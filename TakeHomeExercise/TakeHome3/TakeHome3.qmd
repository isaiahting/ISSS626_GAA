---
title: "Take Home Exercise 3"
author: "Joshua TING"
date: "21 October, 2024"
date-modified: "last-modified"
format:
  html:
    code-fold: true
    code-summary: "Code Chunk"
    number-sections: true
execute:
  eval: true #r will run through all codes
  echo: true #r will display all code chunk
  warning: false #for mark down
  freeze: true #r will not render all existing  html files
  message: false #avoid printing warning message
editor: source
---

# Take-home Exercise 3b: Predicting HDB Resale Prices with Geographically Weighted Machine Learning Methods

## Overview

### Purpose

In this take-home exercise, We will calibrate a predictive model to predict HDB resale prices between July-September 2024 by using HDB resale transaction records in 2023.

## Getting Started

```{r}
pacman::p_load(tidyverse, readr, sf, httr, jsonlite, rvest, dplyr, units, lubridate, tmap, ggplot2,ggpubr, beepr, corrplot, spdep, GWmodel, SpatialML, rsample, Metrics)
```

| **Package** | **Function** | **Explanation** |
|------------------------|------------------------|------------------------|
| **tidyverse** | `dplyr::select`, `dplyr::mutate`, `ggplot2::ggplot`, `tibble::tibble` | A collection of R packages for data manipulation (`dplyr`), data visualization (`ggplot2`), and reading (`readr`). `tibble` is for working with data frames. |
| **readr** | `readr::read_csv`, `readr::write_csv` | Functions for reading and writing CSV files efficiently. |
| **sf** | `sf::st_as_sf`, `sf::st_transform`, `sf::st_drop_geometry`, `sf::st_is_empty`, `sf::st_coordinates` | Functions for handling spatial data, including converting data to `sf` objects, transforming coordinates, and checking for empty geometries. |
| **httr** | `httr::GET`, `httr::POST` | Functions for making HTTP requests, such as GET and POST, to interact with web APIs. |
| **jsonlite** | `jsonlite::fromJSON`, `jsonlite::toJSON` | Functions to parse JSON data into R objects and to convert R objects into JSON format. |
| **rvest** | `rvest::read_html`, `rvest::html_nodes`, `rvest::html_table` | Used for web scraping, including reading HTML content and extracting elements from web pages. |
| **dplyr** | `dplyr::select`, `dplyr::mutate`, `dplyr::filter`, `dplyr::summarize` | A core package for data manipulation, used for selecting columns, creating new columns, filtering data, and summarizing results. |
| **units** | `units::set_units`, `units::as_units` | Provides functions for handling and converting physical units in R. |
| **lubridate** | `lubridate::ymd`, `lubridate::mdy`, `lubridate::today` | Functions to parse, manipulate, and work with date-time objects in a variety of formats. |
| **tmap** | `tmap::tm_shape`, `tmap::tm_borders`, `tmap::tm_fill` | A package for thematic mapping. Functions allow for creating maps with geographic boundaries, fill colors, and more. |
| **ggplot2** | `ggplot2::ggplot`, `ggplot2::geom_point`, `ggplot2::geom_line` | The main visualization package in R, used for creating static graphics such as scatter plots, line charts, histograms, and more. |
| **ggpubr** | `ggpubr::ggarrange`, `ggpubr::stat_cor` | Enhances `ggplot2` by allowing for arranging multiple plots and adding statistical tests, like correlation coefficients, to plots. |
| **beepr** | `beepr::beep` | A simple function to play sounds as notifications, commonly used to alert the user when an operation is complete. |
| **corrplot** | `corrplot::corrplot` | Used to visualize correlation matrices, offering various plotting styles and methods for better presentation of correlations. |
| **spdep** | `spdep::nb2listw`, `spdep::spautolm` | Provides spatial econometrics tools, such as calculating spatial weights and performing spatial autoregressive models. |
| **GWmodel** | `GWmodel::gwr.basic`, `GWmodel::grf`, `GWmodel::bw.gwr` | Functions for geographically weighted regression (GWR), including model fitting (`gwr.basic`), bandwidth selection, and fitting spatial regression trees. |
| **SpatialML** | `SpatialML::SpatialCV`, `SpatialML::SpatialML` | Used for machine learning models in spatial contexts, including spatial cross-validation and spatially explicit machine learning techniques. |
| **rsample** | `rsample::initial_split`, `rsample::training`, `rsample::testing` | Provides tools for resampling and splitting data into training and testing sets for model validation. |
| **Metrics** | `Metrics::rmse`, `Metrics::mae`, `Metrics::mse` | Functions to calculate various performance metrics for model evaluation, including Root Mean Square Error (RMSE), Mean Absolute Error (MAE), and Mean Squared Error (MSE). |

## The Data

1.  Aspatial Dataset:

-   HDB Resale Flat Prices from [Data.gov.s](https://data.gov.sg./datasets?query=HDB+resale+flat+prices&page=1&resultId=189)g

2.  Geospatial Dataset:

-   MPSZ: Boundaries of Singapore from [ura.gov.sg](https://www.ura.gov.sg/Corporate/Planning/Master-Plan)

-   Bus Stops: A list of bus stops locally from [datamall.lta.gov.sg](https://datamall.lta.gov.sg/content/datamall/en/search_datasets.html?searchText=bus%20stop)

-   Eldercare (SHP): A list of elder care centres locally from [data.gov.sg](https://data.gov.sg./datasets?query=eldercare&page=1&resultId=d_3545b068e3f3506c56b2cb6b6117b884)

-   Hawker Centres (KML): a list of hawker centres from [data.gov.sg](https://data.gov.sg./datasets?query=hawker+centre&page=1&resultId=d_ccca3606c337a5c386b9c88dc0dd08b6)

-   Kindergarten (KML) from [data.gov.sg](https://data.gov.sg/datasets?query=kindergarten&page=1&resultId=d_95aa63998d7de94dd4cb3e8e43a5f6d5)

-   Malls (CSV) from [kaggle.com](https://www.kaggle.com/datasets/karthikgangula/shopping-mall-coordinates)

-   Train Station (SHP) from [datamall.lta.gov.sg](https://datamall.lta.gov.sg/content/datamall/en/search_datasets.html?searchText=train%20station)

-   SDCP Park (kml) from [data.gov.sg](https://data.gov.sg/datasets?query=park&page=1&resultId=d_dec52717093b20d30677a938d39f0dac)

-   Schools (CSV) from [data.gov.sg](https://data.gov.sg/datasets?query=general+information+of+schools&page=1&resultId=d_688b934f82c1059ed0a6993d2a829089)

-   Supermarket (KML) from [data.gov.sg](https://data.gov.sg/datasets?query=supermarketsKML&page=1&resultId=d_8a77ee0446716b2ce475a587004afc73)

## Importing Data

1.  Aspatial Data:

-   HDB Resale

```{r}
resale2023 <- read_csv("data/rawdata/aspatial/hdb/resale.csv") %>%
  filter(month >= "2023-01" & month <= "2024-09")
```

2.  Geospatial Data:

-   MPSZ in shapefile format

```{r}
mpsz = st_read(dsn = "data/rawdata/geospatial/mpsz",
               layer = "MP14_SUBZONE_NO_SEA_PL")
```

```{r}
plot(mpsz)
```

-   Bus Stops in shapefile format

```{r}
busstop <- st_read(dsn = "data/rawdata/geospatial/busstop",
                      layer = "BusStop")
```

-   **Eldercare** in shapefile format

```{r}
eldercare <- st_read(dsn = "data/rawdata/geospatial/eldercare",
                      layer = "ELDERCARE")
```

-   Hawker Centres in kml format

```{r}
hawker <- st_read("data/rawdata/geospatial/hawker/HawkerCentres.kml")
```

-   Kindergarten in fml format

```{r}
kindergarten <- st_read("data/rawdata/geospatial/kindergarten/Kindergartens.kml")
kindergarten <- st_transform(kindergarten, 3414)
```

-   Shopping Malls in csv format

```{r}
mall <- read_csv("data/rawdata/geospatial/mall/shopping_mall_coordinates.csv")
```

-   Train Station in shapefile format

```{r}
mrt <- st_read(dsn = "data/rawdata/geospatial/mrt",
                layer = "RapidTransitSystemStation")
```

-   SDCP Park in kml format

```{r}
park <- st_read("data/rawdata/geospatial/park/NParksParksandNatureReservesKML.kml")
```

-   Schools in csv format

```{r}
school <- read_csv("data/rawdata/geospatial/school/Generalinformationofschools.csv") %>% 
  mutate(postal_code = as.character(postal_code))
```

-   Supermarket in kml format

```{r}
supermarket <- st_read("data/rawdata/geospatial/supermarket/SupermarketsKML.kml")
```

```{r}
#| echo: false
#| eval: false
write_rds(resale2023, "data/rds/aspatial/resale2023.rds")
write_rds(mpsz, "data/rds/geopatial/mpsz.rds")
write_rds(eldercare, "data/rds/geospatial/eldercare.rds")
write_rds(hawker, "data/rds/geospatial/hawker.rds")
write_rds(kindergarten, "data/rds/geospatial/kindergarten.rds")
write_rds(mall, "data/rds/geospatial/mall.rds")
write_rds(mrt, "data/rds/geospatial/mrt.rds")
write_rds(park, "data/rds/geospatial/park.rds")
write_rds(school, "data/rds/geospatial/school.rds")
write_rds(supermarket, "data/rds/geospatial/supermarket.rds")
```

```{r}
#| echo: false
#| eval: false
resale2023 = read_rds("data/rds/aspatial/resale2023.rds")
mpsz = read_rds("data/rds/geopatial/mpsz.rds")
eldercare = read_rds("data/rds/geospatial/eldercare.rds")
hawker = read_rds("data/rds/geospatial/hawker.rds")
mall = read_rds("data/rds/geospatial/mall.rds")
kindergarten = read_rds("data/rds/geospatial/kindergarten.rds")
mrt = read_rds("data/rds/geospatial/mrt.rds")
park = read_rds("data/rds/geospatial/park.rds")
school = read_rds("data/rds/geospatial/school.rds")
supermarket = read_rds("data/rds/geospatial/supermarket.rds")
```

::: callout-important
## Standardising CRS

Ironically, all of these data sets are from Singapore, yet, it presents with varied CRS.

Therefore, in this exercise, SVY21 (EPSG 3414) will be the standardised CRS as all data relates to local context.
:::

## Data Wrangling

### HDB Resale

In tidying up the HDB resale data, the column *block* & *street_name* are combined and *remaining_lease* are split in two columns for ease of manipulation. Additionally, we are changing the column *remaining_lease_yr* as an integer and splitting it to month and year.

```{r}
resale_tidy <- resale2023 %>%
  mutate(address = paste(block,street_name)) %>% #combined block & street name
  mutate(remaining_lease_yr = as.integer(
    str_sub(remaining_lease, 0, 2)))%>% #extract remaining lease by yr
  mutate(remaining_lease_mth = as.integer(
    str_sub(remaining_lease, 9, 11))) #extract remaining lease by month
```

```{r}
resale_selected <- resale_tidy %>%
  filter(month >= "2023-01" & month <= "2024-09")
```

```{r}
add_list <- sort(unique(resale_selected$address)) #parse a list as API cannot read df
#unique reduces records to pass to portal
#sort is used to easier to find geo codes
```

Fetching Data from onemap API

As the HDB resale data lack of coordinates, API was used to extract the coordinates.

```{r}
get_coords <- function(add_list){

  # Create a data frame to store all retrieved coordinates
  postal_coords <- data.frame()
    
  for (i in add_list){
    r <- GET('https://www.onemap.gov.sg/api/common/elastic/search?',
           query=list(searchVal=i,
                     returnGeom='Y',
                     getAddrDetails='Y'))
    data <- fromJSON(rawToChar(r$content))
    found <- data$found
    res <- data$results
    
    # Create a new data frame for each address
    new_row <- data.frame()
    
    # If single result, append 
    if (found == 1){
      postal <- res$POSTAL 
      lat <- res$LATITUDE
      lng <- res$LONGITUDE
      new_row <- data.frame(address = i, 
                           postal = postal, 
                           latitude_wgs84 = lat,  # renamed to clarify coordinate system
                           longitude_wgs84 = lng) # renamed to clarify coordinate system
    }
    
    # If multiple results, drop NIL and append top 1
    else if (found > 1){
      # Remove those with NIL as postal
      res_sub <- res[res$POSTAL != "NIL", ]
      
      # Set as NA first if no Postal
      if (nrow(res_sub) == 0) {
          new_row <- data.frame(address = i, 
                               postal = NA, 
                               latitude_wgs84 = NA, 
                               longitude_wgs84 = NA)
      }
      else{
        top1 <- head(res_sub, n = 1)
        postal <- top1$POSTAL 
        lat <- top1$LATITUDE
        lng <- top1$LONGITUDE
        new_row <- data.frame(address = i, 
                             postal = postal, 
                             latitude_wgs84 = lat, 
                             longitude_wgs84 = lng)
      }
    }
    else {
      new_row <- data.frame(address = i, 
                           postal = NA, 
                           latitude_wgs84 = NA, 
                           longitude_wgs84 = NA)
    }
    
    # Add the row
    postal_coords <- rbind(postal_coords, new_row)
  }
  
  # Convert to sf object with WGS84 coordinates (EPSG:4326)
  # Filter out rows with NA coordinates first
  valid_coords <- postal_coords[!is.na(postal_coords$latitude_wgs84) & 
                              !is.na(postal_coords$longitude_wgs84), ]
  
  if(nrow(valid_coords) > 0) {
    coords_sf <- st_as_sf(valid_coords, 
                         coords = c("longitude_wgs84", "latitude_wgs84"),
                         crs = 4326)
    
    # Transform to SVY21 (EPSG:3414)
    coords_svy21 <- st_transform(coords_sf, 3414)
    
    # Extract coordinates
    coords_matrix <- st_coordinates(coords_svy21)
    
    # Add SVY21 coordinates back to the original dataframe with desired column names
    valid_coords$longitude <- coords_matrix[, 1]  # SVY21 X coordinate as longitude
    valid_coords$latitude <- coords_matrix[, 2]   # SVY21 Y coordinate as latitude
    
    # Merge back with rows that had NA coordinates
    result <- merge(postal_coords, valid_coords[c("address", "longitude", "latitude")], 
                   by = "address", all.x = TRUE)
  } else {
    # If no valid coordinates, add empty SVY21 columns
    result <- postal_coords
    result$longitude <- NA  # SVY21 coordinates
    result$latitude <- NA   # SVY21 coordinates
  }
  
  return(result)
}
```

Below is the code chunk that populates the coordinates in longitude, latitude and postal code against the address in the *add_list*.

```{r}
coords <- get_coords(add_list)
```

```{r}
#| echo: false
write_rds(coords, "data/rds/aspatial/coords.rds")
```

```{r}
#| echo: false
coords = read_rds("data/rds/aspatial/coords.rds")
```

The longtitude and latitude is then combined into geometry and the crs has been set to EPSG = 3414.

```{r}
coords_sf <- coords %>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 3414, remove = FALSE) %>%
  select(address, postal, longitude, latitude, geometry)
```

```{r}
#| echo: false
write_rds(coords_sf, "data/rds/aspatial/coords_sf.rds")
```

```{r}
#| echo: false
coords_sf = read_rds("data/rds/aspatial/coords_sf.rds")
```

Following which, the *coords_sf* is then combined to the *resale_selected* df by address, forming a new df *resale_geom*

```{r}
resale_geom <- resale_selected %>% 
  left_join(coords_sf, by = "address")
```

Assigning CRS EPSG 3414 to resale_geom

```{r}
resale_geom <- resale_geom %>% 
  st_as_sf(coords =c("longitude", "latitude"), crs = 3414, remove = FALSE)
```

Using st_crs() to check if it is the correct CRS.

```{r}
st_crs(resale_geom)
```

In ensuring these are the variables we need, we use select() to include the variables. Then, we compute the unit age with the formula of 99 year (convention HDB lease years) - remaining lease year. Lastly, we change the column *month* to POSICxt so we can manipulate in ease later on.

```{r}
resale_geom <- resale_geom %>%
  select(address, town, resale_price, month, flat_type, floor_area_sqm, remaining_lease_yr,longitude, latitude, geometry) %>% 
  mutate(unit_age = 99 - remaining_lease_yr)
```

```{r}
#| echo: false
write_rds(resale_geom, "data/rds/aspatial/resale_geom.rds")
```

```{r}
#| echo: false
resale_geom = read_rds("data/rds/aspatial/resale_geom.rds")
```

### Overview of Data sets in Coordinate Reference System

|    WGS 84    |   SVY21   | No PCS / CSV |
|:------------:|:---------:|:------------:|
|    Hawker    | Bus Stop  |  HDB Resale  |
| Kindergarten | Eldercare |     Mall     |
|     Park     |    MRT    |    School    |
| Supermarket  |           |              |
|              |           |              |

::: callout-note
**Coordinate System**

Viewing the sf, we noticed that the coordinates are in WGS84 - EPSG 4326. This may not be suitable for our data manipulation later on as we are standardising it to our local coordinates system which is SVY 21 EPSG 3414.
:::

::: callout-tip
**Overview of Coordinate System**

WGS84 (EPSG: 4326) - Global Use

1.  **Definition**: WGS84 (World Geodetic System 1984) is a global coordinate system that defines latitude and longitude on a three-dimensional ellipsoid. It is the standard used by the Global Positioning System (GPS).

2.  **Coordinate System**: It uses a geographic coordinate system based on a spheroid, where coordinates are given in degrees (latitude and longitude).

3.  **Application**: WGS84 is used globally for mapping, navigation, and geolocation applications. It provides a standard reference for geographic information systems (GIS) and is essential for interoperability between different systems.

4.  **Units**: The coordinates are expressed in degrees.

SVY21 (EPSG: 3413) - Singapore Use

1.  **Definition**: SVY21 (Singapore Vertical 21) is a national coordinate system specifically designed for Singapore. It is based on the Transverse Mercator projection and is tailored to provide accurate measurements within Singapore's geographic boundaries.

2.  **Coordinate System**: SVY21 uses a projected coordinate system, where coordinates are given in meters. The system is more suitable for local applications because it minimizes distortions in a specific area.

3.  **Application**: SVY21 is primarily used for urban planning, construction, and various governmental applications within Singapore. It provides higher accuracy for local measurements compared to global systems like WGS84.

4.  **Units**: The coordinates are expressed in meters.
:::

### MPSZ

First, we would like to check if there is any shared boudaries in this sf as this will churned as invalid polygons in the data. We will then noticed that there are several "Ring Self-Intersection" specifically, there are 9 polygons with self-intersection issues which is not good for the data.

```{r}
st_is_valid(mpsz, reason = TRUE)
```

The below code chunk visualises the invalid geometries.

```{r}
invalid_polygons <- mpsz[!st_is_valid(mpsz),]
plot(invalid_polygons)
```

In addressing the above point, we will use st_buffer() of sf package to compute a 5-metres buffers around the data.

```{r}
mpsz <- st_buffer(mpsz, dist = 2)
```

```{r}
#| echo: false
write_rds(mpsz, "data/rds/geospatial/mpsz.rds")
```

```{r}
#| echo: false
mpsz = read_rds("data/rds/geospatial/mpsz.rds")
```

### Bus Stops

In inspecting the sf, we noticed that when we sort the column "BUS_STOP_N", there are two rows that retains the geometry however there are nil indication of the bus stop number and location. Hence both of the rows will be deleted.

Additionally, in checking for duplicates through its geometry, we noticed two rows share the geometry. One is named as "YUSEN LOGISTICS" while the other is "yusen logistics"

```{r}
# Check for duplicate geometries in the eldercare sf object
duplicate_busstop <- busstop[duplicated(busstop$geometry), ]

# Display the duplicate geometries if any
if (nrow(duplicate_busstop) > 0) {
  print(duplicate_busstop)
} else {
  print("No duplicate geometries found.")
}
```

Hence, with the above adjustments and extracting coordinates, the data is cleaned.

```{r}
busstop_cleaned <- busstop %>%
  filter(LOC_DESC != "YUSEN LOGISTICS")  # Delete the specified row
```

```{r}
# Extract coordinates after filtering
coordinates <- st_coordinates(busstop_cleaned)

# Add longitude and latitude columns
busstop_cleaned <- busstop_cleaned %>%
  mutate(longitude = coordinates[, 1],    # First column is longitude
         latitude = coordinates[, 2]) %>%  
  select(LOC_DESC, longitude, latitude, geometry) 
```

```{r}
st_crs(busstop_cleaned) <- 3414
```

```{r}
#| echo: false
write_rds(busstop_cleaned, "data/rds/geospatial/cleaned/busstop_cleaned.rds")
```

```{r}
#| echo: false
busstop_cleaned = read_rds("data/rds/geospatial/cleaned/busstop_cleaned.rds")
```

### Eldercare

In ensuring there are no duplicates the below code chunk was churned. However,we noticed that there is two rows that shares the same geometry. Through closer inspection, we realised that both addresses are formatted differently "117 Bukit Merah View" & "Blk 117 Bukit Merah View #01-205". Thus, one of the row will be deleted.

```{r}
# Check for duplicate geometries in the eldercare sf object
duplicate_geometries <- eldercare[duplicated(eldercare$geometry), ]

# Display the duplicate geometries if any
if (nrow(duplicate_geometries) > 0) {
  print(duplicate_geometries)
} else {
  print("No duplicate geometries found.")
}
```

The code below selects the necessary variables of ease of manipulation

```{r}
eldercare_cleaned <- eldercare %>% 
  filter(INC_CRC != "7FF38742987329FE") %>%  #deletes the duplicated row
  rename(longitude = X_ADDR) %>% 
  rename(latitude = Y_ADDR) %>% 
  select(NAME, longitude, latitude, geometry)
```

```{r}
st_crs(eldercare_cleaned) <- 3414
```

```{r}
#| echo: false
write_rds(eldercare_cleaned, "data/rds/geospatial/cleaned/eldercare_cleaned.rds")
```

```{r}
#| echo: false
eldercare_cleaned = read_rds("data/rds/geospatial/cleaned/eldercare_cleaned.rds")
```

### Hawker

The code chunk below checks if there are any duplicates geometries and it returns nil.

```{r}
duplicate_geometries <- hawker[duplicated(st_geometry(hawker)), ]

# Display duplicate geometries if any
if (nrow(duplicate_geometries) > 0) {
  print(duplicate_geometries)
} else {
  print("No duplicate geometries found.")
}
```

As the CRS of hawker sf is in WGS84, st_transform() is then used to transform it to SVY21.

```{r}
hawker <- st_transform(hawker, 3414)
```

The data is then split into longitude and latitude and selected columns were chosen.

```{r}
coordinates <- st_coordinates(hawker)
hawker_cleaned <- hawker %>%
  mutate(longitude = coordinates[, 1],    # First column is longitude
         latitude = coordinates[, 2]) %>%  
  select(longitude, latitude, geometry)
```

```{r}
#| echo: false
write_rds(hawker_cleaned, "data/rds/geospatial/cleaned/hawker_cleaned.rds")
```

```{r}
#| echo: false
hawker_cleaned = read_rds("data/rds/geospatial/cleaned/hawker_cleaned.rds")
```

### Kindergarten

The code chunk below checks if there are any duplicates geometries and it returns nil.

```{r}
duplicate_kindergarten <- kindergarten[duplicated(st_geometry(kindergarten)), ]

# Display duplicate geometries if any
if (nrow(duplicate_kindergarten) > 0) {
  print(duplicate_kindergarten)
} else {
  print("No duplicate geometries found.")
}
```

```{r}
kindergarten <- st_transform(kindergarten, 3414)
```

The data is then split into longitude and latitude and selected columns were chosen.

```{r}
coordinates <- st_coordinates(kindergarten)
kindergarten_cleaned <- kindergarten %>%
  mutate(longitude = coordinates[, 1],    # First column is longitude
         latitude = coordinates[, 2]) %>%  
  select(longitude, latitude, geometry)
```

```{r}
#| echo: false
write_rds(kindergarten_cleaned, "data/rds/geospatial/cleaned/kindergarten_cleaned.rds")
```

```{r}
#| echo: false
kindergarten_cleaned = read_rds("data/rds/geospatial/cleaned/kindergarten_cleaned.rds")
```

### School

Despite importing the data and ensuring that postal code is in character form, there are still incomplete postal code as seen in the below code.

```{r}
list(school$postal_code)
```

The below code chunk churned out 3 incomplete postal codes.

```{r}
# Display postal codes that are not 6 digits
invalid_postal_codes <- school %>%
  filter(nchar(as.character(postal_code)) != 6) %>%
  select(address, postal_code)

# Show the result
print(invalid_postal_codes)
```

The correct postal code has been changed with the assistance of Google Maps and it piped back to the main df school.

```{r}
school <- school %>% 
  mutate(postal_code = ifelse(postal_code == "88256", "088256", postal_code)) %>% 
  mutate(postal_code = ifelse(postal_code == "99757", "099757", postal_code)) %>% 
  mutate(postal_code = ifelse(postal_code == "99840", "099840", postal_code))

```

```{r}
#| echo: false
write_rds(school, "data/rds/aspatial/school.rds")
```

```{r}
#| echo: false
school = read_rds("data/rds/aspatial/school.rds")
```

```{r}
school <- school %>% 
  select(school_name, address, postal_code, mainlevel_code) %>% 
  filter(mainlevel_code =="PRIMARY")
```

```{r}
postal_list <- sort(unique(school$postal_code)) #parse a list as API cannot read df
#unique reduces records to pass to portal
#sort is used to easier to find geo codes
```

Fetching Data from onemap API

As the primary schools lack of geometry coordinates, API was used to extract the coordinates.

This will return WGS84 xy coordinates alongside SVY21 xy coordinates.

```{r}
get_coords <- function(postal_list){
  # Create a data frame to store all retrieved coordinates
  postal_coords <- data.frame()
    
  for (postal in postal_list){
    r <- GET('https://www.onemap.gov.sg/api/common/elastic/search?',
           query=list(searchVal=postal,
                     returnGeom='Y',
                     getAddrDetails='Y'))
    data <- fromJSON(rawToChar(r$content))
    found <- data$found
    res <- data$results
    
    # Create a new data frame for each postal code
    new_row <- data.frame()
    
    # If single result, append 
    if (found == 1){
      postal_code <- res$POSTAL 
      lat <- res$LATITUDE
      lng <- res$LONGITUDE
      new_row <- data.frame(postal_code = postal, 
                           postal_found = postal_code, 
                           latitude_wgs84 = lat,
                           longitude_wgs84 = lng)
    }
    
    # If multiple results, use the exact postal code match
    else if (found > 1){
      # Find exact match for postal code
      res_match <- res[res$POSTAL == postal, ]
      
      # If exact match found, use it
      if (nrow(res_match) > 0) {
        postal_code <- res_match$POSTAL[1]
        lat <- res_match$LATITUDE[1]
        lng <- res_match$LONGITUDE[1]
        new_row <- data.frame(postal_code = postal,
                             postal_found = postal_code,
                             latitude_wgs84 = lat,
                             longitude_wgs84 = lng)
      }
      # If no exact match, set as NA
      else {
        new_row <- data.frame(postal_code = postal,
                             postal_found = NA,
                             latitude_wgs84 = NA,
                             longitude_wgs84 = NA)
      }
    }
    # If no results found
    else {
      new_row <- data.frame(postal_code = postal,
                           postal_found = NA,
                           latitude_wgs84 = NA,
                           longitude_wgs84 = NA)
    }
    
    # Add the row
    postal_coords <- rbind(postal_coords, new_row)
  }
  
  # Convert to sf object with WGS84 coordinates (EPSG:4326)
  # Filter out rows with NA coordinates first
  valid_coords <- postal_coords[!is.na(postal_coords$latitude_wgs84) & 
                              !is.na(postal_coords$longitude_wgs84), ]
  
  if(nrow(valid_coords) > 0) {
    coords_sf <- st_as_sf(valid_coords, 
                         coords = c("longitude_wgs84", "latitude_wgs84"),
                         crs = 4326)
    
    # Transform to SVY21 (EPSG:3414)
    coords_svy21 <- st_transform(coords_sf, 3414)
    
    # Extract coordinates
    coords_matrix <- st_coordinates(coords_svy21)
    
    # Add SVY21 coordinates back to the original dataframe
    valid_coords$longitude <- coords_matrix[, 1]  # SVY21 X coordinate
    valid_coords$latitude <- coords_matrix[, 2]   # SVY21 Y coordinate
    
    # Add geometry column
    valid_coords$geometry <- st_geometry(coords_svy21)
    
    # Merge back with rows that had NA coordinates
    result <- merge(postal_coords, 
                   valid_coords[c("postal_code", "longitude", "latitude", "geometry")], 
                   by = "postal_code", all.x = TRUE)
  } else {
    # If no valid coordinates, add empty SVY21 columns
    result <- postal_coords
    result$longitude <- NA
    result$latitude <- NA
    result$geometry <- NA
  }
  
  return(result)
}

# Usage example:
add_list_school <- sort(unique(school$postal_code))
coords_school <- get_coords(add_list_school)
print(head(coords_school))

# Optional: Convert result to sf object for spatial operations
coords_school_sf <- st_as_sf(coords_school[!is.na(coords_school$geometry), ])
```

Below is the code chunk that populates the coordinates in longitude, latitude and postal code against the address in the *add_list*.

```{r}
coords_school <- get_coords(postal_list)
print(coords_school)
```

```{r}
#| echo: false
write_rds(coords_school, "data/rds/geospatial/cleaned/coords_school.rds")
```

```{r}
#| echo: false
coords_school = read_rds("data/rds/geospatial/cleaned/coords_school.rds")
```

```{r}
# Combine latitude and longitude in `coords` int a single "geometry" column and convert to an sf object
coords_school <- coords_school %>%
  select(postal_code, geometry)
```

`Following which, the coords_sf is then combined to the resale_selected df by address, forming a new df resale_geom`

```{r}
school_cleaned <- school %>% 
  left_join(coords_school, by = "postal_code")
```

In the code chunk, we will extract the longitude and latitude from the geometry to facilitate transforming into sf.

```{r}
school_cleaned <- school_cleaned %>%
  mutate(
    longitude = st_coordinates(geometry)[, 1],  # Extract longitude
    latitude = st_coordinates(geometry)[, 2]    # Extract latitude
  )
```

Then, we are able to transform this into sf with the new longitude and latitude columns.

```{r}
school_cleaned<- st_as_sf(school_cleaned, coords = c("longitude", "latitude"), crs = 3414)
```

```{r}
#| echo: false
write_rds(school_cleaned, "data/rds/geospatial/cleaned/school_cleaned.rds")
```

```{r}
#| echo: false
school_cleaned = read_rds("data/rds/geospatial/cleaned/school_cleaned.rds")
```

### Malls

As the af *mall* was in WGS84, it will transformed into SVY21 EPSG 3414.

```{r}
# Create an sf object with WGS 84 CRS
mall <- st_as_sf(mall, coords = c("LONGITUDE", "LATITUDE"), crs = 4326)
# Transform to EPSG:3414
mall_cleaned <- st_transform(mall, crs = 3414)
```

From the code below, we noticed that there are two malls that are duplicated.

```{r}
geom_text <- st_as_text(st_geometry(mall_cleaned))

# Find duplicated geometries
duplicates <- mall_cleaned[duplicated(geom_text) | duplicated(geom_text, fromLast = TRUE), ]

# Print summary
print(paste("Number of duplicate geometries found:", nrow(duplicates)))

if(nrow(duplicates) > 0) {
  print("\nDuplicate records:")
  print(duplicates)
}

```

Hence, with the code below, we will remove the duplicate.

```{r}
# Convert geometry to text for comparison
geom_text <- st_as_text(st_geometry(mall_cleaned))

# Add row numbers to track which entries we keep
mall_cleaned$row_num <- 1:nrow(mall_cleaned)

# Remove duplicates, keeping first occurrence only
mall_cleaned<- mall_cleaned[!duplicated(geom_text), ]

mall_cleaned <- mall_cleaned %>% 
  select(`Mall Name`, geometry)
```

```{r}
#| echo: false
write_rds(mall, "data/rds/geospatial/cleaned/mall_cleaned.rds")
```

```{r}
#| echo: false
mall_cleaned = read_rds("data/rds/geospatial/cleaned/mall_cleaned.rds")
```

### Park

The code chunk below checks if there are any duplicates geometries and it returns nil.

```{r}
# Get the geometries as text for easier comparison
geom_park <- st_as_text(st_geometry(park))

# Find duplicated geometries
duplicates_park <- park[duplicated(geom_park) | duplicated(geom_park, fromLast = TRUE), ]
```

As the sf park was in WGS84, it will transformed into SVY21 EPSG 3414.

```{r}
# Create an sf object with WGS 84 CRS
park <- st_as_sf(park, coords = c("LONGITUDE", "LATITUDE"), crs = 4326)

# Transform to EPSG:3414
park_cleaned <- st_transform(park, crs = 3414)
```

```{r}
#| echo: false
write_rds(park_cleaned, "data/rds/geospatial/cleaned/park_cleaned.rds")
```

```{r}
#| echo: false
park_cleaned = read_rds("data/rds/geospatial/cleaned/park_cleaned.rds")
```

### MRT

Let's take a quick glimpse at the data. We noticed that there duplicated station names, depots, command centres (i.e. BOCC - to which I derived it as a [Bus Operations Control Centre](https://landtransportguru.net/trapeze-common-fleet-management-system/seletar-bus-depot-interiors-7/), stations that are under construction.

```{r}
glimpse(mrt)
```

Thus, with the code chunk below, we use the duplicate function to check which are the duplciated stations. Noticeably, these stations are actually interchange where it consists of more than 1 metro line (i.e. Outram Park where it houses East-West, Thomson-EastCoast, and North-East Line). However, Bayshore MRT isn't an interchange but there are 2 duplicates of it that is likely due to the construction of Bedok South where both links. Regardless, the decision to keep these variables retains as it serves as an important proximate to the residential areas in geospatial setting.

```{r}
# Check for duplicates in the STN_NAM_DE column and display the whole rows
duplicates <- mrt %>%
  group_by(STN_NAM_DE) %>%
  filter(n() > 1) %>%
  ungroup()

# Display duplicates if any
if (nrow(duplicates) > 0) {
  print(duplicates)
} else {
  print("No duplicates found in the STN_NAM_DE column.")
}

glimpse(duplicates)
```

[![MRT System Map](images/Screenshot%202024-11-03%20at%2014.58.56.png){fig-align="center"}](https://www.lta.gov.sg/content/dam/ltagov/getting_around/public_transport/rail_network/image/SM_TEL4_Eng_(Ver150824)_TL_Main.pdf)

In the below code chunk, we will cross-check with the MRT map above and remove the names are that are depots, operation centres, sub stations and columns that are not needed. The code chunks includes stations with "MRT STATION" or "LRT STATION".

```{r}
mrt_cleaned <- mrt %>%
  select(-TYP_CD, -STN_NAM, -ATTACHEMEN) %>%  # Remove specified columns
  filter(grepl("MRT STATION|LRT STATION", STN_NAM_DE, ignore.case = TRUE))  # Filter rows

# Display the cleaned sf object
print(mrt_cleaned)
```

In the list, we noticed that some of the MRT stations are actually under construction. Hence we cross-checked with the MRT map above and exclude those that are under construction.

First, we create a list of excluded MRT list. Then we use Regex to exclude it using if function.

```{r}
# Specify the words to exclude
words_to_exclude <- c("HUME", "SUNGEI BEDOK", "BEDOK SOUTH", "XILIN", 
                       "PUNGGOL COAST", "BUKIT BROWN", "MOUNT PLEASANT", 
                       "FOUNDERS' MEMORIAL")

# Create a regular expression pattern to match any of the words
pattern <- paste(words_to_exclude, collapse = "|")

# Ensure the train object is an sf object
if (!inherits(mrt_cleaned, "sf")) {
  stop("The 'train' object is not an sf object.")
}

# Filter the train sf object to exclude rows with specified words in STN_NAM_DE
mrt_cleaned  <- mrt_cleaned %>%
  filter(!str_detect(STN_NAM_DE, regex(pattern, ignore_case = TRUE)))

# Verify the results
print(head(mrt_cleaned))  # Print the first few rows of the filtered object
summary(mrt_cleaned )      # Summary of the filtered object


```

We will then view() the mrt_cleaned sf and search for the excluded MRT stations in ensuring that it is not there.

```{r}
view(mrt_cleaned)
```

```{r}
st_crs(mrt_cleaned) <- 3414
```

```{r}
#| echo: false
write_rds(mrt_cleaned, "data/rds/geospatial/cleaned/mrt_cleaned.rds")
```

```{r}
#| echo: false
mrt_cleaned = read_rds("data/rds/geospatial/cleaned/mrt_cleaned.rds")
```

### Supermarket

The code chunk below checks if there are any duplicates geometries and it returns several duplicates.

In the table below, we noticed that there are 33 rows of geometries returned with 2-4 duplicates in each geometry. In explaining this issue, we can zoom into the description column and it will surface different supermarket names sharing the same geometry. This is likely due to the changing of the tenants.

```{r}
supermarket = read_rds("data/rds/geospatial/supermarket.rds")
```

```{r}
# Find duplicated geometries
duplicate_ids <- supermarket %>%
  filter(duplicated(st_geometry(.)) | duplicated(st_geometry(.), fromLast = TRUE)) %>%
  select(geometry) %>%
  unique() %>%
  pull(geometry)

# Create a table with duplicates together in a row and count
duplicates_table <- supermarket %>%
  filter(st_geometry(geometry) %in% duplicate_ids) %>%
  group_by(geometry) %>%
  summarise(duplicate_entries = list(cur_data()), 
            count = n(),         # Count the number of duplicates
            .groups = 'drop')

# View the duplicated entries together in a row with counts
if (nrow(duplicates_table) > 0) {
  print("Duplicated geometries found:")
  print(duplicates_table)
  total_duplicates <- sum(duplicates_table$count)
  cat("Total number of duplicates:", total_duplicates, "\n")
} else {
  print("No duplicated geometries found.")
}

```

In addressing the duplicate, only one geometry will be retained while the rest will be eradicated.

```{r}
# Remove duplicates while retaining unique geometries
supermarket <- supermarket %>%
  distinct(geometry, .keep_all = TRUE)  # Keep all columns for the unique geometries
```

Following which, we can check again to see if there's any duplicates remaining and it returns nil.

```{r}
# Find duplicated geometries
duplicate_ids <- supermarket %>%
  filter(duplicated(st_geometry(.)) | duplicated(st_geometry(.), fromLast = TRUE)) %>%
  select(geometry) %>%
  unique() %>%
  pull(geometry)

# Create a table with duplicates together in a row and count
duplicates_table <- supermarket %>%
  filter(st_geometry(geometry) %in% duplicate_ids) %>%
  group_by(geometry) %>%
  summarise(duplicate_entries = list(cur_data()), 
            count = n(),         # Count the number of duplicates
            .groups = 'drop')

# View the duplicated entries together in a row with counts
if (nrow(duplicates_table) > 0) {
  print("Duplicated geometries found:")
  print(duplicates_table)
  total_duplicates <- sum(duplicates_table$count)
  cat("Total number of duplicates:", total_duplicates, "\n")
} else {
  print("No duplicated geometries found.")
}

```

The data is then transformed from WGS84 to SVY21.

```{r}
# Create an sf object with WGS 84 CRS
supermarket <- st_as_sf(supermarket, coords = c("LONGITUDE", "LATITUDE"), crs = 4326)

# Transform to EPSG:3414
supermarket_cleaned <- st_transform(supermarket, crs = 3414)
```

```{r}
coordinates <- st_coordinates(supermarket_cleaned)
supermarket_cleaned <- supermarket_cleaned %>%
  mutate(longitude = coordinates[, 1],    # First column is longitude
         latitude = coordinates[, 2]) %>%  
  select(longitude, latitude, geometry)
```

```{r}
#| echo: false
write_rds(supermarket_cleaned, "data/rds/geospatial/cleaned/supermarket_cleaned.rds")
```

```{r}
#| echo: false
supermarket_cleaned = read_rds("data/rds/geospatial/cleaned/supermarket_cleaned.rds")
```

## Computing Proximity to Resale Transactions

#### Bus Stop

```{r}
# Create busstop_resale with just the two columns we want
busstop_resale <- data.frame(
  busstop_prox = numeric(nrow(resale_geom)),
  within_350m_busstop = numeric(nrow(resale_geom)))

# Loop through each resale point
for (i in 1:nrow(resale_geom)) {
  # Get current resale point
  current_point <- resale_geom[i, ]
  
  # Calculate distances to all bus stops
  distances <- st_distance(current_point, busstop_cleaned)
  
  # Store minimum distance in km in busstop_prox
  busstop_resale$busstop_prox[i] <- as.numeric(min(distances)) / 1000
  
  # Count bus stops within 350 meters
  busstop_resale$within_350m_busstop[i] <- sum(as.numeric(distances) <= 350)
}

# Verify results
print(head(busstop_resale))             # Print the first few rows
summary(busstop_resale)                 # Summary of both columns

```

```{r}
#| echo: false
write_rds(busstop_resale, "data/rds/prox/busstop_resale.rds")
```

```{r}
#| echo: false
busstop_resale = read_rds("data/rds/prox/busstop_resale.rds")
```

#### Eldercare

```{r}
# Create eldercare_resale with just the two columns we want
eldercare_resale <- data.frame(
  eldercare_prox = numeric(nrow(resale_geom)),
  within_350m_eldercare = numeric(nrow(resale_geom)))

# Loop through each point in busstop_resale
for (i in 1:nrow(resale_geom)) {
  # Get current point
  current_point <- resale_geom[i, ]
  
  # Calculate distances to all eldercare centers
  distances <- st_distance(current_point, eldercare_cleaned)
  
  # Store minimum distance in km in eldercare_prox
  eldercare_resale$eldercare_prox[i] <- as.numeric(min(distances)) / 1000
  
  # Count eldercare facilities within 350 meters
  eldercare_resale$within_350m_eldercare[i] <- sum(as.numeric(distances) <= 350)
}

# Verify results
print(head(eldercare_resale))             # Print the first few rows
summary(eldercare_resale)   

beep()
```

```{r}
#| echo: false
write_rds(eldercare_resale, "data/rds/prox/eldercare_resale.rds")
```

```{r}
#| echo: false
eldercare_resale = read_rds("data/rds/prox/eldercare_resale.rds")
```

#### Hawker

```{r}
# Create eldercare_resale with just the two columns we want
hawker_resale <- data.frame(
  hawker_prox = numeric(nrow(resale_geom)))

# Loop through each point in busstop_resale
for (i in 1:nrow(resale_geom)) {
  # Get current point
  current_point <- resale_geom[i, ]
  
  # Calculate distances to all eldercare centers
  distances <- st_distance(current_point, hawker_cleaned)
  
  # Store minimum distance in km in eldercare_prox
  hawker_resale$hawker_prox[i] <- as.numeric(min(distances)) / 1000
  
}

# Verify results
print(head(hawker_resale))             # Print the first few rows

beep(3)
```

```{r}
#| echo: false
write_rds(hawker_resale, "data/rds/prox/hawker_resale.rds")
```

```{r}
#| echo: false
hawker_resale = read_rds("data/rds/prox/hawker_resale.rds")
```

#### Kindergarten

```{r}
# Create eldercare_resale with just the two columns we want
kindergarten_resale <- data.frame(
  kindergarten_prox = numeric(nrow(resale_geom)),
  within_350m_kindergarten = numeric(nrow(resale_geom)))

# Loop through each point in busstop_resale
for (i in 1:nrow(resale_geom)) {
  # Get current point
  current_point <- resale_geom[i, ]
  
  # Calculate distances to all eldercare centers
  distances <- st_distance(current_point, kindergarten_cleaned)
  
  # Store minimum distance in km in eldercare_prox
  kindergarten_resale$kindergarten_prox[i] <- as.numeric(min(distances)) / 1000
  
  # Count eldercare facilities within 350 meters
  kindergarten_resale$within_350m_kindergarten[i] <- sum(as.numeric(distances) <= 350)
}

# Verify results
print(head(kindergarten_resale))             # Print the first few rows
summary(kindergarten_resale)   # Summary of both columns

beep(3)              
```

```{r}
#| echo: false
write_rds(kindergarten_resale, "data/rds/prox/kindergarten_resale.rds")
```

```{r}
#| echo: false
kindergarten_resale = read_rds("data/rds/prox/kindergarten_resale.rds")
```

#### Malls

```{r}
#change CRS of mall_cleaned to align to CRS EPSG 3414 
st_crs(mall_cleaned) <- 3414

# Create eldercare_resale with just the two columns we want
mall_resale <- data.frame(
  mall_prox = numeric(nrow(resale_geom)))

# Loop through each point in busstop_resale
for (i in 1:nrow(resale_geom)) {
  # Get current point
  current_point <- resale_geom[i, ]
  
  # Calculate distances to all eldercare centers
  distances <- st_distance(current_point, mall_cleaned)
  
  # Store minimum distance in km in eldercare_prox
  mall_resale$mall_prox[i] <- as.numeric(min(distances)) / 1000
  
}

# Verify results
print(head(mall_resale))             # Print the first few rows

beep(3)
```

```{r}
#| echo: false
write_rds(mall_resale, "data/rds/prox/mall_resale.rds")
```

```{r}
#| echo: false
mall_resale = read_rds("data/rds/prox/mall_resale.rds")
```

#### MRT

```{r}
# Create eldercare_resale with just the two columns we want
mrt_resale <- data.frame(
  mrt_prox = numeric(nrow(resale_geom)))

# Loop through each point in busstop_resale
for (i in 1:nrow(resale_geom)) {
  # Get current point
  current_point <- resale_geom[i, ]
  
  # Calculate distances to all eldercare centers
  distances <- st_distance(current_point, mrt_cleaned)
  
  # Store minimum distance in km in eldercare_prox
  mrt_resale$mrt_prox[i] <- as.numeric(min(distances)) / 1000
  
}

# Verify results
print(head(mrt_resale))             # Print the first few rows

beep(3)
```

```{r}
#| echo: false
write_rds(mrt_resale, "data/rds/prox/mrt_resale.rds")
```

```{r}
#| echo: false
mrt_resale = read_rds("data/rds/prox/mrt_resale.rds")
```

#### Park

```{r}
# Create eldercare_resale with just the two columns we want
park_resale <- data.frame(
  park_prox = numeric(nrow(resale_geom)))

# Loop through each point in busstop_resale
for (i in 1:nrow(resale_geom)) {
  # Get current point
  current_point <- resale_geom[i, ]
  
  # Calculate distances to all eldercare centers
  distances <- st_distance(current_point, park_cleaned)
  
  # Store minimum distance in km in eldercare_prox
  park_resale$park_prox[i] <- as.numeric(min(distances)) / 1000
  
}

# Verify results
print(head(park_resale))             # Print the first few rows

beep(3)
```

```{r}
#| echo: false
write_rds(park_resale, "data/rds/prox/park_resale.rds")
```

```{r}
#| echo: false
park_resale = read_rds("data/rds/prox/park_resale.rds")
```

#### School

```{r}
# Create eldercare_resale with just the two columns we want
school_resale <- data.frame(
  school_prox = numeric(nrow(resale_geom)),
  within_1km_school = numeric(nrow(resale_geom)))

# Loop through each point in busstop_resale
for (i in 1:nrow(resale_geom)) {
  # Get current point
  current_point <- resale_geom[i, ]
  
  # Calculate distances to all eldercare centers
  distances <- st_distance(current_point, school_cleaned)
  
  # Store minimum distance in km in eldercare_prox
  school_resale$school_prox[i] <- as.numeric(min(distances)) / 1000
  
  # Count eldercare facilities within 1000 meters
  school_resale$within_1km_school[i] <- sum(as.numeric(distances) <= 1000)
}

# Verify results
print(head(school_resale))             # Print the first few rows
summary(school_resale)   # Summary of both columns

beep(3)    
```

```{r}
#| echo: false
write_rds(school_resale, "data/rds/prox/school_resale.rds")
```

```{r}
#| echo: false
school_resale = read_rds("data/rds/prox/school_resale.rds")
```

#### Supermarkets

```{r}
# Create eldercare_resale with just the two columns we want
supermarket_resale <- data.frame(
  supermarket_prox = numeric(nrow(resale_geom)))

# Loop through each point in busstop_resale
for (i in 1:nrow(resale_geom)) {
  # Get current point
  current_point <- resale_geom[i, ]
  
  # Calculate distances to all eldercare centers
  distances <- st_distance(current_point, supermarket_cleaned)
  
  # Store minimum distance in km in eldercare_prox
  supermarket_resale$supermarket_prox[i] <- as.numeric(min(distances)) / 1000
  
}

# Verify results
print(head(supermarket_resale))             # Print the first few rows

beep(3)
```

```{r}
#| echo: false
write_rds(supermarket_resale, "data/rds/prox/supermarket_resale.rds")
```

```{r}
#| echo: false
supermarket_resale = read_rds("data/rds/prox/supermarket_resale.rds")
```

## Visualisations

In the code chunk below, we are able to view the mininum, maximum, median and quadrants of the data. Interesting, the minimum and maximum resale price were \$150,000 and \$1.58m. And the smallest age of unit was 3 whereas the oldest was 58.

```{r}
summary(resale_geom)
```

```{r}
#| echo: false
#| eval: false
Area_sqm <- ggplot(data=resale_geom, aes(x= resale_price)) + 
  geom_histogram(bins=20, color="black", fill="light blue")

Age <- ggplot(data=resale_geom, aes(x= unit_age)) +
  geom_histogram(bins=20, color="black", fill="light blue")

prox_busstop <- ggplot(data=busstop_resale, aes(x= within_350m_busstop)) +
  geom_histogram(bins=20, color="black", fill="light blue")

prox_eldercare <- ggplot(data=eldercare_resale, aes(x= within_350m_eldercare)) + 
  geom_histogram(bins=20, color="black", fill="light blue")

prox_hawker <- ggplot(data=hawker_resale, aes(x= hawker_prox)) +
  geom_histogram(bins=20, color="black", fill="light blue")

prox_kindergarten <- ggplot(data=kindergarten_resale, 
                               aes(x= within_350m_kindergarten)) +
  geom_histogram(bins=20, color="black", fill="light blue")

prox_mall <- ggplot(data=mall_resale, aes(x= mall_prox)) +
  geom_histogram(bins=20, color="black", fill="light blue")

prox_mrt <- ggplot(data=mrt_resale, aes(x= mrt_prox)) +
  geom_histogram(bins=20, color="black", fill="light blue")

prox_park <- ggplot(data= park_resale, aes(x= park_prox)) +
  geom_histogram(bins=20, color="black", fill="light blue")

prox_school <- ggplot(data=school_resale, aes(x= within_1km_school)) +
  geom_histogram(bins=20, color="black", fill="light blue")

prox_supermarket <- ggplot(data=supermarket_resale, aes(x= supermarket_resale)) +
  geom_histogram(bins=20, color="black", fill="light blue")

ggarrange(Area_sqm, Age, prox_busstop, prox_eldercare, prox_hawker,
          prox_kindergarten, prox_mall, prox_mrt, prox_park,
          prox_school, prox_supermarket,
          ncol = 3, nrow = 4)
```

We will use tmap to create an iteractive map of the resale prices. Noticebly, the "hotter" resale flats are concentrated in the south and middle area with scatters around the east and west.

```{r}
tmap_mode('plot')
tm_shape(mpsz)+
  tm_polygons() +
tm_shape(resale_geom) +  
  tm_dots(col = "resale_price",
          alpha = 0.6,
          style="quantile") +
  tm_view(set.zoom.limits = c(11,14))
```

```{r}
tmap_mode('plot')
tm_shape(mpsz)+
  tm_polygons() +
tm_shape(resale_geom) +  
  tm_dots(col = "unit_age",
          alpha = 0.6,
          style="quantile") +
  tm_view(set.zoom.limits = c(11,14))
```

In barchart below, we have populated the percentage of transactions stratified by town. Visibly, Punggol attains the highest 7.7% followed by Woodlands at 7.6%. Hence We would want to predict the HDB resale prices for these two towns.

```{r}
town_percentages <- resale_geom %>%
  group_by(town) %>%
  summarise(count = n()) %>%
  mutate(percentage = (count/sum(count))*100) %>%
  arrange(desc(percentage))  # Sort in descending order

# Create the bar chart
ggplot(town_percentages, aes(x = reorder(town, -percentage), y = percentage)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  geom_text(aes(label = sprintf("%.1f%%", percentage)), 
            vjust = -0.5, 
            size = 3) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Percentage Distribution of Resale Transactions by Town",
       x = "Town",
       y = "Percentage of Total Transactions (%)")
```

## Final Amendments

```{r}
#| echo: false
resale_geom = read_rds("data/rds/aspatial/resale_geom.rds")
```

```{r}
data <- resale_geom %>%
  cbind(busstop_resale, eldercare_resale, hawker_resale,
            kindergarten_resale, mall_resale, mrt_resale,
            park_resale, school_resale, supermarket_resale)
```

Firstly, we will change the column "month" to POSIXct for easier manipulation.

As we are focusing on 4-room HDB flats, we will filter according to that.

Lastly, as we're working with an sf (spatial features) object, we need to use a different approach to remove columns instead of using dplyr. We will use select(!matches) to remove the column address.

```{r}
data <- data %>%
  mutate(month = as.POSIXct(paste0(month, "-01"), format = "%Y-%m-%d")) %>% 
  filter(flat_type == "4 ROOM") %>% # Change from int to num
  select(-c(address, flat_type)) %>% 
  mutate(remaining_lease_yr = as.numeric(remaining_lease_yr)) # Change from int to num

```

```{r}
#| echo: false
write_rds(data, "data/rds/ml/data.rds")
```

```{r}
#| echo: false
write.csv(data,"data/rds/ml/data.csv", row.names = FALSE)
```

## Machine Learning

### Data Sampling

The entire data are split into training (Jan 2023 - June 2024) and testing (July 2024 - Sept 2024) data sets into by using *initial_split()* of **rsample** package. rsample is one of the package of tigymodels.

```{r}
#| echo: false
data = read_rds("data/rds/ml/data.rds")
```

In this section, we will split the data into training data - Jan 2023 to June 2024 & test Data - July 2024 to Sept 2024 that is stratified by Punggol & Woodlands. We can see there are 1549 training records and 300 testing records for Punggol whereas in Woodlands there are 1391 training records and 238 testing records.

::: panel-tabset
## **Punggol**

```{r}
set.seed(1234)
train_data_punggol <- data %>%
  filter(month >= as.POSIXct("2023-01-01") & 
         month <= as.POSIXct("2024-06-30") &
         town == "PUNGGOL") %>% 
  select(-c(month))

test_data_punggol <- data %>%
  filter(month >= as.POSIXct("2024-07-01") & 
         month <= as.POSIXct("2024-09-30") &
         town == "PUNGGOL") %>% 
  select(-c(month))

# Optional: to check the filtered data
print(paste("Number of training records:", nrow(train_data_punggol)))
print(paste("Number of testing records:", nrow(test_data_punggol)))
```

```{r}
write_rds(train_data_punggol, "data/rds/ml/punggol/train_data_punggol.rds")
write_rds(test_data_punggol, "data/rds/ml/punggol/test_data_punggol.rds")
```

## **Woodlands**

```{r}
set.seed(1234)
train_data_woodlands <- data %>%
  filter(month >= as.POSIXct("2023-01-01") & 
         month <= as.POSIXct("2024-06-30") &
         town == "WOODLANDS") %>% 
  select(-c(month))

test_data_woodlands <- data %>%
  filter(month >= as.POSIXct("2024-07-01") & 
         month <= as.POSIXct("2024-09-30") &
         town == "WOODLANDS") %>% 
  select(-c(month))

# Optional: to check the filtered data
print(paste("Number of training records:", nrow(train_data_woodlands)))
print(paste("Number of testing records:", nrow(test_data_woodlands)))
```

```{r}
write_rds(train_data_woodlands, "data/rds/ml/woodlands/train_data_woodlands.rds")
write_rds(test_data_woodlands, "data/rds/ml/woodlands/test_data_woodlands.rds")
```
:::

### Computing Correlation Matrix

Prior to loading predictors into predictive model, a correlation matrix will be use to check for sign of multicollinearity. From the correlation matrix below, all correlation values are below 0.8, indicating no sign of multicollinearity.

```{r}
data_nogeo <- data %>%
   select(-c(month)) %>%
  st_drop_geometry()
```

```{r}
corrplot::corrplot(cor(data_nogeo[, 2:19]), 
                   diag = FALSE, 
                   order = "AOE",
                   tl.pos = "td", 
                   tl.cex = 0.5, 
                   method = "number", 
                   type = "upper")
```

### Retrieving Stored Data

::: panel-tabset
## **Punggol**

```{r}
train_data_punggol <- read_rds("data/rds/ml/punggol/train_data_punggol.rds")
test_data_punggol <- read_rds("data/rds/ml/punggol/test_data_punggol.rds")
```

## **Woodlands**

```{r}
train_data_woodlands <- read_rds("data/rds/ml/woodlands/train_data_woodlands.rds")
test_data_woodlands <- read_rds("data/rds/ml/woodlands/test_data_woodlands.rds")
```
:::

## Building a non-spatial multiple linear regression

::: panel-tabset
## **Punggol**

```{r}
# Fit the linear model for resale_price using the specified variables
price_mlr_p <- lm(resale_price ~ floor_area_sqm +
                 unit_age + within_350m_busstop +
                 within_350m_eldercare + hawker_prox + 
                 within_350m_kindergarten + mall_prox +
                 mrt_prox + park_prox +
                 within_1km_school +
                 supermarket_prox,
               data = train_data_punggol)

# Summary of the linear model
summary(price_mlr_p)

beep(3)
```

```{r}
write_rds(price_mlr_p, "data/rds/ml/punggol/price_mlr_p.rds" ) 
```

## **Woodlands**

```{r}
# Fit the linear model for resale_price using the specified variables
price_mlr_w <- lm(resale_price ~ floor_area_sqm +
                 unit_age + within_350m_busstop +
                 within_350m_eldercare + hawker_prox + 
                 within_350m_kindergarten + mall_prox +
                 mrt_prox + park_prox +
                 within_1km_school +
                 supermarket_prox,
               data = train_data_woodlands)

# Summary of the linear model
summary(price_mlr_w)

beep(3)
```

```{r}
write_rds(price_mlr_w, "data/rds/ml/woodlands/price_mlr_w.rds" ) 
```
:::

## Geographically Weighted Regression

Now, we will calibrate a model to predict HDB resale price by using geographically weighted regression method of [**GWmodel**](https://cran.r-project.org/web/packages/GWmodel/index.html) package.

### Converting the sf data.frame to SpatialPointDataFrame

::: panel-tabset
## **Punggol**

```{r}
train_data_sp_p <- as_Spatial(train_data_punggol)
train_data_sp_p
```

## **Woodlands**

```{r}
train_data_sp_w <- as_Spatial(train_data_woodlands)
train_data_sp_w
```
:::

### Computing adaptive bandwidth

Next, `bw.gwr()` of **GWmodel** package will be used to determine the optimal bandwidth to be used.

::::: panel-tabset
## **Punggol**

```{r}
bw_adaptive_p <- bw.gwr(resale_price ~ floor_area_sqm +
                 unit_age + within_350m_busstop +
                 within_350m_eldercare + hawker_prox + 
                 within_350m_kindergarten + mall_prox +
                 mrt_prox + park_prox +
                 within_1km_school +
                 supermarket_prox,
                  data=train_data_sp_p,
                  approach="CV",
                  kernel="gaussian",
                  adaptive=TRUE,
                  longlat=FALSE)

beep(3)
```

The result shows that 31 neighbour points will be the optimal bandwidth to be used if adaptive bandwidth is used for this data set.

::: callout-note
Insights

The result shows that 86 neighbour points will be the optimal bandwidth to be used if adaptive bandwidth is used for this data set.
:::

```{r}
write_rds(bw_adaptive_p, "data/rds/ml/punggol/bw_adaptive_p.rds")
```

## **Woodlands**

```{r}
bw_adaptive_w <- bw.gwr(resale_price ~ floor_area_sqm +
                 unit_age + within_350m_busstop +
                 within_350m_eldercare + hawker_prox + 
                 within_350m_kindergarten + mall_prox +
                 mrt_prox + park_prox +
                 within_1km_school +
                 supermarket_prox,
                  data=train_data_sp_w,
                  approach="CV",
                  kernel="gaussian",
                  adaptive=TRUE,
                  longlat=FALSE)

beep(3)
```

::: callout-note
Insights

The results shows that 48 neighbour points will be the optimal bandwidth to be used if adaptive bandwidth is used for this data set.
:::

```{r}
write_rds(bw_adaptive_w, "data/rds/ml/woodlands/bw_adaptive_w.rds")
```
:::::

### Constructing the adaptive bandwidth gwr model

```{r}
bw_adaptive_p <- read_rds("data/rds/ml/punggol/bw_adaptive_p.rds")
```

```{r}
bw_adaptive_w <- read_rds("data/rds/ml/woodlands/bw_adaptive_w.rds")
```

Now, we can go ahead to calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and Gaussian kernel as shown in the code chunk below.

::: panel-tabset
## **Punggol**

```{r}
gwr_adaptive_p <- gwr.basic(formula = resale_price ~ floor_area_sqm +
                 unit_age + within_350m_busstop +
                 within_350m_eldercare + hawker_prox + 
                 within_350m_kindergarten + mall_prox +
                 mrt_prox + park_prox +
                 within_1km_school +
                 supermarket_prox,
                  data=train_data_sp_p,
                          bw=bw_adaptive_p, 
                          kernel = 'gaussian', 
                          adaptive=TRUE,
                          longlat = FALSE)
beep(3)
```

```{r}
write_rds(gwr_adaptive_p, "data/rds/ml/punggol/gwr_adaptive_p.rds")
```

## **Woodlands**

```{r}
gwr_adaptive_w <- gwr.basic(formula = resale_price ~ floor_area_sqm +
                 unit_age + within_350m_busstop +
                 within_350m_eldercare + hawker_prox + 
                 within_350m_kindergarten + mall_prox +
                 mrt_prox + park_prox +
                 within_1km_school +
                 supermarket_prox,
                  data=train_data_sp_w,
                          bw=bw_adaptive_w, 
                          kernel = 'gaussian', 
                          adaptive=TRUE,
                          longlat = FALSE)
beep(3)
```

```{r}
write_rds(gwr_adaptive_w, "data/rds/ml/woodlands/gwr_adaptive_w.rds")
```
:::

### Retrieve gwr output object

The code below can be used to display the model output.

::::: panel-tabset
## **Punggol**

```{r}
gwr_adaptive_p <- read_rds("data/rds/ml/punggol/gwr_adaptive_p.rds")
```

```{r}
gwr_adaptive_p
```

::: callout-note
**Insights of 2 Regression Analysis in predicting resale price - Punggol**

1.  Global Regression

    -   **Residuals**: The residuals indicate the difference between the observed and predicted resale prices. The residuals range from -144,029 to 135,047, with a median of -1,271, suggesting that the model may not perfectly fit all data points, particularly those with extreme values.

    -   **Coefficients**: The estimated coefficients for each predictor variable indicate their respective impact on resale price.

        -   **Significant Variables**: Variables like `floor_area_sqm`, `unit_age`, and `hawker_prox` are statistically significant with p-values less than 0.001, suggesting strong relationships with the resale price.

        -   **Non-significant Variables**: `within_350m_busstop` did not show significant effects on resale price, with p-values greater than 0.05.

    -   **Model Fit**: The global regression model explains about **51.57%** of the variance in resale price (R-squared = 0.5157). The **Adjusted R-squared** is 0.5123, showing a modest fit after adjusting for the number of predictors. The **F-statistic** of 148.8, with a p-value less than 2.2e-16, indicates that the overall model is highly significant.

    -   **AIC/BIC**: The Akaike Information Criterion (AIC = 37,298.31) and the Bayesian Information Criterion (BIC = 35,914.29) suggest that the global model provides a reasonable fit, although these values are used for model comparison rather than absolute goodness-of-fit assessment.

2.  Geographically Weighted Regression

This model was calibrated using the **adaptive Gaussian kernel** with an **adaptive bandwidth** of 31, meaning the model uses data from the 31 nearest neighbors to each observation.

-   **Model Calibration**: The GWR model was fitted using the same set of predictor variables as the global regression model. The use of an adaptive kernel and Euclidean distance metric enables the model to account for spatial heterogeneity in the relationships between predictors and resale price.

-   **Coefficient Estimates**:

    -   The GWR model shows **significant variation** in the coefficient estimates across geographic locations. For instance:

        -   The coefficient for `floor_area_sqm` ranges from negative to positive values, indicating that the impact of floor area on resale price varies spatially.

        -   The coefficient for `unit_age` shows a similar spatial variation, with the impact differing from location to location.

        -   `hawker_prox` and `supermarket_prox` also exhibit varying effects on resale price across locations.

-   **Model Fit**:

    **R-squared**: The GWR model explains approximately **71.98%** of the variance in resale price (R-squared = 0.7198), which is a substantial improvement over the global regression model. The **Adjusted R-squared** is 0.6743, indicating that the model explains a significant portion of the variance while accounting for the number of predictors.

    **AIC/BIC**: The GWR model shows lower AIC (36,602.13) and BIC (36,179.57) values than the global regression model, suggesting that the GWR model provides a better fit to the data when accounting for spatial variability.

**Overall**

Spatially varying coefficients highlight that the impact of certain factors, such as `floor_area_sqm`, `unit_age`, and proximity to amenities like `hawker centers` and `supermarkets`, can differ significantly across different locations, suggesting that location-specific interventions or policies might be more effective in real estate pricing.
:::

## **Woodlands**

```{r}
gwr_adaptive_w <- read_rds("data/rds/ml/woodlands/gwr_adaptive_w.rds")
```

```{r}
gwr_adaptive_w
```

::: callout-note
**Insights of 2 Regression Analysis in predicting resale price - Woodlands**

1.  Global Regression

-   **Residuals**: The residuals range from -110,060 to 118,690, with a median of -926. This shows that the model can produce both underestimations and overestimations of resale price, with some extreme residuals indicating potential outliers.

-   **Coefficients**:

    -   **Intercept**: The base resale price when all predictors are zero is estimated at **435,172.15**.

    -   **Significant Variables**:

        -   **`floor_area_sqm`**: Each additional square meter increases the resale price by **2,247.33**.

        -   **`unit_age`**: Each additional year of unit age decreases the resale price by **4,322.06**.

        -   **`hawker_prox`**: Proximity to hawker centers decreases resale price by **-14,811.35**.

        -   **`mrt_prox`**: Proximity to MRT stations has a significant negative effect of **-45,257.65** on resale price.

        -   **`park_prox`**: Proximity to parks also significantly decreases resale price by **-15,817.05**.

        -   **`within_350m_kindergarten`**: The presence of a kindergarten within 350 meters increases resale price by **3,083.36**.

-   **Model Fit**:

    -   **R-squared**: The model explains about **74.83%** of the variance in resale price, indicating a strong overall fit.

    -   **Adjusted R-squared**: **74.63%**, which adjusts for the number of predictors.

    -   **F-statistic**: The F-statistic is **372.7**, with a p-value of less than 2.2e-16, indicating that the model is highly significant.

-   **AIC/BIC**:

    -   **AIC**: 32,503.3, and **BIC**: 31,274.49, which provide a basis for model comparison and indicate that the global model fits the data reasonably well.

2.  Geographical Weighted Regression

-   **Model Calibration**:

    -   **Adaptive Bandwidth**: 48 neighbors were used in the adaptive bandwidth, ensuring that the local influence of each observation is based on the proximity of other observations.

    -   **Kernel Function**: A Gaussian kernel was used, which gives more weight to observations closer to each location.

-   **Coefficient Estimates**: The coefficients for the predictors vary significantly across geographic locations:

    -   **`floor_area_sqm`**: The effect of floor area on resale price ranges from negative to positive values. In some areas, additional floor space leads to a large increase in resale price, while in others, the impact is smaller.

    -   **`unit_age`**: The effect of unit age varies spatially, with some areas showing a significant negative impact on resale price, while in others, the impact is less pronounced.

    -   **`hawker_prox`**: The proximity to hawker centers shows a negative relationship with resale price in some regions, with a large range in coefficient estimates, suggesting that the effect of hawker centers on property prices varies significantly by location.

    -   **`mrt_prox`**: Proximity to MRT stations shows a negative effect in certain locations, with varying strength across different regions in Woodlands.

    -   **`park_prox`**: Proximity to parks also varies across locations, with some areas showing a stronger negative impact on resale price than others.

-   **Model Fit**:

    -   **R-squared**: The GWR model explains **80.58%** of the variance in resale price, which represents a significant improvement over the global model (74.83%). This indicates that incorporating spatial variability greatly enhances model accuracy.

    -   **Adjusted R-squared**: **78.52%**, further indicating that the GWR model accounts for a significant amount of variance while adjusting for the number of predictors.

-   **AIC/BIC**:

    -   **AIC**: 32,223.42 and **BIC**: 31,497.62, which are lower than the values for the global model, indicating that the GWR model provides a better fit for the data when accounting for spatial differences.

-   **Residual Sum of Squares (RSS)**: The GWR model has a residual sum of squares of **8.67507e+11**, suggesting that the model has reduced unexplained variance compared to the global model.

**Overall**

The GWR model provides a much better fit for the resale price data in Woodlands compared to the global regression model. The improvement in **R-squared** (from 74.83% to 80.58%) suggests that there are significant spatial differences in the factors affecting resale prices.

-   **Floor Area**: The positive relationship between floor area and resale price varies significantly across Woodlands. In some areas, floor area is a strong predictor of resale price, while in others, it has a smaller effect.

-   **Unit Age**: The negative effect of unit age on resale price is more pronounced in some parts of Woodlands, indicating that older units in certain locations have a larger impact on resale price.

-   **Proximity to Amenities**: Variables such as proximity to MRT stations, parks, and hawker centers show varying effects across locations. For example, properties near MRT stations tend to have lower resale prices in some regions, while in others, the proximity to parks and kindergartens can increase prices.
:::
:::::

### Converting the test data from sf data.frame to SpatialPointDataFrame

::: panel-tabset
## **Punggol**

```{r}
test_data_sp_p <- as_Spatial(test_data_punggol)
test_data_sp_p
```

## **Woodlands**

```{r}
test_data_sp_w <- as_Spatial(test_data_woodlands)
test_data_sp_w
```
:::

### Computing adaptive bandwidth for the test data

::: panel-tabset
## **Punggol**

```{r}
gwr_bw_test_adaptive_p <- bw.gwr(resale_price ~ floor_area_sqm +
                 unit_age + within_350m_busstop +
                 within_350m_eldercare + hawker_prox + 
                 within_350m_kindergarten + mall_prox +
                 mrt_prox + park_prox +
                 within_1km_school +
                 supermarket_prox,
                  data=test_data_sp_p,
                  approach="CV",
                  kernel="gaussian",
                  adaptive=TRUE,
                  longlat=FALSE)

beep(3)
```

```{r}
write_rds(gwr_bw_test_adaptive_p, "data/rds/ml/punggol/gwr_bw_test_adaptive_p.rds")
```

## **Woodlands**

```{r}
gwr_bw_test_adaptive_w<- bw.gwr(resale_price ~ floor_area_sqm +
                 unit_age + within_350m_busstop +
                 within_350m_eldercare + hawker_prox + 
                 within_350m_kindergarten + mall_prox +
                 mrt_prox + park_prox +
                 within_1km_school +
                 supermarket_prox,
                  data=test_data_sp_w,
                  approach="CV",
                  kernel="gaussian",
                  adaptive=TRUE,
                  longlat=FALSE)

beep(3)
```

```{r}
write_rds(gwr_bw_test_adaptive_w, "data/rds/ml/woodlands/gwr_bw_test_adaptive_w.rds")
```
:::

### Computing predicted values of the test data

::::: panel-tabset
## **Punggol**

```{r}
gwr_pred_p <- gwr.predict(formula = resale_price ~ floor_area_sqm +
                 unit_age + within_350m_busstop +
                 within_350m_eldercare + hawker_prox + 
                 within_350m_kindergarten + mall_prox +
                 mrt_prox + park_prox +
                 within_1km_school +
                 supermarket_prox, 
                        data=train_data_sp_p, 
                        predictdata = test_data_sp_p, 
                        bw=100, 
                        kernel = 'gaussian', 
                        adaptive=TRUE, 
                        longlat = FALSE)

beep(3)
```

```{r}
gwr_pred_p
```

```{r}
write_rds(gwr_pred_p, "data/rds/ml/punggol/gwr_pred_p.rds")
```

::: callout-note
**GWR Coefficient Estimates**

**Interpretation**:

-   The **Intercept** varies greatly across locations, ranging from a negative coefficient (-1,441,251.41) to a positive coefficient (786,268.8), indicating that baseline resale prices are influenced differently across locations.

-   **floor_area_sqm**: The effect of floor area on resale price ranges from negative to positive. In some locations, a larger floor area has a smaller impact, while in others it has a much larger effect.

-   **unit_age**: The negative relationship between unit age and resale price is more pronounced in some areas, suggesting that older units may have a larger negative impact on property values in certain locations.

-   **Proximity to amenities**: Variables such as **hawker_prox**, **mrt_prox**, **park_prox**, and **supermarket_prox** show a wide range of coefficients. For example, proximity to hawker centers has a strong negative impact in some regions, while proximity to parks and supermarkets has a positive impact in others.

**Prediction Results**

**Interpretation**:

-   The predicted resale prices range from **\$463,945** to **\$707,938**, with a median prediction of **\$609,348**. These values are within a reasonable range given the data and suggest a good predictive performance.

-   The **prediction variance** shows a substantial range, from about **1.25 billion** to **1.4 billion**, indicating that the predicted resale prices vary considerably. This is expected, as some locations may have more variability in resale price due to factors such as proximity to amenities and the overall market conditions.

**Overall**

Key findings include:

-   The **effect of floor area**, **unit age**, and **proximity to amenities** varies significantly across different locations in Woodlands.

-   The **prediction results** show a reasonable range of predicted resale prices, with substantial variance indicating the sensitivity of prices to local factors.
:::

## **Woodlands**

```{r}
gwr_pred_w <- gwr.predict(formula = resale_price ~ floor_area_sqm +
                 unit_age + within_350m_busstop +
                 within_350m_eldercare + hawker_prox + 
                 within_350m_kindergarten + mall_prox +
                 mrt_prox + park_prox +
                 within_1km_school +
                 supermarket_prox, 
                        data=train_data_sp_w, 
                        predictdata = test_data_sp_w, 
                        bw=100, 
                        kernel = 'gaussian', 
                        adaptive=TRUE, 
                        longlat = FALSE)

beep(3)
```

```{r}
gwr_pred_w
```

```{r}
write_rds(gwr_pred_w, "data/rds/ml/woodlands/gwr_pred_w.rds")
```

::: callout-note
**Summary of GWR Coefficients**

**Interpretation**:

-   **Intercept**: The baseline resale price in Woodlands varies significantly across locations, with a range from 373,726.15 to 929,191.50. This suggests that the average resale price can be strongly influenced by local spatial factors.

-   **Floor Area**: The effect of floor area on resale price varies from a negative to a positive coefficient. In some locations, additional floor area has a relatively small positive impact, while in other locations, the effect is stronger.

-   **Unit Age**: The negative impact of unit age on resale price is consistent across locations. Older units tend to have a more significant negative effect on resale prices, particularly in areas where newer properties dominate.

-   **Proximity to Amenities**:

    -   **Bus Stops**: Proximity to bus stops generally has a small positive effect in certain areas (range from -1,933.97 to 3,494.30).

    -   **Elder Care Centers**: The proximity to eldercare centers has a more varied effect, with the coefficient ranging from negative to positive. In some regions, this proximity significantly increases resale prices.

    -   **Hawker Centers**: The proximity to hawker centers shows a highly negative impact in many areas, especially in places where hawker centers are abundant and could be seen as less desirable.

    -   **Kindergartens**: Proximity to kindergartens has a positive effect on property prices in certain locations, especially in family-oriented neighborhoods.

    -   **Malls, MRT, Parks**: Proximity to malls and MRT stations tends to have a negative impact on resale prices in specific areas, while proximity to parks shows a more varied effect, with some areas benefiting from this proximity more than others.

    -   **Supermarkets**: Proximity to supermarkets has a small negative effect on resale prices in some areas but is generally less influential compared to other amenities.

**Prediction Results**

**Interpretation**:

-   The predicted resale prices range from **\$359,170** to **\$616,562**, with a median prediction of **\$493,786**. This shows that the resale price in Woodlands varies significantly across locations.

-   The **prediction variance** is substantial, ranging from **735 million** to **792 million**. This suggests considerable variability in the predicted prices, likely due to differing local factors influencing property values across different areas of Woodlands.

**Overall**

The **Geographically Weighted Regression (GWR)** model provides a more accurate prediction of resale prices in **Woodlands** compared to traditional global models. By accounting for spatial variability, the GWR model captures how the relationship between property features and resale price changes across different locations.

**Key findings**:

-   **Floor area** and **unit age** have spatially varying impacts, with some regions showing stronger relationships between these variables and resale prices.

-   **Proximity to amenities** such as **hawker centers**, **MRT stations**, and **supermarkets** shows spatial heterogeneity, with some areas benefiting more from proximity to these amenities than others.

-   The **prediction results** indicate that resale prices vary widely across Woodlands, with certain regions experiencing higher values based on local factors like proximity to key amenities.
:::
:::::

## Random Forest

### Extracting coordinates data

::: panel-tabset
```{r}
coords <- st_coordinates(data)
```

## **Punggol**

```{r}
coords_train_p <- st_coordinates(train_data_punggol)
coords_test_p <- st_coordinates(test_data_punggol)
```

Before continue, we write all the output into rds for future used.

```{r}
coords_train_p <- write_rds(coords_train_p, "data/rds/ml/punggol/coords_train_p.rds" )
coords_test_p <- write_rds(coords_test_p, "data/rds/ml/punggol/coords_test_p.rds" )
```

## **Woodlands**

```{r}
coords_train_w <- st_coordinates(train_data_woodlands)
coords_test_w <- st_coordinates(test_data_woodlands)
```

Before continue, we write all the output into rds for future used.

```{r}
coords_train_w <- write_rds(coords_train_w, "data/rds/ml/coords_train_w.rds" )
coords_test_w <- write_rds(coords_test_w, "data/rds/ml/coords_test_w.rds" )
```
:::

### Dropping geometry field

First, we will drop geometry column of the sf data.frame by using `st_drop_geometry()` of sf package.

::: panel-tabset
## **Punggol**

```{r}
train_data_punggol <- train_data_punggol %>% 
  st_drop_geometry() %>% 
  select(-town)
```

## **Woodlands**

```{r}
train_data_woodlands <- train_data_woodlands %>% 
  st_drop_geometry() %>% 
  select(-town)
```
:::

## Calibrating Random Forest Model

In this section, you will learn how to calibrate a model to predict HDB resale price by using random forest function of [**ranger**](https://cran.r-project.org/web/packages/ranger/index.html) package.

::: panel-tabset
## **Punggol**

```{r}
set.seed(1234)
rf_p <- ranger(resale_price ~ floor_area_sqm +
                 unit_age + within_350m_busstop +
                 within_350m_eldercare + hawker_prox + 
                 within_350m_kindergarten + mall_prox +
                 mrt_prox + park_prox +
                 within_1km_school +
                 supermarket_prox,
             data=train_data_punggol)
rf_p
```

```{r}
write_rds(rf_p, "data/rds/ml/punggol/rf_p.rds")
```

```{r}
rf_p <- read_rds("data/rds/ml/punggol/rf_p.rds")
rf_p
```

## **Woodlands**

```{r}
set.seed(1234)
rf_w <- ranger(resale_price ~ floor_area_sqm +
                 unit_age + within_350m_busstop +
                 within_350m_eldercare + hawker_prox + 
                 within_350m_kindergarten + mall_prox +
                 mrt_prox + park_prox +
                 within_1km_school +
                 supermarket_prox,
             data=train_data_woodlands)
rf_w
```

```{r}
write_rds(rf_w, "data/rds/ml/woodlands/rf_w.rds")
```

```{r}
rf_w <- read_rds("data/rds/ml/woodlands/rf_w.rds")
rf_w
```
:::

## Calibrating Geographical Random Forest Model

In this section, you will learn how to calibrate a model to predict HDB resale price by using `grf()` of [**SpatialML**](https://cran.r-project.org/web/packages/ranger/index.html) package.

::: panel-tabset
## **Punggol**

```{r}
set.seed(1234)
gwRF_adaptive_p <- grf(formula = resale_price ~ floor_area_sqm +
                 unit_age + within_350m_busstop +
                 within_350m_eldercare + hawker_prox + 
                 within_350m_kindergarten + mall_prox +
                 mrt_prox + park_prox +
                 within_1km_school +
                 supermarket_prox,
                      dframe=train_data_punggol, 
                     bw=55,
                     kernel="adaptive",
                     coords=coords_train_p,
                     ntree = 50)

beep(3)
```

```{r}
write_rds(gwRF_adaptive_p, "data/rds/ml/punggol/gwRF_adaptive_p.rds")
```

```{r}
gwRF_adaptive_p <- read_rds("data/rds/ml/punggol/gwRF_adaptive_p.rds")
```

## **Woodlands**

```{r}
set.seed(1234)
gwRF_adaptive_w <- grf(formula = resale_price ~ floor_area_sqm +
                 unit_age + within_350m_busstop +
                 within_350m_eldercare + hawker_prox + 
                 within_350m_kindergarten + mall_prox +
                 mrt_prox + park_prox +
                 within_1km_school +
                 supermarket_prox,
                      dframe=train_data_woodlands, 
                     bw=55,
                     kernel="adaptive",
                     coords=coords_train_w,
                     ntree = 50)

beep(3)
```

```{r}
write_rds(gwRF_adaptive_w, "data/rds/ml/woodlands/gwRF_adaptive_w.rds")
```

```{r}
gwRF_adaptive_w <- read_rds("data/rds/ml/woodlands/gwRF_adaptive_w.rds")
```
:::

### Predicting by using test data

#### Preparing the test data

The code chunk below will be used to combine the test data with its corresponding coordinates data.

::: panel-tabset
## **Punggol**

```{r}
test_data_punggol <- cbind(test_data_punggol, coords_test_p) %>%
  st_drop_geometry()
```

## **Woodlands**

```{r}
test_data_woodlands <- cbind(test_data_woodlands, coords_test_w) %>%
  st_drop_geometry()
```
:::

#### Predicting with test data

Next, `predict.grf()` of spatialML package will be used to predict the resale value by using the test data and gwRF_adaptive model calibrated earlier.

::: panel-tabset
## **Punggol**

```{r}
gwRF_pred_p <- predict.grf(gwRF_adaptive_p, 
                           test_data_punggol, 
                           x.var.name="X",
                           y.var.name="Y", 
                           local.w=1,
                           global.w=0)
```

```{r}
GRF_pred_p <- write_rds(gwRF_pred_p, "data/rds/ml/punggol/gwRF_adaptive_p.rds")
```

## **Woodlands**

```{r}
gwRF_pred_w <- predict.grf(gwRF_adaptive_w, 
                           test_data_woodlands, 
                           x.var.name="X",
                           y.var.name="Y", 
                           local.w=1,
                           global.w=0)
```

```{r}
GRF_pred_w <- write_rds(gwRF_pred_w, "data/rds/ml/woodlands/gwRF_adaptive_w.rds")
```
:::

#### Converting the predicting output into a data frame

The output of the `predict.grf()` is a vector of predicted values. It is wiser to convert it into a data frame for further visualisation and analysis.

::: panel-tabset
## **Punggol**

```{r}
GRF_pred_p <- read_rds("data/rds/ml/punggol/gwRF_adaptive_p.rds")
```

```{r}
GRF_pred_df_p <- as.data.frame(GRF_pred_p)
```

In the code chunk below, `cbind()` is used to append the predicted values onto test_data

```{r}
test_data_p <- cbind(test_data_punggol, GRF_pred_df_p)
```

```{r}
write_rds(test_data_p, "data/rds/ml/punggol/test_data_p.rds")
```

## **Woodlands**

```{r}
GRF_pred_w <- read_rds("data/rds/ml/woodlands/gwRF_adaptive_w.rds")
```

```{r}
GRF_pred_df_w <- as.data.frame(GRF_pred_w)
```

In the code chunk below, `cbind()` is used to append the predicted values onto test_data

```{r}
test_data_w <- cbind(test_data_woodlands, GRF_pred_df_w)
```

```{r}
write_rds(test_data_w, "data/rds/ml/woodlands/test_data_w.rds")
```
:::

### Calculating Root Mean Square Error

The root mean square error (RMSE) allows us to measure how far predicted values are from observed values in a regression analysis. In the code chunk below, rmse() of Metrics package is used to compute the RMSE.

::::: panel-tabset
## **Punggol**

```{r}
rmse(test_data_punggol$resale_price, 
     test_data_p$GRF_pred_p)
```

```{r}
summary(train_data_punggol$resale_price)
```

```{r}
summary(test_data_p$resale_price)
```

::: callout-note
**Interpretation**

1.  **Magnitude of the Error**:

    -   The **RMSE of** \$49,423.08 indicates that, on average, the model’s predicted resale prices deviate from the actual resale prices by approximately \$**49,423.08.**

    -   Given that the **mean resale price** for the **test dataset** is \$**650,865**, this RMSE represents about **7.5%** of the mean resale price. This indicates a relatively reasonable model performance, with predictions being on average about **7.5%** off from actual resale prices.

2.  **Comparison to Median**:

    -   The **median resale price** in the **test dataset** is \$**648,000**.

    -   The **RMSE of \$49,423.08** represents about **7.6%** of the median resale price, which suggests that the model’s predictions are, on average, off by **7.6%** from the actual resale price at the median.

    -   Since the RMSE is around **7.5-8%** of both the **mean** and **median resale prices**, it suggests that the model performs fairly well, but there is still some degree of prediction error.

3.  **Model Performance**:

    -   An RMSE of \$**49,423.08** is typical for real estate predictive models. It shows that while the model has a strong performance, especially considering it accounts for the variance in resale prices, there's still room for improvement, particularly for properties at the lower and higher price ranges.

4.  **Range of Resale Prices**:

    -   The resale prices in the test dataset range from \$495,000 to \$788,000. Given this price range, the RMSE of \$**49,423.08** suggests that the model is accurate within **7-8%** for most properties, but the error could be higher for properties at the extremes (either low or high price points).

    -   The **training dataset** has similar price distributions, but with a slightly lower mean of \$606,094 compared to the test dataset’s mean of \$**650,865**, which suggests that the model may be slightly more optimistic for higher resale prices in the test set.

5.  **Comparison to the Training Data**:

    -   The **training dataset** has a slightly higher mean resale price (\$**606,094**) compared to the test dataset (\$**650,865**), suggesting that the model might have been slightly undertrained or overfitted. This could be further analyzed by evaluating how well the model generalizes to the test data.

    -   The relatively large **spread** between the lower and upper bounds of the resale prices (from \$456,000 to \$**800,000** in the training set) might also contribute to some variance in the predictions, though the RMSE indicates that the model is generally effective.

**Conclusion:**

The **RMSE of** \$\$49,423.08 for the Punggol test dataset, where the median resale price is **648,000**, indicates that the model’s predictions are on average about **7.6%** off from the actual resale prices. This suggests that the model performs reasonably well, with error margins that are typical for real estate prediction models. While this level of error is acceptable, there is still room for improvement, particularly for properties at the lower and higher ends of the price spectrum.
:::

## **Woodlands**

```{r}
rmse(test_data_woodlands$resale_price, 
     test_data_w$GRF_pred_w)
```

```{r}
summary(train_data_woodlands$resale_price)
```

```{r}
summary(test_data_w$resale_price)
```

::: callout-note
**Interpretation**

1\. **Magnitude of the Error**

-   The **RMSE of** \$42,038.73 indicates that, on average, the model’s predicted resale prices deviate from the actual resale prices by \$**42,038.73**

-   This is a **7.8%** error when compared to the **mean resale price** in the test dataset (\$**540,097**). This suggests that, on average, the model’s predictions are off by about **7.8%** of the actual resale price, which is a reasonable error margin for many predictive models.

2\. **Comparison to the Median**

-   The **median resale price** in the **test dataset** is \$**530,000**.

-   The **RMSE of \$42,038.73** represents about **7.9%** of the median resale price. This level of error is typical for real estate predictive models, and the fact that the RMSE is about **7.9%** of the median indicates that the model performs well for most of the data points, but there may still be some variability in predictions for properties priced far below or above the median.

3\. **Model Performance**

-   The **RMSE of \$42,038.73** shows that the model has a **moderate level of predictive accuracy**. An RMSE that is around **7-8%** of the median and mean resale prices is common for real estate prediction models, which often deal with diverse factors influencing property prices (e.g., location, condition of the property, etc.).

-   The fact that the RMSE is relatively small compared to the range of resale prices suggests that the model captures the general trends well. However, there could be room for improvement, especially for properties with prices that are on the lower or upper extremes of the price spectrum.

4\. **Range of Resale Prices**

-   The **range of resale prices** in the test dataset spans from \$355,000 to \$701,000, with the median at \$530,000 and the mean at \$**540,097**.

-   The RMSE of \$42,038.73 indicates that, on average, the predicted resale prices are within 7-8% of the actual resale prices. This is reasonable for most properties, but properties at the lower end (\$355,000) or the **upper end** (\$701,000) of the price range might have higher prediction errors. This suggests that the model works well for mid-range properties, but its accuracy decreases as we move toward the extremes of the price range.

5\. **Comparison to Training Data**

-   The **median resale price** in the **training dataset** is \$\$500,000, which is 30,000 lower than the median resale price in the test dataset (5\$**30,000**).

-   The **mean** resale price in the training dataset is \$507,168, which is also slightly lower than the mean of \$**540,097** in the test dataset.

-   This suggests that the test data might contain higher-value properties than the training data. Despite this difference, the model’s performance (as indicated by the RMSE) is fairly consistent, with the error remaining around **7.8%** of the test dataset's mean resale price.

-   The **RMSE of** \$42,038.73 relative to the training dataset's median resale price of \$**500,000** is approximately **8.4%**, which is slightly higher than the RMSE relative to the test dataset’s median. This indicates that the model performs slightly better on the test dataset, potentially due to the difference in price distributions.

**Conclusion:**

The **RMSE of** \$42,038.73 for the Woodlands test dataset, where the median resale price is \$**530,000**, suggests that the model's predictions are, on average, about **7.9%** off from the actual resale prices. This is a reasonable level of predictive accuracy, indicating that the model is performing well overall, with small but acceptable prediction errors for properties within the typical price range.

The model appears to perform slightly better on the **test dataset**, which has higher property prices than the training dataset. Further model refinement or feature adjustment may help reduce errors for properties at the extremes of the price range. However, for general purposes, the model seems reliable for predicting resale prices within the observed price ranges.
:::
:::::

## Visualising the predicted values

Alternatively, scatterplot can be used to visualise the actual resale price and the predicted resale price by using the code chunk below.

::::: panel-tabset
## **Punggol**

```{r}
ggplot(data = test_data_p,
       aes(x = GRF_pred_p,
           y = resale_price)) +
  geom_point()
```

::: callout-note
Observation

1.  **Positive Correlation**:

    -   There is a clear **positive relationship** between the predicted resale prices (**GRF_pred_p**) and the actual resale prices (**resale_price**). As the predicted resale prices increase, the actual resale prices also tend to increase.

    -   This suggests that the model's predictions are generally in line with the observed data, as higher predicted values correspond to higher actual resale prices.

2.  **Trend and Distribution**:

    -   The points follow an upward trend, with a generally **tight clustering** around the line of best fit (not shown, but implied by the alignment of points). This indicates that the model is effectively capturing the main trend in the data.

    -   The scatter is somewhat **spread out**, indicating some variance between the predicted and actual resale prices. This is typical in any real-world prediction model, as it shows that the model is not perfect and that there are some discrepancies between predicted and actual values.

3.  **Outliers**:

    -   Although the points generally follow a clear upward trend, there might be a few **outliers** or data points that deviate significantly from the general trend. These outliers could represent cases where the model performs poorly or where there are unusual observations in the data.

4.  **Model Fit**:

    -   The strong linear trend suggests that the model has captured the general relationship between the variables, but some level of variability remains. To assess how well the model performs, you would typically calculate performance metrics like **R-squared**, **RMSE**, or **MAE** to quantify how well the model's predictions match the actual values.

Conclusion:

The scatterplot shows that there is a positive relationship between predicted resale prices (**GRF_pred_p**) and actual resale prices (**resale_price**). This suggests that the model is successful in capturing the general trend of the data, but some discrepancies remain, which is expected in any predictive model. Further analysis, such as residual analysis or model evaluation metrics, could be done to refine the model and assess its performance more thoroughly.
:::

## **Woodlands**

::: callout-note
**Observation**

1.  **Positive Correlation**:

    -   There is a **strong positive correlation** between **GRF_pred_w** (predicted resale prices) and **resale_price** (actual resale prices). As the predicted resale prices increase, the actual resale prices also tend to increase.

    -   This indicates that the model's predictions are generally aligned with the observed data, and the higher predicted values tend to correspond to higher actual resale prices.

2.  **Trend and Distribution**:

    -   The points show a clear **upward trend**, indicating a **good linear relationship** between the predicted and actual values.

    -   The scatter around the trend is **relatively tight**, meaning that the model's predictions are fairly close to the actual resale prices. This suggests that the model is capturing the trend in the data well.

3.  **Outliers**:

    -   Some **outliers** may exist at the higher end of the resale price range. These points appear to deviate slightly from the general trend, but they do not seem to be too far from the line of best fit. These outliers could represent cases where the model does not predict well for certain properties, possibly due to extreme values or unique characteristics of those observations.

4.  **Model Fit**:

    -   The strong linear relationship in the scatterplot indicates that the model fits the data well. However, there may still be room for improvement in reducing the variance in predictions, as indicated by the slight spread of points, especially for lower resale prices.

5.  **Comparison to the Previous Scatterplot**:

    -   Compared to the first scatterplot with **GRF_pred_p**, this scatterplot (with **GRF_pred_w**) shows a **similar pattern** of positive correlation between predicted and actual resale prices. Both scatterplots indicate good predictive performance, with minor deviations in predicted values at the lower and higher ends.

Conclusion:

The scatterplot suggests that the model with **GRF_pred_w** effectively captures the relationship between the predictors and actual resale prices, with strong predictive accuracy. The positive correlation and tight clustering of points indicate that the model is reliable for predicting resale prices. However, further evaluation using residual analysis or model evaluation metrics could help identify areas for improvement, particularly in the presence of outliers.
:::
:::::

## Evaluation of Model: Geographically Weighted Regression Vs Random Forest

::: panel-tabset
## **Punggol** 

To compare the two methods—Random Forest (RF) and Geographically Weighted Regression (GWR)—for predicting resale prices in Punggol, let's focus on the prediction accuracy and model performance.

1\. Prediction Accuracy:

Random Forest:

-   RMSE: The Random Forest model has an RMSE of 49,423.08, which measures the average error between the predicted and actual resale prices. A lower RMSE indicates better accuracy.

-   Price Distribution:

    -   **Training Data**: Min 456,000, Median 605,000, Max 800,000.

    -   **Test Data**: Min 495,000, Median 648,000, Max 788,000.

The predicted resale prices in the test data are within the expected range and the RMSE indicates that the Random Forest model performs fairly well.

GWR:

-   Predicted Values: The predicted resale prices from the GWR model range from 463,945 to 707,938, with a median of 609,348.

-   Prediction Variance: The variance of predictions ranges from 1.25 billion to 1.4 billion, indicating some spread in the predictions, which is typical of GWR since it accounts for spatial variability.

The predicted resale prices from GWR are also within the expected range, with a similar spread as the actual test data.

2.  Model Calibration:

Random Forest:

-   Strength: Random Forest is well-suited for handling complex, non-linear relationships between predictors without explicit assumptions about their functional form.

-   Weakness: It is harder to interpret and doesn't directly provide insights into how individual variables (like distance to bus stops, malls, etc.) influence the resale prices.

GWR:

-   Strength: GWR allows for spatial variation in the coefficients, meaning it can show how the effect of each predictor (like `floor_area_sqm`, `unit_age`, etc.) changes across different locations. This makes GWR especially valuable for understanding local variations in resale prices.

-   Weakness: GWR requires more complex interpretation, as it provides different coefficients for each observation based on the local neighborhood.

3.  Which Model is Better?

-   Prediction Accuracy: Both models provide similar ranges of predicted resale prices. However, Random Forest has a slightly better RMSE (49,423.08), which suggests it may perform slightly better in terms of overall prediction accuracy.

-   Interpretability: GWR excels in interpretability, as it provides insights into how different factors affect resale prices in different parts of Punggol, thanks to its spatially varying coefficients. This is valuable if you need to understand the localized impact of each predictor.

## **Woodlands**

1\. Prediction Accuracy:

Random Forest:

-   RMSE: The RMSE for the Random Forest model is 42,038.73. Lower RMSE indicates better prediction accuracy, and this value is quite reasonable, suggesting good performance for predicting resale prices.

-   Price Summary:

    -   **Training Data**: Min 350,000, Median 500,000, Max 690,000.

    -   **Test Data**: Min 355,000, Median 530,000, Max 701,000.

    The test data's price distribution suggests that the model is predicting within the expected range.

GWR:

-   Prediction: The GWR model gives predicted resale prices ranging from 359,170 to 616,562, with a median of 493,786. This is fairly close to the actual resale prices in the test data (from 355,000 to 701,000), indicating that GWR is also performing reasonably well for predictions.

-   Prediction Variance: The prediction variance ranges from 735,748,898 to 792,698,476, suggesting that the GWR model's predictions have some spread, but this is expected since GWR accounts for spatial variation.

2.  Model Calibration:

Random Forest:

-   Random Forest tends to perform well when capturing complex, non-linear relationships between predictors. It doesn't offer easy interpretability, but it can model interactions between variables without explicitly specifying them. Given that the RMSE is relatively low (42,038.73), the model seems to be doing well in predicting resale prices.

GWR:

-   Model Calibration Information: The coefficients for the GWR model vary spatially, and the values seem reasonable (e.g., for `floor_area_sqm`, the coefficient ranges from -3419.735 to 3037.7). These coefficients represent the local effect of each variable on resale prices.

-   Interpretability: One key strength of GWR is that it allows you to understand how each variable affects resale prices differently in various spatial locations (e.g., the effect of proximity to bus stops, hawker centers, or supermarkets varies across the region).

3.  Which Model is Better?

-   Prediction Accuracy: Both models give similar ranges for predicted resale prices, and both seem to predict the prices reasonably well. However, Random Forest generally provides a more reliable prediction with lower RMSE (42,038.73), indicating better overall performance in terms of predictive accuracy.

-   Interpretability: If you need to understand how each factor influences resale prices in different regions of Woodlands, GWR is a better choice because of its spatially varying coefficients and ability to show how variables interact with location.
:::

## Limitations

This study only looks into two the top two towns that has the most transactions and the transactions are slightly below 2 years. Hence, the sample size may be size which inevitably affecting the model. Thus, future proposal may look into top 5 - 10 towns over a span of 5 years. Additonally, only two predictive models were used. Future studies may consider other models such as Xboost in comparing across the models predictive abilities.

## Conclusion

To summarise, Random Forest is better in predicting HDB resale prices. However, geospatial details are heavily embedded in HDB resale data, we shouldn't discount the spatial aspects that will affect the resale prices too. If the study's purpose is to understand spatial patterns, GWR is more suited. Conversely, if accuracy is a priority, Random Forest would be a better choice.
