[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS626 Geospatial Analytics & Applications",
    "section": "",
    "text": "Hello! Welcome to my Page!\nThis website is crafted as part of a module’s requirement for the Master’s programme (Geospatial Analytics & Application) under the esteemed guidance of A/Prof KAM Tin Seong.\nMotivation in Undertaking Geospatial Analytics\nInspired by the pioneering work of John Snow in epidemiology, whose spatial mapping of cholera outbreaks in the 1850s revolutionized disease tracking, I aim to apply these principles to contemporary challenges, using spatial data to analyze patterns, uncover insights, and contribute to more informed decision-making in fields like public health and beyond, especially with the rise of Mpox.\n\n\n\n\n\n\n\n\nNaively Confident Indeed\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On 9\n\n\n\n\n\n\nJoshua TING\n\n\nOct 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTake Home Exercise 3\n\n\n\n\n\n\nJoshua TING\n\n\nOct 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIn-Class Exercise 8\n\n\n\n\n\n\nJoshua TING\n\n\nOct 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On Exercise 8\n\n\n\n\n\n\nJoshua TING\n\n\nOct 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIn-Class Exercise 7\n\n\n\n\n\n\nJoshua TING\n\n\nOct 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On Exercise 7\n\n\n\n\n\n\nJoshua TING\n\n\nOct 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTake-Home Exercise 2\n\n\n\n\n\n\nJoshua TING\n\n\nSep 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIn-Class Exercise 6\n\n\n\n\n\n\nJoshua TING\n\n\nSep 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On Exercise 5a\n\n\n\n\n\n\nJoshua TING\n\n\nSep 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On Exercise 5b\n\n\n\n\n\n\nJoshua TING\n\n\nSep 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIn-Class Exercise 4\n\n\n\n\n\n\nJoshua TING\n\n\nSep 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIn-Class Exercise 5\n\n\n\n\n\n\nJoshua TING\n\n\nSep 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On Exercise 3\n\n\n\n\n\n\nJoshua TING\n\n\nSep 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTake-Home Exercise 1\n\n\n\n\n\n\nJoshua TING\n\n\nSep 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On Exercise 6\n\n\n\n\n\n\nJoshua TING\n\n\nSep 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIn-Class Exercise 2\n\n\n\n\n\n\nJoshua TING\n\n\nSep 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On Exercise 2a\n\n\n\n\n\n\nJoshua TING\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On Exercise 2b\n\n\n\n\n\n\nJoshua TING\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIn Class Exercise 1\n\n\n\n\n\n\nJoshua TING\n\n\nAug 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands On Exercise 1a\n\n\n\n\n\n\nJoshua TING\n\n\nAug 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands On Exercise 1b\n\n\n\n\n\n\nJoshua TING\n\n\nAug 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On Exercise 4\n\n\n\n\n\n\nJoshua TING\n\n\nInvalid Date\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "GeoSpatial Analytics",
    "section": "",
    "text": "Hello there! Welcome to my portfolio! :)\nI’m  Joshua TING\nA Master of IT in Business (Analytics) student with Singapore Management University\nBefore pursuing my Master’s degree, I worked as a healthcare professional in a restructured hospital. Prior to that, I obtained a Bachelor of Science in Nursing from the Singapore Institute of Technology through their university Scholars’ Programme.\nConnect with Me:\n  \nBack to Home:"
  },
  {
    "objectID": "about.html#about-the-author.",
    "href": "about.html#about-the-author.",
    "title": "GeoSpatial Analytics",
    "section": "about the author.",
    "text": "about the author.\n\n\n\n\n\nHello there! Welcome to my portfolio! :)\nI’m  Joshua TING\nA Master of IT in Business (Analytics) student with Singapore Management University\nBefore pursuing my Master’s degree, I worked as a healthcare professional in a restructured hospital. Prior to that, I obtained a Bachelor of Science in Nursing from the Singapore Institute of Technology through their university Scholars’ Programme.\nConnect with Me:\n  \nBack to Home:"
  },
  {
    "objectID": "HandsOnExercise/HandsOn1/HandsOn1.html",
    "href": "HandsOnExercise/HandsOn1/HandsOn1.html",
    "title": "Hands On Exercise 1a",
    "section": "",
    "text": "Geospatial Data Science is a process of importing, wrangling, integrating, and processing geographically referenced data sets. In this hands-on exercise, you will learn how to perform geospatial data science tasks in R by using sf package.\nBy the end of this hands-on exercise, you should acquire the following competencies:\n\ninstalling and loading sf and tidyverse packages into R environment,\nimporting geospatial data by using appropriate functions of sf package,\nimporting aspatial data by using appropriate function of readr package,\nexploring the content of simple feature data frame by using appropriate Base R and sf functions,\nassigning or transforming coordinate systems by using using appropriate sf functions,\nconverting an aspatial data into a sf data frame by using appropriate function of sf package,\nperforming geoprocessing tasks by using appropriate functions of sf package,\nperforming data wrangling tasks by using appropriate functions of dplyr package and\nperforming Exploratory Data Analysis (EDA) by using appropriate functions from ggplot2 package.\n\n\nNote: Students are encouraged to read the reference guide of each function, especially the input data requirements, syntaxt and argument option before using them.\n\n\n\n\nData are key to data analytics including geospatial analytics. Hence, before analysing, we need to assemble the necessary data. In this hands-on exercise, you are required to extract the necessary data sets from the following sources:\n\nMaster Plan 2014 Subzone Boundary (Web) from data.gov.sg\nPre-Schools Location from data.gov.sg\nCycling Path from LTADataMall\nLatest version of Singapore Airbnb listing data from Inside Airbnb\n\n\nNote: The purpose of this section is not merely extracting the necessary data sets. It also aims to introduce you to public available data sets. Students are encouraged to explore the rest of the available data sets in these three data sources.\n\n\n\nNext, at the Hands-on_Ex01 folder, create a sub-folder called data. Then, inside the data sub-folder, create two sub-folders and name them geospatial and aspatial respectively.\nPlace Master Plan 2014 Subzone Boundary (Web), Pre-Schools Location and Cycling Path zipped files into geospatial sub-folder and unzipped them. Copy the unzipped files from their respective sub-folders and place them inside geospatial sub-folder.\n\n\n\nNow, you will extract the downloaded listing data file. At Downloads folder, cut and paste listing.csv into aspatial sub-folder.\n\n\n\n\nIn this hands-on exercise, two R packages will be used. They are:\n\nsf for importing, managing, and processing geospatial data, and\ntidyverse for performing data science tasks such as importing, wrangling and visualising data.\n\nTidyverse consists of a family of R packages. In this hands-on exercise, the following packages will be used:\n\nreadr for importing csv data,\nreadxl for importing Excel worksheet,\ntidyr for manipulating data,\ndplyr for transforming data, and\nggplot2 for visualising data\n\nCode Chunk\n\n\nCode Chunk\npacman::p_load(sf, tidyverse)\n\n\nWhat we can learn from the code chunk above:\n\np_load function pf pacman package is used to install and load sf and tidyverse pacages into R environment.\n\n\n\n\nIn this section, you will learn how to import the following geospatial data into R by using st_read() of sf package:\n\nMP14_SUBZONE_WEB_PL, a polygon feature layer in ESRI shapefile format,\nCyclingPath, a line feature layer in ESRI shapefile format, and\nPreSchool, a point feature layer in kml file format.\n\n\n\nThe code chunk below uses st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a polygon feature data frame. Note that when the input geospatial data is in shapefile format, two arguments will be used, namely: dsn to define the data path and layer to provide the shapefile name. Also note that no extension such as .shp, .dbf, .prj and .shx are needed.\n\n\nCode Chunk\nmpsz = st_read(dsn = \"data/geospatial\", \n                  layer = \"MP14_SUBZONE_WEB_PL\")\n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/HandsOnExercise/HandsOn1/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nThe message above reveals that the geospatial objects are multipolygon features. There are a total of 323 multipolygon features and 15 fields in mpsz simple feature data frame. mpsz is in svy21 projected coordinates systems. The bounding box provides the x extend and y extend of the data.\n\n\n\nThe code chunk below uses st_read() function of sf package to import CyclingPath shapefile into R as line feature data frame.\n\n\nCode Chunk\ncyclingpath = st_read(dsn = \"data/geospatial/CyclingPath_Jul2024\", \n                         layer = \"CyclingPathGazette\")\n\n\nReading layer `CyclingPathGazette' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/HandsOnExercise/HandsOn1/data/geospatial/CyclingPath_Jul2024' \n  using driver `ESRI Shapefile'\nSimple feature collection with 3138 features and 2 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 11854.32 ymin: 28347.98 xmax: 42644.17 ymax: 48948.15\nProjected CRS: SVY21\n\n\nThe message above reveals that there are a total of 2558 features and 2 fields in cyclingpath linestring feature data frame and it is in svy21 projected coordinates system too.\n\n\n\nThe PreSchoolsLocation is in kml format. The code chunk below will be used to import the kml into R. Notice that in the code chunk below, the complete path and the kml file extension were provided.\n\n\nCode Chunk\npreschool = st_read(\"data/geospatial/PreSchoolsLocation.kml\")\n\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/HandsOnExercise/HandsOn1/data/geospatial/PreSchoolsLocation.kml' \n  using driver `KML'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\nThe message above reveals that preschool is a point feature data frame. There are a total of 2290 features and 2 fields. Different from the previous two simple feature data frame, preschool is in wgs84 coordinates system.\n\n\n\n\nIn this sub-section, you will learn different ways to retrieve information related to the content of a simple feature data frame.\n\n\nThe column in the sf data.frame that contains the geometries is a list, of class sfc. We can retrieve the geometry list-column in this case by mpsz$geom or mpsz[[1]], but the more general way uses st_geometry() as shown in the code chunk below.\n\n\nCode Chunk\nst_geometry(mpsz)\n\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\nNotice that the print only displays basic information of the feature class such as type of geometry, the geographic extent of the features and the coordinate system of the data.\n\n\n\nBeside the basic feature information, we also would like to learn more about the associated attribute information in the data frame. This is the time you will find glimpse() of dplyr. very handy as shown in the code chunk below.\n\n\nCode Chunk\nglimpse(mpsz)\n\n\nRows: 323\nColumns: 16\n$ OBJECTID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO &lt;int&gt; 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  &lt;chr&gt; \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N &lt;chr&gt; \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C &lt;chr&gt; \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    &lt;chr&gt; \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D &lt;date&gt; 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     &lt;dbl&gt; 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     &lt;dbl&gt; 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng &lt;dbl&gt; 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area &lt;dbl&gt; 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\n\nglimpse() report reveals the data type of each fields. For example FMEL-UPD_D field is in date data type and X_ADDR, Y_ADDR, SHAPE_L and SHAPE_AREA fields are all in double-precision values.\n\n\n\nSometimes we would like to reveal complete information of a feature object, this is the job of head() of Base R\n\n\nCode Chunk\nhead(mpsz, n=5)\n\n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n\n\nNote: One of the useful argument of head() is it allows user to select the numbers of record to display (i.e. the n argument).\n\n\n\n\nIn geospatial data science, by looking at the feature information is not enough. We are also interested to visualise the geospatial features. This is the time you will find plot() of R Graphic comes in very handy as shown in the code chunk below.\n\n\nCode Chunk\nplot(mpsz)\n\n\n\n\n\n\n\n\n\nThe default plot of an sf object is a multi-plot of all attributes, up to a reasonable maximum as shown above. We can, however, choose to plot only the geometry by using the code chunk below.\n\n\nCode Chunk\nplot(st_geometry(mpsz))\n\n\n\n\n\n\n\n\n\nAlternatively, we can also choose the plot the sf object by using a specific attribute as shown in the code chunk below.\n\n\nCode Chunk\nplot(mpsz[\"PLN_AREA_N\"])\n\n\n\n\n\n\n\n\n\nNote: plot() is mean for plotting the geospatial object for quick look. For high cartographic quality plot, other R package such as tmap should be used.\n\n\n\nMap projection is an important property of a geospatial data. In order to perform geoprocessing using two geospatial data, we need to ensure that both geospatial data are projected using similar coordinate system.\nIn this section, you will learn how to project a simple feature data frame from one coordinate system to another coordinate system. The technical term of this process is called projection transformation.\n\n\nOne of the common issue that can happen during importing geospatial data into R is that the coordinate system of the source data was either missing (such as due to missing .proj for ESRI shapefile) or wrongly assigned during the importing process.\nThis is an example the coordinate system of mpsz simple feature data frame by using st_crs() of sf package as shown in the code chunk below.\n\n\nCode Chunk\nst_crs(mpsz)\n\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nAlthough mpsz data frame is projected in svy21 but when we read until the end of the print, it indicates that the EPSG is 9001. This is a wrong EPSG code because the correct EPSG code for svy21 should be 3414.\nIn order to assign the correct EPSG code to mpsz data frame, st_set_crs() of sf package is used as shown in the code chunk below.\n\n\nCode Chunk\nmpsz3414 &lt;- st_set_crs(mpsz, 3414)\n\n\nNow, let us check the CSR again by using the code chunk below.\n\n\nCode Chunk\nst_crs(mpsz3414)\n\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\n\n\nIn geospatial analytics, it is very common for us to transform the original data from geographic coordinate system to projected coordinate system. This is because geographic coordinate system is not appropriate if the analysis need to use distance or/and area measurements.\nLet us take preschool simple feature data frame as an example. The print below reveals that it is in wgs84 coordinate system.\nGeometry set for 2290 features \nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\nThis is a scenario that st_set_crs() is not appropriate and st_transform() of sf package should be used. This is because we need to reproject preschool from one coordinate system to another coordinate system mathemetically.\nLet us perform the projection transformation by using the code chunk below.\n\n\nCode Chunk\npreschool3414 &lt;- st_transform(preschool, \n                              crs = 3414)\n\n\n\nNote: In practice, we need find out the appropriate project coordinate system to use before performing the projection transformation.\n\nNext, let us display the content of preschool3414 sf data frame as shown below.\nGeometry set for 2290 features \nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 11810.03 ymin: 25596.33 xmax: 45404.24 ymax: 49300.88\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\nFirst 5 geometries:\nNotice that it is in svy21 projected coordinate system now. Furthermore, if you refer to Bounding box:, the values are greater than 0-360 range of decimal degree commonly used by most of the geographic coordinate systems.\n\n\n\n\nIn practice, it is not unusual that we will come across data such as listing of Inside Airbnb. We call this kind of data aspatial data. This is because it is not a geospatial data but among the data fields, there are two fields that capture the x- and y-coordinates of the data points.\nIn this section, you will learn how to import an aspatial data into R environment and save it as a tibble data frame. Next, you will convert it into a simple feature data frame.\nFor the purpose of this exercise, the listings.csv data downloaded from AirBnb will be used.\n\n\nSince listings data set is in csv file format, we will use read_csv() of readr package to import listing.csv as shown the code chunk below. The output R object is called listings and it is a tibble data frame.\n\n\nCode Chunk\nlistings &lt;- read_csv(\"data/geospatial/listings.csv\")\n\n\nAfter importing the data file into R, it is important for us to examine if the data file has been imported correctly.\nThe code chunk below shows list() of Base R instead of glimpse() is used to do the job.\n\n\nCode Chunk\nlist(listings)\n\n\n[[1]]\n# A tibble: 3,540 × 18\n       id name      host_id host_name neighbourhood_group neighbourhood latitude\n    &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;               &lt;chr&gt;            &lt;dbl&gt;\n 1  71609 Ensuite …  367042 Belinda   East Region         Tampines          1.35\n 2  71896 B&B  Roo…  367042 Belinda   East Region         Tampines          1.35\n 3  71903 Room 2-n…  367042 Belinda   East Region         Tampines          1.35\n 4 275343 10min wa… 1439258 Kay       Central Region      Bukit Merah       1.29\n 5 275344 15 mins … 1439258 Kay       Central Region      Bukit Merah       1.29\n 6 289234 Booking …  367042 Belinda   East Region         Tampines          1.34\n 7 294281 5 mins w… 1521514 Elizabeth Central Region      Newton            1.31\n 8 324945 Comforta… 1439258 Kay       Central Region      Bukit Merah       1.29\n 9 330095 Relaxing… 1439258 Kay       Central Region      Bukit Merah       1.29\n10 344803 Budget s…  367042 Belinda   East Region         Tampines          1.35\n# ℹ 3,530 more rows\n# ℹ 11 more variables: longitude &lt;dbl&gt;, room_type &lt;chr&gt;, price &lt;dbl&gt;,\n#   minimum_nights &lt;dbl&gt;, number_of_reviews &lt;dbl&gt;, last_review &lt;date&gt;,\n#   reviews_per_month &lt;dbl&gt;, calculated_host_listings_count &lt;dbl&gt;,\n#   availability_365 &lt;dbl&gt;, number_of_reviews_ltm &lt;dbl&gt;, license &lt;chr&gt;\n\n\nThe output reveals that listing tibble data frame consists of 4252 rows and 16 columns. Two useful fields we are going to use in the next phase are latitude and longitude. Note that they are in decimal degree format. As a best guess, we will assume that the data is in wgs84 Geographic Coordinate System.\n\n\n\nThe code chunk below converts listing data frame into a simple feature data frame by using st_as_sf() of sf packages\n\n\nCode Chunk\nlistings_sf &lt;- st_as_sf(listings,\n                        coords = c(\"longitude\", \"latitude\"),\n                        crs = 4326) %&gt;%\n  st_transform(crs=3414)\n\n\n\n\n\n\n\n\nLearning from Above Arguments\n\n\n\n\ncoords argument requires you to provide the column name of the x-coordinates first then followed by the column name of the y-coordinates.\ncrs argument requires you to provide the coordinates system in epsg format. EPSG: 4326 is wgs84 Geographic Coordinate System and EPSG: 3414 is Singapore SVY21 Projected Coordinate System. You can search for other country’s epsg code by referring to epsg.io.\n%&gt;% is used to nest st_transform() to transform the newly created simple feature data frame into svy21 projected coordinates system.\n\n\n\nLet us examine the content of this newly created simple feature data frame.\n\n\nCode Chunk\nglimpse(listings_sf)\n\n\nRows: 3,540\nColumns: 17\n$ id                             &lt;dbl&gt; 71609, 71896, 71903, 275343, 275344, 28…\n$ name                           &lt;chr&gt; \"Ensuite Room (Room 1 & 2) near EXPO\", …\n$ host_id                        &lt;dbl&gt; 367042, 367042, 367042, 1439258, 143925…\n$ host_name                      &lt;chr&gt; \"Belinda\", \"Belinda\", \"Belinda\", \"Kay\",…\n$ neighbourhood_group            &lt;chr&gt; \"East Region\", \"East Region\", \"East Reg…\n$ neighbourhood                  &lt;chr&gt; \"Tampines\", \"Tampines\", \"Tampines\", \"Bu…\n$ room_type                      &lt;chr&gt; \"Private room\", \"Private room\", \"Privat…\n$ price                          &lt;dbl&gt; NA, 80, 80, 50, 50, NA, 85, 65, 45, 54,…\n$ minimum_nights                 &lt;dbl&gt; 92, 92, 92, 180, 180, 92, 92, 180, 180,…\n$ number_of_reviews              &lt;dbl&gt; 19, 24, 46, 20, 16, 12, 131, 17, 5, 60,…\n$ last_review                    &lt;date&gt; 2020-01-17, 2019-10-13, 2020-01-09, 20…\n$ reviews_per_month              &lt;dbl&gt; 0.12, 0.15, 0.29, 0.15, 0.11, 0.08, 0.8…\n$ calculated_host_listings_count &lt;dbl&gt; 6, 6, 6, 49, 49, 6, 7, 49, 49, 6, 7, 7,…\n$ availability_365               &lt;dbl&gt; 89, 148, 90, 62, 0, 88, 365, 0, 0, 365,…\n$ number_of_reviews_ltm          &lt;dbl&gt; 0, 0, 0, 0, 2, 0, 0, 1, 1, 1, 0, 0, 0, …\n$ license                        &lt;chr&gt; NA, NA, NA, \"S0399\", \"S0399\", NA, NA, \"…\n$ geometry                       &lt;POINT [m]&gt; POINT (41972.5 36390.05), POINT (…\n\n\nTable above shows the content of listing_sf. Notice that a new column called geometry has been added into the data frame. On the other hand, the longitude and latitude columns have been dropped from the data frame.\n\n\n\n\nBesides providing functions to handling (i.e. importing, exporting, assigning projection, transforming projection etc) geospatial data, sf package also offers a wide range of geoprocessing (also known as GIS analysis) functions.\nIn this section, you will learn how to perform two commonly used geoprocessing functions, namely buffering and point in polygon count.\n\n\nThe scenario:\nThe authority is planning to upgrade the exiting cycling path. To do so, they need to acquire 5 metres of reserved land on the both sides of the current cycling path. You are tasked to determine the extend of the land need to be acquired and their total area.\nThe solution:\nFirstly, st_buffer() of sf package is used to compute the 5-meter buffers around cycling paths.\n\n\nCode Chunk\nbuffer_cycling &lt;- st_buffer(cyclingpath, \n                               dist=5, nQuadSegs = 30)\n\n\nSecondly, calculating the area of the buffers as shown in the code chunk below.\n\n\nCode Chunk\nbuffer_cycling$AREA &lt;- st_area(buffer_cycling)\n\n\nLastly, sum() of Base R will be used to derive the total land involved\n\n\nCode Chunk\nsum(buffer_cycling$AREA)\n\n\n2218855 [m^2]\n\n\n\n\n\nThe scenario:\nA pre-school service group want to find out the numbers of pre-schools in each Planning Subzone.\nThe solution:\nThe code chunk below performs two operations at one go. Firstly, identify pre-schools located inside each Planning Subzone by using st_intersects(). Next, length() of Base R is used to calculate numbers of pre-schools that fall inside each planning subzone.\n\n\nCode Chunk\nmpsz3414$`PreSch Count`&lt;- lengths(st_intersects(mpsz3414, preschool3414))\n\n\nWarning: You should not confuse with st_intersection().\nYou can check the summary statistics of the newly derived PreSch Count field by using summary() as shown in the code chunk below.\n\n\nCode Chunk\nsummary(mpsz3414$`PreSch Count`)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    4.00    7.09   10.00   72.00 \n\n\nTo list the planning subzone with the most number of pre-school, the top_n() of dplyr package is used as shown in the code chunk below.\n\n\nCode Chunk\ntop_n(mpsz3414, 1, `PreSch Count`)\n\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 39655.33 ymin: 35966 xmax: 42940.57 ymax: 38622.37\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO     SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      189          2 TAMPINES EAST    TMSZ02      N   TAMPINES         TM\n     REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR SHAPE_Leng\n1 EAST REGION       ER 21658EAAF84F4D8D 2014-12-05 41122.55 37392.39   10180.62\n  SHAPE_Area                       geometry PreSch Count\n1    4339824 MULTIPOLYGON (((42196.76 38...           72\n\n\nCalculating density of pre-school by planning subzone\nFirstly, the code chunk below uses st_area() of sf package to derive the area of each planning subzone.\n\n\nCode Chunk\nmpsz3414$Area &lt;- mpsz3414 %&gt;%\n  st_area()\n\n\nNext, mutate() of dplyr package is used to compute the density by using the code chunk below.\n\n\nCode Chunk\nmpsz3414 &lt;- mpsz3414 %&gt;%\n  mutate(`PreSch Density` = `PreSch Count`/Area * 1000000)\n\n\n\n\n\n\nIn practice, many geospatial analytics start with Exploratory Data Analysis. In this section, you will learn how to use appropriate ggplot2 functions to create functional and yet truthful statistical graphs for EDA purposes.\nFirstly, we will plot a histogram to reveal the distribution of PreSch Density. Conventionally, hist() of R Graphics will be used as shown in the code chunk below.\n\n\nCode Chunk\nhist(mpsz3414$`PreSch Density`)\n\n\n\n\n\n\n\n\n\nAlthough the syntax is very easy to use however the output is far from meeting publication quality. Furthermore, the function has limited room for further customisation.\nIn the code chunk below, appropriate ggplot2 functions will be used.\n\n\nCode Chunk\nggplot(data=mpsz3414, \n       aes(x= as.numeric(`PreSch Density`)))+\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  labs(title = \"Are pre-school even distributed in Singapore?\",\n       subtitle= \"There are many planning sub-zones with a single pre-school, on the other hand, \\nthere are two planning sub-zones with at least 20 pre-schools\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Frequency\")\n\n\n\n\n\n\n\n\n\nUsing ggplot2 method, plot a scatterplot showing the relationship between Pre-school Density and Pre-school Count.\n\n\nCode Chunk\nggplot(data=mpsz3414, \n       aes(y = `PreSch Count`, \n           x= as.numeric(`PreSch Density`)))+\n  geom_point(color=\"black\", \n             fill=\"light blue\") +\n  xlim(0, 40) +\n  ylim(0, 40) +\n  labs(title = \"\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Pre-school count\")"
  },
  {
    "objectID": "HandsOnExercise/HandsOn1/HandsOn1.html#learning-outcome",
    "href": "HandsOnExercise/HandsOn1/HandsOn1.html#learning-outcome",
    "title": "Hands On Exercise 1a",
    "section": "",
    "text": "Geospatial Data Science is a process of importing, wrangling, integrating, and processing geographically referenced data sets. In this hands-on exercise, you will learn how to perform geospatial data science tasks in R by using sf package.\nBy the end of this hands-on exercise, you should acquire the following competencies:\n\ninstalling and loading sf and tidyverse packages into R environment,\nimporting geospatial data by using appropriate functions of sf package,\nimporting aspatial data by using appropriate function of readr package,\nexploring the content of simple feature data frame by using appropriate Base R and sf functions,\nassigning or transforming coordinate systems by using using appropriate sf functions,\nconverting an aspatial data into a sf data frame by using appropriate function of sf package,\nperforming geoprocessing tasks by using appropriate functions of sf package,\nperforming data wrangling tasks by using appropriate functions of dplyr package and\nperforming Exploratory Data Analysis (EDA) by using appropriate functions from ggplot2 package.\n\n\nNote: Students are encouraged to read the reference guide of each function, especially the input data requirements, syntaxt and argument option before using them."
  },
  {
    "objectID": "HandsOnExercise/HandsOn1/HandsOn1.html#data-acquisition",
    "href": "HandsOnExercise/HandsOn1/HandsOn1.html#data-acquisition",
    "title": "Hands On Exercise 1a",
    "section": "",
    "text": "Data are key to data analytics including geospatial analytics. Hence, before analysing, we need to assemble the necessary data. In this hands-on exercise, you are required to extract the necessary data sets from the following sources:\n\nMaster Plan 2014 Subzone Boundary (Web) from data.gov.sg\nPre-Schools Location from data.gov.sg\nCycling Path from LTADataMall\nLatest version of Singapore Airbnb listing data from Inside Airbnb\n\n\nNote: The purpose of this section is not merely extracting the necessary data sets. It also aims to introduce you to public available data sets. Students are encouraged to explore the rest of the available data sets in these three data sources.\n\n\n\nNext, at the Hands-on_Ex01 folder, create a sub-folder called data. Then, inside the data sub-folder, create two sub-folders and name them geospatial and aspatial respectively.\nPlace Master Plan 2014 Subzone Boundary (Web), Pre-Schools Location and Cycling Path zipped files into geospatial sub-folder and unzipped them. Copy the unzipped files from their respective sub-folders and place them inside geospatial sub-folder.\n\n\n\nNow, you will extract the downloaded listing data file. At Downloads folder, cut and paste listing.csv into aspatial sub-folder."
  },
  {
    "objectID": "HandsOnExercise/HandsOn1/HandsOn1.html#getting-started",
    "href": "HandsOnExercise/HandsOn1/HandsOn1.html#getting-started",
    "title": "Hands On Exercise 1a",
    "section": "",
    "text": "In this hands-on exercise, two R packages will be used. They are:\n\nsf for importing, managing, and processing geospatial data, and\ntidyverse for performing data science tasks such as importing, wrangling and visualising data.\n\nTidyverse consists of a family of R packages. In this hands-on exercise, the following packages will be used:\n\nreadr for importing csv data,\nreadxl for importing Excel worksheet,\ntidyr for manipulating data,\ndplyr for transforming data, and\nggplot2 for visualising data\n\nCode Chunk\n\n\nCode Chunk\npacman::p_load(sf, tidyverse)\n\n\nWhat we can learn from the code chunk above:\n\np_load function pf pacman package is used to install and load sf and tidyverse pacages into R environment."
  },
  {
    "objectID": "HandsOnExercise/HandsOn1/HandsOn1.html#importing-geospatial-data",
    "href": "HandsOnExercise/HandsOn1/HandsOn1.html#importing-geospatial-data",
    "title": "Hands On Exercise 1a",
    "section": "",
    "text": "In this section, you will learn how to import the following geospatial data into R by using st_read() of sf package:\n\nMP14_SUBZONE_WEB_PL, a polygon feature layer in ESRI shapefile format,\nCyclingPath, a line feature layer in ESRI shapefile format, and\nPreSchool, a point feature layer in kml file format.\n\n\n\nThe code chunk below uses st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a polygon feature data frame. Note that when the input geospatial data is in shapefile format, two arguments will be used, namely: dsn to define the data path and layer to provide the shapefile name. Also note that no extension such as .shp, .dbf, .prj and .shx are needed.\n\n\nCode Chunk\nmpsz = st_read(dsn = \"data/geospatial\", \n                  layer = \"MP14_SUBZONE_WEB_PL\")\n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/HandsOnExercise/HandsOn1/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nThe message above reveals that the geospatial objects are multipolygon features. There are a total of 323 multipolygon features and 15 fields in mpsz simple feature data frame. mpsz is in svy21 projected coordinates systems. The bounding box provides the x extend and y extend of the data.\n\n\n\nThe code chunk below uses st_read() function of sf package to import CyclingPath shapefile into R as line feature data frame.\n\n\nCode Chunk\ncyclingpath = st_read(dsn = \"data/geospatial/CyclingPath_Jul2024\", \n                         layer = \"CyclingPathGazette\")\n\n\nReading layer `CyclingPathGazette' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/HandsOnExercise/HandsOn1/data/geospatial/CyclingPath_Jul2024' \n  using driver `ESRI Shapefile'\nSimple feature collection with 3138 features and 2 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 11854.32 ymin: 28347.98 xmax: 42644.17 ymax: 48948.15\nProjected CRS: SVY21\n\n\nThe message above reveals that there are a total of 2558 features and 2 fields in cyclingpath linestring feature data frame and it is in svy21 projected coordinates system too.\n\n\n\nThe PreSchoolsLocation is in kml format. The code chunk below will be used to import the kml into R. Notice that in the code chunk below, the complete path and the kml file extension were provided.\n\n\nCode Chunk\npreschool = st_read(\"data/geospatial/PreSchoolsLocation.kml\")\n\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/HandsOnExercise/HandsOn1/data/geospatial/PreSchoolsLocation.kml' \n  using driver `KML'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\nThe message above reveals that preschool is a point feature data frame. There are a total of 2290 features and 2 fields. Different from the previous two simple feature data frame, preschool is in wgs84 coordinates system."
  },
  {
    "objectID": "HandsOnExercise/HandsOn1/HandsOn1.html#checking-the-content-of-a-simple-feature-data-frame",
    "href": "HandsOnExercise/HandsOn1/HandsOn1.html#checking-the-content-of-a-simple-feature-data-frame",
    "title": "Hands On Exercise 1a",
    "section": "",
    "text": "In this sub-section, you will learn different ways to retrieve information related to the content of a simple feature data frame.\n\n\nThe column in the sf data.frame that contains the geometries is a list, of class sfc. We can retrieve the geometry list-column in this case by mpsz$geom or mpsz[[1]], but the more general way uses st_geometry() as shown in the code chunk below.\n\n\nCode Chunk\nst_geometry(mpsz)\n\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\nNotice that the print only displays basic information of the feature class such as type of geometry, the geographic extent of the features and the coordinate system of the data.\n\n\n\nBeside the basic feature information, we also would like to learn more about the associated attribute information in the data frame. This is the time you will find glimpse() of dplyr. very handy as shown in the code chunk below.\n\n\nCode Chunk\nglimpse(mpsz)\n\n\nRows: 323\nColumns: 16\n$ OBJECTID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO &lt;int&gt; 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  &lt;chr&gt; \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N &lt;chr&gt; \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C &lt;chr&gt; \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    &lt;chr&gt; \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D &lt;date&gt; 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     &lt;dbl&gt; 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     &lt;dbl&gt; 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng &lt;dbl&gt; 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area &lt;dbl&gt; 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\n\nglimpse() report reveals the data type of each fields. For example FMEL-UPD_D field is in date data type and X_ADDR, Y_ADDR, SHAPE_L and SHAPE_AREA fields are all in double-precision values.\n\n\n\nSometimes we would like to reveal complete information of a feature object, this is the job of head() of Base R\n\n\nCode Chunk\nhead(mpsz, n=5)\n\n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n\n\nNote: One of the useful argument of head() is it allows user to select the numbers of record to display (i.e. the n argument)."
  },
  {
    "objectID": "HandsOnExercise/HandsOn1/HandsOn1.html#plotting-the-geospatial-data",
    "href": "HandsOnExercise/HandsOn1/HandsOn1.html#plotting-the-geospatial-data",
    "title": "Hands On Exercise 1a",
    "section": "",
    "text": "In geospatial data science, by looking at the feature information is not enough. We are also interested to visualise the geospatial features. This is the time you will find plot() of R Graphic comes in very handy as shown in the code chunk below.\n\n\nCode Chunk\nplot(mpsz)\n\n\n\n\n\n\n\n\n\nThe default plot of an sf object is a multi-plot of all attributes, up to a reasonable maximum as shown above. We can, however, choose to plot only the geometry by using the code chunk below.\n\n\nCode Chunk\nplot(st_geometry(mpsz))\n\n\n\n\n\n\n\n\n\nAlternatively, we can also choose the plot the sf object by using a specific attribute as shown in the code chunk below.\n\n\nCode Chunk\nplot(mpsz[\"PLN_AREA_N\"])\n\n\n\n\n\n\n\n\n\nNote: plot() is mean for plotting the geospatial object for quick look. For high cartographic quality plot, other R package such as tmap should be used."
  },
  {
    "objectID": "HandsOnExercise/HandsOn1/HandsOn1.html#working-with-projection",
    "href": "HandsOnExercise/HandsOn1/HandsOn1.html#working-with-projection",
    "title": "Hands On Exercise 1a",
    "section": "",
    "text": "Map projection is an important property of a geospatial data. In order to perform geoprocessing using two geospatial data, we need to ensure that both geospatial data are projected using similar coordinate system.\nIn this section, you will learn how to project a simple feature data frame from one coordinate system to another coordinate system. The technical term of this process is called projection transformation.\n\n\nOne of the common issue that can happen during importing geospatial data into R is that the coordinate system of the source data was either missing (such as due to missing .proj for ESRI shapefile) or wrongly assigned during the importing process.\nThis is an example the coordinate system of mpsz simple feature data frame by using st_crs() of sf package as shown in the code chunk below.\n\n\nCode Chunk\nst_crs(mpsz)\n\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nAlthough mpsz data frame is projected in svy21 but when we read until the end of the print, it indicates that the EPSG is 9001. This is a wrong EPSG code because the correct EPSG code for svy21 should be 3414.\nIn order to assign the correct EPSG code to mpsz data frame, st_set_crs() of sf package is used as shown in the code chunk below.\n\n\nCode Chunk\nmpsz3414 &lt;- st_set_crs(mpsz, 3414)\n\n\nNow, let us check the CSR again by using the code chunk below.\n\n\nCode Chunk\nst_crs(mpsz3414)\n\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\n\n\nIn geospatial analytics, it is very common for us to transform the original data from geographic coordinate system to projected coordinate system. This is because geographic coordinate system is not appropriate if the analysis need to use distance or/and area measurements.\nLet us take preschool simple feature data frame as an example. The print below reveals that it is in wgs84 coordinate system.\nGeometry set for 2290 features \nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\nThis is a scenario that st_set_crs() is not appropriate and st_transform() of sf package should be used. This is because we need to reproject preschool from one coordinate system to another coordinate system mathemetically.\nLet us perform the projection transformation by using the code chunk below.\n\n\nCode Chunk\npreschool3414 &lt;- st_transform(preschool, \n                              crs = 3414)\n\n\n\nNote: In practice, we need find out the appropriate project coordinate system to use before performing the projection transformation.\n\nNext, let us display the content of preschool3414 sf data frame as shown below.\nGeometry set for 2290 features \nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 11810.03 ymin: 25596.33 xmax: 45404.24 ymax: 49300.88\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\nFirst 5 geometries:\nNotice that it is in svy21 projected coordinate system now. Furthermore, if you refer to Bounding box:, the values are greater than 0-360 range of decimal degree commonly used by most of the geographic coordinate systems."
  },
  {
    "objectID": "HandsOnExercise/HandsOn1/HandsOn1.html#importing-and-converting-an-aspatial-data",
    "href": "HandsOnExercise/HandsOn1/HandsOn1.html#importing-and-converting-an-aspatial-data",
    "title": "Hands On Exercise 1a",
    "section": "",
    "text": "In practice, it is not unusual that we will come across data such as listing of Inside Airbnb. We call this kind of data aspatial data. This is because it is not a geospatial data but among the data fields, there are two fields that capture the x- and y-coordinates of the data points.\nIn this section, you will learn how to import an aspatial data into R environment and save it as a tibble data frame. Next, you will convert it into a simple feature data frame.\nFor the purpose of this exercise, the listings.csv data downloaded from AirBnb will be used.\n\n\nSince listings data set is in csv file format, we will use read_csv() of readr package to import listing.csv as shown the code chunk below. The output R object is called listings and it is a tibble data frame.\n\n\nCode Chunk\nlistings &lt;- read_csv(\"data/geospatial/listings.csv\")\n\n\nAfter importing the data file into R, it is important for us to examine if the data file has been imported correctly.\nThe code chunk below shows list() of Base R instead of glimpse() is used to do the job.\n\n\nCode Chunk\nlist(listings)\n\n\n[[1]]\n# A tibble: 3,540 × 18\n       id name      host_id host_name neighbourhood_group neighbourhood latitude\n    &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;               &lt;chr&gt;            &lt;dbl&gt;\n 1  71609 Ensuite …  367042 Belinda   East Region         Tampines          1.35\n 2  71896 B&B  Roo…  367042 Belinda   East Region         Tampines          1.35\n 3  71903 Room 2-n…  367042 Belinda   East Region         Tampines          1.35\n 4 275343 10min wa… 1439258 Kay       Central Region      Bukit Merah       1.29\n 5 275344 15 mins … 1439258 Kay       Central Region      Bukit Merah       1.29\n 6 289234 Booking …  367042 Belinda   East Region         Tampines          1.34\n 7 294281 5 mins w… 1521514 Elizabeth Central Region      Newton            1.31\n 8 324945 Comforta… 1439258 Kay       Central Region      Bukit Merah       1.29\n 9 330095 Relaxing… 1439258 Kay       Central Region      Bukit Merah       1.29\n10 344803 Budget s…  367042 Belinda   East Region         Tampines          1.35\n# ℹ 3,530 more rows\n# ℹ 11 more variables: longitude &lt;dbl&gt;, room_type &lt;chr&gt;, price &lt;dbl&gt;,\n#   minimum_nights &lt;dbl&gt;, number_of_reviews &lt;dbl&gt;, last_review &lt;date&gt;,\n#   reviews_per_month &lt;dbl&gt;, calculated_host_listings_count &lt;dbl&gt;,\n#   availability_365 &lt;dbl&gt;, number_of_reviews_ltm &lt;dbl&gt;, license &lt;chr&gt;\n\n\nThe output reveals that listing tibble data frame consists of 4252 rows and 16 columns. Two useful fields we are going to use in the next phase are latitude and longitude. Note that they are in decimal degree format. As a best guess, we will assume that the data is in wgs84 Geographic Coordinate System.\n\n\n\nThe code chunk below converts listing data frame into a simple feature data frame by using st_as_sf() of sf packages\n\n\nCode Chunk\nlistings_sf &lt;- st_as_sf(listings,\n                        coords = c(\"longitude\", \"latitude\"),\n                        crs = 4326) %&gt;%\n  st_transform(crs=3414)\n\n\n\n\n\n\n\n\nLearning from Above Arguments\n\n\n\n\ncoords argument requires you to provide the column name of the x-coordinates first then followed by the column name of the y-coordinates.\ncrs argument requires you to provide the coordinates system in epsg format. EPSG: 4326 is wgs84 Geographic Coordinate System and EPSG: 3414 is Singapore SVY21 Projected Coordinate System. You can search for other country’s epsg code by referring to epsg.io.\n%&gt;% is used to nest st_transform() to transform the newly created simple feature data frame into svy21 projected coordinates system.\n\n\n\nLet us examine the content of this newly created simple feature data frame.\n\n\nCode Chunk\nglimpse(listings_sf)\n\n\nRows: 3,540\nColumns: 17\n$ id                             &lt;dbl&gt; 71609, 71896, 71903, 275343, 275344, 28…\n$ name                           &lt;chr&gt; \"Ensuite Room (Room 1 & 2) near EXPO\", …\n$ host_id                        &lt;dbl&gt; 367042, 367042, 367042, 1439258, 143925…\n$ host_name                      &lt;chr&gt; \"Belinda\", \"Belinda\", \"Belinda\", \"Kay\",…\n$ neighbourhood_group            &lt;chr&gt; \"East Region\", \"East Region\", \"East Reg…\n$ neighbourhood                  &lt;chr&gt; \"Tampines\", \"Tampines\", \"Tampines\", \"Bu…\n$ room_type                      &lt;chr&gt; \"Private room\", \"Private room\", \"Privat…\n$ price                          &lt;dbl&gt; NA, 80, 80, 50, 50, NA, 85, 65, 45, 54,…\n$ minimum_nights                 &lt;dbl&gt; 92, 92, 92, 180, 180, 92, 92, 180, 180,…\n$ number_of_reviews              &lt;dbl&gt; 19, 24, 46, 20, 16, 12, 131, 17, 5, 60,…\n$ last_review                    &lt;date&gt; 2020-01-17, 2019-10-13, 2020-01-09, 20…\n$ reviews_per_month              &lt;dbl&gt; 0.12, 0.15, 0.29, 0.15, 0.11, 0.08, 0.8…\n$ calculated_host_listings_count &lt;dbl&gt; 6, 6, 6, 49, 49, 6, 7, 49, 49, 6, 7, 7,…\n$ availability_365               &lt;dbl&gt; 89, 148, 90, 62, 0, 88, 365, 0, 0, 365,…\n$ number_of_reviews_ltm          &lt;dbl&gt; 0, 0, 0, 0, 2, 0, 0, 1, 1, 1, 0, 0, 0, …\n$ license                        &lt;chr&gt; NA, NA, NA, \"S0399\", \"S0399\", NA, NA, \"…\n$ geometry                       &lt;POINT [m]&gt; POINT (41972.5 36390.05), POINT (…\n\n\nTable above shows the content of listing_sf. Notice that a new column called geometry has been added into the data frame. On the other hand, the longitude and latitude columns have been dropped from the data frame."
  },
  {
    "objectID": "HandsOnExercise/HandsOn1/HandsOn1.html#geoprocessing-with-sf-package",
    "href": "HandsOnExercise/HandsOn1/HandsOn1.html#geoprocessing-with-sf-package",
    "title": "Hands On Exercise 1a",
    "section": "",
    "text": "Besides providing functions to handling (i.e. importing, exporting, assigning projection, transforming projection etc) geospatial data, sf package also offers a wide range of geoprocessing (also known as GIS analysis) functions.\nIn this section, you will learn how to perform two commonly used geoprocessing functions, namely buffering and point in polygon count.\n\n\nThe scenario:\nThe authority is planning to upgrade the exiting cycling path. To do so, they need to acquire 5 metres of reserved land on the both sides of the current cycling path. You are tasked to determine the extend of the land need to be acquired and their total area.\nThe solution:\nFirstly, st_buffer() of sf package is used to compute the 5-meter buffers around cycling paths.\n\n\nCode Chunk\nbuffer_cycling &lt;- st_buffer(cyclingpath, \n                               dist=5, nQuadSegs = 30)\n\n\nSecondly, calculating the area of the buffers as shown in the code chunk below.\n\n\nCode Chunk\nbuffer_cycling$AREA &lt;- st_area(buffer_cycling)\n\n\nLastly, sum() of Base R will be used to derive the total land involved\n\n\nCode Chunk\nsum(buffer_cycling$AREA)\n\n\n2218855 [m^2]\n\n\n\n\n\nThe scenario:\nA pre-school service group want to find out the numbers of pre-schools in each Planning Subzone.\nThe solution:\nThe code chunk below performs two operations at one go. Firstly, identify pre-schools located inside each Planning Subzone by using st_intersects(). Next, length() of Base R is used to calculate numbers of pre-schools that fall inside each planning subzone.\n\n\nCode Chunk\nmpsz3414$`PreSch Count`&lt;- lengths(st_intersects(mpsz3414, preschool3414))\n\n\nWarning: You should not confuse with st_intersection().\nYou can check the summary statistics of the newly derived PreSch Count field by using summary() as shown in the code chunk below.\n\n\nCode Chunk\nsummary(mpsz3414$`PreSch Count`)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    4.00    7.09   10.00   72.00 \n\n\nTo list the planning subzone with the most number of pre-school, the top_n() of dplyr package is used as shown in the code chunk below.\n\n\nCode Chunk\ntop_n(mpsz3414, 1, `PreSch Count`)\n\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 39655.33 ymin: 35966 xmax: 42940.57 ymax: 38622.37\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO     SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      189          2 TAMPINES EAST    TMSZ02      N   TAMPINES         TM\n     REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR SHAPE_Leng\n1 EAST REGION       ER 21658EAAF84F4D8D 2014-12-05 41122.55 37392.39   10180.62\n  SHAPE_Area                       geometry PreSch Count\n1    4339824 MULTIPOLYGON (((42196.76 38...           72\n\n\nCalculating density of pre-school by planning subzone\nFirstly, the code chunk below uses st_area() of sf package to derive the area of each planning subzone.\n\n\nCode Chunk\nmpsz3414$Area &lt;- mpsz3414 %&gt;%\n  st_area()\n\n\nNext, mutate() of dplyr package is used to compute the density by using the code chunk below.\n\n\nCode Chunk\nmpsz3414 &lt;- mpsz3414 %&gt;%\n  mutate(`PreSch Density` = `PreSch Count`/Area * 1000000)"
  },
  {
    "objectID": "HandsOnExercise/HandsOn1/HandsOn1.html#exploratory-data-analysis-eda",
    "href": "HandsOnExercise/HandsOn1/HandsOn1.html#exploratory-data-analysis-eda",
    "title": "Hands On Exercise 1a",
    "section": "",
    "text": "In practice, many geospatial analytics start with Exploratory Data Analysis. In this section, you will learn how to use appropriate ggplot2 functions to create functional and yet truthful statistical graphs for EDA purposes.\nFirstly, we will plot a histogram to reveal the distribution of PreSch Density. Conventionally, hist() of R Graphics will be used as shown in the code chunk below.\n\n\nCode Chunk\nhist(mpsz3414$`PreSch Density`)\n\n\n\n\n\n\n\n\n\nAlthough the syntax is very easy to use however the output is far from meeting publication quality. Furthermore, the function has limited room for further customisation.\nIn the code chunk below, appropriate ggplot2 functions will be used.\n\n\nCode Chunk\nggplot(data=mpsz3414, \n       aes(x= as.numeric(`PreSch Density`)))+\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  labs(title = \"Are pre-school even distributed in Singapore?\",\n       subtitle= \"There are many planning sub-zones with a single pre-school, on the other hand, \\nthere are two planning sub-zones with at least 20 pre-schools\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Frequency\")\n\n\n\n\n\n\n\n\n\nUsing ggplot2 method, plot a scatterplot showing the relationship between Pre-school Density and Pre-school Count.\n\n\nCode Chunk\nggplot(data=mpsz3414, \n       aes(y = `PreSch Count`, \n           x= as.numeric(`PreSch Density`)))+\n  geom_point(color=\"black\", \n             fill=\"light blue\") +\n  xlim(0, 40) +\n  ylim(0, 40) +\n  labs(title = \"\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Pre-school count\")"
  },
  {
    "objectID": "HandsOnExercise/HandsOn1b/HandsOn1b.html",
    "href": "HandsOnExercise/HandsOn1b/HandsOn1b.html",
    "title": "Hands On Exercise 1b",
    "section": "",
    "text": "In general, thematic mapping involves the use of map symbols to visualize selected properties of geographic features that are not naturally visible, such as population, temperature, crime rate, and property prices, just to mention a few of them.\nGeovisualisation, on the other hand, works by providing graphical ideation to render a place, a phenomenon or a process visible, enabling human’s most powerful information-processing abilities – those of spatial cognition associated with our eye–brain vision system – to be directly brought to bear.\nIn this chapter, you will learn how to plot functional and truthful choropleth maps by using an R package called **tmap** package.\n\n\n\nIn this hands-on exercise, the key R package use is tmap package in R. Beside tmap package, four other R packages will be used. They are:\n\nreadr for importing delimited text file,\ntidyr for tidying data,\ndplyr for wrangling data and\nsf for handling geospatial data.\n\nAmong the four packages, readr, tidyr and dplyr are part of tidyverse package.\nThe code chunk below will be used to install and load these packages in RStudio.\n\n\nCode Chunk\npacman::p_load(sf, tidyverse, tmap)\n\n\n\n\n\n\n\nTwo data set will be used to create the choropleth map. They are:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\n\n\n\nThe code chunk below uses the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz.\n\n\nCode Chunk\nmpsz &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/HandsOnExercise/HandsOn1b/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nExamining the content of mpsz by using the code chunk below:\n\n\nCode Chunk\nmpsz\n\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\nNotice that only the first ten records will be displayed.\n\n\n\nNext, we will import respopagsex2011to2020.csv file into RStudio and save the file into an R dataframe called popdata.\nThe task will be performed by using read_csv() function of readr package as shown in the code chunk below.\n\n\nCode Chunk\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020/respopagesextod2011to2020.csv\")\n\n\n\n\n\nBefore a thematic map can be prepared, you are required to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\n\nThe following data wrangling and transformation functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\n\nCode Chunk\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup()%&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\n\n\n\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\n\n\nCode Chunk\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = list(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\n\nCode Chunk\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\n\nThing to learn from the code chunk above:\n\nleft_join() of dplyr package is used with mpsz simple feature data frame as the left data table is to ensure that the output will be a simple features data frame.\n\n\n\nCode Chunk\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")\n\n\n\n\n\n\n\nChoropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\nTwo approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\n\n\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\nThe code chunk below will draw a cartographic standard choropleth map as shown below.\nThings to learn from the code chunk above:\n\ntmap_mode() with “plot” option is used to produce a static map. For interactive mode, “view” option should be used.\nfill argument is used to map the attribute (i.e. DEPENDENCY)\n\n\n\nCode Chunk\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\n\n\n\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes aesthetics of individual layers harder to control. To draw a high quality cartographic choropleth map as shown in the figure below, tmap’s drawing elements should be used.\n\n\nCode Chunk\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\nIn the following sub-section, we will share with you tmap functions that used to plot these elements.\n\n\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\nIn the code chunk below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons\n\n\nCode Chunk\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n\n\n\n\n\n\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\n\nCode Chunk\ntm_shape(mpsz_pop2020) +\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\nThings to learn from tm_polygons():\n\nThe default interval binning used to draw the choropleth map is called “pretty”. A detailed discussion of the data classification methods supported by tmap will be provided in sub-section 4.3.\nThe default colour scheme used is YlOrRd of ColorBrewer. You will learn more about the color scheme in sub-section 4.4.\nBy default, Missing value will be shaded in grey.\n\n\n\n\nActually, tm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nThe code chunk below draws a choropleth map by using tm_fill() alone.\n\n\nCode Chunk\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\nNotice that the planning subzones are shared according to the respective dependecy values\nTo add the boundary of the planning subzones, tm_borders will be used as shown in the code chunk below.\n\n\nCode Chunk\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\n\n\n\n\nNotice that the planning subzones are shared according to the respective dependecy values\nTo add the boundary of the planning subzones, tm_borders will be used as shown in the code chunk below.\n\n\nCode Chunk\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\n\n\n\n\nNotice that light-gray border lines have been added on the choropleth map.\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the col is used (normally 1).\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is “solid”.\n\n\n\n\n\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\n\nThe code chunk below shows a quantile data classification that used 5 classes.\n\n\nCode Chunk\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nIn the code chunk below, equal data classification method is used.\n\n\nCode Chunk\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nNotice that the distribution of quantile data classification method are more evenly distributed then equal data classification method.\n\n\n\n\n\n\nMaps Lie!\n\n\n\n\n\n\n\n\n\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points. Code chunk below will be used to compute and display the descriptive statistics of DEPENDENCY field.\n\n\nCode Chunk\nsummary(mpsz_pop2020$DEPENDENCY)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00)\nNow, we will plot the choropleth map by using the code chunk below.\n\n\nCode Chunk\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\n\n\nTo change the colour, we assign the preferred colour to palette argument of tm_fill() as shown in the code chunk below.\n\n\nCode Chunk\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nNotice that the choropleth map is shaded in green.\nTo reverse the colour shading, add a “-” prefix.\n\n\nCode Chunk\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\nMap layout refers to the combination of all map elements into a cohensive map. Map elements include among others the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios. Colour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\n\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\n\nCode Chunk\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\ntmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style().\nThe code chunk below shows the classic style is used.\n\n\nCode Chunk\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\n\n\n\n\n\n\nBeside map style, tmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\n\nCode Chunk\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\nReturning to default style. code chunk:\n\n\nCode Chunk\ntmap_style(\"white\")\n\n\n\n\n\n\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\n\nIn this example, small multiple choropleth maps are created by defining ncols in tm_fill()\n\n\nCode Chunk\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\n\n\n\n\n\nIn this example, small multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments.\n\n\nCode Chunk\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\n\n\nIn this example, multiple small choropleth maps are created by using tm_facets().\n\n\nCode Chunk\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\nIn this example, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\n\nCode Chunk\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstead of creating small multiple choropleth map, you can also use selection funtion to map spatial objects meeting the selection criterion.\n\n\nCode Chunk\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntmap: Thematic Maps in R\ntmap\ntmap: get started!\ntmap: changes in version 2.0\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)\n\n\n\n\n\nsf: Simple Features for R\nSimple Features for R: StandardizedSupport for Spatial Vector Data\nReading, Writing and Converting Simple Features\n\n\n\n\n\ndplyr\nTidy data\ntidyr: Easily Tidy Data with ‘spread()’ and ‘gather()’ Functions"
  },
  {
    "objectID": "HandsOnExercise/HandsOn1b/HandsOn1b.html#overview",
    "href": "HandsOnExercise/HandsOn1b/HandsOn1b.html#overview",
    "title": "Hands On Exercise 1b",
    "section": "",
    "text": "In general, thematic mapping involves the use of map symbols to visualize selected properties of geographic features that are not naturally visible, such as population, temperature, crime rate, and property prices, just to mention a few of them.\nGeovisualisation, on the other hand, works by providing graphical ideation to render a place, a phenomenon or a process visible, enabling human’s most powerful information-processing abilities – those of spatial cognition associated with our eye–brain vision system – to be directly brought to bear.\nIn this chapter, you will learn how to plot functional and truthful choropleth maps by using an R package called **tmap** package."
  },
  {
    "objectID": "HandsOnExercise/HandsOn1b/HandsOn1b.html#getting-started",
    "href": "HandsOnExercise/HandsOn1b/HandsOn1b.html#getting-started",
    "title": "Hands On Exercise 1b",
    "section": "",
    "text": "In this hands-on exercise, the key R package use is tmap package in R. Beside tmap package, four other R packages will be used. They are:\n\nreadr for importing delimited text file,\ntidyr for tidying data,\ndplyr for wrangling data and\nsf for handling geospatial data.\n\nAmong the four packages, readr, tidyr and dplyr are part of tidyverse package.\nThe code chunk below will be used to install and load these packages in RStudio.\n\n\nCode Chunk\npacman::p_load(sf, tidyverse, tmap)"
  },
  {
    "objectID": "HandsOnExercise/HandsOn1b/HandsOn1b.html#importing-data-into-r",
    "href": "HandsOnExercise/HandsOn1b/HandsOn1b.html#importing-data-into-r",
    "title": "Hands On Exercise 1b",
    "section": "",
    "text": "Two data set will be used to create the choropleth map. They are:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\n\n\n\nThe code chunk below uses the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz.\n\n\nCode Chunk\nmpsz &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/HandsOnExercise/HandsOn1b/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nExamining the content of mpsz by using the code chunk below:\n\n\nCode Chunk\nmpsz\n\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\nNotice that only the first ten records will be displayed.\n\n\n\nNext, we will import respopagsex2011to2020.csv file into RStudio and save the file into an R dataframe called popdata.\nThe task will be performed by using read_csv() function of readr package as shown in the code chunk below.\n\n\nCode Chunk\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020/respopagesextod2011to2020.csv\")\n\n\n\n\n\nBefore a thematic map can be prepared, you are required to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\n\nThe following data wrangling and transformation functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\n\nCode Chunk\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup()%&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\n\n\n\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\n\n\nCode Chunk\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = list(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\n\nCode Chunk\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\n\nThing to learn from the code chunk above:\n\nleft_join() of dplyr package is used with mpsz simple feature data frame as the left data table is to ensure that the output will be a simple features data frame.\n\n\n\nCode Chunk\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")"
  },
  {
    "objectID": "HandsOnExercise/HandsOn1b/HandsOn1b.html#choropleth-mapping-geospatial-data-using-tmap",
    "href": "HandsOnExercise/HandsOn1b/HandsOn1b.html#choropleth-mapping-geospatial-data-using-tmap",
    "title": "Hands On Exercise 1b",
    "section": "",
    "text": "Choropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\nTwo approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\n\n\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\nThe code chunk below will draw a cartographic standard choropleth map as shown below.\nThings to learn from the code chunk above:\n\ntmap_mode() with “plot” option is used to produce a static map. For interactive mode, “view” option should be used.\nfill argument is used to map the attribute (i.e. DEPENDENCY)\n\n\n\nCode Chunk\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\n\n\n\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes aesthetics of individual layers harder to control. To draw a high quality cartographic choropleth map as shown in the figure below, tmap’s drawing elements should be used.\n\n\nCode Chunk\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\nIn the following sub-section, we will share with you tmap functions that used to plot these elements.\n\n\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\nIn the code chunk below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons\n\n\nCode Chunk\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n\n\n\n\n\n\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\n\nCode Chunk\ntm_shape(mpsz_pop2020) +\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\nThings to learn from tm_polygons():\n\nThe default interval binning used to draw the choropleth map is called “pretty”. A detailed discussion of the data classification methods supported by tmap will be provided in sub-section 4.3.\nThe default colour scheme used is YlOrRd of ColorBrewer. You will learn more about the color scheme in sub-section 4.4.\nBy default, Missing value will be shaded in grey.\n\n\n\n\nActually, tm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nThe code chunk below draws a choropleth map by using tm_fill() alone.\n\n\nCode Chunk\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\nNotice that the planning subzones are shared according to the respective dependecy values\nTo add the boundary of the planning subzones, tm_borders will be used as shown in the code chunk below.\n\n\nCode Chunk\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\n\n\n\n\nNotice that the planning subzones are shared according to the respective dependecy values\nTo add the boundary of the planning subzones, tm_borders will be used as shown in the code chunk below.\n\n\nCode Chunk\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\n\n\n\n\nNotice that light-gray border lines have been added on the choropleth map.\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the col is used (normally 1).\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is “solid”.\n\n\n\n\n\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\n\nThe code chunk below shows a quantile data classification that used 5 classes.\n\n\nCode Chunk\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nIn the code chunk below, equal data classification method is used.\n\n\nCode Chunk\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nNotice that the distribution of quantile data classification method are more evenly distributed then equal data classification method.\n\n\n\n\n\n\nMaps Lie!\n\n\n\n\n\n\n\n\n\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points. Code chunk below will be used to compute and display the descriptive statistics of DEPENDENCY field.\n\n\nCode Chunk\nsummary(mpsz_pop2020$DEPENDENCY)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00)\nNow, we will plot the choropleth map by using the code chunk below.\n\n\nCode Chunk\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\n\n\nTo change the colour, we assign the preferred colour to palette argument of tm_fill() as shown in the code chunk below.\n\n\nCode Chunk\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nNotice that the choropleth map is shaded in green.\nTo reverse the colour shading, add a “-” prefix.\n\n\nCode Chunk\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\nMap layout refers to the combination of all map elements into a cohensive map. Map elements include among others the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios. Colour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\n\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\n\nCode Chunk\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\ntmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style().\nThe code chunk below shows the classic style is used.\n\n\nCode Chunk\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\n\n\n\n\n\n\nBeside map style, tmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\n\nCode Chunk\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\nReturning to default style. code chunk:\n\n\nCode Chunk\ntmap_style(\"white\")\n\n\n\n\n\n\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\n\nIn this example, small multiple choropleth maps are created by defining ncols in tm_fill()\n\n\nCode Chunk\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\n\n\n\n\n\nIn this example, small multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments.\n\n\nCode Chunk\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\n\n\nIn this example, multiple small choropleth maps are created by using tm_facets().\n\n\nCode Chunk\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\nIn this example, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\n\nCode Chunk\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstead of creating small multiple choropleth map, you can also use selection funtion to map spatial objects meeting the selection criterion.\n\n\nCode Chunk\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "HandsOnExercise/HandsOn1b/HandsOn1b.html#reference",
    "href": "HandsOnExercise/HandsOn1b/HandsOn1b.html#reference",
    "title": "Hands On Exercise 1b",
    "section": "",
    "text": "tmap: Thematic Maps in R\ntmap\ntmap: get started!\ntmap: changes in version 2.0\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)\n\n\n\n\n\nsf: Simple Features for R\nSimple Features for R: StandardizedSupport for Spatial Vector Data\nReading, Writing and Converting Simple Features\n\n\n\n\n\ndplyr\nTidy data\ntidyr: Easily Tidy Data with ‘spread()’ and ‘gather()’ Functions"
  },
  {
    "objectID": "InClassExercise/InClass1/InClass1.html",
    "href": "InClassExercise/InClass1/InClass1.html",
    "title": "In Class Exercise 1",
    "section": "",
    "text": "Code Chunk\npacman::p_load(sf, tidyverse, tmap, ggstatsplot)\n\n\n\n\n\n\n\n\n\nCode Chunk\nmpsz14_shp = st_read(dsn = \"data/\", \n                  layer = \"MP14_SUBZONE_WEB_PL\")\n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/InClassExercise/InClass1/data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\n\n\n\nCode Chunk\nmpsz14_kml &lt;- st_read(\"data/MasterPlan2014SubzoneBoundaryWebKML.kml\")\n\n\n\n\n\n\n\nCode Chunk\nst_write(mpsz14_shp,\n         \"data/MP14_SUBZONE_WEB_PL.kml\",\n         delete_dsn = TRUE) #delete the existing file with same name first\n\n\nDeleting source `data/MP14_SUBZONE_WEB_PL.kml' using driver `KML'\nWriting layer `MP14_SUBZONE_WEB_PL' to data source \n  `data/MP14_SUBZONE_WEB_PL.kml' using driver `KML'\nWriting 323 features with 15 fields and geometry type Multi Polygon.\n\n\n\n\n\n\n\nCode Chunk\nmpsz19_shp = st_read(dsn = \"data/\", \n                  layer = \"MPSZ-2019\")\n\n\nReading layer `MPSZ-2019' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/InClassExercise/InClass1/data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\nCode Chunk\nmpsz19_shp &lt;-  st_read(\"data/MasterPlan2019SubzoneBoundaryNoSeaKML.kml\")\n\n\nReading layer `URA_MP19_SUBZONE_NO_SEA_PL' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/InClassExercise/InClass1/data/MasterPlan2019SubzoneBoundaryNoSeaKML.kml' \n  using driver `KML'\nSimple feature collection with 332 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY, XYZ\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\n\nPCS over GCS\n\n\n\nTo use Projected Coordinate System (PCS) as geography is flattened and measured using (i.e. metres) instead of Geography Coordinate System (GCS) as it is not accurate. For an example, one degree in the equator and north pole makes a significant difference and GCS will distort it.\n\n\n\n\n\n\nThis mathematically converts one coordinate to another coordinate.\n\n\nCode Chunk\nmpsz19_shp &lt;- st_read(dsn = \"data/\",\n                      layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs = 3414)\n\n\nReading layer `MPSZ-2019' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/InClassExercise/InClass1/data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\n\nChoosing CSV over XLS\n\n\n\nCSV. over XLS as there are multiple hindrances (i.e. extra space, symbols) that requires extra effort to clean. Use text files without structures.\n\n\n\n\n\n\n\n\n\nCode Chunk\npopdata &lt;- read.csv(\"data/respopagesextod2023/respopagesextod2023.csv\")\n\n\n\n\n\n\n\nCode Chunk\npopdata2023 &lt;- popdata %&gt;%\n  group_by(PA, SZ, AG) %&gt;% #aggregation\n  summarise('POP'=sum('Pop')) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(names_from=AG,\n              values_from=POP)\n\n\n\n\nCode Chunk\npopdata2023 &lt;- popdata %&gt;%\n  mutate(YOUNG=rowSums(.[3:6]))"
  },
  {
    "objectID": "InClassExercise/InClass1/InClass1.html#installing-packages",
    "href": "InClassExercise/InClass1/InClass1.html#installing-packages",
    "title": "In Class Exercise 1",
    "section": "",
    "text": "Code Chunk\npacman::p_load(sf, tidyverse, tmap, ggstatsplot)"
  },
  {
    "objectID": "InClassExercise/InClass1/InClass1.html#reading-data",
    "href": "InClassExercise/InClass1/InClass1.html#reading-data",
    "title": "In Class Exercise 1",
    "section": "",
    "text": "Code Chunk\nmpsz14_shp = st_read(dsn = \"data/\", \n                  layer = \"MP14_SUBZONE_WEB_PL\")\n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/InClassExercise/InClass1/data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\n\n\n\nCode Chunk\nmpsz14_kml &lt;- st_read(\"data/MasterPlan2014SubzoneBoundaryWebKML.kml\")\n\n\n\n\n\n\n\nCode Chunk\nst_write(mpsz14_shp,\n         \"data/MP14_SUBZONE_WEB_PL.kml\",\n         delete_dsn = TRUE) #delete the existing file with same name first\n\n\nDeleting source `data/MP14_SUBZONE_WEB_PL.kml' using driver `KML'\nWriting layer `MP14_SUBZONE_WEB_PL' to data source \n  `data/MP14_SUBZONE_WEB_PL.kml' using driver `KML'\nWriting 323 features with 15 fields and geometry type Multi Polygon.\n\n\n\n\n\n\n\nCode Chunk\nmpsz19_shp = st_read(dsn = \"data/\", \n                  layer = \"MPSZ-2019\")\n\n\nReading layer `MPSZ-2019' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/InClassExercise/InClass1/data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\nCode Chunk\nmpsz19_shp &lt;-  st_read(\"data/MasterPlan2019SubzoneBoundaryNoSeaKML.kml\")\n\n\nReading layer `URA_MP19_SUBZONE_NO_SEA_PL' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/InClassExercise/InClass1/data/MasterPlan2019SubzoneBoundaryNoSeaKML.kml' \n  using driver `KML'\nSimple feature collection with 332 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY, XYZ\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\n\nPCS over GCS\n\n\n\nTo use Projected Coordinate System (PCS) as geography is flattened and measured using (i.e. metres) instead of Geography Coordinate System (GCS) as it is not accurate. For an example, one degree in the equator and north pole makes a significant difference and GCS will distort it."
  },
  {
    "objectID": "InClassExercise/InClass1/InClass1.html#transforming-coordinate-system-from-gcs-to-pcs",
    "href": "InClassExercise/InClass1/InClass1.html#transforming-coordinate-system-from-gcs-to-pcs",
    "title": "In Class Exercise 1",
    "section": "",
    "text": "This mathematically converts one coordinate to another coordinate.\n\n\nCode Chunk\nmpsz19_shp &lt;- st_read(dsn = \"data/\",\n                      layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs = 3414)\n\n\nReading layer `MPSZ-2019' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/InClassExercise/InClass1/data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\n\nChoosing CSV over XLS\n\n\n\nCSV. over XLS as there are multiple hindrances (i.e. extra space, symbols) that requires extra effort to clean. Use text files without structures."
  },
  {
    "objectID": "InClassExercise/InClass1/InClass1.html#new-practice",
    "href": "InClassExercise/InClass1/InClass1.html#new-practice",
    "title": "In Class Exercise 1",
    "section": "",
    "text": "Code Chunk\npopdata &lt;- read.csv(\"data/respopagesextod2023/respopagesextod2023.csv\")\n\n\n\n\n\n\n\nCode Chunk\npopdata2023 &lt;- popdata %&gt;%\n  group_by(PA, SZ, AG) %&gt;% #aggregation\n  summarise('POP'=sum('Pop')) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(names_from=AG,\n              values_from=POP)\n\n\n\n\nCode Chunk\npopdata2023 &lt;- popdata %&gt;%\n  mutate(YOUNG=rowSums(.[3:6]))"
  },
  {
    "objectID": "InClassExercise/InClass1/data/MPSZ-2019.html",
    "href": "InClassExercise/InClass1/data/MPSZ-2019.html",
    "title": "Navigating ",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "HandsOnExercise/HandsOn2a/HandsOn2a.html",
    "href": "HandsOnExercise/HandsOn2a/HandsOn2a.html",
    "title": "Hands-On Exercise 2a",
    "section": "",
    "text": "Spatial Point Pattern Analysis is the evaluation of the pattern or distribution, of a set of points on a surface. The point can be location of:\n\nevents such as crime, traffic accident and disease onset, or\nbusiness services (coffee and fastfood outlets) or facilities such as childcare and eldercare.\n\nUsing appropriate functions of spatstat, this hands-on exercise aims to discover the spatial point processes of childecare centres in Singapore.\nProblem Statement:\n\nAre the childcare centres in Singapore randomly distributed across the country?\nif no, what are the locations with higher concentration of childcare centres?\n\nTo provide answers to the questions above, three data sets will be used. They are:\n\nCHILDCARE, a point feature data providing both location and attribute information of childcare centres. It was downloaded from Data.gov.sg and is in geojson format.\nMP14_SUBZONE_WEB_PL, a polygon feature data providing information of URA 2014 Master Plan Planning Subzone boundary data. It is in ESRI shapefile format. This data set was also downloaded from Data.gov.sg.\nCostalOutline, a polygon feature data showing the national boundary of Singapore. It is provided by SLA and is in ESRI shapefile format.\n\n\n\n\nIn this hands-on exercise, five R packages will be used, they are:\n\nsf, a relatively new R package specially designed to import, manage and process vector-based geospatial data in R.\nspatstat, which has a wide range of useful functions for point pattern analysis. In this hands-on exercise, it will be used to perform 1st- and 2nd-order spatial point patterns analysis and derive kernel density estimation (KDE) layer.\nraster which reads, writes, manipulates, analyses and model of gridded spatial data (i.e. raster). In this hands-on exercise, it will be used to convert image output generate by spatstat into raster format.\nmaptools which provides a set of tools for manipulating geographic data. In this hands-on exercise, we mainly use it to convert Spatial objects into ppp format of spatstat.\ntmap which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\n\nUse the code chunk below to install and launch the five R packages.\n\n\nCode Chunk\npacman::p_load(sf, raster, spatstat, tmap, tidyverse, BiocManager)\n\n\n\n\n\n\n\nIn this section, st_read() of sf package will be used to import these three geospatial data sets into R.\n\n\nCode Chunk\nchildcare_sf &lt;- st_read(\"data/child-care-services-geojson.geojson\") %&gt;%\n  st_transform(crs= 3414)\n\n\nReading layer `child-care-services-geojson' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/HandsOnExercise/HandsOn2a/data/child-care-services-geojson.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1545 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.248403 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\n\nCode Chunk\nsg_sf &lt;- st_read(\"data\", layer = \"CostalOutline\")\n\n\nReading layer `CostalOutline' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/HandsOnExercise/HandsOn2a/data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 60 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2663.926 ymin: 16357.98 xmax: 56047.79 ymax: 50244.03\nProjected CRS: SVY21\n\n\n\n\nCode Chunk\nmpsz_sf &lt;- st_read(dsn=\"data\",\n                    layer = \"MP14_SUBZONE_WEB_PL\")\n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/HandsOnExercise/HandsOn2a/data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nBefore we can use these data for analysis, it is important for us to ensure that they are projected in same projection system.\n\nDIY: Using the appropriate sf function you learned in Hands-on Exercise 2, retrieve the referencing system information of these geospatial data.\n\n\n\nCode Chunk\nst_crs(mpsz_sf)\n\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\n\n\nCode Chunk\nmpsz3414 &lt;- st_set_crs(mpsz_sf, 3414)\n\n\n\n\nCode Chunk\nst_crs(mpsz3414)\n\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n`{r} st_crs(sg_sf)\nNotice that except childcare_sf, both mpsz_sf and sg_sf do not have proper crs information.\n\nDIY: Using the method you learned in Lesson 2, assign the correct crs to mpsz_sf and sg_sf simple feature data frames.\n\n\nDIY: If necessary, changing the referencing system to Singapore national projected coordinate system.\n\n\n\n\nAfter checking the referencing system of each geospatial data data frame, it is also useful for us to plot a map to show their spatial patterns.\n\nDIY: Using the mapping methods you learned in Hands-on Exercise 3, prepare a map as shown below.\n\n\n\nCode Chunk\nchildcare_sf &lt;- st_transform(childcare_sf, \n                              crs=3414)\n\n\n\n\nCode Chunk\ntmap_mode('view')\ntm_shape(childcare_sf)+\n  tm_dots()\n\n\n\n\n\n\n\n\nCode Chunk\ntmap_mode('plot')\n\n\nNotice that at the interactive mode, tmap is using leaflet for R API. The advantage of this interactive pin map is it allows us to navigate and zoom around the map freely. We can also query the information of each simple feature (i.e. the point) by clicking of them. Last but not least, you can also change the background of the internet map layer. Currently, three internet map layers are provided. They are: ESRI.WorldGrayCanvas, OpenStreetMap, and ESRI.WorldTopoMap. The default is ESRI.WorldGrayCanvas.\n\nReminder: Always remember to switch back to plot mode after the interactive map. This is because, each interactive mode will consume a connection. You should also avoid displaying excessive numbers of interactive maps (i.e. not more than 10) in one RMarkdown document when publish on Netlify.\n\n\n\n\n\nAlthough simple feature data frame is gaining popularity again sp’s Spatial* classes, there are, however, many geospatial analysis packages require the input geospatial data in sp’s Spatial* classes. In this section, you will learn how to convert simple feature data frame to sp’s Spatial* class.\n\n\nThe code chunk below uses as_Spatial() of sf package to convert the three geospatial data from simple feature data frame to sp’s Spatial* class.\n\n\nCode Chunk\nchildcare &lt;- as_Spatial(childcare_sf)\nmpsz &lt;- as_Spatial(mpsz_sf)\nsg &lt;- as_Spatial(sg_sf)\n\n\nDIY: Using appropriate function, display the information of these three Spatial* classes as shown below.\n\n\nCode Chunk\nchildcare\n\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1545 \nextent      : 11203.01, 45404.24, 25667.6, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 2\nnames       :    Name,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Description \nmin values  :   kml_1, &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;018989&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;1, MARINA BOULEVARD, #B1 - 01, ONE MARINA BOULEVARD, SINGAPORE 018989&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;THE LITTLE SKOOL-HOUSE INTERNATIONAL PTE. LTD.&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;08F73931F4A691F4&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt; \nmax values  : kml_999,                  &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;829646&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;200, PONGGOL SEVENTEENTH AVENUE, SINGAPORE 829646&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Child Care Services&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;RAFFLES KIDZ @ PUNGGOL PTE LTD&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;379D017BF244B0FA&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt; \n\n\n\n\nCode Chunk\nmpsz\n\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 323 \nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +datum=WGS84 +units=m +no_defs \nvariables   : 15\nnames       : OBJECTID, SUBZONE_NO, SUBZONE_N, SUBZONE_C, CA_IND, PLN_AREA_N, PLN_AREA_C,       REGION_N, REGION_C,          INC_CRC, FMEL_UPD_D,     X_ADDR,     Y_ADDR,    SHAPE_Leng,    SHAPE_Area \nmin values  :        1,          1, ADMIRALTY,    AMSZ01,      N, ANG MO KIO,         AM, CENTRAL REGION,       CR, 00F5E30B5C9B7AD8,      16409,  5092.8949,  19579.069, 871.554887798, 39437.9352703 \nmax values  :      323,         17,    YUNNAN,    YSSZ09,      Y,     YISHUN,         YS,    WEST REGION,       WR, FFCCF172717C2EAF,      16409, 50424.7923, 49552.7904, 68083.9364708,  69748298.792 \n\n\n\n\nCode Chunk\nsg\n\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 60 \nextent      : 2663.926, 56047.79, 16357.98, 50244.03  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +datum=WGS84 +units=m +no_defs \nvariables   : 4\nnames       : GDO_GID, MSLINK, MAPID,              COSTAL_NAM \nmin values  :       1,      1,     0,             ISLAND LINK \nmax values  :      60,     67,     0, SINGAPORE - MAIN ISLAND \n\n\nNotice that the geospatial data have been converted into their respective sp’s Spatial* classes now.\n\n\n\nspatstat requires the analytical data in ppp object form. There is no direct way to convert a Spatial* classes into ppp object. We need to convert the Spatial classes* into Spatial object first.\nThe codes chunk below converts the Spatial* classes into generic sp objects.\n\n\nCode Chunk\nchildcare_sp &lt;- as(childcare, \"SpatialPoints\")\nsg_sp &lt;- as(sg, \"SpatialPolygons\")\n\n\nNext, you should display the sp objects properties as shown below.\n\n\nCode Chunk\nchildcare_sp\n\n\nclass       : SpatialPoints \nfeatures    : 1545 \nextent      : 11203.01, 45404.24, 25667.6, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \n\n\n\n\nCode Chunk\nsg_sp\n\n\nclass       : SpatialPolygons \nfeatures    : 60 \nextent      : 2663.926, 56047.79, 16357.98, 50244.03  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +datum=WGS84 +units=m +no_defs \n\n\nChallenge: Do you know what are the differences between Spatial* classes and generic sp object?\n\n\n\nNow, we will use as.ppp() function of spatstat to convert the spatial data into spatstat’s ppp object format.\n\n\nCode Chunk\nchildcare_ppp &lt;- as.ppp(childcare_sf)\nchildcare_ppp\n\n\nMarked planar point pattern: 1545 points\nmarks are of storage type  'character'\nwindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n\n\nNow, let us plot childcare_ppp and examine the different.\n\n\nCode Chunk\nplot(childcare_ppp)\n\n\n\n\n\n\n\n\n\nYou can take a quick look at the summary statistics of the newly created ppp object by using the code chunk below.\n\n\nCode Chunk\nsummary(childcare_ppp)\n\n\nMarked planar point pattern:  1545 points\nAverage intensity 1.91145e-06 points per square unit\n\nCoordinates are given to 11 decimal places\n\nmarks are of type 'character'\nSummary:\n   Length     Class      Mode \n     1545 character character \n\nWindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n                    (34200 x 23630 units)\nWindow area = 808287000 square units\n\n\nNotice the warning message about duplicates. In spatial point patterns analysis an issue of significant is the presence of duplicates. The statistical methodology used for spatial point patterns processes is based largely on the assumption that process are simple, that is, that the points cannot be coincident.\n\n\n\nWe can check the duplication in a ppp object by using the code chunk below.\n\n\nCode Chunk\nany(duplicated(childcare_ppp))\n\n\n[1] FALSE\n\n\nTo count the number of co-indicence point, we will use the multiplicity() function as shown in the code chunk below.\n\n\nCode Chunk\nmultiplicity(childcare_ppp)\n\n\n   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [112] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [149] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [186] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [223] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [260] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [297] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [334] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [371] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [408] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [445] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [482] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [519] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [556] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [593] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [630] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [667] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [704] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [741] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [778] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [815] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [852] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [889] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [926] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [963] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1000] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1037] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1074] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1111] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1148] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1185] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1222] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1259] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1296] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1333] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1370] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1407] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1444] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1481] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1518] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nIf we want to know how many locations have more than one point event, we can use the code chunk below.\n\n\nCode Chunk\nsum(multiplicity(childcare_ppp) &gt; 1)\n\n\n[1] 0\n\n\nThe output shows that there are 128 duplicated point events.\nTo view the locations of these duplicate point events, we will plot childcare data by using the code chunk below.\n\n\nCode Chunk\ntmap_mode('view')\ntm_shape(childcare) +\n  tm_dots(alpha=0.4,\n            size=0.01)\n\n\n\n\n\n\nMakes the map interactive\n\n\nCode Chunk\ntmap_mode('plot') \n\n\nChallenge: Do you know how to spot the duplicate points from the map shown above?\nThere are three ways to overcome this problem. The easiest way is to delete the duplicates. But, that will also mean that some useful point events will be lost.\nThe second solution is use jittering, which will add a small perturbation to the duplicate points so that they do not occupy the exact same space.\nThe third solution is to make each point “unique” and then attach the duplicates of the points to the patterns as marks, as attributes of the points. Then you would need analytical techniques that take into account these marks.\nThe code chunk below implements the jittering approach.\n\n\nCode Chunk\nchildcare_ppp_jit &lt;- rjitter(childcare_ppp,\n                              retry = TRUE,\n                              nsim=1,\n                              drop=TRUE)\n\n\nDIY: Using the method you learned in previous section, check if any dusplicated point in this geospatial data.\n\n\nCode Chunk\nany(duplicated(childcare_ppp_jit))\n\n\n[1] FALSE\n\n\n\n\n\nWhen analysing spatial point patterns, it is a good practice to confine the analysis with a geographical area like Singapore boundary. In spatstat, an object called owin is specially designed to represent this polygonal region.\nThe code chunk below is used to covert sg SpatialPolygon object into owin object of spatstat.\n\n\nCode Chunk\nsg_owin &lt;- as.owin(sg_sf)\n\n\nThe ouput object can be displayed by using plot() function\n\n\nCode Chunk\nplot(sg_owin)\n\n\n\n\n\n\n\n\n\nand summary() function of Base R.\n\n\nCode Chunk\nsummary(sg_owin)\n\n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401\n\n\n\n\n\nIn this last step of geospatial data wrangling, we will extract childcare events that are located within Singapore by using the code chunk below.\n\n\nCode Chunk\nchildcareSG_ppp = childcare_ppp[sg_owin]\n\n\nThe output object combined both the point and polygon feature in one ppp object class as shown below.\n\n\nCode Chunk\nsummary(childcareSG_ppp)\n\n\nMarked planar point pattern:  1545 points\nAverage intensity 2.129929e-06 points per square unit\n\nCoordinates are given to 11 decimal places\n\nmarks are of type 'character'\nSummary:\n   Length     Class      Mode \n     1545 character character \n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401\n\n\nDIY: Using the method you learned in previous exercise, plot the newly derived childcareSG_ppp as shown below.\n\n\nCode Chunk\nchildcareSG_ppp &lt;- as.ppp(childcare_sf)\nchildcareSG_ppp\n\n\nMarked planar point pattern: 1545 points\nmarks are of storage type  'character'\nwindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n\n\n\n\nCode Chunk\nplot(childcareSG_ppp)\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this section, you will learn how to perform first-order SPPA by using spatstat package. The hands-on exercise will focus on:\n\nderiving kernel density estimation (KDE) layer for visualising and exploring the intensity of point processes,\nperforming Confirmatory Spatial Point Patterns Analysis by using Nearest Neighbour statistics.\n\n\n\nIn this section, you will learn how to compute the kernel density estimation (KDE) of childcare services in Singapore.\n\n\nThe code chunk below computes a kernel density by using the following configurations of density() of spatstat:\n\nbw.diggle() automatic bandwidth selection method. Other recommended methods are bw.CvL(), bw.scott() or bw.ppl().\nThe smoothing kernel used is gaussian, which is the default. Other smoothing methods are: “epanechnikov”, “quartic” or “disc”.\nThe intensity estimate is corrected for edge effect bias by using method described by Jones (1993) and Diggle (2010, equation 18.9). The default is FALSE.\n\n\n\nCode Chunk\nkde_childcareSG_bw &lt;- density(childcareSG_ppp,\n                              sigma = bw.diggle,\n                              edge = TRUE,\n                              kernel = \"gaussian\")\n\n\nThe plot() function of Base R is then used to display the kernel density derived.\n\n\nCode Chunk\nplot(kde_childcareSG_bw)\n\n\n\n\n\n\n\n\n\nThe density values of the output range from 0 to 0.000035 which is way too small to comprehend. This is because the default unit of measurement of svy21 is in meter. As a result, the density values computed is in “number of points per square meter”.\nBefore we move on to next section, it is good to know that you can retrieve the bandwidth used to compute the kde layer by using the code chunk below.\n\n\nCode Chunk\nbw &lt;- bw.diggle(childcareSG_ppp)\nbw\n\n\n   sigma \n294.8378 \n\n\n\n\n\nIn the code chunk below, rescale.ppp() is used to covert the unit of measurement from meter to kilometer.\n\n\nCode Chunk\nchildcareSG_ppp.km &lt;- rescale.ppp(childcareSG_ppp, 1000, \"km\")\n\n\nNow, we can re-run density() using the resale data set and plot the output kde map.\n\n\nCode Chunk\nkde_childrenSG.bw &lt;- density(childcareSG_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childrenSG.bw)\n\n\n\n\n\n\n\n\n\nNotice that output image looks identical to the earlier version, the only changes in the data values (refer to the legend).\n\n\n\n\nBeside bw.diggle(), there are three other spatstat functions can be used to determine the bandwidth, they are: bw.CvL(), bw.scott(), and bw.ppl().\nLet us take a look at the bandwidth return by these automatic bandwidth calculation methods by using the code chunk below.\n\n\nCode Chunk\n bw.CvL(childcareSG_ppp.km)\n\n\n   sigma \n6.354327 \n\n\n\n\nCode Chunk\nbw.ppl(childcareSG_ppp.km)\n\n\n    sigma \n0.3283284 \n\n\n\n\nCode Chunk\nbw.diggle(childcareSG_ppp.km)\n\n\n    sigma \n0.2948378 \n\n\nBaddeley et. (2016) suggested the use of the bw.ppl() algorithm because in ther experience it tends to produce the more appropriate values when the pattern consists predominantly of tight clusters. But they also insist that if the purpose of once study is to detect a single tight cluster in the midst of random noise then the bw.diggle() method seems to work best.\nThe code chunk beow will be used to compare the output of using bw.diggle and bw.ppl methods.\n\n\nCode Chunk\nkde_childcareSG.ppl &lt;- density(childcareSG_ppp.km,\n                               sigma=bw.ppl,\n                               edge=TRUE,\n                               kernel = \"gaussian\")\n\npar(mfrow=c(1,2))\nplot(kde_childrenSG.bw, main = \"bw.diggle\")\nplot(kde_childcareSG.ppl, main=\"bw.ppl\")\n\n\n\n\n\n\n\n\n\n\n\n\nBy default, the kernel method used in density.ppp() is gaussian. But there are three other options, namely: Epanechnikov, Quartic and Dics.\nThe code chunk below will be used to compute three more kernel density estimations by using these three kernel function.\n```\n\n\nCode Chunk\npar(mfrow=c(2,2))\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"gaussian\"), \n     main=\"Gaussian\")\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"epanechnikov\"), \n     main=\"Epanechnikov\")\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"quartic\"), \n     main=\"Quartic\")\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"disc\"), \n     main=\"Disc\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext, you will compute a KDE layer by defining a bandwidth of 600 meter. Notice that in the code chunk below, the sigma value used is 0.6. This is because the unit of measurement of childcareSG_ppp.km object is in kilometer, hence the 600m is 0.6km.\n\n\nCode Chunk\nkde_childcareSG_600 &lt;- density(childcareSG_ppp.km, sigma=0.6,\n                                edge=TRUE, kernel = \"gaussian\")\nplot(kde_childcareSG_600)\n\n\n\n\n\n\n\n\n\n\n\n\nFixed bandwidth method is very sensitive to highly skew distribution of spatial point patterns over geographical units for example urban versus rural. One way to overcome this problem is by using adaptive bandwidth instead.\nIn this section, you will learn how to derive adaptive kernel density estimation by using density.adaptive() of spatstat.\n\n\nCode Chunk\nkde_childcareSG_adaptive &lt;- adaptive.density(childcareSG_ppp.km, method = \"kernel\")\nplot(kde_childcareSG_adaptive)\n\n\n\n\n\n\n\n\n\nWe can compare the fixed and adaptive kernel density estimation outputs by using the code chunk below.\n\n\nCode Chunk\npar(mfrow=c(1,2))\nplot(kde_childcareSG_bw, main = \"Fixed bandwidth\")\nplot(kde_childcareSG_adaptive, main = \"Adaptive bandwidth\")\n\n\n\n\n\n\n\n\n\n\n\n\nThe result is the same, we just convert it so that it is suitable for mapping purposes\n\n\nCode Chunk\n# Convert the 'im' object to a 'SpatialGridDataFrame'\ngridded_kde_childcareSG_bw &lt;- as(kde_childcareSG_bw, \"SpatialGridDataFrame\")\nspplot(gridded_kde_childcareSG_bw)\n\n\n\n\n\n\n\n\n\n\n\nNext, we will convert the gridded kernal density objects into RasterLayer object by using raster() of raster package.\n\n\nCode Chunk\nkde_childcareSG_bw_raster &lt;- raster(kde_childrenSG.bw)\n\n\n\n\nCode Chunk\nkde_childcareSG_bw_raster\n\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.2671971, 0.184635  (x, y)\nextent     : 11.20301, 45.40424, 25.6676, 49.30088  (xmin, xmax, ymin, ymax)\ncrs        : NA \nsource     : memory\nnames      : layer \nvalues     : -3.782421e-15, 28.48563  (min, max)\n\n\nNotice that the crs property is NA.\n\n\n\nThe code chunk below will be used to include the CRS information on kde_childcareSG_bw_raster RasterLayer.\n\n\nCode Chunk\nprojection(kde_childcareSG_bw_raster) &lt;- CRS(\"+init=EPSG:3414\")\nkde_childcareSG_bw_raster\n\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.2671971, 0.184635  (x, y)\nextent     : 11.20301, 45.40424, 25.6676, 49.30088  (xmin, xmax, ymin, ymax)\ncrs        : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +units=m +no_defs \nsource     : memory\nnames      : layer \nvalues     : -3.782421e-15, 28.48563  (min, max)\n\n\nNotice that the crs property is completed.\n\n\n\n\nFinally, we will display the raster in cartographic quality map using tmap package.\n\n\nCode Chunk\ntm_shape(kde_childcareSG_bw_raster) +\n  tm_raster(\"layer\", palette = \"viridis\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), frame=FALSE)\n\n\n\n\n\n\n\n\n\n\nNotice that the raster values are encoded explicitly onto the raster pixel using the values in “v”” field.\n\n\n\nIn this section, you will learn how to compare KDE of childcare at Ponggol, Tampines, Chua Chu Kang and Jurong West planning areas.\n\n\nThe code chunk below will be used to extract the target planning areas.\n\n\nCode Chunk\npg &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"PUNGGOL\")\ntm &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"TAMPINES\")\nck &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"CHOA CHU KANG\")\njw &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"JURONG WEST\")\n\n\nPlotting planning areas of the above\n\n\nCode Chunk\npar(mfrow = c(2,2))\nplot(pg, main = \"PUNGGOL\")\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\nplot(tm, main = \"Tampines\")\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\nplot(ck, main=\"Choa Chu Kang\")\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\nplot(jw, main = \"Jurong West\")\n\n\n\n\n\n\n\n\n\n\n\n\nNow, we will convert these sf objects into owin objects that is required by spatstat.\n\n\nCode Chunk\npg_owin = as.owin(pg)\ntm_owin = as.owin(tm)\nck_owin = as.owin(ck)\njw_owin = as.owin(jw)\n\n\n\n\n\nBy using the code chunk below, we are able to extract childcare that is within the specific region to do our analysis later on.\n\n\nCode Chunk\nchildcare_pg_ppp = childcare_ppp_jit[pg_owin]\nchildcare_tm_ppp = childcare_ppp_jit[tm_owin]\nchildcare_ck_ppp = childcare_ppp_jit[ck_owin]\nchildcare_jw_ppp = childcare_ppp_jit[jw_owin]\n\n\nNext, rescale.ppp() function is used to trasnform the unit of measurement from metre to kilometre.\n\n\nCode Chunk\nchildcare_pg_ppp.km = rescale.ppp(childcare_pg_ppp, 1000, \"km\")\nchildcare_tm_ppp.km = rescale.ppp(childcare_tm_ppp, 1000, \"km\")\nchildcare_ck_ppp.km = rescale.ppp(childcare_ck_ppp, 1000, \"km\")\nchildcare_jw_ppp.km = rescale.ppp(childcare_jw_ppp, 1000, \"km\")\n\n\nThe code chunk below is used to plot these four study areas and the locations of the childcare centre.\n\n\nCode Chunk\npar(mfrow=c(2,2))\nplot(childcare_pg_ppp.km, main=\"Punggol\")\nplot(childcare_tm_ppp.km, main=\"Tampines\")\nplot(childcare_ck_ppp.km, main=\"Choa Chu Kang\")\nplot(childcare_jw_ppp.km, main=\"Jurong West\")\n\n\n\n\n\n\n\n\n\n\n\n\nThe code chunk below will be used to compute the KDE of these four planning area. bw.diggle method is used to derive the bandwidth of each.\n\n\nCode Chunk\npar(mfrow=c(2,2))\nplot(density(childcare_pg_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tempines\")\nplot(density(childcare_ck_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Choa Chu Kang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"JUrong West\")\n\n\n\n\n\n\n\n\n\n\n\n\nFor comparison purposes, we will use 250m as the bandwidth.\n\n\nCode Chunk\npar(mfrow=c(2,2))\nplot(density(childcare_ck_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Chou Chu Kang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"JUrong West\")\nplot(density(childcare_pg_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tampines\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this section, we will perform the Clark-Evans test of aggregation for a spatial point pattern by using clarkevans.test() of statspat.\nThe test hypotheses are:\nHo = The distribution of childcare services are randomly distributed.\nH1= The distribution of childcare services are not randomly distributed.\nThe 95% confident interval will be used.\n\n\n\n\nCode Chunk\nclarkevans.test(childcareSG_ppp,\n                correction = \"none\",\n                clipregion=\"sg_owin\",\n                alterative = c(\"clustered\"),\n                nsim=99)\n\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcareSG_ppp\nR = 0.527, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided\n\n\n\n\n\nIn the code chunk below, clarkevans.test() of spatstat is used to performs Clark-Evans test of aggregation for childcare centre in Choa Chu Kang planning area.\n\n\nCode Chunk\nclarkevans.test(childcare_ck_ppp,\n                correction=\"none\",\n                clipregion = NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_ck_ppp\nR = 0.92448, p-value = 0.2592\nalternative hypothesis: two-sided\n\n\n\n\n\nIn the code chunk below, the similar test is used to analyse the spatial point patterns of childcare centre in Tampines planning area.\n\n\nCode Chunk\nclarkevans.test(childcare_tm_ppp,\n                correction=\"none\",\n                clipregion = NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_tm_ppp\nR = 0.80096, p-value = 0.0003278\nalternative hypothesis: two-sided"
  },
  {
    "objectID": "HandsOnExercise/HandsOn2a/HandsOn2a.html#overview",
    "href": "HandsOnExercise/HandsOn2a/HandsOn2a.html#overview",
    "title": "Hands-On Exercise 2a",
    "section": "",
    "text": "Spatial Point Pattern Analysis is the evaluation of the pattern or distribution, of a set of points on a surface. The point can be location of:\n\nevents such as crime, traffic accident and disease onset, or\nbusiness services (coffee and fastfood outlets) or facilities such as childcare and eldercare.\n\nUsing appropriate functions of spatstat, this hands-on exercise aims to discover the spatial point processes of childecare centres in Singapore.\nProblem Statement:\n\nAre the childcare centres in Singapore randomly distributed across the country?\nif no, what are the locations with higher concentration of childcare centres?\n\nTo provide answers to the questions above, three data sets will be used. They are:\n\nCHILDCARE, a point feature data providing both location and attribute information of childcare centres. It was downloaded from Data.gov.sg and is in geojson format.\nMP14_SUBZONE_WEB_PL, a polygon feature data providing information of URA 2014 Master Plan Planning Subzone boundary data. It is in ESRI shapefile format. This data set was also downloaded from Data.gov.sg.\nCostalOutline, a polygon feature data showing the national boundary of Singapore. It is provided by SLA and is in ESRI shapefile format."
  },
  {
    "objectID": "HandsOnExercise/HandsOn2a/HandsOn2a.html#installing-and-loading-the-r-packages",
    "href": "HandsOnExercise/HandsOn2a/HandsOn2a.html#installing-and-loading-the-r-packages",
    "title": "Hands-On Exercise 2a",
    "section": "",
    "text": "In this hands-on exercise, five R packages will be used, they are:\n\nsf, a relatively new R package specially designed to import, manage and process vector-based geospatial data in R.\nspatstat, which has a wide range of useful functions for point pattern analysis. In this hands-on exercise, it will be used to perform 1st- and 2nd-order spatial point patterns analysis and derive kernel density estimation (KDE) layer.\nraster which reads, writes, manipulates, analyses and model of gridded spatial data (i.e. raster). In this hands-on exercise, it will be used to convert image output generate by spatstat into raster format.\nmaptools which provides a set of tools for manipulating geographic data. In this hands-on exercise, we mainly use it to convert Spatial objects into ppp format of spatstat.\ntmap which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\n\nUse the code chunk below to install and launch the five R packages.\n\n\nCode Chunk\npacman::p_load(sf, raster, spatstat, tmap, tidyverse, BiocManager)"
  },
  {
    "objectID": "HandsOnExercise/HandsOn2a/HandsOn2a.html#spatial-data-wrangling",
    "href": "HandsOnExercise/HandsOn2a/HandsOn2a.html#spatial-data-wrangling",
    "title": "Hands-On Exercise 2a",
    "section": "",
    "text": "In this section, st_read() of sf package will be used to import these three geospatial data sets into R.\n\n\nCode Chunk\nchildcare_sf &lt;- st_read(\"data/child-care-services-geojson.geojson\") %&gt;%\n  st_transform(crs= 3414)\n\n\nReading layer `child-care-services-geojson' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/HandsOnExercise/HandsOn2a/data/child-care-services-geojson.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1545 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.248403 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\n\nCode Chunk\nsg_sf &lt;- st_read(\"data\", layer = \"CostalOutline\")\n\n\nReading layer `CostalOutline' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/HandsOnExercise/HandsOn2a/data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 60 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2663.926 ymin: 16357.98 xmax: 56047.79 ymax: 50244.03\nProjected CRS: SVY21\n\n\n\n\nCode Chunk\nmpsz_sf &lt;- st_read(dsn=\"data\",\n                    layer = \"MP14_SUBZONE_WEB_PL\")\n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/HandsOnExercise/HandsOn2a/data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nBefore we can use these data for analysis, it is important for us to ensure that they are projected in same projection system.\n\nDIY: Using the appropriate sf function you learned in Hands-on Exercise 2, retrieve the referencing system information of these geospatial data.\n\n\n\nCode Chunk\nst_crs(mpsz_sf)\n\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\n\n\nCode Chunk\nmpsz3414 &lt;- st_set_crs(mpsz_sf, 3414)\n\n\n\n\nCode Chunk\nst_crs(mpsz3414)\n\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n`{r} st_crs(sg_sf)\nNotice that except childcare_sf, both mpsz_sf and sg_sf do not have proper crs information.\n\nDIY: Using the method you learned in Lesson 2, assign the correct crs to mpsz_sf and sg_sf simple feature data frames.\n\n\nDIY: If necessary, changing the referencing system to Singapore national projected coordinate system.\n\n\n\n\nAfter checking the referencing system of each geospatial data data frame, it is also useful for us to plot a map to show their spatial patterns.\n\nDIY: Using the mapping methods you learned in Hands-on Exercise 3, prepare a map as shown below.\n\n\n\nCode Chunk\nchildcare_sf &lt;- st_transform(childcare_sf, \n                              crs=3414)\n\n\n\n\nCode Chunk\ntmap_mode('view')\ntm_shape(childcare_sf)+\n  tm_dots()\n\n\n\n\n\n\n\n\nCode Chunk\ntmap_mode('plot')\n\n\nNotice that at the interactive mode, tmap is using leaflet for R API. The advantage of this interactive pin map is it allows us to navigate and zoom around the map freely. We can also query the information of each simple feature (i.e. the point) by clicking of them. Last but not least, you can also change the background of the internet map layer. Currently, three internet map layers are provided. They are: ESRI.WorldGrayCanvas, OpenStreetMap, and ESRI.WorldTopoMap. The default is ESRI.WorldGrayCanvas.\n\nReminder: Always remember to switch back to plot mode after the interactive map. This is because, each interactive mode will consume a connection. You should also avoid displaying excessive numbers of interactive maps (i.e. not more than 10) in one RMarkdown document when publish on Netlify."
  },
  {
    "objectID": "HandsOnExercise/HandsOn2a/HandsOn2a.html#geospatial-data-wrangling",
    "href": "HandsOnExercise/HandsOn2a/HandsOn2a.html#geospatial-data-wrangling",
    "title": "Hands-On Exercise 2a",
    "section": "",
    "text": "Although simple feature data frame is gaining popularity again sp’s Spatial* classes, there are, however, many geospatial analysis packages require the input geospatial data in sp’s Spatial* classes. In this section, you will learn how to convert simple feature data frame to sp’s Spatial* class.\n\n\nThe code chunk below uses as_Spatial() of sf package to convert the three geospatial data from simple feature data frame to sp’s Spatial* class.\n\n\nCode Chunk\nchildcare &lt;- as_Spatial(childcare_sf)\nmpsz &lt;- as_Spatial(mpsz_sf)\nsg &lt;- as_Spatial(sg_sf)\n\n\nDIY: Using appropriate function, display the information of these three Spatial* classes as shown below.\n\n\nCode Chunk\nchildcare\n\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1545 \nextent      : 11203.01, 45404.24, 25667.6, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 2\nnames       :    Name,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Description \nmin values  :   kml_1, &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;018989&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;1, MARINA BOULEVARD, #B1 - 01, ONE MARINA BOULEVARD, SINGAPORE 018989&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;THE LITTLE SKOOL-HOUSE INTERNATIONAL PTE. LTD.&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;08F73931F4A691F4&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt; \nmax values  : kml_999,                  &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;829646&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;200, PONGGOL SEVENTEENTH AVENUE, SINGAPORE 829646&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Child Care Services&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;RAFFLES KIDZ @ PUNGGOL PTE LTD&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;379D017BF244B0FA&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt; \n\n\n\n\nCode Chunk\nmpsz\n\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 323 \nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +datum=WGS84 +units=m +no_defs \nvariables   : 15\nnames       : OBJECTID, SUBZONE_NO, SUBZONE_N, SUBZONE_C, CA_IND, PLN_AREA_N, PLN_AREA_C,       REGION_N, REGION_C,          INC_CRC, FMEL_UPD_D,     X_ADDR,     Y_ADDR,    SHAPE_Leng,    SHAPE_Area \nmin values  :        1,          1, ADMIRALTY,    AMSZ01,      N, ANG MO KIO,         AM, CENTRAL REGION,       CR, 00F5E30B5C9B7AD8,      16409,  5092.8949,  19579.069, 871.554887798, 39437.9352703 \nmax values  :      323,         17,    YUNNAN,    YSSZ09,      Y,     YISHUN,         YS,    WEST REGION,       WR, FFCCF172717C2EAF,      16409, 50424.7923, 49552.7904, 68083.9364708,  69748298.792 \n\n\n\n\nCode Chunk\nsg\n\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 60 \nextent      : 2663.926, 56047.79, 16357.98, 50244.03  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +datum=WGS84 +units=m +no_defs \nvariables   : 4\nnames       : GDO_GID, MSLINK, MAPID,              COSTAL_NAM \nmin values  :       1,      1,     0,             ISLAND LINK \nmax values  :      60,     67,     0, SINGAPORE - MAIN ISLAND \n\n\nNotice that the geospatial data have been converted into their respective sp’s Spatial* classes now.\n\n\n\nspatstat requires the analytical data in ppp object form. There is no direct way to convert a Spatial* classes into ppp object. We need to convert the Spatial classes* into Spatial object first.\nThe codes chunk below converts the Spatial* classes into generic sp objects.\n\n\nCode Chunk\nchildcare_sp &lt;- as(childcare, \"SpatialPoints\")\nsg_sp &lt;- as(sg, \"SpatialPolygons\")\n\n\nNext, you should display the sp objects properties as shown below.\n\n\nCode Chunk\nchildcare_sp\n\n\nclass       : SpatialPoints \nfeatures    : 1545 \nextent      : 11203.01, 45404.24, 25667.6, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \n\n\n\n\nCode Chunk\nsg_sp\n\n\nclass       : SpatialPolygons \nfeatures    : 60 \nextent      : 2663.926, 56047.79, 16357.98, 50244.03  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +datum=WGS84 +units=m +no_defs \n\n\nChallenge: Do you know what are the differences between Spatial* classes and generic sp object?\n\n\n\nNow, we will use as.ppp() function of spatstat to convert the spatial data into spatstat’s ppp object format.\n\n\nCode Chunk\nchildcare_ppp &lt;- as.ppp(childcare_sf)\nchildcare_ppp\n\n\nMarked planar point pattern: 1545 points\nmarks are of storage type  'character'\nwindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n\n\nNow, let us plot childcare_ppp and examine the different.\n\n\nCode Chunk\nplot(childcare_ppp)\n\n\n\n\n\n\n\n\n\nYou can take a quick look at the summary statistics of the newly created ppp object by using the code chunk below.\n\n\nCode Chunk\nsummary(childcare_ppp)\n\n\nMarked planar point pattern:  1545 points\nAverage intensity 1.91145e-06 points per square unit\n\nCoordinates are given to 11 decimal places\n\nmarks are of type 'character'\nSummary:\n   Length     Class      Mode \n     1545 character character \n\nWindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n                    (34200 x 23630 units)\nWindow area = 808287000 square units\n\n\nNotice the warning message about duplicates. In spatial point patterns analysis an issue of significant is the presence of duplicates. The statistical methodology used for spatial point patterns processes is based largely on the assumption that process are simple, that is, that the points cannot be coincident.\n\n\n\nWe can check the duplication in a ppp object by using the code chunk below.\n\n\nCode Chunk\nany(duplicated(childcare_ppp))\n\n\n[1] FALSE\n\n\nTo count the number of co-indicence point, we will use the multiplicity() function as shown in the code chunk below.\n\n\nCode Chunk\nmultiplicity(childcare_ppp)\n\n\n   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [112] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [149] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [186] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [223] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [260] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [297] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [334] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [371] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [408] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [445] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [482] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [519] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [556] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [593] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [630] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [667] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [704] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [741] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [778] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [815] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [852] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [889] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [926] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [963] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1000] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1037] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1074] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1111] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1148] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1185] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1222] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1259] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1296] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1333] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1370] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1407] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1444] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1481] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1518] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nIf we want to know how many locations have more than one point event, we can use the code chunk below.\n\n\nCode Chunk\nsum(multiplicity(childcare_ppp) &gt; 1)\n\n\n[1] 0\n\n\nThe output shows that there are 128 duplicated point events.\nTo view the locations of these duplicate point events, we will plot childcare data by using the code chunk below.\n\n\nCode Chunk\ntmap_mode('view')\ntm_shape(childcare) +\n  tm_dots(alpha=0.4,\n            size=0.01)\n\n\n\n\n\n\nMakes the map interactive\n\n\nCode Chunk\ntmap_mode('plot') \n\n\nChallenge: Do you know how to spot the duplicate points from the map shown above?\nThere are three ways to overcome this problem. The easiest way is to delete the duplicates. But, that will also mean that some useful point events will be lost.\nThe second solution is use jittering, which will add a small perturbation to the duplicate points so that they do not occupy the exact same space.\nThe third solution is to make each point “unique” and then attach the duplicates of the points to the patterns as marks, as attributes of the points. Then you would need analytical techniques that take into account these marks.\nThe code chunk below implements the jittering approach.\n\n\nCode Chunk\nchildcare_ppp_jit &lt;- rjitter(childcare_ppp,\n                              retry = TRUE,\n                              nsim=1,\n                              drop=TRUE)\n\n\nDIY: Using the method you learned in previous section, check if any dusplicated point in this geospatial data.\n\n\nCode Chunk\nany(duplicated(childcare_ppp_jit))\n\n\n[1] FALSE\n\n\n\n\n\nWhen analysing spatial point patterns, it is a good practice to confine the analysis with a geographical area like Singapore boundary. In spatstat, an object called owin is specially designed to represent this polygonal region.\nThe code chunk below is used to covert sg SpatialPolygon object into owin object of spatstat.\n\n\nCode Chunk\nsg_owin &lt;- as.owin(sg_sf)\n\n\nThe ouput object can be displayed by using plot() function\n\n\nCode Chunk\nplot(sg_owin)\n\n\n\n\n\n\n\n\n\nand summary() function of Base R.\n\n\nCode Chunk\nsummary(sg_owin)\n\n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401\n\n\n\n\n\nIn this last step of geospatial data wrangling, we will extract childcare events that are located within Singapore by using the code chunk below.\n\n\nCode Chunk\nchildcareSG_ppp = childcare_ppp[sg_owin]\n\n\nThe output object combined both the point and polygon feature in one ppp object class as shown below.\n\n\nCode Chunk\nsummary(childcareSG_ppp)\n\n\nMarked planar point pattern:  1545 points\nAverage intensity 2.129929e-06 points per square unit\n\nCoordinates are given to 11 decimal places\n\nmarks are of type 'character'\nSummary:\n   Length     Class      Mode \n     1545 character character \n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401\n\n\nDIY: Using the method you learned in previous exercise, plot the newly derived childcareSG_ppp as shown below.\n\n\nCode Chunk\nchildcareSG_ppp &lt;- as.ppp(childcare_sf)\nchildcareSG_ppp\n\n\nMarked planar point pattern: 1545 points\nmarks are of storage type  'character'\nwindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n\n\n\n\nCode Chunk\nplot(childcareSG_ppp)"
  },
  {
    "objectID": "HandsOnExercise/HandsOn2a/HandsOn2a.html#first-order-spatial-point-patterns-analysis",
    "href": "HandsOnExercise/HandsOn2a/HandsOn2a.html#first-order-spatial-point-patterns-analysis",
    "title": "Hands-On Exercise 2a",
    "section": "",
    "text": "In this section, you will learn how to perform first-order SPPA by using spatstat package. The hands-on exercise will focus on:\n\nderiving kernel density estimation (KDE) layer for visualising and exploring the intensity of point processes,\nperforming Confirmatory Spatial Point Patterns Analysis by using Nearest Neighbour statistics.\n\n\n\nIn this section, you will learn how to compute the kernel density estimation (KDE) of childcare services in Singapore.\n\n\nThe code chunk below computes a kernel density by using the following configurations of density() of spatstat:\n\nbw.diggle() automatic bandwidth selection method. Other recommended methods are bw.CvL(), bw.scott() or bw.ppl().\nThe smoothing kernel used is gaussian, which is the default. Other smoothing methods are: “epanechnikov”, “quartic” or “disc”.\nThe intensity estimate is corrected for edge effect bias by using method described by Jones (1993) and Diggle (2010, equation 18.9). The default is FALSE.\n\n\n\nCode Chunk\nkde_childcareSG_bw &lt;- density(childcareSG_ppp,\n                              sigma = bw.diggle,\n                              edge = TRUE,\n                              kernel = \"gaussian\")\n\n\nThe plot() function of Base R is then used to display the kernel density derived.\n\n\nCode Chunk\nplot(kde_childcareSG_bw)\n\n\n\n\n\n\n\n\n\nThe density values of the output range from 0 to 0.000035 which is way too small to comprehend. This is because the default unit of measurement of svy21 is in meter. As a result, the density values computed is in “number of points per square meter”.\nBefore we move on to next section, it is good to know that you can retrieve the bandwidth used to compute the kde layer by using the code chunk below.\n\n\nCode Chunk\nbw &lt;- bw.diggle(childcareSG_ppp)\nbw\n\n\n   sigma \n294.8378 \n\n\n\n\n\nIn the code chunk below, rescale.ppp() is used to covert the unit of measurement from meter to kilometer.\n\n\nCode Chunk\nchildcareSG_ppp.km &lt;- rescale.ppp(childcareSG_ppp, 1000, \"km\")\n\n\nNow, we can re-run density() using the resale data set and plot the output kde map.\n\n\nCode Chunk\nkde_childrenSG.bw &lt;- density(childcareSG_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childrenSG.bw)\n\n\n\n\n\n\n\n\n\nNotice that output image looks identical to the earlier version, the only changes in the data values (refer to the legend).\n\n\n\n\nBeside bw.diggle(), there are three other spatstat functions can be used to determine the bandwidth, they are: bw.CvL(), bw.scott(), and bw.ppl().\nLet us take a look at the bandwidth return by these automatic bandwidth calculation methods by using the code chunk below.\n\n\nCode Chunk\n bw.CvL(childcareSG_ppp.km)\n\n\n   sigma \n6.354327 \n\n\n\n\nCode Chunk\nbw.ppl(childcareSG_ppp.km)\n\n\n    sigma \n0.3283284 \n\n\n\n\nCode Chunk\nbw.diggle(childcareSG_ppp.km)\n\n\n    sigma \n0.2948378 \n\n\nBaddeley et. (2016) suggested the use of the bw.ppl() algorithm because in ther experience it tends to produce the more appropriate values when the pattern consists predominantly of tight clusters. But they also insist that if the purpose of once study is to detect a single tight cluster in the midst of random noise then the bw.diggle() method seems to work best.\nThe code chunk beow will be used to compare the output of using bw.diggle and bw.ppl methods.\n\n\nCode Chunk\nkde_childcareSG.ppl &lt;- density(childcareSG_ppp.km,\n                               sigma=bw.ppl,\n                               edge=TRUE,\n                               kernel = \"gaussian\")\n\npar(mfrow=c(1,2))\nplot(kde_childrenSG.bw, main = \"bw.diggle\")\nplot(kde_childcareSG.ppl, main=\"bw.ppl\")\n\n\n\n\n\n\n\n\n\n\n\n\nBy default, the kernel method used in density.ppp() is gaussian. But there are three other options, namely: Epanechnikov, Quartic and Dics.\nThe code chunk below will be used to compute three more kernel density estimations by using these three kernel function.\n```\n\n\nCode Chunk\npar(mfrow=c(2,2))\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"gaussian\"), \n     main=\"Gaussian\")\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"epanechnikov\"), \n     main=\"Epanechnikov\")\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"quartic\"), \n     main=\"Quartic\")\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"disc\"), \n     main=\"Disc\")"
  },
  {
    "objectID": "HandsOnExercise/HandsOn2a/HandsOn2a.html#fixed-and-adaptive-kde",
    "href": "HandsOnExercise/HandsOn2a/HandsOn2a.html#fixed-and-adaptive-kde",
    "title": "Hands-On Exercise 2a",
    "section": "",
    "text": "Next, you will compute a KDE layer by defining a bandwidth of 600 meter. Notice that in the code chunk below, the sigma value used is 0.6. This is because the unit of measurement of childcareSG_ppp.km object is in kilometer, hence the 600m is 0.6km.\n\n\nCode Chunk\nkde_childcareSG_600 &lt;- density(childcareSG_ppp.km, sigma=0.6,\n                                edge=TRUE, kernel = \"gaussian\")\nplot(kde_childcareSG_600)\n\n\n\n\n\n\n\n\n\n\n\n\nFixed bandwidth method is very sensitive to highly skew distribution of spatial point patterns over geographical units for example urban versus rural. One way to overcome this problem is by using adaptive bandwidth instead.\nIn this section, you will learn how to derive adaptive kernel density estimation by using density.adaptive() of spatstat.\n\n\nCode Chunk\nkde_childcareSG_adaptive &lt;- adaptive.density(childcareSG_ppp.km, method = \"kernel\")\nplot(kde_childcareSG_adaptive)\n\n\n\n\n\n\n\n\n\nWe can compare the fixed and adaptive kernel density estimation outputs by using the code chunk below.\n\n\nCode Chunk\npar(mfrow=c(1,2))\nplot(kde_childcareSG_bw, main = \"Fixed bandwidth\")\nplot(kde_childcareSG_adaptive, main = \"Adaptive bandwidth\")\n\n\n\n\n\n\n\n\n\n\n\n\nThe result is the same, we just convert it so that it is suitable for mapping purposes\n\n\nCode Chunk\n# Convert the 'im' object to a 'SpatialGridDataFrame'\ngridded_kde_childcareSG_bw &lt;- as(kde_childcareSG_bw, \"SpatialGridDataFrame\")\nspplot(gridded_kde_childcareSG_bw)\n\n\n\n\n\n\n\n\n\n\n\nNext, we will convert the gridded kernal density objects into RasterLayer object by using raster() of raster package.\n\n\nCode Chunk\nkde_childcareSG_bw_raster &lt;- raster(kde_childrenSG.bw)\n\n\n\n\nCode Chunk\nkde_childcareSG_bw_raster\n\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.2671971, 0.184635  (x, y)\nextent     : 11.20301, 45.40424, 25.6676, 49.30088  (xmin, xmax, ymin, ymax)\ncrs        : NA \nsource     : memory\nnames      : layer \nvalues     : -3.782421e-15, 28.48563  (min, max)\n\n\nNotice that the crs property is NA.\n\n\n\nThe code chunk below will be used to include the CRS information on kde_childcareSG_bw_raster RasterLayer.\n\n\nCode Chunk\nprojection(kde_childcareSG_bw_raster) &lt;- CRS(\"+init=EPSG:3414\")\nkde_childcareSG_bw_raster\n\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.2671971, 0.184635  (x, y)\nextent     : 11.20301, 45.40424, 25.6676, 49.30088  (xmin, xmax, ymin, ymax)\ncrs        : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +units=m +no_defs \nsource     : memory\nnames      : layer \nvalues     : -3.782421e-15, 28.48563  (min, max)\n\n\nNotice that the crs property is completed.\n\n\n\n\nFinally, we will display the raster in cartographic quality map using tmap package.\n\n\nCode Chunk\ntm_shape(kde_childcareSG_bw_raster) +\n  tm_raster(\"layer\", palette = \"viridis\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), frame=FALSE)\n\n\n\n\n\n\n\n\n\n\nNotice that the raster values are encoded explicitly onto the raster pixel using the values in “v”” field.\n\n\n\nIn this section, you will learn how to compare KDE of childcare at Ponggol, Tampines, Chua Chu Kang and Jurong West planning areas.\n\n\nThe code chunk below will be used to extract the target planning areas.\n\n\nCode Chunk\npg &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"PUNGGOL\")\ntm &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"TAMPINES\")\nck &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"CHOA CHU KANG\")\njw &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"JURONG WEST\")\n\n\nPlotting planning areas of the above\n\n\nCode Chunk\npar(mfrow = c(2,2))\nplot(pg, main = \"PUNGGOL\")\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\nplot(tm, main = \"Tampines\")\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\nplot(ck, main=\"Choa Chu Kang\")\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\nplot(jw, main = \"Jurong West\")\n\n\n\n\n\n\n\n\n\n\n\n\nNow, we will convert these sf objects into owin objects that is required by spatstat.\n\n\nCode Chunk\npg_owin = as.owin(pg)\ntm_owin = as.owin(tm)\nck_owin = as.owin(ck)\njw_owin = as.owin(jw)\n\n\n\n\n\nBy using the code chunk below, we are able to extract childcare that is within the specific region to do our analysis later on.\n\n\nCode Chunk\nchildcare_pg_ppp = childcare_ppp_jit[pg_owin]\nchildcare_tm_ppp = childcare_ppp_jit[tm_owin]\nchildcare_ck_ppp = childcare_ppp_jit[ck_owin]\nchildcare_jw_ppp = childcare_ppp_jit[jw_owin]\n\n\nNext, rescale.ppp() function is used to trasnform the unit of measurement from metre to kilometre.\n\n\nCode Chunk\nchildcare_pg_ppp.km = rescale.ppp(childcare_pg_ppp, 1000, \"km\")\nchildcare_tm_ppp.km = rescale.ppp(childcare_tm_ppp, 1000, \"km\")\nchildcare_ck_ppp.km = rescale.ppp(childcare_ck_ppp, 1000, \"km\")\nchildcare_jw_ppp.km = rescale.ppp(childcare_jw_ppp, 1000, \"km\")\n\n\nThe code chunk below is used to plot these four study areas and the locations of the childcare centre.\n\n\nCode Chunk\npar(mfrow=c(2,2))\nplot(childcare_pg_ppp.km, main=\"Punggol\")\nplot(childcare_tm_ppp.km, main=\"Tampines\")\nplot(childcare_ck_ppp.km, main=\"Choa Chu Kang\")\nplot(childcare_jw_ppp.km, main=\"Jurong West\")\n\n\n\n\n\n\n\n\n\n\n\n\nThe code chunk below will be used to compute the KDE of these four planning area. bw.diggle method is used to derive the bandwidth of each.\n\n\nCode Chunk\npar(mfrow=c(2,2))\nplot(density(childcare_pg_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tempines\")\nplot(density(childcare_ck_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Choa Chu Kang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"JUrong West\")\n\n\n\n\n\n\n\n\n\n\n\n\nFor comparison purposes, we will use 250m as the bandwidth.\n\n\nCode Chunk\npar(mfrow=c(2,2))\nplot(density(childcare_ck_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Chou Chu Kang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"JUrong West\")\nplot(density(childcare_pg_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tampines\")"
  },
  {
    "objectID": "HandsOnExercise/HandsOn2a/HandsOn2a.html#nearest-neighbour-analysis",
    "href": "HandsOnExercise/HandsOn2a/HandsOn2a.html#nearest-neighbour-analysis",
    "title": "Hands-On Exercise 2a",
    "section": "",
    "text": "In this section, we will perform the Clark-Evans test of aggregation for a spatial point pattern by using clarkevans.test() of statspat.\nThe test hypotheses are:\nHo = The distribution of childcare services are randomly distributed.\nH1= The distribution of childcare services are not randomly distributed.\nThe 95% confident interval will be used.\n\n\n\n\nCode Chunk\nclarkevans.test(childcareSG_ppp,\n                correction = \"none\",\n                clipregion=\"sg_owin\",\n                alterative = c(\"clustered\"),\n                nsim=99)\n\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcareSG_ppp\nR = 0.527, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided\n\n\n\n\n\nIn the code chunk below, clarkevans.test() of spatstat is used to performs Clark-Evans test of aggregation for childcare centre in Choa Chu Kang planning area.\n\n\nCode Chunk\nclarkevans.test(childcare_ck_ppp,\n                correction=\"none\",\n                clipregion = NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_ck_ppp\nR = 0.92448, p-value = 0.2592\nalternative hypothesis: two-sided\n\n\n\n\n\nIn the code chunk below, the similar test is used to analyse the spatial point patterns of childcare centre in Tampines planning area.\n\n\nCode Chunk\nclarkevans.test(childcare_tm_ppp,\n                correction=\"none\",\n                clipregion = NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_tm_ppp\nR = 0.80096, p-value = 0.0003278\nalternative hypothesis: two-sided"
  },
  {
    "objectID": "HandsOnExercise/HandsOn2b/HandsOn2b.html",
    "href": "HandsOnExercise/HandsOn2b/HandsOn2b.html",
    "title": "Hands-On Exercise 2b",
    "section": "",
    "text": "Spatial Point Pattern Analysis is the evaluation of the pattern or distribution, of a set of points on a surface. The point can be location of:\n\nevents such as crime, traffic accident and disease onset, or\nbusiness services (coffee and fastfood outlets) or facilities such as childcare and eldercare.\n\nUsing appropriate functions of spatstat, this hands-on exercise aims to discover the spatial point processes of childecare centres in Singapore.\nProblem Statements:\n\nare the childcare centres in Singapore randomly distributed throughout the country?\nif the answer is not, then the next logical question is where are the locations with higher concentration of childcare centres?\n\n\n\n\nTo provide answers to the questions above, three data sets will be used. They are:\n\nCHILDCARE, a point feature data providing both location and attribute information of childcare centres. It was downloaded from Data.gov.sg and is in geojson format.\nMP14_SUBZONE_WEB_PL, a polygon feature data providing information of URA 2014 Master Plan Planning Subzone boundary data. It is in ESRI shapefile format. This data set was also downloaded from Data.gov.sg.\nCostalOutline, a polygon feature data showing the national boundary of Singapore. It is provided by SLA and is in ESRI shapefile format.\n\n\n\n\nIn this hands-on exercise, five R packages will be used, they are:\n\nsf, a relatively new R package specially designed to import, manage and process vector-based geospatial data in R.\nspatstat, which has a wide range of useful functions for point pattern analysis. In this hands-on exercise, it will be used to perform 1st- and 2nd-order spatial point patterns analysis and derive kernel density estimation (KDE) layer.\nraster which reads, writes, manipulates, analyses and model of gridded spatial data (i.e. raster). In this hands-on exercise, it will be used to convert image output generate by spatstat into raster format.\nmaptools which provides a set of tools for manipulating geographic data. In this hands-on exercise, we mainly use it to convert Spatial objects into ppp format of spatstat.\ntmap which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\n\nUse the code chunk below to install and launch the five R packages.\n\n\nCode Chunk\npacman::p_load(tmap, sf, raster, spatstat, tidyverse)\n\n\n\n\n\n\n\nIn this section, st_read() of sf package will be used to import these three geospatial data sets into R.\n\n\nCode Chunk\nchildcare_sf &lt;- st_read(\"data/child-care-services-geojson.geojson\") %&gt;%\n  st_transform(crs= 3414)\n\n\nReading layer `child-care-services-geojson' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/HandsOnExercise/HandsOn2b/data/child-care-services-geojson.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1545 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.248403 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\n\nCode Chunk\nsg_sf &lt;- st_read(dsn = \"data\", layer = \"CostalOutline\")\n\n\nReading layer `CostalOutline' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/HandsOnExercise/HandsOn2b/data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 60 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2663.926 ymin: 16357.98 xmax: 56047.79 ymax: 50244.03\nProjected CRS: SVY21\n\n\n\n\nCode Chunk\nmpsz_sf &lt;- st_read(dsn = \"data\", \n                   layer = \"MP14_SUBZONE_WEB_PL\")\n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/HandsOnExercise/HandsOn2b/data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nBefore we can use these data for analysis, it is important for us to ensure that they are projected in same projection system.\n\nDIY: Using the appropriate sf function you learned in Hands-on Exercise 2, retrieve the referencing system information of these geospatial data.\n\nNotice that except childcare_sf, both mpsz_sf and sg_sf do not have proper crs information.\n\nDIY: Using the method you learned in Lesson 2, assign the correct crs to mpsz_sf and sg_sf simple feature data frames.\n\n\n\nCode Chunk\nst_crs(mpsz_sf)\n\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\n\n\nCode Chunk\nmpsz3414 &lt;- st_set_crs(mpsz_sf, 3414)\n\n\n\n\nCode Chunk\nst_crs(mpsz3414)\n\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\n\nCode Chunk\nst_crs(sg_sf)\n\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\n\nDIY: If necessary, changing the referencing system to Singapore national projected coordinate system.\n\n\n\n\nAfter checking the referencing system of each geospatial data data frame, it is also useful for us to plot a map to show their spatial patterns.\n\nDIY: Using the mapping methods you learned in Hands-on Exercise 3, prepare a map as shown below.\n\n\n\nCode Chunk\nchildcare_sf &lt;- st_transform(childcare_sf, crs=3414)\n\n\n\n\nCode Chunk\ntmap_mode('view')\ntm_shape(childcare_sf)+\n  tm_dots()\n\n\n\n\n\n\n\n\nCode Chunk\ntmap_mode('plot')\n\n\nNotice that at the interactive mode, tmap is using leaflet for R API. The advantage of this interactive pin map is it allows us to navigate and zoom around the map freely. We can also query the information of each simple feature (i.e. the point) by clicking of them. Last but not least, you can also change the background of the internet map layer. Currently, three internet map layers are provided. They are: ESRI.WorldGrayCanvas, OpenStreetMap, and ESRI.WorldTopoMap. The default is ESRI.WorldGrayCanvas.\n\nReminder: Always remember to switch back to plot mode after the interactive map. This is because, each interactive mode will consume a connection. You should also avoid displaying ecessive numbers of interactive maps (i.e. not more than 10) in one RMarkdown document when publish on Netlify.\n\n\n\n\n\nAlthough simple feature data frame is gaining popularity again sp’s Spatial* classes, there are, however, many geospatial analysis packages require the input geospatial data in sp’s Spatial* classes. In this section, you will learn how to convert simple feature data frame to sp’s Spatial* class.\n\n\nNow, we will use as.ppp() function of spatstat to convert the spatial data into spatstat’s ppp object format.\n\n\nCode Chunk\nchildcare_ppp &lt;- as.ppp(childcare_sf)\nchildcare_ppp\n\n\nMarked planar point pattern: 1545 points\nmarks are of storage type  'character'\nwindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n\n\nNow, let us plot childcare_ppp and examine the different.\n\n\nCode Chunk\nplot(childcare_ppp)\n\n\n\n\n\n\n\n\n\nYou can take a quick look at the summary statistics of the newly created ppp object by using the code chunk below.\n\n\nCode Chunk\nsummary(childcare_ppp)\n\n\nMarked planar point pattern:  1545 points\nAverage intensity 1.91145e-06 points per square unit\n\nCoordinates are given to 11 decimal places\n\nmarks are of type 'character'\nSummary:\n   Length     Class      Mode \n     1545 character character \n\nWindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n                    (34200 x 23630 units)\nWindow area = 808287000 square units\n\n\nNotice the warning message about duplicates. In spatial point patterns analysis an issue of significant is the presence of duplicates. The statistical methodology used for spatial point patterns processes is based largely on the assumption that process are simple, that is, that the points cannot be coincident.\n\n\n\nWe can check the duplication in a ppp object by using the code chunk below.\n\n\nCode Chunk\nany(duplicated(childcare_ppp))\n\n\n[1] FALSE\n\n\nTo count the number of co-indicence point, we will use the multiplicity() function as shown in the code chunk below.\n\n\nCode Chunk\nsum(multiplicity(childcare_ppp) &gt; 1)\n\n\n[1] 0\n\n\nThe output shows that there are 128 duplicated point events.\nTo view the locations of these duplicate point events, we will plot childcare data by using the code chunk below.\n\n\nCode Chunk\ntmap_mode(\"view\")\ntm_shape(childcare_sf) +\n  tm_dots(alpha=0.4,\n          size =0.05)\n\n\n\n\n\n\n\n\nCode Chunk\ntmap_mode('plot')\n\n\nThere are three ways to overcome this problem. The easiest way is to delete the duplicates. But, that will also mean that some useful point events will be lost.\nThe second solution is use jittering, which will add a small perturbation to the duplicate points so that they do not occupy the exact same space.\nThe third solution is to make each point “unique” and then attach the duplicates of the points to the patterns as marks, as attributes of the points. Then you would need analytical techniques that take into account these marks.\nThe code chunk below implements the jittering approach.\n\n\nCode Chunk\nchildcare_ppp_jit &lt;- rjitter(childcare_ppp,\n                             retry=TRUE,\n                             nsim=1,\n                             drop=TRUE)\n\n\n\n\nCode Chunk\nany(duplicated(childcare_ppp_jit))\n\n\n[1] FALSE\n\n\n\n\n\nWhen analysing spatial point patterns, it is a good practice to confine the analysis with a geographical area like Singapore boundary. In spatstat, an object called owin is specially designed to represent this polygonal region.\nThe code chunk below is used to covert sg SpatialPolygon object into owin object of spatstat.\n\n\nCode Chunk\nsg_owin &lt;- as.owin(sg_sf)\n\n\nThe output object can be displayed by using plot() function.\n\n\nCode Chunk\nplot(sg_owin)\n\n\n\n\n\n\n\n\n\nand summary() function of Base R.\n\n\nCode Chunk\nsummary(sg_owin)\n\n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401\n\n\n\n\n\nIn this last step of geospatial data wrangling, we will extract childcare events that are located within Singapore by using the code chunk below.\n\n\nCode Chunk\nchildcareSG_ppp = childcare_ppp[sg_owin]\n\n\nThe output object combined both the point and polygon feature in one ppp object class as shown below.\n\n\nCode Chunk\nsummary(childcareSG_ppp)\n\n\nMarked planar point pattern:  1545 points\nAverage intensity 2.129929e-06 points per square unit\n\nCoordinates are given to 11 decimal places\n\nmarks are of type 'character'\nSummary:\n   Length     Class      Mode \n     1545 character character \n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401\n\n\n\n\nThe code chunk below will be used to extract the target planning areas.\n\n\nCode Chunk\npg &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"PUNGGOL\")\ntm &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"TAMPINES\")\nck &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"CHOA CHU KANG\")\njw &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"JURONG WEST\")\n\n\nPlotting target planning areas\n\n\nCode Chunk\npar(mfrow=c(2,2))\nplot(pg, main = \"Ponggol\")\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\nplot(tm, main=\"Tampines\")\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\nplot(tm,main=\"Tampines\")\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\nplot(ck, main = \"Choa Chu Kang\")\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\nplot(jw, main=\"Jurong West\")\n\n\n\n\n\n\n\n\n\n\n\n\nNow, we will convert these sf objects into owin objects that is required by spatstat.\n\n\nCode Chunk\npg_owin = as.owin(pg)\ntm_owin = as.owin(tm)\nck_owin = as.owin(ck)\njw_owin = as.owin(jw)\n\n\n\n\n\nBy using the code chunk below, we are able to extract childcare that is within the specific region to do our analysis later on.\n\n\nCode Chunk\nchildcare_pg_ppp = childcare_ppp_jit[pg_owin = as.owin(pg)]\nchildcare_tm_ppp = childcare_ppp_jit[tm_owin = as.owin(tm)]\nchildcare_ck_ppp = childcare_ppp_jit[ck_owin = as.owin(ck)]\nchildcare_jw_ppp = childcare_ppp_jit[jw_owin = as.owin(jw)]\n\n\nNext, rescale() function is used to trasnform the unit of measurement from metre to kilometre.\n\n\nCode Chunk\nchildcare_pg_ppp.km = rescale(childcare_pg_ppp, 1000, \"km\")\nchildcare_tm_ppp.km = rescale(childcare_tm_ppp, 1000, \"km\")\nchildcare_ck_ppp.km = rescale(childcare_ck_ppp, 1000, \"km\")\nchildcare_jw_ppp.km = rescale(childcare_jw_ppp, 1000, \"km\")\n\n\nThe code chunk below is used to plot these four study areas and the locations of the childcare centres.\n\n\nCode Chunk\npar(mfrow=c(2,2))\nplot(childcare_pg_ppp.km, main=\"Punggol\")\nplot(childcare_tm_ppp.km, main=\"Tampines\")\nplot(childcare_ck_ppp.km, main=\"Choa Chu Kang\")\nplot(childcare_jw_ppp.km, main=\"Jurong West\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe G function measures the distribution of the distances from an arbitrary event to its nearest event. In this section, you will learn how to compute G-function estimation by using Gest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\n\n\n\n\nThe code chunk below is used to compute G-function using Gest() of spatat package.\n\n\nCode Chunk\nG_CK = Gest(childcare_ck_ppp, correction = \"border\")\nplot(G_CK, xlim=c(0,500))\n\n\n\n\n\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nMonte Carlo test with G-fucntion\n\n\nCode Chunk\nG_CK.csr &lt;- envelope(childcare_ck_ppp, Gest, nsim = 999)\n\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\n\nCode Chunk\nplot(G_CK.csr)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\nG_tm = Gest(childcare_tm_ppp, correction = \"best\")\nplot(G_tm)\n\n\n\n\n\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected is p-value is smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\n\nCode Chunk\nG_tm.csr &lt;- envelope(childcare_tm_ppp, Gest, correction = \"all\", nsim = 999)\n\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\n\nCode Chunk\nplot(G_tm.csr)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe F function estimates the empty space function F(r) or its hazard rate h(r) from a point pattern in a window of arbitrary shape. In this section, you will learn how to compute F-function estimation by using Fest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\n\n\n\n\nThe code chunk below is used to compute F-function using Fest() of spatat package.\n\n\nCode Chunk\nF_CK = Fest(childcare_ck_ppp)\nplot(F_CK)\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nMonte Carlo test with F-fucntion\n\n\nCode Chunk\nF_CK.csr &lt;- envelope(childcare_ck_ppp, Fest, nsim = 999)\n\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\n\nCode Chunk\nplot(F_CK.csr)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonte Carlo test with F-fucntion\n\n\nCode Chunk\nF_tm = Fest(childcare_tm_ppp, correction = \"best\")\nplot(F_tm)\n\n\n\n\n\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected is p-value is smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\n\nCode Chunk\nF_tm.csr &lt;- envelope(childcare_tm_ppp, Fest, correction = \"all\", nsim = 999)\n\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\n\nCode Chunk\nplot(F_tm.csr)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nK-function measures the number of events found up to a given distance of any particular event. In this section, you will learn how to compute K-function estimates by using Kest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\n\n\n\n\n\n\nCode Chunk\nK_ck = Kest(childcare_ck_ppp, correction = \"Ripley\")\nplot(K_ck, . -r ~ r, ylab= \"K(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\n\nCode Chunk\nK_ck.csr &lt;- envelope(childcare_ck_ppp, Kest, nsim = 99, rank = 1, glocal=TRUE)\n\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\n\nCode Chunk\nplot(K_ck.csr, . - r ~ r, xlab=\"d\", ylab=\"K(d)-r\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\nK_tm = Kest(childcare_tm_ppp, correction = \"Ripley\")\nplot(K_tm, . -r ~ r, \n     ylab= \"K(d)-r\", xlab = \"d(m)\", \n     xlim=c(0,1000))\n\n\n\n\n\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\n\nCode Chunk\nK_tm.csr &lt;- envelope(childcare_tm_ppp, Kest, nsim = 99, rank = 1, glocal=TRUE)\n\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\n\nCode Chunk\nplot(K_tm.csr, . - r ~ r, \n     xlab=\"d\", ylab=\"K(d)-r\", xlim=c(0,500))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this section, you will learn how to compute L-function estimation by using Lest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\n\n\n\n\n\n\nCode Chunk\nL_ck = Lest(childcare_ck_ppp, correction = \"Ripley\")\nplot(L_ck, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value if smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\n\nCode Chunk\nL_ck.csr &lt;- envelope(childcare_ck_ppp, Lest, nsim = 99, rank = 1, glocal=TRUE)\n\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\n\nCode Chunk\nplot(L_ck.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\nL_ck.csr = Lest(childcare_tm_ppp, correction = \"Ripley\")\nplot(L_ck.csr, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\", \n     xlim=c(0,1000))\n\n\n\n\n\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nThe code chunk below will be used to perform the hypothesis testing.\n\n\nCode Chunk\nL_tm.csr &lt;- envelope(childcare_tm_ppp, Lest, nsim = 99, rank = 1, glocal=TRUE)\n\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\nThen, plot the model output by using the code chunk below.\n\n\nCode Chunk\nplot(L_tm.csr, . - r ~ r, \n     xlab=\"d\", ylab=\"L(d)-r\", xlim=c(0,500))"
  },
  {
    "objectID": "HandsOnExercise/HandsOn2b/HandsOn2b.html#overview",
    "href": "HandsOnExercise/HandsOn2b/HandsOn2b.html#overview",
    "title": "Hands-On Exercise 2b",
    "section": "",
    "text": "Spatial Point Pattern Analysis is the evaluation of the pattern or distribution, of a set of points on a surface. The point can be location of:\n\nevents such as crime, traffic accident and disease onset, or\nbusiness services (coffee and fastfood outlets) or facilities such as childcare and eldercare.\n\nUsing appropriate functions of spatstat, this hands-on exercise aims to discover the spatial point processes of childecare centres in Singapore.\nProblem Statements:\n\nare the childcare centres in Singapore randomly distributed throughout the country?\nif the answer is not, then the next logical question is where are the locations with higher concentration of childcare centres?"
  },
  {
    "objectID": "HandsOnExercise/HandsOn2b/HandsOn2b.html#data",
    "href": "HandsOnExercise/HandsOn2b/HandsOn2b.html#data",
    "title": "Hands-On Exercise 2b",
    "section": "",
    "text": "To provide answers to the questions above, three data sets will be used. They are:\n\nCHILDCARE, a point feature data providing both location and attribute information of childcare centres. It was downloaded from Data.gov.sg and is in geojson format.\nMP14_SUBZONE_WEB_PL, a polygon feature data providing information of URA 2014 Master Plan Planning Subzone boundary data. It is in ESRI shapefile format. This data set was also downloaded from Data.gov.sg.\nCostalOutline, a polygon feature data showing the national boundary of Singapore. It is provided by SLA and is in ESRI shapefile format."
  },
  {
    "objectID": "HandsOnExercise/HandsOn2b/HandsOn2b.html#installing-and-loading-the-r-packages",
    "href": "HandsOnExercise/HandsOn2b/HandsOn2b.html#installing-and-loading-the-r-packages",
    "title": "Hands-On Exercise 2b",
    "section": "",
    "text": "In this hands-on exercise, five R packages will be used, they are:\n\nsf, a relatively new R package specially designed to import, manage and process vector-based geospatial data in R.\nspatstat, which has a wide range of useful functions for point pattern analysis. In this hands-on exercise, it will be used to perform 1st- and 2nd-order spatial point patterns analysis and derive kernel density estimation (KDE) layer.\nraster which reads, writes, manipulates, analyses and model of gridded spatial data (i.e. raster). In this hands-on exercise, it will be used to convert image output generate by spatstat into raster format.\nmaptools which provides a set of tools for manipulating geographic data. In this hands-on exercise, we mainly use it to convert Spatial objects into ppp format of spatstat.\ntmap which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\n\nUse the code chunk below to install and launch the five R packages.\n\n\nCode Chunk\npacman::p_load(tmap, sf, raster, spatstat, tidyverse)"
  },
  {
    "objectID": "HandsOnExercise/HandsOn2b/HandsOn2b.html#spatial-data-wrangling",
    "href": "HandsOnExercise/HandsOn2b/HandsOn2b.html#spatial-data-wrangling",
    "title": "Hands-On Exercise 2b",
    "section": "",
    "text": "In this section, st_read() of sf package will be used to import these three geospatial data sets into R.\n\n\nCode Chunk\nchildcare_sf &lt;- st_read(\"data/child-care-services-geojson.geojson\") %&gt;%\n  st_transform(crs= 3414)\n\n\nReading layer `child-care-services-geojson' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/HandsOnExercise/HandsOn2b/data/child-care-services-geojson.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1545 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.248403 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\n\nCode Chunk\nsg_sf &lt;- st_read(dsn = \"data\", layer = \"CostalOutline\")\n\n\nReading layer `CostalOutline' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/HandsOnExercise/HandsOn2b/data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 60 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2663.926 ymin: 16357.98 xmax: 56047.79 ymax: 50244.03\nProjected CRS: SVY21\n\n\n\n\nCode Chunk\nmpsz_sf &lt;- st_read(dsn = \"data\", \n                   layer = \"MP14_SUBZONE_WEB_PL\")\n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/HandsOnExercise/HandsOn2b/data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nBefore we can use these data for analysis, it is important for us to ensure that they are projected in same projection system.\n\nDIY: Using the appropriate sf function you learned in Hands-on Exercise 2, retrieve the referencing system information of these geospatial data.\n\nNotice that except childcare_sf, both mpsz_sf and sg_sf do not have proper crs information.\n\nDIY: Using the method you learned in Lesson 2, assign the correct crs to mpsz_sf and sg_sf simple feature data frames.\n\n\n\nCode Chunk\nst_crs(mpsz_sf)\n\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\n\n\nCode Chunk\nmpsz3414 &lt;- st_set_crs(mpsz_sf, 3414)\n\n\n\n\nCode Chunk\nst_crs(mpsz3414)\n\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\n\nCode Chunk\nst_crs(sg_sf)\n\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\n\nDIY: If necessary, changing the referencing system to Singapore national projected coordinate system.\n\n\n\n\nAfter checking the referencing system of each geospatial data data frame, it is also useful for us to plot a map to show their spatial patterns.\n\nDIY: Using the mapping methods you learned in Hands-on Exercise 3, prepare a map as shown below.\n\n\n\nCode Chunk\nchildcare_sf &lt;- st_transform(childcare_sf, crs=3414)\n\n\n\n\nCode Chunk\ntmap_mode('view')\ntm_shape(childcare_sf)+\n  tm_dots()\n\n\n\n\n\n\n\n\nCode Chunk\ntmap_mode('plot')\n\n\nNotice that at the interactive mode, tmap is using leaflet for R API. The advantage of this interactive pin map is it allows us to navigate and zoom around the map freely. We can also query the information of each simple feature (i.e. the point) by clicking of them. Last but not least, you can also change the background of the internet map layer. Currently, three internet map layers are provided. They are: ESRI.WorldGrayCanvas, OpenStreetMap, and ESRI.WorldTopoMap. The default is ESRI.WorldGrayCanvas.\n\nReminder: Always remember to switch back to plot mode after the interactive map. This is because, each interactive mode will consume a connection. You should also avoid displaying ecessive numbers of interactive maps (i.e. not more than 10) in one RMarkdown document when publish on Netlify."
  },
  {
    "objectID": "HandsOnExercise/HandsOn2b/HandsOn2b.html#geospatial-data-wrangling",
    "href": "HandsOnExercise/HandsOn2b/HandsOn2b.html#geospatial-data-wrangling",
    "title": "Hands-On Exercise 2b",
    "section": "",
    "text": "Although simple feature data frame is gaining popularity again sp’s Spatial* classes, there are, however, many geospatial analysis packages require the input geospatial data in sp’s Spatial* classes. In this section, you will learn how to convert simple feature data frame to sp’s Spatial* class.\n\n\nNow, we will use as.ppp() function of spatstat to convert the spatial data into spatstat’s ppp object format.\n\n\nCode Chunk\nchildcare_ppp &lt;- as.ppp(childcare_sf)\nchildcare_ppp\n\n\nMarked planar point pattern: 1545 points\nmarks are of storage type  'character'\nwindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n\n\nNow, let us plot childcare_ppp and examine the different.\n\n\nCode Chunk\nplot(childcare_ppp)\n\n\n\n\n\n\n\n\n\nYou can take a quick look at the summary statistics of the newly created ppp object by using the code chunk below.\n\n\nCode Chunk\nsummary(childcare_ppp)\n\n\nMarked planar point pattern:  1545 points\nAverage intensity 1.91145e-06 points per square unit\n\nCoordinates are given to 11 decimal places\n\nmarks are of type 'character'\nSummary:\n   Length     Class      Mode \n     1545 character character \n\nWindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n                    (34200 x 23630 units)\nWindow area = 808287000 square units\n\n\nNotice the warning message about duplicates. In spatial point patterns analysis an issue of significant is the presence of duplicates. The statistical methodology used for spatial point patterns processes is based largely on the assumption that process are simple, that is, that the points cannot be coincident.\n\n\n\nWe can check the duplication in a ppp object by using the code chunk below.\n\n\nCode Chunk\nany(duplicated(childcare_ppp))\n\n\n[1] FALSE\n\n\nTo count the number of co-indicence point, we will use the multiplicity() function as shown in the code chunk below.\n\n\nCode Chunk\nsum(multiplicity(childcare_ppp) &gt; 1)\n\n\n[1] 0\n\n\nThe output shows that there are 128 duplicated point events.\nTo view the locations of these duplicate point events, we will plot childcare data by using the code chunk below.\n\n\nCode Chunk\ntmap_mode(\"view\")\ntm_shape(childcare_sf) +\n  tm_dots(alpha=0.4,\n          size =0.05)\n\n\n\n\n\n\n\n\nCode Chunk\ntmap_mode('plot')\n\n\nThere are three ways to overcome this problem. The easiest way is to delete the duplicates. But, that will also mean that some useful point events will be lost.\nThe second solution is use jittering, which will add a small perturbation to the duplicate points so that they do not occupy the exact same space.\nThe third solution is to make each point “unique” and then attach the duplicates of the points to the patterns as marks, as attributes of the points. Then you would need analytical techniques that take into account these marks.\nThe code chunk below implements the jittering approach.\n\n\nCode Chunk\nchildcare_ppp_jit &lt;- rjitter(childcare_ppp,\n                             retry=TRUE,\n                             nsim=1,\n                             drop=TRUE)\n\n\n\n\nCode Chunk\nany(duplicated(childcare_ppp_jit))\n\n\n[1] FALSE\n\n\n\n\n\nWhen analysing spatial point patterns, it is a good practice to confine the analysis with a geographical area like Singapore boundary. In spatstat, an object called owin is specially designed to represent this polygonal region.\nThe code chunk below is used to covert sg SpatialPolygon object into owin object of spatstat.\n\n\nCode Chunk\nsg_owin &lt;- as.owin(sg_sf)\n\n\nThe output object can be displayed by using plot() function.\n\n\nCode Chunk\nplot(sg_owin)\n\n\n\n\n\n\n\n\n\nand summary() function of Base R.\n\n\nCode Chunk\nsummary(sg_owin)\n\n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401\n\n\n\n\n\nIn this last step of geospatial data wrangling, we will extract childcare events that are located within Singapore by using the code chunk below.\n\n\nCode Chunk\nchildcareSG_ppp = childcare_ppp[sg_owin]\n\n\nThe output object combined both the point and polygon feature in one ppp object class as shown below.\n\n\nCode Chunk\nsummary(childcareSG_ppp)\n\n\nMarked planar point pattern:  1545 points\nAverage intensity 2.129929e-06 points per square unit\n\nCoordinates are given to 11 decimal places\n\nmarks are of type 'character'\nSummary:\n   Length     Class      Mode \n     1545 character character \n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401\n\n\n\n\nThe code chunk below will be used to extract the target planning areas.\n\n\nCode Chunk\npg &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"PUNGGOL\")\ntm &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"TAMPINES\")\nck &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"CHOA CHU KANG\")\njw &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"JURONG WEST\")\n\n\nPlotting target planning areas\n\n\nCode Chunk\npar(mfrow=c(2,2))\nplot(pg, main = \"Ponggol\")\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\nplot(tm, main=\"Tampines\")\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\nplot(tm,main=\"Tampines\")\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\nplot(ck, main = \"Choa Chu Kang\")\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\nplot(jw, main=\"Jurong West\")\n\n\n\n\n\n\n\n\n\n\n\n\nNow, we will convert these sf objects into owin objects that is required by spatstat.\n\n\nCode Chunk\npg_owin = as.owin(pg)\ntm_owin = as.owin(tm)\nck_owin = as.owin(ck)\njw_owin = as.owin(jw)\n\n\n\n\n\nBy using the code chunk below, we are able to extract childcare that is within the specific region to do our analysis later on.\n\n\nCode Chunk\nchildcare_pg_ppp = childcare_ppp_jit[pg_owin = as.owin(pg)]\nchildcare_tm_ppp = childcare_ppp_jit[tm_owin = as.owin(tm)]\nchildcare_ck_ppp = childcare_ppp_jit[ck_owin = as.owin(ck)]\nchildcare_jw_ppp = childcare_ppp_jit[jw_owin = as.owin(jw)]\n\n\nNext, rescale() function is used to trasnform the unit of measurement from metre to kilometre.\n\n\nCode Chunk\nchildcare_pg_ppp.km = rescale(childcare_pg_ppp, 1000, \"km\")\nchildcare_tm_ppp.km = rescale(childcare_tm_ppp, 1000, \"km\")\nchildcare_ck_ppp.km = rescale(childcare_ck_ppp, 1000, \"km\")\nchildcare_jw_ppp.km = rescale(childcare_jw_ppp, 1000, \"km\")\n\n\nThe code chunk below is used to plot these four study areas and the locations of the childcare centres.\n\n\nCode Chunk\npar(mfrow=c(2,2))\nplot(childcare_pg_ppp.km, main=\"Punggol\")\nplot(childcare_tm_ppp.km, main=\"Tampines\")\nplot(childcare_ck_ppp.km, main=\"Choa Chu Kang\")\nplot(childcare_jw_ppp.km, main=\"Jurong West\")"
  },
  {
    "objectID": "HandsOnExercise/HandsOn2b/HandsOn2b.html#analysing-spatial-point-process-using-g-function",
    "href": "HandsOnExercise/HandsOn2b/HandsOn2b.html#analysing-spatial-point-process-using-g-function",
    "title": "Hands-On Exercise 2b",
    "section": "",
    "text": "The G function measures the distribution of the distances from an arbitrary event to its nearest event. In this section, you will learn how to compute G-function estimation by using Gest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\n\n\n\n\nThe code chunk below is used to compute G-function using Gest() of spatat package.\n\n\nCode Chunk\nG_CK = Gest(childcare_ck_ppp, correction = \"border\")\nplot(G_CK, xlim=c(0,500))\n\n\n\n\n\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nMonte Carlo test with G-fucntion\n\n\nCode Chunk\nG_CK.csr &lt;- envelope(childcare_ck_ppp, Gest, nsim = 999)\n\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\n\nCode Chunk\nplot(G_CK.csr)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\nG_tm = Gest(childcare_tm_ppp, correction = \"best\")\nplot(G_tm)\n\n\n\n\n\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected is p-value is smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\n\nCode Chunk\nG_tm.csr &lt;- envelope(childcare_tm_ppp, Gest, correction = \"all\", nsim = 999)\n\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\n\nCode Chunk\nplot(G_tm.csr)"
  },
  {
    "objectID": "HandsOnExercise/HandsOn2b/HandsOn2b.html#analysing-spatial-point-process-using-f-function",
    "href": "HandsOnExercise/HandsOn2b/HandsOn2b.html#analysing-spatial-point-process-using-f-function",
    "title": "Hands-On Exercise 2b",
    "section": "",
    "text": "The F function estimates the empty space function F(r) or its hazard rate h(r) from a point pattern in a window of arbitrary shape. In this section, you will learn how to compute F-function estimation by using Fest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\n\n\n\n\nThe code chunk below is used to compute F-function using Fest() of spatat package.\n\n\nCode Chunk\nF_CK = Fest(childcare_ck_ppp)\nplot(F_CK)\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nMonte Carlo test with F-fucntion\n\n\nCode Chunk\nF_CK.csr &lt;- envelope(childcare_ck_ppp, Fest, nsim = 999)\n\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\n\nCode Chunk\nplot(F_CK.csr)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonte Carlo test with F-fucntion\n\n\nCode Chunk\nF_tm = Fest(childcare_tm_ppp, correction = \"best\")\nplot(F_tm)\n\n\n\n\n\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected is p-value is smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\n\nCode Chunk\nF_tm.csr &lt;- envelope(childcare_tm_ppp, Fest, correction = \"all\", nsim = 999)\n\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\n\nCode Chunk\nplot(F_tm.csr)"
  },
  {
    "objectID": "HandsOnExercise/HandsOn2b/HandsOn2b.html#analysing-spatial-point-process-using-k-function",
    "href": "HandsOnExercise/HandsOn2b/HandsOn2b.html#analysing-spatial-point-process-using-k-function",
    "title": "Hands-On Exercise 2b",
    "section": "",
    "text": "K-function measures the number of events found up to a given distance of any particular event. In this section, you will learn how to compute K-function estimates by using Kest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\n\n\n\n\n\n\nCode Chunk\nK_ck = Kest(childcare_ck_ppp, correction = \"Ripley\")\nplot(K_ck, . -r ~ r, ylab= \"K(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\n\nCode Chunk\nK_ck.csr &lt;- envelope(childcare_ck_ppp, Kest, nsim = 99, rank = 1, glocal=TRUE)\n\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\n\nCode Chunk\nplot(K_ck.csr, . - r ~ r, xlab=\"d\", ylab=\"K(d)-r\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\nK_tm = Kest(childcare_tm_ppp, correction = \"Ripley\")\nplot(K_tm, . -r ~ r, \n     ylab= \"K(d)-r\", xlab = \"d(m)\", \n     xlim=c(0,1000))\n\n\n\n\n\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\n\nCode Chunk\nK_tm.csr &lt;- envelope(childcare_tm_ppp, Kest, nsim = 99, rank = 1, glocal=TRUE)\n\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\n\nCode Chunk\nplot(K_tm.csr, . - r ~ r, \n     xlab=\"d\", ylab=\"K(d)-r\", xlim=c(0,500))"
  },
  {
    "objectID": "HandsOnExercise/HandsOn2b/HandsOn2b.html#analysing-spatial-point-process-using-l-function",
    "href": "HandsOnExercise/HandsOn2b/HandsOn2b.html#analysing-spatial-point-process-using-l-function",
    "title": "Hands-On Exercise 2b",
    "section": "",
    "text": "In this section, you will learn how to compute L-function estimation by using Lest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\n\n\n\n\n\n\nCode Chunk\nL_ck = Lest(childcare_ck_ppp, correction = \"Ripley\")\nplot(L_ck, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value if smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\n\nCode Chunk\nL_ck.csr &lt;- envelope(childcare_ck_ppp, Lest, nsim = 99, rank = 1, glocal=TRUE)\n\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\n\nCode Chunk\nplot(L_ck.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\nL_ck.csr = Lest(childcare_tm_ppp, correction = \"Ripley\")\nplot(L_ck.csr, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\", \n     xlim=c(0,1000))\n\n\n\n\n\n\n\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nThe code chunk below will be used to perform the hypothesis testing.\n\n\nCode Chunk\nL_tm.csr &lt;- envelope(childcare_tm_ppp, Lest, nsim = 99, rank = 1, glocal=TRUE)\n\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\nThen, plot the model output by using the code chunk below.\n\n\nCode Chunk\nplot(L_tm.csr, . - r ~ r, \n     xlab=\"d\", ylab=\"L(d)-r\", xlim=c(0,500))"
  },
  {
    "objectID": "InClassExercise/InClass2/InClass2.html",
    "href": "InClassExercise/InClass2/InClass2.html",
    "title": "In-Class Exercise 2",
    "section": "",
    "text": "In Class Exercise 2\nMaptools is retired and binary is removed from CRAN. If you would want to use it, you can retrieve from CRAN. However, R version must support the package.\n\n\nCode Chunk\ninstall.packages(\"maptools\",\n                 repos = \"https://packagemanager.posit.co/cran/2023-10-13\")\n\n\n\n\nCode Chunk\npacman::p_load()\n\n\nIssue 2: Creating coastal outline\nin sf package, there are two functions allow us to combine multiple simple features into one simple features. They are st_combine and st_union().\nsf: https://r-spatial.github.io/sf/\nWorking with st_union()\nThe code chunk below st_union() is used to derive the coastal outline sf tibble data.frame.\n\n\nCode Chunk\nsg_sf &lt;- mpsz_sf %&gt;%\n  st_union()\n\n\nsg_sf will look similar to the figure below.\n\n\nCode Chunk\nprint(sg_sf)\n\n\nas.ppp() allows to create point object and as.owin creates boundary.\n```{r}\n\n\n\nCode Chunk\npar(bg=\"#E4D5C9\")\n\ngridded_kde_childcareSG_ad &lt;- as(\n  kde_childcareSG_adaptive,\n  \"SpatialGridDataFrame\")\nspplot(gridded_kde_childcareSG_ad)\n\n\n\n\n\n\n\n\nMonte Carlo Simulation\n\n\n\nIn order to ensure reproducibility, it’s important to include the code chunk below before using spatstat functions involve Monte Carlo Simulation.\n\n\n\n\nCode Chunk\nset.seed(1234)\n\n\n\nensure km are m in PCS"
  },
  {
    "objectID": "HandsOnExercise/HandsOn3/HandsOn3.html",
    "href": "HandsOnExercise/HandsOn3/HandsOn3.html",
    "title": "Hands-On Exercise 3",
    "section": "",
    "text": "Network constrained Spatial Point Patterns Analysis (NetSPAA) is a collection of spatial point patterns analysis methods special developed for analysing spatial point event occurs on or alongside network. The spatial point event can be locations of traffic accident or childcare centre for example. The network, on the other hand can be a road network or river network.\nIn this hands-on exercise, you are going to gain hands-on experience on using appropriate functions of spNetwork package:\n\nto derive network kernel density estimation (NKDE), and\nto perform network G-function and k-function analysis\n\n\n\n\nIn this study, we will analyse the spatial distribution of childcare centre in Punggol planning area. For the purpose of this study, two geospatial data sets will be used. They are:\n\nPunggol_St, a line features geospatial data which store the road network within Punggol Planning Area.\nPunggol_CC, a point feature geospatial data which store the location of childcare centres within Punggol Planning Area.\n\nBoth data sets are in ESRI shapefile format.\n\n\n\nIn this hands-on exercise, four R packages will be used, they are:\n\nspNetwork, which provides functions to perform Spatial Point Patterns Analysis such as kernel density estimation (KDE) and K-function on network. It also can be used to build spatial matrices (‘listw’ objects like in ‘spdep’ package) to conduct any kind of traditional spatial analysis with spatial weights based on reticular distances.\nsf package provides functions to manage, processing, and manipulate Simple Features, a formal geospatial data standard that specifies a storage and access model of spatial geometries such as points, lines, and polygons.\ntmap which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\n\nUse the code chunk below to install and launch the four R packages.\n\n\nCode Chunk\npacman::p_load(sf, spNetwork, tmap, tidyverse)\n\n\n\n\n\nThe code chunk below uses st_read() of sf package to important Punggol_St and Punggol_CC geospatial data sets into RStudio as sf data frames.\n\n\nCode Chunk\nnetwork &lt;- st_read(dsn=\"data/geospatial\",\n                   layer=\"Punggol_St\")\n\n\nReading layer `Punggol_St' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/HandsOnExercise/HandsOn3/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 2642 features and 2 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 34038.56 ymin: 40941.11 xmax: 38882.85 ymax: 44801.27\nProjected CRS: SVY21 / Singapore TM\n\n\n\n\nCode Chunk\nchildcare &lt;- st_read(dsn=\"data/geospatial\",\n                      layer = \"Punggol_CC\")\n\n\nReading layer `Punggol_CC' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/HandsOnExercise/HandsOn3/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 61 features and 1 field\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 34423.98 ymin: 41503.6 xmax: 37619.47 ymax: 44685.77\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\n\n\n\n\nCode Chunk\nchildcare3414 &lt;- st_transform(childcare, \n                                crs = 3414)\n\n\n\n\nCode Chunk\nlist(childcare)\n\n\n[[1]]\nSimple feature collection with 61 features and 1 field\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 34423.98 ymin: 41503.6 xmax: 37619.47 ymax: 44685.77\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n      Name                      geometry\n1   kml_10 POINT Z (36173.81 42550.33 0)\n2   kml_99 POINT Z (36479.56 42405.21 0)\n3  kml_100 POINT Z (36618.72 41989.13 0)\n4  kml_101 POINT Z (36285.37 42261.42 0)\n5  kml_122  POINT Z (35414.54 42625.1 0)\n6  kml_161 POINT Z (36545.16 42580.09 0)\n7  kml_172 POINT Z (35289.44 44083.57 0)\n8  kml_188 POINT Z (36520.56 42844.74 0)\n9  kml_205  POINT Z (36924.01 41503.6 0)\n10 kml_222 POINT Z (37141.76 42326.36 0)\n\n\n\n\nCode Chunk\nchildcare_sf &lt;- st_as_sf(childcare,\n                          coords = c(\"longitutde\", \"latitude\"),\n                          crs = 4326) %&gt;%\n  st_transform(crs = 3414)\n\n\n\n\nIn geospatial analytics, it is very common for us to transform the original data from geographic coordinate system to projected coordinate system. This is because geographic coordinate system is not appropriate if the analysis need to use distance or/and area measurements.\n\n\n\n\n\n\nReferenced from Chapter 1.7.2 of R4GDSA\n\n\n\nhttps://r4gdsa.netlify.app/chap01.html\n\n\n\n\nCode Chunk\nnetwork3414 &lt;- st_set_crs(network, 3414)\n\n\n\n\nCode Chunk\nst_crs(network3414)\n\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\n\nCode Chunk\nplot(network3414)\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\nchildcare3414 &lt;- st_zm(childcare3414)\n\n\nWe can examine the structure of the output simple features data tables in RStudio. Alternative, code chunk below can be used to print the content of network and childcare simple features objects by using the code chunk below.\n\nChildcareNetwork\n\n\n\n\nCode Chunk\nchildcare\n\n\nSimple feature collection with 61 features and 1 field\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 34423.98 ymin: 41503.6 xmax: 37619.47 ymax: 44685.77\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n      Name                      geometry\n1   kml_10 POINT Z (36173.81 42550.33 0)\n2   kml_99 POINT Z (36479.56 42405.21 0)\n3  kml_100 POINT Z (36618.72 41989.13 0)\n4  kml_101 POINT Z (36285.37 42261.42 0)\n5  kml_122  POINT Z (35414.54 42625.1 0)\n6  kml_161 POINT Z (36545.16 42580.09 0)\n7  kml_172 POINT Z (35289.44 44083.57 0)\n8  kml_188 POINT Z (36520.56 42844.74 0)\n9  kml_205  POINT Z (36924.01 41503.6 0)\n10 kml_222 POINT Z (37141.76 42326.36 0)\n\n\n\n\n\n\nCode Chunk\nnetwork\n\n\nSimple feature collection with 2642 features and 2 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 34038.56 ymin: 40941.11 xmax: 38882.85 ymax: 44801.27\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n     LINK_ID                   ST_NAME                       geometry\n1  116130894                PUNGGOL RD LINESTRING (36546.89 44574....\n2  116130897 PONGGOL TWENTY-FOURTH AVE LINESTRING (36546.89 44574....\n3  116130901   PONGGOL SEVENTEENTH AVE LINESTRING (36012.73 44154....\n4  116130902   PONGGOL SEVENTEENTH AVE LINESTRING (36062.81 44197....\n5  116130907           PUNGGOL CENTRAL LINESTRING (36131.85 42755....\n6  116130908                PUNGGOL RD LINESTRING (36112.93 42752....\n7  116130909           PUNGGOL CENTRAL LINESTRING (36127.4 42744.5...\n8  116130910               PUNGGOL FLD LINESTRING (35994.98 42428....\n9  116130911               PUNGGOL FLD LINESTRING (35984.97 42407....\n10 116130912            EDGEFIELD PLNS LINESTRING (36200.87 42219....\n\n\n\n\n\nWhen exploring spNetwork’s functions, it came to my attention that spNetwork is expecting the geospatial data contains complete CRS information.\n\n\n\n\nBefore we jump into the analysis, it is a good practice to visualise the geospatial data. There are at least two ways to visualise the geospatial data. One way is by using plot() of Base R as shown in the code chunk below.\n\n\nCode Chunk\nhead(childcare, n=5)\n\n\nSimple feature collection with 5 features and 1 field\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 35414.54 ymin: 41989.13 xmax: 36618.72 ymax: 42625.1\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\n     Name                      geometry\n1  kml_10 POINT Z (36173.81 42550.33 0)\n2  kml_99 POINT Z (36479.56 42405.21 0)\n3 kml_100 POINT Z (36618.72 41989.13 0)\n4 kml_101 POINT Z (36285.37 42261.42 0)\n5 kml_122  POINT Z (35414.54 42625.1 0)\n\n\n\n\nCode Chunk\nplot(st_geometry(network))\nplot(childcare,add=T, col='red', pch=19)\n\n\n\n\n\n\n\n\n\nTo visualise the geospatial data with high cartographic quality and interactive manner, the mapping function of tmap package can be used as shown in the code chunk below.\n\n\nCode Chunk\ntmap_mode('view')\ntm_shape(childcare) +\n  tm_dots() +\n  tm_shape(network) +\n  tm_lines()\n\n\n\n\n\n\n\n\n\nIn this section, we will perform NKDE analysis by using appropriate functions provided in spNetwork package.\n\n\nBefore computing NKDE, the SpatialLines object need to be cut into lixels with a specified minimal distance. This task can be performed by using with lixelize_lines() of spNetwork as shown in the code chunk below.\n\n\nCode Chunk\nlixels &lt;- lixelize_lines(network,\n                         700,\n                         mindist = 375)\n\n\nWhat can we learned from the code chunk above:\n\nThe length of a lixel, lx_length is set to 700m, and\nThe minimum length of a lixel, mindist is set to 350m.\n\nAfter cut, if the length of the final lixel is shorter than the minimum distance, then it is added to the previous lixel. If NULL, then mindist = maxdist/10. Also note that the segments that are already shorter than the minimum distance are not modified\nNote: There is another function called lixelize_lines.mc() which provide multicore support.\n\n\n\nNext, lines_center() of spNetwork will be used to generate a SpatialPointsDataFrame (i.e. samples) with line centre points as shown in the code chunk below.\n\n\nCode Chunk\nsamples &lt;- lines_center(lixels)\n\n\nThe points are located at center of the line based on the length of the line.\n\n\n\nWe are ready to computer the NKDE by using the code chunk below.\n\n\nCode Chunk\ndensities &lt;- nkde(network3414,\n                  events= childcare3414,\n                  w = rep(1, nrow(childcare3414)),\n                  samples = samples,\n                  kernel_name = \"quartic\",\n                  bw = 300,\n                  div = \"bw\",\n                  method = \"simple\",\n                  digits = 1,\n                  tol = 1,\n                  grid_shape = c(1,1),\n                  max_depth = 8,\n                  agg = 5,\n                  sparse = TRUE,\n                  verbose = FALSE)\n\n\nWhat can we learn from the code chunk above?\n\nkernel_name argument indicates that quartic kernel is used. Are possible kernel methods supported by spNetwork are: triangle, gaussian, scaled gaussian, tricube, cosine ,triweight, epanechnikov or uniform.\nmethod argument indicates that simple method is used to calculate the NKDE. Currently, spNetwork support three popular methods, they are:\n\nmethod=“simple”. This first method was presented by Xie et al. (2008) and proposes an intuitive solution. The distances between events and sampling points are replaced by network distances, and the formula of the kernel is adapted to calculate the density over a linear unit instead of an areal unit.\nmethod=“discontinuous”. The method is proposed by Okabe et al (2008), which equally “divides” the mass density of an event at intersections of lixels.\nmethod=“continuous”. If the discontinuous method is unbiased, it leads to a discontinuous kernel function which is a bit counter-intuitive. Okabe et al (2008) proposed another version of the kernel, that divide the mass of the density at intersection but adjusts the density before the intersection to make the function continuous.\n\n\nThe user guide of spNetwork package provide a comprehensive discussion of nkde(). You should read them at least once to have a basic understanding of the various parameters that can be used to calibrate the NKDE model.\n\n\nBefore we can visualise the NKDE values, code chunk below will be used to insert the computed density values (i.e. densities) into samples and lixels objects as density field.\n\n\nCode Chunk\nsamples$density &lt;- densities\nlixels$density &lt;- densities\n\n\nSince svy21 projection system is in meter, the computed density values are very small i.e. 0.0000005. The code chunk below is used to resale the density values from number of events per meter to number of events per kilometer.\n\n\nCode Chunk\n# rescaling to help the mapping\nsamples$density &lt;- samples$density*1000\nlixels$density &lt;- lixels$density*1000\n\n\nThe code below uses appropriate functions of tmap package to prepare interactive and high cartographic quality map visualisation.\n\n\nCode Chunk\ntmap_mode('view')\ntm_shape(lixels)+\n  tm_lines(col=\"density\")+\ntm_shape(childcare)+\n  tm_dots()\ntmap_mode('plot')\n\n\nThe interactive map above effectively reveals road segments (darker color) with relatively higher density of childcare centres than road segments with relatively lower density of childcare centres (lighter color)\n\n\n\n\n\nIn this section, we are going to perform complete spatial randomness (CSR) test by using kfunctions() of spNetwork package. The null hypothesis is defined as:\nHo: The observed spatial point events (i.e distribution of childcare centres) are uniformly distributed over a street network in Punggol Planning Area.\nThe CSR test is based on the assumption of the binomial point process which implies the hypothesis that the childcare centres are randomly and independently distributed over the street network.\nIf this hypothesis is rejected, we may infer that the distribution of childcare centres are spatially interacting and dependent on each other; as a result, they may form nonrandom patterns.\n\n\nCode Chunk\nkfun_childcare &lt;- kfunctions(network3414, \n                             childcare,\n                             start = 0, \n                             end = 1000, \n                             step = 50, \n                             width = 50, \n                             nsim = 50, \n                             resolution = 50,\n                             verbose = FALSE, \n                             conf_int = 0.05)\n\n\nWhat can we learn from the code chunk above?\nThere are ten arguments used in the code chunk above they are:\n\nlines: A SpatialLinesDataFrame with the sampling points. The geometries must be a SpatialLinesDataFrame (may crash if some geometries are invalid).\npoints: A SpatialPointsDataFrame representing the points on the network. These points will be snapped on the network.\nstart: A double, the start value for evaluating the k and g functions.\nend: A double, the last value for evaluating the k and g functions.\nstep: A double, the jump between two evaluations of the k and g function.\nwidth: The width of each donut for the g-function.\nnsim: An integer indicating the number of Monte Carlo simulations required. In the above example, 50 simulation was performed. Note: most of the time, more simulations are required for inference\nresolution: When simulating random points on the network, selecting a resolution will reduce greatly the calculation time. When resolution is null the random points can occur everywhere on the graph. If a value is specified, the edges are split according to this value and the random points are selected vertices on the new network.\nconf_int: A double indicating the width confidence interval (default = 0.05).\n\nFor the usage of other arguments, you should refer to the user guide of spNetwork package.\nThe output of kfunctions() is a list with the following values:\n\nplotkA, a ggplot2 object representing the values of the k-function\nplotgA, a ggplot2 object representing the values of the g-function\nvaluesA, a DataFrame with the values used to build the plots\n\nFor example, we can visualise the ggplot2 object of k-function by using the code chunk below.\n\n\nCode Chunk\nkfun_childcare$plotk\n\n\n\n\n\n\n\n\n\n\n\n\n\nspNetwork: Spatial Analysis on Network\nNetwork Kernel Density Estimate\nDetails about NKDE\nNetwork k Functions"
  },
  {
    "objectID": "HandsOnExercise/HandsOn3/HandsOn3.html#overview",
    "href": "HandsOnExercise/HandsOn3/HandsOn3.html#overview",
    "title": "Hands-On Exercise 3",
    "section": "",
    "text": "Network constrained Spatial Point Patterns Analysis (NetSPAA) is a collection of spatial point patterns analysis methods special developed for analysing spatial point event occurs on or alongside network. The spatial point event can be locations of traffic accident or childcare centre for example. The network, on the other hand can be a road network or river network.\nIn this hands-on exercise, you are going to gain hands-on experience on using appropriate functions of spNetwork package:\n\nto derive network kernel density estimation (NKDE), and\nto perform network G-function and k-function analysis"
  },
  {
    "objectID": "HandsOnExercise/HandsOn3/HandsOn3.html#data",
    "href": "HandsOnExercise/HandsOn3/HandsOn3.html#data",
    "title": "Hands-On Exercise 3",
    "section": "",
    "text": "In this study, we will analyse the spatial distribution of childcare centre in Punggol planning area. For the purpose of this study, two geospatial data sets will be used. They are:\n\nPunggol_St, a line features geospatial data which store the road network within Punggol Planning Area.\nPunggol_CC, a point feature geospatial data which store the location of childcare centres within Punggol Planning Area.\n\nBoth data sets are in ESRI shapefile format."
  },
  {
    "objectID": "HandsOnExercise/HandsOn3/HandsOn3.html#installing-and-launching-the-r-packages",
    "href": "HandsOnExercise/HandsOn3/HandsOn3.html#installing-and-launching-the-r-packages",
    "title": "Hands-On Exercise 3",
    "section": "",
    "text": "In this hands-on exercise, four R packages will be used, they are:\n\nspNetwork, which provides functions to perform Spatial Point Patterns Analysis such as kernel density estimation (KDE) and K-function on network. It also can be used to build spatial matrices (‘listw’ objects like in ‘spdep’ package) to conduct any kind of traditional spatial analysis with spatial weights based on reticular distances.\nsf package provides functions to manage, processing, and manipulate Simple Features, a formal geospatial data standard that specifies a storage and access model of spatial geometries such as points, lines, and polygons.\ntmap which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\n\nUse the code chunk below to install and launch the four R packages.\n\n\nCode Chunk\npacman::p_load(sf, spNetwork, tmap, tidyverse)"
  },
  {
    "objectID": "HandsOnExercise/HandsOn3/HandsOn3.html#data-import-and-preparation",
    "href": "HandsOnExercise/HandsOn3/HandsOn3.html#data-import-and-preparation",
    "title": "Hands-On Exercise 3",
    "section": "",
    "text": "The code chunk below uses st_read() of sf package to important Punggol_St and Punggol_CC geospatial data sets into RStudio as sf data frames.\n\n\nCode Chunk\nnetwork &lt;- st_read(dsn=\"data/geospatial\",\n                   layer=\"Punggol_St\")\n\n\nReading layer `Punggol_St' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/HandsOnExercise/HandsOn3/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 2642 features and 2 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 34038.56 ymin: 40941.11 xmax: 38882.85 ymax: 44801.27\nProjected CRS: SVY21 / Singapore TM\n\n\n\n\nCode Chunk\nchildcare &lt;- st_read(dsn=\"data/geospatial\",\n                      layer = \"Punggol_CC\")\n\n\nReading layer `Punggol_CC' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/HandsOnExercise/HandsOn3/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 61 features and 1 field\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 34423.98 ymin: 41503.6 xmax: 37619.47 ymax: 44685.77\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\n\n\n\n\nCode Chunk\nchildcare3414 &lt;- st_transform(childcare, \n                                crs = 3414)\n\n\n\n\nCode Chunk\nlist(childcare)\n\n\n[[1]]\nSimple feature collection with 61 features and 1 field\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 34423.98 ymin: 41503.6 xmax: 37619.47 ymax: 44685.77\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n      Name                      geometry\n1   kml_10 POINT Z (36173.81 42550.33 0)\n2   kml_99 POINT Z (36479.56 42405.21 0)\n3  kml_100 POINT Z (36618.72 41989.13 0)\n4  kml_101 POINT Z (36285.37 42261.42 0)\n5  kml_122  POINT Z (35414.54 42625.1 0)\n6  kml_161 POINT Z (36545.16 42580.09 0)\n7  kml_172 POINT Z (35289.44 44083.57 0)\n8  kml_188 POINT Z (36520.56 42844.74 0)\n9  kml_205  POINT Z (36924.01 41503.6 0)\n10 kml_222 POINT Z (37141.76 42326.36 0)\n\n\n\n\nCode Chunk\nchildcare_sf &lt;- st_as_sf(childcare,\n                          coords = c(\"longitutde\", \"latitude\"),\n                          crs = 4326) %&gt;%\n  st_transform(crs = 3414)\n\n\n\n\nIn geospatial analytics, it is very common for us to transform the original data from geographic coordinate system to projected coordinate system. This is because geographic coordinate system is not appropriate if the analysis need to use distance or/and area measurements.\n\n\n\n\n\n\nReferenced from Chapter 1.7.2 of R4GDSA\n\n\n\nhttps://r4gdsa.netlify.app/chap01.html\n\n\n\n\nCode Chunk\nnetwork3414 &lt;- st_set_crs(network, 3414)\n\n\n\n\nCode Chunk\nst_crs(network3414)\n\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\n\nCode Chunk\nplot(network3414)\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\nchildcare3414 &lt;- st_zm(childcare3414)\n\n\nWe can examine the structure of the output simple features data tables in RStudio. Alternative, code chunk below can be used to print the content of network and childcare simple features objects by using the code chunk below.\n\nChildcareNetwork\n\n\n\n\nCode Chunk\nchildcare\n\n\nSimple feature collection with 61 features and 1 field\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 34423.98 ymin: 41503.6 xmax: 37619.47 ymax: 44685.77\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n      Name                      geometry\n1   kml_10 POINT Z (36173.81 42550.33 0)\n2   kml_99 POINT Z (36479.56 42405.21 0)\n3  kml_100 POINT Z (36618.72 41989.13 0)\n4  kml_101 POINT Z (36285.37 42261.42 0)\n5  kml_122  POINT Z (35414.54 42625.1 0)\n6  kml_161 POINT Z (36545.16 42580.09 0)\n7  kml_172 POINT Z (35289.44 44083.57 0)\n8  kml_188 POINT Z (36520.56 42844.74 0)\n9  kml_205  POINT Z (36924.01 41503.6 0)\n10 kml_222 POINT Z (37141.76 42326.36 0)\n\n\n\n\n\n\nCode Chunk\nnetwork\n\n\nSimple feature collection with 2642 features and 2 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 34038.56 ymin: 40941.11 xmax: 38882.85 ymax: 44801.27\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n     LINK_ID                   ST_NAME                       geometry\n1  116130894                PUNGGOL RD LINESTRING (36546.89 44574....\n2  116130897 PONGGOL TWENTY-FOURTH AVE LINESTRING (36546.89 44574....\n3  116130901   PONGGOL SEVENTEENTH AVE LINESTRING (36012.73 44154....\n4  116130902   PONGGOL SEVENTEENTH AVE LINESTRING (36062.81 44197....\n5  116130907           PUNGGOL CENTRAL LINESTRING (36131.85 42755....\n6  116130908                PUNGGOL RD LINESTRING (36112.93 42752....\n7  116130909           PUNGGOL CENTRAL LINESTRING (36127.4 42744.5...\n8  116130910               PUNGGOL FLD LINESTRING (35994.98 42428....\n9  116130911               PUNGGOL FLD LINESTRING (35984.97 42407....\n10 116130912            EDGEFIELD PLNS LINESTRING (36200.87 42219....\n\n\n\n\n\nWhen exploring spNetwork’s functions, it came to my attention that spNetwork is expecting the geospatial data contains complete CRS information."
  },
  {
    "objectID": "HandsOnExercise/HandsOn3/HandsOn3.html#visualising-geospatial-data",
    "href": "HandsOnExercise/HandsOn3/HandsOn3.html#visualising-geospatial-data",
    "title": "Hands-On Exercise 3",
    "section": "",
    "text": "Before we jump into the analysis, it is a good practice to visualise the geospatial data. There are at least two ways to visualise the geospatial data. One way is by using plot() of Base R as shown in the code chunk below.\n\n\nCode Chunk\nhead(childcare, n=5)\n\n\nSimple feature collection with 5 features and 1 field\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 35414.54 ymin: 41989.13 xmax: 36618.72 ymax: 42625.1\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\n     Name                      geometry\n1  kml_10 POINT Z (36173.81 42550.33 0)\n2  kml_99 POINT Z (36479.56 42405.21 0)\n3 kml_100 POINT Z (36618.72 41989.13 0)\n4 kml_101 POINT Z (36285.37 42261.42 0)\n5 kml_122  POINT Z (35414.54 42625.1 0)\n\n\n\n\nCode Chunk\nplot(st_geometry(network))\nplot(childcare,add=T, col='red', pch=19)\n\n\n\n\n\n\n\n\n\nTo visualise the geospatial data with high cartographic quality and interactive manner, the mapping function of tmap package can be used as shown in the code chunk below.\n\n\nCode Chunk\ntmap_mode('view')\ntm_shape(childcare) +\n  tm_dots() +\n  tm_shape(network) +\n  tm_lines()"
  },
  {
    "objectID": "HandsOnExercise/HandsOn3/HandsOn3.html#network-kde-nkde-analysis",
    "href": "HandsOnExercise/HandsOn3/HandsOn3.html#network-kde-nkde-analysis",
    "title": "Hands-On Exercise 3",
    "section": "",
    "text": "In this section, we will perform NKDE analysis by using appropriate functions provided in spNetwork package.\n\n\nBefore computing NKDE, the SpatialLines object need to be cut into lixels with a specified minimal distance. This task can be performed by using with lixelize_lines() of spNetwork as shown in the code chunk below.\n\n\nCode Chunk\nlixels &lt;- lixelize_lines(network,\n                         700,\n                         mindist = 375)\n\n\nWhat can we learned from the code chunk above:\n\nThe length of a lixel, lx_length is set to 700m, and\nThe minimum length of a lixel, mindist is set to 350m.\n\nAfter cut, if the length of the final lixel is shorter than the minimum distance, then it is added to the previous lixel. If NULL, then mindist = maxdist/10. Also note that the segments that are already shorter than the minimum distance are not modified\nNote: There is another function called lixelize_lines.mc() which provide multicore support.\n\n\n\nNext, lines_center() of spNetwork will be used to generate a SpatialPointsDataFrame (i.e. samples) with line centre points as shown in the code chunk below.\n\n\nCode Chunk\nsamples &lt;- lines_center(lixels)\n\n\nThe points are located at center of the line based on the length of the line.\n\n\n\nWe are ready to computer the NKDE by using the code chunk below.\n\n\nCode Chunk\ndensities &lt;- nkde(network3414,\n                  events= childcare3414,\n                  w = rep(1, nrow(childcare3414)),\n                  samples = samples,\n                  kernel_name = \"quartic\",\n                  bw = 300,\n                  div = \"bw\",\n                  method = \"simple\",\n                  digits = 1,\n                  tol = 1,\n                  grid_shape = c(1,1),\n                  max_depth = 8,\n                  agg = 5,\n                  sparse = TRUE,\n                  verbose = FALSE)\n\n\nWhat can we learn from the code chunk above?\n\nkernel_name argument indicates that quartic kernel is used. Are possible kernel methods supported by spNetwork are: triangle, gaussian, scaled gaussian, tricube, cosine ,triweight, epanechnikov or uniform.\nmethod argument indicates that simple method is used to calculate the NKDE. Currently, spNetwork support three popular methods, they are:\n\nmethod=“simple”. This first method was presented by Xie et al. (2008) and proposes an intuitive solution. The distances between events and sampling points are replaced by network distances, and the formula of the kernel is adapted to calculate the density over a linear unit instead of an areal unit.\nmethod=“discontinuous”. The method is proposed by Okabe et al (2008), which equally “divides” the mass density of an event at intersections of lixels.\nmethod=“continuous”. If the discontinuous method is unbiased, it leads to a discontinuous kernel function which is a bit counter-intuitive. Okabe et al (2008) proposed another version of the kernel, that divide the mass of the density at intersection but adjusts the density before the intersection to make the function continuous.\n\n\nThe user guide of spNetwork package provide a comprehensive discussion of nkde(). You should read them at least once to have a basic understanding of the various parameters that can be used to calibrate the NKDE model.\n\n\nBefore we can visualise the NKDE values, code chunk below will be used to insert the computed density values (i.e. densities) into samples and lixels objects as density field.\n\n\nCode Chunk\nsamples$density &lt;- densities\nlixels$density &lt;- densities\n\n\nSince svy21 projection system is in meter, the computed density values are very small i.e. 0.0000005. The code chunk below is used to resale the density values from number of events per meter to number of events per kilometer.\n\n\nCode Chunk\n# rescaling to help the mapping\nsamples$density &lt;- samples$density*1000\nlixels$density &lt;- lixels$density*1000\n\n\nThe code below uses appropriate functions of tmap package to prepare interactive and high cartographic quality map visualisation.\n\n\nCode Chunk\ntmap_mode('view')\ntm_shape(lixels)+\n  tm_lines(col=\"density\")+\ntm_shape(childcare)+\n  tm_dots()\ntmap_mode('plot')\n\n\nThe interactive map above effectively reveals road segments (darker color) with relatively higher density of childcare centres than road segments with relatively lower density of childcare centres (lighter color)"
  },
  {
    "objectID": "HandsOnExercise/HandsOn3/HandsOn3.html#network-constrained-g--and-k-function-analysis",
    "href": "HandsOnExercise/HandsOn3/HandsOn3.html#network-constrained-g--and-k-function-analysis",
    "title": "Hands-On Exercise 3",
    "section": "",
    "text": "In this section, we are going to perform complete spatial randomness (CSR) test by using kfunctions() of spNetwork package. The null hypothesis is defined as:\nHo: The observed spatial point events (i.e distribution of childcare centres) are uniformly distributed over a street network in Punggol Planning Area.\nThe CSR test is based on the assumption of the binomial point process which implies the hypothesis that the childcare centres are randomly and independently distributed over the street network.\nIf this hypothesis is rejected, we may infer that the distribution of childcare centres are spatially interacting and dependent on each other; as a result, they may form nonrandom patterns.\n\n\nCode Chunk\nkfun_childcare &lt;- kfunctions(network3414, \n                             childcare,\n                             start = 0, \n                             end = 1000, \n                             step = 50, \n                             width = 50, \n                             nsim = 50, \n                             resolution = 50,\n                             verbose = FALSE, \n                             conf_int = 0.05)\n\n\nWhat can we learn from the code chunk above?\nThere are ten arguments used in the code chunk above they are:\n\nlines: A SpatialLinesDataFrame with the sampling points. The geometries must be a SpatialLinesDataFrame (may crash if some geometries are invalid).\npoints: A SpatialPointsDataFrame representing the points on the network. These points will be snapped on the network.\nstart: A double, the start value for evaluating the k and g functions.\nend: A double, the last value for evaluating the k and g functions.\nstep: A double, the jump between two evaluations of the k and g function.\nwidth: The width of each donut for the g-function.\nnsim: An integer indicating the number of Monte Carlo simulations required. In the above example, 50 simulation was performed. Note: most of the time, more simulations are required for inference\nresolution: When simulating random points on the network, selecting a resolution will reduce greatly the calculation time. When resolution is null the random points can occur everywhere on the graph. If a value is specified, the edges are split according to this value and the random points are selected vertices on the new network.\nconf_int: A double indicating the width confidence interval (default = 0.05).\n\nFor the usage of other arguments, you should refer to the user guide of spNetwork package.\nThe output of kfunctions() is a list with the following values:\n\nplotkA, a ggplot2 object representing the values of the k-function\nplotgA, a ggplot2 object representing the values of the g-function\nvaluesA, a DataFrame with the values used to build the plots\n\nFor example, we can visualise the ggplot2 object of k-function by using the code chunk below.\n\n\nCode Chunk\nkfun_childcare$plotk"
  },
  {
    "objectID": "HandsOnExercise/HandsOn3/HandsOn3.html#references",
    "href": "HandsOnExercise/HandsOn3/HandsOn3.html#references",
    "title": "Hands-On Exercise 3",
    "section": "",
    "text": "spNetwork: Spatial Analysis on Network\nNetwork Kernel Density Estimate\nDetails about NKDE\nNetwork k Functions"
  },
  {
    "objectID": "TakeHomeExercise/TakeHome1/TakeHome1.html",
    "href": "TakeHomeExercise/TakeHome1/TakeHome1.html",
    "title": "Take-Home Exercise 1",
    "section": "",
    "text": "According to World Health Organisation (WHO), road traffic accidents cause the death of approximately 1.19 million people each year leave between 20 and 50 million people with non-fatal injuries. More than half of all road traffic deaths occur among vulnerable road users, such as pedestrians, cyclists and motorcyclists.\nRoad traffic injuries are the leading cause of death for children and young adults aged 5–29. Yet two thirds of road traffic fatalities occur among people of working age (18–59 years). Nine in 10 fatalities on the roads occur in low- and middle-income countries, even though these countries have around 60% of the world’s vehicles.\nIn addition to the human suffering caused by road traffic injuries, they also incur a heavy economic burden on victims and their families, both through treatment costs for the injured and through loss of productivity of those killed or disabled. More broadly, road traffic injuries have a serious impact on national economies, costing countries 3% of their annual gross domestic product.\nThailand’s roads are the deadliest in Southeast Asia and among the worst in the world, according to the World Health Organisation. About 20,000 people die in road accidents each year, or about 56 deaths a day (WHO).\nBetween 2014 and 2021, Thailand experienced a notable increase in accident frequencies. Specifically, 19% of all accidents in Thailand occurred on the national highways, which constituted the primary public thoroughfares connecting various regions, provinces, districts, and significant locations within a comprehensive network. Within the broader context of accidents across the country, there existed a considerable 66% likelihood of encountering accident-prone zones, often termed ‘black spots,’ distributed as follows: 66% on straight road segments, 13% at curves, 6% at median points of cross-shaped intersections, 5% at T-shaped intersections and Y-shaped intersections, 3% at cross-shaped intersections, 2% on bridges, and 2% on steep slopes, respectively.\n\n\n\nBy and large, road traffic accidents can be attributed by two major factors, namely: behavioural and environmental factors. Behavioural factors in driving are considered to be major causes of traffic accidents either in direct or indirect manner (Lewin, 1982). These factors can be further grouped into two as, driver behavior (also called driver/driving style) and driver performance, in other words, driver/driving skills (Elander, West, & French, 1993). Environmental factors, on the other hand, includes but not limited to weather condition such as poor visibility during heavy rain or foggy and road conditions such as sharp bend road, slippery slope road, and blind spot.\nPrevious studies have demonstrated the significant potential of Spatial Point Patterns Analysis (SPPA) in exploring and identifying factors influencing road traffic accidents. However, these studies often focus solely on either behavioral or environmental factors, with limited consideration of temporal factors such as season, day of the week, or time of day.\nIn view of this, you are tasked to discover factors affecting road traffic accidents in the Bangkok Metropolitan Region BMR by employing both spatial spatio-temporal point patterns analysis methods.\nThe specific objectives of this take-home exercise are as follows:\n\nTo visualize the spatio-temporal dynamics of road traffic accidents in BMR using appropriate statistical graphics and geovisualization methods.\nTo conduct detailed spatial analysis of road traffic accidents using appropriate Network Spatial Point Patterns Analysis methods.\nTo conduct detailed spatio-temporal analysis of road traffic accidents using appropriate Temporal Network Spatial Point Patterns Analysis methods.\n\n\n\n\nFor the purpose of this exercise, three basic data sets must be used:\n\nThailand Road Accident [2019-2022] on Kaggle\nThailand Roads (OpenStreetMap Export) on HDX.\nThailand - Subnational Administrative Boundaries on HDX.\n\n\n\nCode Chunk\npacman::p_load(tidyverse, sf, spatstat, ggplot2, ggmap, tmap, dplyr, lubridate, raster)\n\n\n\nPackages\n\n\nPackages\nFunction\n\n\n\n\nsf\nTo import, manage, and hande geospatial data\n\n\ntidyverse\nFor non-spatial data wrangling\n\n\nsfdep\nTo compute spatial weights, global and local spatial autocorrelation statistics\n\n\nspatstat\nFor analysing spatial points\n\n\nggplot2\nFor data divisualisation\n\n\nggmap\nRetrieve raster map tiles from online mapping services\n\n\ntmap\nCreating thematic maps\n\n\nlubridate\nFor robust date-time usage\n\n\nleaflet\nFor interactive maps\n\n\nknitr\nFor dynamic report generation\n\n\nraster\n\n\n\n\n\nRoad Traffic Accidents (RTA)Road NetworkAdministrative BoundariesImporting Aspatial & Geospatial Data\n\n\nThailand Road Traffic Accident Data [2019-2022] (from Kaggle): This contains the road accident records, including spatial (longitude/latitude) and temporal information (date and time).\n\n\nCode Chunk\nrta_sf &lt;- read_csv(\"data/aspatial/archive/thai_road_accident_2019_2022.csv\") %&gt;%\n  filter(!is.na(longitude) & longitude !='',\n        !is.na(latitude) & latitude != '') %&gt;%\n  st_as_sf(coords = c(\"longitude\", 'latitude'),\n           crs=4326) %&gt;% #WGS 84 - USE BY ALL GPS\n  st_transform(crs=32647) %&gt;% #EPSG \n  mutate(Month_num = month(incident_datetime)) %&gt;%\n  mutate(Month_fac = month(incident_datetime, #fac is factor\n                       label = TRUE, \n                       abbr = TRUE)) %&gt;% #CAN CHANGE TO MON TUES \n  mutate(dayofweek = day(incident_datetime))\n#PROJECTED PCS UTM IS 326, 47 NORTH \n\n\nSaving this geometry with corrected projection for plotting use.\n\n\nCode Chunk\nwrite_rds(rta_sf, \"data/rds/rta_sf.rds\")\n\n\n\n\n\nThailand Roads (OpenStreetMap Export) (from HDX): This will provide the road network to conduct network-based analysis.\n\n\n\nCode Chunk\nroadlines &lt;- st_read(dsn = \"data/geospatial/roadlines/\",\n                      layer = \"hotosm_tha_roads_lines_shp\")\n\n\nReading layer `hotosm_tha_roads_lines_shp' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/TakeHomeExercise/TakeHome1/data/geospatial/roadlines' \n  using driver `ESRI Shapefile'\nSimple feature collection with 2792590 features and 14 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 97.34457 ymin: 5.643645 xmax: 105.6528 ymax: 20.47168\nCRS:           NA\n\n\n\n\nThailand Subnational Administrative Boundaries (from HDX): These boundaries will help in restricting the analysis to the Bangkok Metropolitan Region 9BMR) and may also serve for regional analysis.\n\n\nCode Chunk\nthaiadmin &lt;- st_read(dsn = \"data/geospatial/tha_adm_rtsd_itos_20210121_shp/\",\n                      layer = \"tha_admbnda_adm1_rtsd_20220121\")\n\n\nReading layer `tha_admbnda_adm1_rtsd_20220121' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/TakeHomeExercise/TakeHome1/data/geospatial/tha_adm_rtsd_itos_20210121_shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing the function glimpse() in of dplyr in the tidyverse family, we are able to see the data type of each variable.\n\n\nCode Chunk\nglimpse(thaiadmin)\n\n\nRows: 77\nColumns: 17\n$ Shape_Leng &lt;dbl&gt; 2.417227, 1.695100, 1.251111, 1.884945, 3.041716, 1.739908,…\n$ Shape_Area &lt;dbl&gt; 0.13133873, 0.07926199, 0.05323766, 0.12698345, 0.21393797,…\n$ ADM1_EN    &lt;chr&gt; \"Bangkok\", \"Samut Prakan\", \"Nonthaburi\", \"Pathum Thani\", \"P…\n$ ADM1_TH    &lt;chr&gt; \"กรุงเทพมหานคร\", \"สมุทรปราการ\", \"นนทบุรี\", \"ปทุมธานี\", \"พระนครศรีอ…\n$ ADM1_PCODE &lt;chr&gt; \"TH10\", \"TH11\", \"TH12\", \"TH13\", \"TH14\", \"TH15\", \"TH16\", \"TH…\n$ ADM1_REF   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM1ALT1EN &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM1ALT2EN &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM1ALT1TH &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM1ALT2TH &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM0_EN    &lt;chr&gt; \"Thailand\", \"Thailand\", \"Thailand\", \"Thailand\", \"Thailand\",…\n$ ADM0_TH    &lt;chr&gt; \"ประเทศไทย\", \"ประเทศไทย\", \"ประเทศไทย\", \"ประเทศไทย\", \"ประเทศ…\n$ ADM0_PCODE &lt;chr&gt; \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\",…\n$ date       &lt;date&gt; 2019-02-18, 2019-02-18, 2019-02-18, 2019-02-18, 2019-02-18…\n$ validOn    &lt;date&gt; 2022-01-22, 2022-01-22, 2022-01-22, 2022-01-22, 2022-01-22…\n$ validTo    &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ geometry   &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((100.6139 13..., MULTIPOLYGON (…\n\n\n\n\nCode Chunk\nplot(thaiadmin)\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\nplot(st_geometry(thaiadmin))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\nst_crs(thaiadmin)\n\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\n\n\nCode Chunk\nthaiadmin32647 &lt;- st_transform(thaiadmin, crs = 32647)\n\n\n\n\nCode Chunk\nst_crs(thaiadmin32647)\n\n\nCoordinate Reference System:\n  User input: EPSG:32647 \n  wkt:\nPROJCRS[\"WGS 84 / UTM zone 47N\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"UTM zone 47N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",99,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"Between 96°E and 102°E, northern hemisphere between equator and 84°N, onshore and offshore. China. Indonesia. Laos. Malaysia - West Malaysia. Mongolia. Myanmar (Burma). Russian Federation. Thailand.\"],\n        BBOX[0,96,84,102]],\n    ID[\"EPSG\",32647]]\n\n\nCurrently, the EPSG code in thaiadmin is 32647.\n\n\n\n\n\nCode Chunk\nthaiadmin32647$\"acc_code\" &lt;- lengths(st_intersects(thaiadmin32647, rta_sf))\n\n\n\n\nCode Chunk\nsummary(thaiadmin32647$'acc_code')\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    140     503     822    1057    1165    6090 \n\n\nn_distinct() of the dyplr package counts the number of incidents.\n\n\nCode Chunk\nn_distinct(rta_sf$acc_code)\n\n\n[1] 81376\n\n\n\n\n\n\n\n\n\n\nThe output reflected that there are no duplicates in the data.\n\n\nCode Chunk\nduplicate &lt;- rta_sf %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\nduplicate\n\n\nSimple feature collection with 0 features and 19 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nProjected CRS: WGS 84 / UTM zone 47N\n# A tibble: 0 × 20\n# ℹ 20 variables: acc_code &lt;dbl&gt;, incident_datetime &lt;dttm&gt;,\n#   report_datetime &lt;dttm&gt;, province_th &lt;chr&gt;, province_en &lt;chr&gt;, agency &lt;chr&gt;,\n#   route &lt;chr&gt;, vehicle_type &lt;chr&gt;, presumed_cause &lt;chr&gt;, accident_type &lt;chr&gt;,\n#   number_of_vehicles_involved &lt;dbl&gt;, number_of_fatalities &lt;dbl&gt;,\n#   number_of_injuries &lt;dbl&gt;, weather_condition &lt;chr&gt;, road_description &lt;chr&gt;,\n#   slope_description &lt;chr&gt;, geometry &lt;GEOMETRY [m]&gt;, Month_num &lt;dbl&gt;,\n#   Month_fac &lt;ord&gt;, dayofweek &lt;int&gt;\n\n\n\n\n\n“dplyr::select” was used as an error message “Error: unable to find an inherited method for function ‘select’ for signature ‘x = “sf”’”.\n\n\nCode Chunk\nrta_sf_mbr &lt;- rta_sf %&gt;%\n  dplyr::select(-province_th, -route) %&gt;%\n  filter(province_en %in% c(\"Nakhon Pathom\", \"Pathum Thani\", \"Nonthaburi\", \"Samut Prakan\", \"Samut Sakhon\", \"Bangkok\"))\n\n\nThe two variables were dropped as it were in Thai language. Additionally, the scope has been narrowed to only 6 provinces that are in the BMR.\n\n\n\nThe incident_datetime and report_datetime column were separated for ease of manipulation.\n\n\nCode Chunk\nrta_sf_mbr_split &lt;- rta_sf_mbr %&gt;%\n  separate(incident_datetime, into = c(\"incident_date\", \"incident_time\"), sep = \" \") %&gt;%\n  separate(report_datetime, into = c(\"report_date\", \"report_time\"), sep = \" \")\n\n\nIn ensuring that the data is in POSIXct format, ymd_hms was used.\n\n\nCode Chunk\nrta_sf_mbr_split &lt;- rta_sf_mbr %&gt;%\n  mutate(\n    datetime_parsed = ymd_hms(incident_datetime),\n    date = ymd_hms(datetime_parsed),\n    time = format(datetime_parsed, \"%H:%M:%S\")\n  )\n\n\nPulse Check: To check all of the current columns.\n\n\nCode Chunk\ncolnames(rta_sf_mbr_split)\n\n\n [1] \"acc_code\"                    \"incident_datetime\"          \n [3] \"report_datetime\"             \"province_en\"                \n [5] \"agency\"                      \"vehicle_type\"               \n [7] \"presumed_cause\"              \"accident_type\"              \n [9] \"number_of_vehicles_involved\" \"number_of_fatalities\"       \n[11] \"number_of_injuries\"          \"weather_condition\"          \n[13] \"road_description\"            \"slope_description\"          \n[15] \"geometry\"                    \"Month_num\"                  \n[17] \"Month_fac\"                   \"dayofweek\"                  \n[19] \"datetime_parsed\"             \"date\"                       \n[21] \"time\"                       \n\n\n\n\n\n\nThe variables that were not used in the analysis were dropped and it was filtered to the BMR.\n\n\nCode Chunk\nthaiadmin_bmr &lt;- thaiadmin %&gt;%\n  dplyr::select(-ADM1_TH, -ADM0_TH, -ADM1_REF, -ADM1ALT1EN, -ADM1ALT2EN, -ADM1ALT1TH,\n         -ADM1ALT2TH, -ADM0_PCODE, -validTo, -validOn) %&gt;%\n  filter(ADM1_EN %in% c(\"Nakhon Pathom\", \"Pathum Thani\", \"Nonthaburi\", \"Samut Prakan\", \"Samut Sakhon\", \"Bangkok\"))\n\n\n\n\nWhen analysing spatial point patterns, it is a good practice to confine the analysis with a geographical area like Singapore boundary. In spatstat, an object called owin is specially designed to represent this polygonal region.\nThe code chunk below is used to covert sg SpatialPolygon object into owin object of spatstat.\n\n\nCode Chunk\nbmr_owin &lt;- as.owin(thaiadmin32647)\n\n\n\n\nCode Chunk\nplot(bmr_owin)\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\ncoords &lt;- st_coordinates(rta_sf_mbr_split)\n# Create the ppp object with the same window as bmr_owin\nthaiadmin_ppp &lt;- as.ppp(coords, W = bmr_owin)\n\n# View the result\nplot(thaiadmin_ppp)\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\nsummary(thaiadmin_ppp)\n\n\nPlanar point pattern:  12986 points\nAverage intensity 2.516079e-08 points per square unit\n\n*Pattern contains duplicated points*\n\nCoordinates are given to 10 decimal places\n\nWindow: polygonal boundary\n728 separate polygons (no holes)\n             vertices        area relative.area\npolygon 1          53 2.53355e+03      4.91e-09\npolygon 2        1523 6.23850e+06      1.21e-05\npolygon 3          42 2.49113e+04      4.83e-08\npolygon 4          62 2.23046e+04      4.32e-08\npolygon 5          56 4.39488e+05      8.52e-07\npolygon 6          46 7.65029e+03      1.48e-08\npolygon 7          29 9.27916e+03      1.80e-08\npolygon 8          59 8.02014e+03      1.55e-08\npolygon 9         300 3.40852e+05      6.60e-07\npolygon 10        112 6.79950e+05      1.32e-06\npolygon 11         42 3.37968e+04      6.55e-08\npolygon 12        219 5.29394e+06      1.03e-05\npolygon 13         50 2.05900e+05      3.99e-07\npolygon 14        268 6.79882e+06      1.32e-05\npolygon 15         34 1.55825e+05      3.02e-07\npolygon 16         51 3.93134e+04      7.62e-08\npolygon 17         78 1.75837e+05      3.41e-07\npolygon 18        101 5.42686e+05      1.05e-06\npolygon 19        256 4.74527e+05      9.19e-07\npolygon 20         58 9.19444e+03      1.78e-08\npolygon 21         59 2.87919e+05      5.58e-07\npolygon 22         60 6.27905e+04      1.22e-07\npolygon 23        485 1.43133e+07      2.77e-05\npolygon 24        117 1.38538e+05      2.68e-07\npolygon 25        104 5.70710e+04      1.11e-07\npolygon 26        195 4.59998e+05      8.91e-07\npolygon 27         73 2.41793e+04      4.68e-08\npolygon 28        210 9.62852e+05      1.87e-06\npolygon 29         97 1.59662e+05      3.09e-07\npolygon 30        178 8.27111e+05      1.60e-06\npolygon 31        156 1.18428e+05      2.29e-07\npolygon 32         47 1.13996e+04      2.21e-08\npolygon 33         95 6.90830e+04      1.34e-07\npolygon 34         83 3.93177e+05      7.62e-07\npolygon 35         51 2.10677e+04      4.08e-08\npolygon 36         49 6.59279e+04      1.28e-07\npolygon 37         58 5.17956e+03      1.00e-08\npolygon 38         78 2.71442e+04      5.26e-08\npolygon 39        175 6.79048e+05      1.32e-06\npolygon 40        108 7.31483e+04      1.42e-07\npolygon 41        350 4.09442e+06      7.93e-06\npolygon 42        996 5.02741e+06      9.74e-06\npolygon 43         37 4.46504e+03      8.65e-09\npolygon 44         54 3.50271e+05      6.79e-07\npolygon 45        120 1.24977e+05      2.42e-07\npolygon 46        140 1.09669e+05      2.12e-07\npolygon 47         50 1.03162e+05      2.00e-07\npolygon 48         46 1.22206e+04      2.37e-08\npolygon 49         61 2.95012e+04      5.72e-08\npolygon 50        189 1.49213e+06      2.89e-06\npolygon 51         67 6.80527e+05      1.32e-06\npolygon 52        104 1.24896e+05      2.42e-07\npolygon 53         70 2.76957e+04      5.37e-08\npolygon 54         38 3.01574e+05      5.84e-07\npolygon 55         53 1.26348e+04      2.45e-08\npolygon 56         73 6.96475e+04      1.35e-07\npolygon 57         97 1.16021e+05      2.25e-07\npolygon 58        288 6.13520e+05      1.19e-06\npolygon 59        125 9.76483e+04      1.89e-07\npolygon 60        290 4.90893e+05      9.51e-07\npolygon 61         86 2.96223e+03      5.74e-09\npolygon 62        107 7.10532e+04      1.38e-07\npolygon 63         42 2.42729e+03      4.70e-09\npolygon 64         82 9.69627e+03      1.88e-08\npolygon 65        116 4.07177e+03      7.89e-09\npolygon 66        284 4.24081e+05      8.22e-07\npolygon 67        167 5.62031e+04      1.09e-07\npolygon 68        120 2.36548e+05      4.58e-07\npolygon 69       4581 2.10438e+08      4.08e-04\npolygon 70         86 5.63982e+04      1.09e-07\npolygon 71         64 8.12083e+03      1.57e-08\npolygon 72         62 6.58777e+04      1.28e-07\npolygon 73         52 5.27989e+04      1.02e-07\npolygon 74        152 2.86297e+03      5.55e-09\npolygon 75         54 9.21124e+04      1.78e-07\npolygon 76         52 3.61962e+04      7.01e-08\npolygon 77         69 1.02026e+05      1.98e-07\npolygon 78         49 4.19303e+04      8.12e-08\npolygon 79         37 1.40027e+04      2.71e-08\npolygon 80        226 3.76927e+05      7.30e-07\npolygon 81         86 2.75512e+05      5.34e-07\npolygon 82          8 7.96410e+03      1.54e-08\npolygon 83         38 7.22431e+04      1.40e-07\npolygon 84         48 3.33832e+04      6.47e-08\npolygon 85         46 2.94345e+04      5.70e-08\npolygon 86         33 6.35164e+03      1.23e-08\npolygon 87        344 4.03961e+06      7.83e-06\npolygon 88        441 7.31057e+05      1.42e-06\npolygon 89        249 4.78911e+05      9.28e-07\npolygon 90         52 5.01728e+03      9.72e-09\npolygon 91        116 3.62860e+05      7.03e-07\npolygon 92        125 7.50948e+04      1.45e-07\npolygon 93        123 8.15184e+04      1.58e-07\npolygon 94        108 2.78359e+06      5.39e-06\npolygon 95         56 2.11673e+05      4.10e-07\npolygon 96        364 1.86320e+06      3.61e-06\npolygon 97        161 5.21457e+05      1.01e-06\npolygon 98         34 6.70203e+03      1.30e-08\npolygon 99         32 7.97567e+03      1.55e-08\npolygon 100       130 1.97501e+06      3.83e-06\npolygon 101      1416 1.22482e+07      2.37e-05\npolygon 102        57 9.17313e+04      1.78e-07\npolygon 103        60 8.59325e+03      1.66e-08\npolygon 104        89 6.72186e+04      1.30e-07\npolygon 105        79 3.91725e+04      7.59e-08\npolygon 106        58 1.80916e+04      3.51e-08\npolygon 107        49 1.25443e+04      2.43e-08\npolygon 108       632 2.95702e+06      5.73e-06\npolygon 109        52 4.08946e+04      7.92e-08\npolygon 110       130 1.28922e+05      2.50e-07\npolygon 111        54 3.41374e+04      6.61e-08\npolygon 112        88 3.10409e+04      6.01e-08\npolygon 113       155 1.31462e+05      2.55e-07\npolygon 114        57 1.35510e+04      2.63e-08\npolygon 115        58 1.74498e+04      3.38e-08\npolygon 116       203 4.73856e+04      9.18e-08\npolygon 117       114 1.28611e+05      2.49e-07\npolygon 118       105 4.43573e+05      8.59e-07\npolygon 119       255 3.36132e+04      6.51e-08\npolygon 120       190 3.98132e+04      7.71e-08\npolygon 121      2874 1.10463e+08      2.14e-04\npolygon 122       143 5.49569e+04      1.06e-07\npolygon 123       186 2.29056e+05      4.44e-07\npolygon 124       154 1.53180e+06      2.97e-06\npolygon 125       440 2.10619e+04      4.08e-08\npolygon 126       302 4.71123e+04      9.13e-08\npolygon 127        56 1.17346e+05      2.27e-07\npolygon 128        31 3.09800e+04      6.00e-08\npolygon 129       130 4.69193e+04      9.09e-08\npolygon 130       271 1.22079e+06      2.37e-06\npolygon 131        81 2.68329e+04      5.20e-08\npolygon 132        26 2.60408e+03      5.05e-09\npolygon 133       400 1.88826e+06      3.66e-06\npolygon 134        46 1.37856e+04      2.67e-08\npolygon 135        26 7.52813e+04      1.46e-07\npolygon 136        11 1.09345e+04      2.12e-08\npolygon 137        12 1.21607e+04      2.36e-08\npolygon 138       107 1.61052e+05      3.12e-07\npolygon 139        39 1.02309e+05      1.98e-07\npolygon 140        13 2.16292e+04      4.19e-08\npolygon 141        31 8.74137e+04      1.69e-07\npolygon 142        10 1.61048e+04      3.12e-08\npolygon 143        12 5.99257e+03      1.16e-08\npolygon 144        21 8.49665e+04      1.65e-07\npolygon 145       227 1.97629e+06      3.83e-06\npolygon 146        56 3.94580e+04      7.65e-08\npolygon 147        54 4.76934e+04      9.24e-08\npolygon 148       293 8.85374e+05      1.72e-06\npolygon 149        71 5.28210e+04      1.02e-07\npolygon 150        37 1.12415e+04      2.18e-08\npolygon 151        76 1.68825e+05      3.27e-07\npolygon 152        59 8.12766e+04      1.57e-07\npolygon 153        80 8.32872e+04      1.61e-07\npolygon 154        60 2.66550e+04      5.16e-08\npolygon 155       116 1.79496e+05      3.48e-07\npolygon 156       197 7.99359e+04      1.55e-07\npolygon 157        58 9.33090e+04      1.81e-07\npolygon 158        44 9.01237e+03      1.75e-08\npolygon 159       152 8.10480e+04      1.57e-07\npolygon 160        95 7.92271e+05      1.54e-06\npolygon 161       104 4.56811e+04      8.85e-08\npolygon 162        53 7.07697e+04      1.37e-07\npolygon 163        35 3.21371e+04      6.23e-08\npolygon 164        30 2.47659e+03      4.80e-09\npolygon 165        76 3.78024e+04      7.32e-08\npolygon 166        27 4.82358e+04      9.35e-08\npolygon 167        23 7.26466e+03      1.41e-08\npolygon 168        52 3.88426e+03      7.53e-09\npolygon 169        40 7.02522e+04      1.36e-07\npolygon 170        30 5.40608e+04      1.05e-07\npolygon 171        99 2.16592e+05      4.20e-07\npolygon 172        83 2.81683e+04      5.46e-08\npolygon 173        40 7.75391e+03      1.50e-08\npolygon 174       157 1.83378e+07      3.55e-05\npolygon 175       167 2.71430e+05      5.26e-07\npolygon 176        86 2.35305e+04      4.56e-08\npolygon 177        14 5.15806e+03      9.99e-09\npolygon 178        62 4.49328e+04      8.71e-08\npolygon 179        27 6.19557e+04      1.20e-07\npolygon 180       133 5.92845e+05      1.15e-06\npolygon 181        86 6.04232e+05      1.17e-06\npolygon 182        72 1.31340e+05      2.54e-07\npolygon 183       138 1.16705e+06      2.26e-06\npolygon 184       245 3.45624e+06      6.70e-06\npolygon 185       109 4.18371e+05      8.11e-07\npolygon 186       472 2.39033e+07      4.63e-05\npolygon 187      1779 1.22523e+08      2.37e-04\npolygon 188        60 1.00340e+05      1.94e-07\npolygon 189        20 4.44892e+04      8.62e-08\npolygon 190        13 6.19586e+03      1.20e-08\npolygon 191       219 2.40052e+05      4.65e-07\npolygon 192        61 3.03300e+05      5.88e-07\npolygon 193        97 7.00283e+04      1.36e-07\npolygon 194      1062 1.69975e+07      3.29e-05\npolygon 195       145 3.01654e+05      5.84e-07\npolygon 196        91 5.83270e+04      1.13e-07\npolygon 197       105 4.14904e+04      8.04e-08\npolygon 198        93 3.63466e+04      7.04e-08\npolygon 199        39 1.01887e+04      1.97e-08\npolygon 200       130 2.63366e+05      5.10e-07\npolygon 201        77 5.34847e+04      1.04e-07\npolygon 202       100 1.23871e+05      2.40e-07\npolygon 203       202 1.95658e+05      3.79e-07\npolygon 204        52 2.04966e+05      3.97e-07\npolygon 205        25 2.58576e+04      5.01e-08\npolygon 206       417 1.51241e+06      2.93e-06\npolygon 207        51 5.71169e+03      1.11e-08\npolygon 208        53 1.52662e+04      2.96e-08\npolygon 209       119 2.77607e+04      5.38e-08\npolygon 210       116 2.76633e+05      5.36e-07\npolygon 211       253 7.77594e+05      1.51e-06\npolygon 212        65 5.53954e+04      1.07e-07\npolygon 213        62 2.17422e+04      4.21e-08\npolygon 214        42 2.64518e+04      5.13e-08\npolygon 215       113 5.21417e+04      1.01e-07\npolygon 216        11 2.24699e+03      4.35e-09\npolygon 217       233 4.05953e+05      7.87e-07\npolygon 218        74 3.72021e+04      7.21e-08\npolygon 219        61 2.21703e+04      4.30e-08\npolygon 220        43 3.47592e+04      6.73e-08\npolygon 221        43 7.09465e+03      1.37e-08\npolygon 222        63 8.21115e+04      1.59e-07\npolygon 223        68 7.42576e+04      1.44e-07\npolygon 224        52 6.13990e+04      1.19e-07\npolygon 225        57 6.14708e+04      1.19e-07\npolygon 226       112 3.32886e+05      6.45e-07\npolygon 227       142 1.09433e+06      2.12e-06\npolygon 228        50 7.74627e+04      1.50e-07\npolygon 229        34 5.71566e+03      1.11e-08\npolygon 230        47 3.28406e+04      6.36e-08\npolygon 231       243 1.32610e+06      2.57e-06\npolygon 232        34 2.24836e+03      4.36e-09\npolygon 233       157 1.42622e+05      2.76e-07\npolygon 234       179 5.63039e+06      1.09e-05\npolygon 235        39 6.77867e+04      1.31e-07\npolygon 236        42 8.65515e+02      1.68e-09\npolygon 237       205 6.69319e+05      1.30e-06\npolygon 238        54 2.77612e+03      5.38e-09\npolygon 239        90 6.76635e+04      1.31e-07\npolygon 240        67 7.75670e+04      1.50e-07\npolygon 241       143 1.74168e+05      3.37e-07\npolygon 242        40 1.60737e+05      3.11e-07\npolygon 243      2552 2.35426e+08      4.56e-04\npolygon 244        70 3.94070e+04      7.64e-08\npolygon 245       208 1.63232e+06      3.16e-06\npolygon 246        83 2.82963e+05      5.48e-07\npolygon 247       175 1.36324e+06      2.64e-06\npolygon 248        65 6.64185e+05      1.29e-06\npolygon 249       138 6.64843e+04      1.29e-07\npolygon 250        31 3.78277e+05      7.33e-07\npolygon 251        32 5.35809e+03      1.04e-08\npolygon 252       147 9.30384e+05      1.80e-06\npolygon 253        40 1.12524e+05      2.18e-07\npolygon 254        25 2.10691e+03      4.08e-09\npolygon 255       629 1.85471e+07      3.59e-05\npolygon 256        42 1.21267e+04      2.35e-08\npolygon 257        58 5.54077e+04      1.07e-07\npolygon 258       218 3.28794e+05      6.37e-07\npolygon 259        71 7.82792e+04      1.52e-07\npolygon 260        67 4.87781e+05      9.45e-07\npolygon 261        57 2.08458e+05      4.04e-07\npolygon 262       226 4.85861e+06      9.41e-06\npolygon 263        88 8.14478e+04      1.58e-07\npolygon 264        56 2.16487e+06      4.19e-06\npolygon 265        79 1.29659e+05      2.51e-07\npolygon 266       132 1.79898e+05      3.49e-07\npolygon 267        33 1.02731e+04      1.99e-08\npolygon 268       142 1.54527e+06      2.99e-06\npolygon 269       729 1.48280e+07      2.87e-05\npolygon 270       133 1.05890e+05      2.05e-07\npolygon 271        83 2.34362e+05      4.54e-07\npolygon 272        14 1.07965e+04      2.09e-08\npolygon 273       168 4.15326e+05      8.05e-07\npolygon 274        36 2.35428e+04      4.56e-08\npolygon 275        23 1.15661e+04      2.24e-08\npolygon 276        45 5.15325e+04      9.98e-08\npolygon 277        53 6.94478e+05      1.35e-06\npolygon 278        59 5.77518e+04      1.12e-07\npolygon 279       190 3.61314e+05      7.00e-07\npolygon 280       976 8.68567e+06      1.68e-05\npolygon 281        29 2.06981e+04      4.01e-08\npolygon 282        22 1.58427e+04      3.07e-08\npolygon 283        93 2.83389e+06      5.49e-06\npolygon 284        96 1.01050e+05      1.96e-07\npolygon 285       213 9.57989e+06      1.86e-05\npolygon 286       133 8.18763e+05      1.59e-06\npolygon 287       187 6.84325e+05      1.33e-06\npolygon 288        46 1.34985e+05      2.62e-07\npolygon 289        28 3.30205e+04      6.40e-08\npolygon 290        32 2.18078e+04      4.23e-08\npolygon 291       113 2.19925e+05      4.26e-07\npolygon 292        84 6.51097e+04      1.26e-07\npolygon 293       312 5.67054e+05      1.10e-06\npolygon 294        33 1.32885e+04      2.57e-08\npolygon 295        88 3.10315e+05      6.01e-07\npolygon 296        69 3.45741e+05      6.70e-07\npolygon 297       109 7.10583e+05      1.38e-06\npolygon 298       496 1.97740e+07      3.83e-05\npolygon 299        76 7.29808e+05      1.41e-06\npolygon 300        32 2.14783e+04      4.16e-08\npolygon 301        76 5.01893e+04      9.72e-08\npolygon 302        49 6.44866e+04      1.25e-07\npolygon 303        38 2.44360e+04      4.73e-08\npolygon 304        34 1.44985e+04      2.81e-08\npolygon 305        47 9.61114e+04      1.86e-07\npolygon 306       282 9.25103e+06      1.79e-05\npolygon 307        31 5.93875e+04      1.15e-07\npolygon 308       750 1.01758e+08      1.97e-04\npolygon 309        36 2.17645e+04      4.22e-08\npolygon 310        20 3.07083e+04      5.95e-08\npolygon 311        56 1.37639e+05      2.67e-07\npolygon 312        54 2.17419e+05      4.21e-07\npolygon 313        76 2.35627e+06      4.57e-06\npolygon 314        37 1.48907e+05      2.89e-07\npolygon 315        31 2.74226e+04      5.31e-08\npolygon 316        18 1.59275e+04      3.09e-08\npolygon 317        46 9.19515e+04      1.78e-07\npolygon 318       545 6.85244e+07      1.33e-04\npolygon 319        47 2.53689e+05      4.92e-07\npolygon 320        46 1.16386e+05      2.26e-07\npolygon 321        37 1.09409e+04      2.12e-08\npolygon 322        63 1.04438e+06      2.02e-06\npolygon 323       177 6.53342e+05      1.27e-06\npolygon 324       696 3.67112e+06      7.11e-06\npolygon 325       204 3.91698e+05      7.59e-07\npolygon 326       415 6.92778e+05      1.34e-06\npolygon 327        61 7.78251e+04      1.51e-07\npolygon 328        37 3.82324e+04      7.41e-08\npolygon 329       106 6.70020e+05      1.30e-06\npolygon 330       183 7.86389e+05      1.52e-06\npolygon 331        17 5.99620e+03      1.16e-08\npolygon 332        36 5.59015e+04      1.08e-07\npolygon 333        24 5.14841e+04      9.98e-08\npolygon 334        73 6.15783e+04      1.19e-07\npolygon 335        67 5.42200e+04      1.05e-07\npolygon 336        40 9.26211e+04      1.79e-07\npolygon 337        28 2.40995e+05      4.67e-07\npolygon 338        72 1.12406e+04      2.18e-08\npolygon 339        37 1.95637e+03      3.79e-09\npolygon 340        58 2.73734e+05      5.30e-07\npolygon 341       105 2.50438e+05      4.85e-07\npolygon 342        96 3.31618e+06      6.43e-06\npolygon 343        23 2.20023e+04      4.26e-08\npolygon 344        46 5.56469e+04      1.08e-07\npolygon 345       146 2.19599e+06      4.25e-06\npolygon 346       858 2.06853e+06      4.01e-06\npolygon 347       119 4.60709e+05      8.93e-07\npolygon 348        40 5.63947e+03      1.09e-08\npolygon 349        34 2.67960e+04      5.19e-08\npolygon 350       144 5.21384e+04      1.01e-07\npolygon 351        23 1.52352e+04      2.95e-08\npolygon 352        50 1.43738e+04      2.78e-08\npolygon 353        29 9.78437e+04      1.90e-07\npolygon 354        70 1.55289e+05      3.01e-07\npolygon 355        50 2.49115e+04      4.83e-08\npolygon 356        69 3.49514e+04      6.77e-08\npolygon 357        18 1.63675e+04      3.17e-08\npolygon 358        61 1.93303e+04      3.75e-08\npolygon 359        68 3.55635e+04      6.89e-08\npolygon 360       121 4.20225e+05      8.14e-07\npolygon 361        55 4.37920e+03      8.48e-09\npolygon 362        79 3.72043e+04      7.21e-08\npolygon 363        48 7.44105e+03      1.44e-08\npolygon 364        53 1.05232e+04      2.04e-08\npolygon 365        53 1.78218e+04      3.45e-08\npolygon 366        59 4.19901e+04      8.14e-08\npolygon 367       359 6.41671e+05      1.24e-06\npolygon 368       185 7.41242e+04      1.44e-07\npolygon 369        48 5.59843e+03      1.08e-08\npolygon 370       103 7.90648e+04      1.53e-07\npolygon 371        75 5.70544e+04      1.11e-07\npolygon 372        49 1.36485e+03      2.64e-09\npolygon 373        14 5.72691e+03      1.11e-08\npolygon 374        55 2.69781e+04      5.23e-08\npolygon 375        32 3.31282e+03      6.42e-09\npolygon 376       223 2.07929e+05      4.03e-07\npolygon 377        70 5.49744e+04      1.07e-07\npolygon 378        52 6.89398e+04      1.34e-07\npolygon 379        63 7.92309e+03      1.54e-08\npolygon 380        21 1.19439e+03      2.31e-09\npolygon 381        60 1.45064e+04      2.81e-08\npolygon 382        27 1.58312e+03      3.07e-09\npolygon 383        42 8.58139e+04      1.66e-07\npolygon 384        17 6.68861e+03      1.30e-08\npolygon 385        15 2.10492e+03      4.08e-09\npolygon 386        24 5.18389e+03      1.00e-08\npolygon 387       174 2.03651e+05      3.95e-07\npolygon 388        30 1.10485e+04      2.14e-08\npolygon 389       273 7.80652e+05      1.51e-06\npolygon 390       189 3.83543e+05      7.43e-07\npolygon 391       112 1.70948e+05      3.31e-07\npolygon 392       158 1.50097e+05      2.91e-07\npolygon 393        15 2.33364e+03      4.52e-09\npolygon 394        35 1.25320e+03      2.43e-09\npolygon 395        31 4.97341e+04      9.64e-08\npolygon 396        67 6.77975e+04      1.31e-07\npolygon 397        26 5.68397e+04      1.10e-07\npolygon 398        88 8.06683e+04      1.56e-07\npolygon 399        27 5.35406e+04      1.04e-07\npolygon 400       647 2.23542e+06      4.33e-06\npolygon 401      7658 5.21946e+08      1.01e-03\npolygon 402       168 3.93488e+05      7.62e-07\npolygon 403        44 6.48599e+04      1.26e-07\npolygon 404        37 2.30143e+04      4.46e-08\npolygon 405        18 2.87844e+03      5.58e-09\npolygon 406        70 2.27072e+05      4.40e-07\npolygon 407        58 4.98893e+03      9.67e-09\npolygon 408        81 2.28360e+04      4.42e-08\npolygon 409        22 4.83019e+03      9.36e-09\npolygon 410        37 5.19766e+03      1.01e-08\npolygon 411        17 6.41580e+02      1.24e-09\npolygon 412      1042 3.66538e+07      7.10e-05\npolygon 413        37 9.83199e+05      1.90e-06\npolygon 414        97 4.08270e+04      7.91e-08\npolygon 415        26 1.97750e+04      3.83e-08\npolygon 416        22 3.43179e+03      6.65e-09\npolygon 417        37 2.62906e+03      5.09e-09\npolygon 418        88 7.04665e+04      1.37e-07\npolygon 419        19 5.05037e+04      9.79e-08\npolygon 420        35 9.15793e+04      1.77e-07\npolygon 421        15 3.98830e+04      7.73e-08\npolygon 422        66 4.32073e+05      8.37e-07\npolygon 423       151 3.94490e+06      7.64e-06\npolygon 424        18 6.74356e+04      1.31e-07\npolygon 425        36 8.12520e+04      1.57e-07\npolygon 426        28 2.19188e+04      4.25e-08\npolygon 427        33 1.46144e+03      2.83e-09\npolygon 428        29 6.08303e+03      1.18e-08\npolygon 429         9 5.53715e+03      1.07e-08\npolygon 430        57 5.48791e+05      1.06e-06\npolygon 431       132 7.72824e+04      1.50e-07\npolygon 432        26 8.00521e+03      1.55e-08\npolygon 433        25 1.11667e+04      2.16e-08\npolygon 434        55 1.67508e+05      3.25e-07\npolygon 435        31 1.15254e+04      2.23e-08\npolygon 436        24 7.19219e+04      1.39e-07\npolygon 437        76 2.37555e+05      4.60e-07\npolygon 438        69 1.35858e+04      2.63e-08\npolygon 439        65 4.23839e+04      8.21e-08\npolygon 440        36 1.14927e+05      2.23e-07\npolygon 441      2287 9.12824e+07      1.77e-04\npolygon 442        95 4.42889e+04      8.58e-08\npolygon 443       109 1.03446e+05      2.00e-07\npolygon 444        77 1.59703e+05      3.09e-07\npolygon 445       227 1.99692e+05      3.87e-07\npolygon 446        49 1.07047e+05      2.07e-07\npolygon 447       146 5.99463e+04      1.16e-07\npolygon 448       159 2.65301e+05      5.14e-07\npolygon 449        65 1.19625e+04      2.32e-08\npolygon 450        71 1.52377e+04      2.95e-08\npolygon 451        23 1.31938e+04      2.56e-08\npolygon 452        93 3.58871e+04      6.95e-08\npolygon 453       278 4.58306e+05      8.88e-07\npolygon 454       113 3.79180e+04      7.35e-08\npolygon 455       159 4.96168e+05      9.61e-07\npolygon 456        96 2.93744e+06      5.69e-06\npolygon 457        45 2.67684e+04      5.19e-08\npolygon 458        49 1.08557e+04      2.10e-08\npolygon 459        19 6.24117e+03      1.21e-08\npolygon 460        16 2.96742e+03      5.75e-09\npolygon 461        12 7.15038e+03      1.39e-08\npolygon 462        79 1.51253e+05      2.93e-07\npolygon 463        22 1.01888e+04      1.97e-08\npolygon 464        14 1.57671e+04      3.05e-08\npolygon 465        23 1.80620e+04      3.50e-08\npolygon 466        47 3.75673e+04      7.28e-08\npolygon 467       160 2.73650e+05      5.30e-07\npolygon 468        68 1.23958e+05      2.40e-07\npolygon 469        37 4.24857e+04      8.23e-08\npolygon 470        38 1.04510e+04      2.02e-08\npolygon 471        50 1.47986e+04      2.87e-08\npolygon 472        27 1.22110e+05      2.37e-07\npolygon 473        67 1.78475e+04      3.46e-08\npolygon 474        66 2.85792e+04      5.54e-08\npolygon 475       126 4.96218e+05      9.61e-07\npolygon 476       105 4.42466e+04      8.57e-08\npolygon 477       114 6.63473e+05      1.29e-06\npolygon 478        41 1.33234e+04      2.58e-08\npolygon 479        65 1.59051e+04      3.08e-08\npolygon 480       113 2.05741e+05      3.99e-07\npolygon 481       267 5.51487e+05      1.07e-06\npolygon 482        39 1.14095e+04      2.21e-08\npolygon 483        56 8.94248e+03      1.73e-08\npolygon 484        74 1.72258e+04      3.34e-08\npolygon 485       107 2.28611e+04      4.43e-08\npolygon 486        62 1.90286e+04      3.69e-08\npolygon 487        98 4.19191e+04      8.12e-08\npolygon 488        65 1.80129e+04      3.49e-08\npolygon 489       342 3.86996e+06      7.50e-06\npolygon 490        43 2.19587e+04      4.25e-08\npolygon 491       107 3.13084e+04      6.07e-08\npolygon 492       320 1.58487e+07      3.07e-05\npolygon 493        58 9.88911e+03      1.92e-08\npolygon 494        79 2.49221e+04      4.83e-08\npolygon 495        89 2.40401e+06      4.66e-06\npolygon 496        19 2.92454e+04      5.67e-08\npolygon 497        35 1.37126e+04      2.66e-08\npolygon 498        82 8.40732e+05      1.63e-06\npolygon 499       155 1.87816e+05      3.64e-07\npolygon 500        36 5.77116e+04      1.12e-07\npolygon 501        30 4.80004e+04      9.30e-08\npolygon 502        52 1.48710e+05      2.88e-07\npolygon 503       315 1.42816e+07      2.77e-05\npolygon 504        40 9.87368e+03      1.91e-08\npolygon 505        50 2.38974e+05      4.63e-07\npolygon 506        54 1.58622e+03      3.07e-09\npolygon 507        27 2.28487e+05      4.43e-07\npolygon 508        30 5.07905e+04      9.84e-08\npolygon 509       601 2.03002e+07      3.93e-05\npolygon 510        53 3.78138e+05      7.33e-07\npolygon 511        34 2.43995e+05      4.73e-07\npolygon 512       148 2.57773e+06      4.99e-06\npolygon 513        95 1.56768e+05      3.04e-07\npolygon 514        24 4.73557e+03      9.18e-09\npolygon 515        96 4.15899e+05      8.06e-07\npolygon 516       124 3.99783e+05      7.75e-07\npolygon 517        56 1.70067e+05      3.30e-07\npolygon 518        28 1.53720e+04      2.98e-08\npolygon 519        55 1.33170e+06      2.58e-06\npolygon 520       118 3.29217e+05      6.38e-07\npolygon 521        29 6.20959e+04      1.20e-07\npolygon 522        33 2.15361e+05      4.17e-07\npolygon 523        58 4.16817e+05      8.08e-07\npolygon 524        69 3.35637e+04      6.50e-08\npolygon 525       343 4.66814e+06      9.04e-06\npolygon 526        42 1.16465e+05      2.26e-07\npolygon 527        11 5.06855e+03      9.82e-09\npolygon 528         9 5.15857e+03      9.99e-09\npolygon 529         9 5.35523e+03      1.04e-08\npolygon 530         9 3.83108e+03      7.42e-09\npolygon 531         9 3.58652e+03      6.95e-09\npolygon 532       507 1.40987e+07      2.73e-05\npolygon 533       176 4.33569e+04      8.40e-08\npolygon 534       145 9.41676e+05      1.82e-06\npolygon 535        50 8.12779e+05      1.57e-06\npolygon 536       132 1.36344e+05      2.64e-07\npolygon 537        64 6.09134e+05      1.18e-06\npolygon 538        58 4.28221e+03      8.30e-09\npolygon 539        86 6.10087e+05      1.18e-06\npolygon 540        33 8.51220e+04      1.65e-07\npolygon 541        36 1.29299e+05      2.51e-07\npolygon 542        36 3.09126e+05      5.99e-07\npolygon 543        78 2.41537e+05      4.68e-07\npolygon 544       391 2.05521e+06      3.98e-06\npolygon 545        55 7.68289e+04      1.49e-07\npolygon 546       149 3.98786e+04      7.73e-08\npolygon 547        53 3.68511e+04      7.14e-08\npolygon 548        68 9.62080e+03      1.86e-08\npolygon 549       111 6.66904e+05      1.29e-06\npolygon 550        41 6.36567e+03      1.23e-08\npolygon 551        23 3.28407e+04      6.36e-08\npolygon 552        24 2.01316e+04      3.90e-08\npolygon 553        64 1.78360e+04      3.46e-08\npolygon 554        46 2.35588e+05      4.56e-07\npolygon 555        28 1.43876e+04      2.79e-08\npolygon 556        42 5.42452e+04      1.05e-07\npolygon 557       103 9.55404e+05      1.85e-06\npolygon 558       409 1.59832e+06      3.10e-06\npolygon 559        26 1.43374e+04      2.78e-08\npolygon 560        23 1.56599e+04      3.03e-08\npolygon 561        56 1.07634e+05      2.09e-07\npolygon 562        29 2.08616e+04      4.04e-08\npolygon 563        23 8.70746e+04      1.69e-07\npolygon 564        22 2.44345e+04      4.73e-08\npolygon 565        29 2.12011e+05      4.11e-07\npolygon 566        26 4.30850e+04      8.35e-08\npolygon 567      3076 1.54644e+08      3.00e-04\npolygon 568        33 3.30314e+04      6.40e-08\npolygon 569        45 1.36371e+04      2.64e-08\npolygon 570        42 9.78281e+03      1.90e-08\npolygon 571        42 1.95779e+04      3.79e-08\npolygon 572        77 1.36570e+04      2.65e-08\npolygon 573        56 4.17267e+04      8.08e-08\npolygon 574        42 7.73459e+04      1.50e-07\npolygon 575       366 1.55299e+06      3.01e-06\npolygon 576       186 3.88337e+05      7.52e-07\npolygon 577       265 4.75199e+06      9.21e-06\npolygon 578        97 1.36906e+05      2.65e-07\npolygon 579       212 7.21442e+05      1.40e-06\npolygon 580        37 1.94568e+05      3.77e-07\npolygon 581        46 2.01957e+04      3.91e-08\npolygon 582        48 2.22218e+05      4.31e-07\npolygon 583        97 1.20950e+05      2.34e-07\npolygon 584        42 1.54741e+04      3.00e-08\npolygon 585        39 1.73207e+05      3.36e-07\npolygon 586        28 3.91697e+04      7.59e-08\npolygon 587        22 1.78201e+03      3.45e-09\npolygon 588        33 2.45257e+03      4.75e-09\npolygon 589       277 1.51027e+06      2.93e-06\npolygon 590       168 1.47433e+06      2.86e-06\npolygon 591        38 2.26133e+05      4.38e-07\npolygon 592       144 4.08278e+05      7.91e-07\npolygon 593        68 3.78086e+05      7.33e-07\npolygon 594        72 2.32205e+04      4.50e-08\npolygon 595       121 5.13480e+06      9.95e-06\npolygon 596        58 2.13844e+04      4.14e-08\npolygon 597        81 4.79942e+04      9.30e-08\npolygon 598       137 3.33007e+04      6.45e-08\npolygon 599       473 7.49755e+06      1.45e-05\npolygon 600        39 1.67970e+05      3.25e-07\npolygon 601       365 1.69664e+06      3.29e-06\npolygon 602        40 1.23577e+05      2.39e-07\npolygon 603       134 2.23991e+04      4.34e-08\npolygon 604       452 3.35999e+07      6.51e-05\npolygon 605        44 2.67716e+04      5.19e-08\npolygon 606        64 4.78819e+04      9.28e-08\npolygon 607        53 3.69509e+03      7.16e-09\npolygon 608       142 1.11369e+05      2.16e-07\npolygon 609       125 2.17926e+06      4.22e-06\npolygon 610        19 3.20952e+04      6.22e-08\npolygon 611       255 2.24402e+06      4.35e-06\npolygon 612        37 1.97936e+05      3.84e-07\npolygon 613        64 4.30987e+03      8.35e-09\npolygon 614       119 1.14005e+04      2.21e-08\npolygon 615        57 9.63602e+04      1.87e-07\npolygon 616        24 2.27351e+05      4.41e-07\npolygon 617       420 1.18945e+07      2.30e-05\npolygon 618       135 3.00968e+05      5.83e-07\npolygon 619        76 9.77677e+03      1.89e-08\npolygon 620        98 3.97489e+05      7.70e-07\npolygon 621        90 4.44495e+05      8.61e-07\npolygon 622        30 4.92474e+03      9.54e-09\npolygon 623        20 7.02674e+04      1.36e-07\npolygon 624        13 9.92488e+03      1.92e-08\npolygon 625        10 1.09966e+04      2.13e-08\npolygon 626       105 8.67378e+05      1.68e-06\npolygon 627         8 5.19214e+03      1.01e-08\npolygon 628        12 5.26167e+03      1.02e-08\npolygon 629        58 2.33547e+05      4.53e-07\npolygon 630        86 1.84186e+05      3.57e-07\npolygon 631        26 3.08047e+04      5.97e-08\npolygon 632        15 1.59462e+04      3.09e-08\npolygon 633        15 1.40602e+04      2.72e-08\npolygon 634        24 6.15344e+02      1.19e-09\npolygon 635        28 7.59425e+04      1.47e-07\npolygon 636        46 4.21602e+05      8.17e-07\npolygon 637       973 4.53393e+06      8.78e-06\npolygon 638       236 3.69682e+04      7.16e-08\npolygon 639       123 2.05167e+04      3.98e-08\npolygon 640        16 1.78479e+04      3.46e-08\npolygon 641        46 2.64240e+04      5.12e-08\npolygon 642        21 1.58657e+03      3.07e-09\npolygon 643       130 1.46585e+06      2.84e-06\npolygon 644        54 2.63840e+05      5.11e-07\npolygon 645       569 8.23323e+05      1.60e-06\npolygon 646        15 2.47074e+04      4.79e-08\npolygon 647        21 1.30712e+04      2.53e-08\npolygon 648        21 3.12432e+04      6.05e-08\npolygon 649        46 1.23221e+05      2.39e-07\npolygon 650        83 1.21574e+06      2.36e-06\npolygon 651        12 5.86805e+03      1.14e-08\npolygon 652       101 3.42115e+05      6.63e-07\npolygon 653      1497 1.51898e+08      2.94e-04\npolygon 654        57 1.39542e+05      2.70e-07\npolygon 655        27 1.20628e+05      2.34e-07\npolygon 656       204 5.88042e+06      1.14e-05\npolygon 657       141 8.66534e+05      1.68e-06\npolygon 658        38 6.23377e+05      1.21e-06\npolygon 659        48 2.47230e+05      4.79e-07\npolygon 660       179 1.30495e+06      2.53e-06\npolygon 661        36 1.14201e+05      2.21e-07\npolygon 662        15 1.45050e+04      2.81e-08\npolygon 663       143 8.24087e+05      1.60e-06\npolygon 664        18 4.19441e+03      8.13e-09\npolygon 665        40 1.93887e+04      3.76e-08\npolygon 666       105 2.81658e+05      5.46e-07\npolygon 667        51 2.19842e+05      4.26e-07\npolygon 668       505 6.33277e+05      1.23e-06\npolygon 669       106 1.03407e+05      2.00e-07\npolygon 670        53 2.72546e+04      5.28e-08\npolygon 671        37 6.11314e+04      1.18e-07\npolygon 672       841 1.83412e+06      3.55e-06\npolygon 673        36 1.25970e+05      2.44e-07\npolygon 674        34 5.79091e+03      1.12e-08\npolygon 675        37 3.76989e+04      7.30e-08\npolygon 676        16 1.25536e+04      2.43e-08\npolygon 677        78 8.69543e+04      1.68e-07\npolygon 678        16 3.04913e+04      5.91e-08\npolygon 679        34 4.63023e+04      8.97e-08\npolygon 680       139 7.82568e+04      1.52e-07\npolygon 681        42 7.95697e+04      1.54e-07\npolygon 682        49 1.18881e+05      2.30e-07\npolygon 683        58 9.55949e+04      1.85e-07\npolygon 684        21 2.08564e+04      4.04e-08\npolygon 685        65 5.29567e+05      1.03e-06\npolygon 686        41 1.73404e+04      3.36e-08\npolygon 687        65 1.69682e+04      3.29e-08\npolygon 688        13 1.46245e+04      2.83e-08\npolygon 689      1066 2.88894e+07      5.60e-05\npolygon 690        18 1.26335e+04      2.45e-08\npolygon 691       125 1.17609e+06      2.28e-06\npolygon 692       291 2.89175e+07      5.60e-05\npolygon 693        39 3.25381e+04      6.30e-08\npolygon 694        77 9.05555e+05      1.75e-06\npolygon 695       133 1.39549e+05      2.70e-07\npolygon 696        30 1.39928e+04      2.71e-08\npolygon 697        31 2.58780e+04      5.01e-08\npolygon 698        19 4.77690e+04      9.26e-08\npolygon 699        18 1.19413e+04      2.31e-08\npolygon 700        27 7.06965e+04      1.37e-07\npolygon 701        94 2.93367e+05      5.68e-07\npolygon 702       357 4.19642e+06      8.13e-06\npolygon 703        50 1.72523e+04      3.34e-08\npolygon 704         9 3.97401e+03      7.70e-09\npolygon 705        55 1.36144e+04      2.64e-08\npolygon 706       109 3.14625e+05      6.10e-07\npolygon 707        23 2.00716e+04      3.89e-08\npolygon 708       123 3.01503e+05      5.84e-07\npolygon 709        33 9.99966e+03      1.94e-08\npolygon 710        39 4.26341e+04      8.26e-08\npolygon 711        21 2.44897e+04      4.74e-08\npolygon 712        14 7.89863e+03      1.53e-08\npolygon 713        44 2.38081e+04      4.61e-08\npolygon 714        35 3.59738e+04      6.97e-08\npolygon 715        66 8.56579e+04      1.66e-07\npolygon 716        25 1.69954e+04      3.29e-08\npolygon 717        16 1.02405e+04      1.98e-08\npolygon 718        23 4.72763e+04      9.16e-08\npolygon 719        30 1.71468e+04      3.32e-08\npolygon 720       206 4.10359e+06      7.95e-06\npolygon 721        35 1.63422e+04      3.17e-08\npolygon 722        26 6.61320e+04      1.28e-07\npolygon 723        24 3.00670e+04      5.83e-08\npolygon 724        60 5.43431e+05      1.05e-06\npolygon 725        52 1.33290e+05      2.58e-07\npolygon 726        51 1.57518e+05      3.05e-07\npolygon 727        46 2.22838e+05      4.32e-07\npolygon 728    177339 5.13717e+11      9.95e-01\nenclosing rectangle: [325178.8, 1213655.7] x [620860.6, 2263241] units\n                     (888500 x 1642000 units)\nWindow area = 5.1612e+11 square units\nFraction of frame area: 0.354\n\n\ncolnames was used to list all of the columns in the df.\n\n\nCode Chunk\ncolnames(thaiadmin32647)\n\n\n [1] \"Shape_Leng\" \"Shape_Area\" \"ADM1_EN\"    \"ADM1_TH\"    \"ADM1_PCODE\"\n [6] \"ADM1_REF\"   \"ADM1ALT1EN\" \"ADM1ALT2EN\" \"ADM1ALT1TH\" \"ADM1ALT2TH\"\n[11] \"ADM0_EN\"    \"ADM0_TH\"    \"ADM0_PCODE\" \"date\"       \"validOn\"   \n[16] \"validTo\"    \"geometry\"   \"acc_code\"  \n\n\n\n\n\n\nNoticed that the geometry type is in multi-line string, not suitable for maniuplation.\n\n\nCode Chunk\nst_geometry(roadlines)\n\n\nGeometry set for 2792590 features \nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 97.34457 ymin: 5.643645 xmax: 105.6528 ymax: 20.47168\nCRS:           NA\nFirst 5 geometries:\n\n\n\n\n\n\nCode Chunk\nroadlines &lt;- st_cast(roadlines, \"LINESTRING\")\n\n\nHead() function allows to view the first 5 rows of the data.\n\n\nCode Chunk\nhead(roadlines, n=5)\n\n\nSimple feature collection with 5 features and 14 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 100.7831 ymin: 13.73046 xmax: 100.7913 ymax: 13.74091\nCRS:           NA\n            name               name_en        highway  surface smoothness width\n1     ถนนฉลองกรุง    Chalong Krung Road      secondary    paved       &lt;NA&gt;  &lt;NA&gt;\n2 ซอยฉลองกรุง 1/1 Soi Chalong Krung 1/1    residential     &lt;NA&gt;       &lt;NA&gt;  &lt;NA&gt;\n3           &lt;NA&gt;                  &lt;NA&gt; secondary_link     &lt;NA&gt;       &lt;NA&gt;  &lt;NA&gt;\n4           &lt;NA&gt;                  &lt;NA&gt;        service     &lt;NA&gt;       &lt;NA&gt;  &lt;NA&gt;\n5     ถนนฉลองกรุง    Chalong Krung Road      secondary concrete       &lt;NA&gt;  &lt;NA&gt;\n  lanes oneway bridge layer source        name_th     osm_id  osm_type\n1  &lt;NA&gt;    yes   &lt;NA&gt;  &lt;NA&gt;   &lt;NA&gt;     ถนนฉลองกรุง 1125681229 ways_line\n2  &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;   &lt;NA&gt; ซอยฉลองกรุง 1/1  594401607 ways_line\n3  &lt;NA&gt;    yes   &lt;NA&gt;  &lt;NA&gt;   &lt;NA&gt;           &lt;NA&gt;  472283206 ways_line\n4  &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;   &lt;NA&gt;           &lt;NA&gt;  594401608 ways_line\n5     2    yes    yes     1   Bing     ถนนฉลองกรุง  116847248 ways_line\n                        geometry\n1 LINESTRING (100.7913 13.740...\n2 LINESTRING (100.7882 13.736...\n3 LINESTRING (100.7844 13.732...\n4 LINESTRING (100.7873 13.735...\n5 LINESTRING (100.7831 13.730...\n\n\n\n\n\n\n\n\nCode Chunk\nroadlines &lt;- st_set_crs(roadlines, 4326)\nroadlines_transformed &lt;- st_transform(roadlines, crs = 32647)\n\n\n\n\nCode Chunk\nmerged_data &lt;- st_join(roadlines_transformed, thaiadmin32647, join = st_intersects)\n\n\n\n\nCode Chunk\nwrite_rds(merged_data, \"data/rds/merged_data.rds\")\n\n\nEXTRACT ROADLINES UNDER BMR then put it accidents data to DR\nTASK 1:To visualize the spatio-temporal dynamics of road traffic accidents in BMR using appropriate statistical graphics and geovisualization methods.\n\nGoal: Create visuals to show how road accidents are distributed over space and time.\nMethods: Use statistical graphics and geovisualization techniques such as:\n\nHeatmaps for spatial density of accidents.\nTime series charts for accident trends over time.\nAnimated maps to show changes in accident locations over time (spatio-temporal dynamics).\n\n\nFrom this heatmap, we can see Bangkok notoriously topped the charts in the number of RTAs every year followed by Samut Prakan.\n\n\nCode Chunk\n# Step 1: Extract the year from the date column\nrta_sf_mbr_split &lt;- rta_sf_mbr_split %&gt;%\n  mutate(year = year(date))  # Extract year using lubridate's year() function\n\n# Step 2: Summarize the data to count accidents per province and year\naccidents_summary &lt;- rta_sf_mbr_split %&gt;%\n  group_by(province_en, year) %&gt;%\n  summarise(total_accidents = n(), .groups = \"drop\")\n\n# Step 3: Create the heatmap with number of accidents in each tile\nggplot(accidents_summary, aes(x = year, y = province_en, fill = total_accidents)) +\n  geom_tile(color = \"white\") +  # Use geom_tile to create the heatmap\n  geom_text(aes(label = total_accidents), color = \"black\", size = 3) +  # Add the number of accidents in each tile\n  scale_fill_gradient(low = \"lightblue\", high = \"red\", na.value = \"white\", name = \"Number of Accidents\") +\n  labs(title = \"Accident Heatmap by Province and Year\",\n       x = \"Year\",\n       y = \"Province\",\n       fill = \"Total Accidents\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability\n\n\n\n\n\n\n\n\n\nA heatmap segregated of months was populated below to gain a better understanding of the accidents in each month.\n\n\nCode Chunk\n# Step 1: Extract the year and month name from the date column\nrta_sf_mbr_split &lt;- rta_sf_mbr_split %&gt;%\n  mutate(\n    year = year(date),                # Extract year\n    month = month(date, label = TRUE, abbr = FALSE)  # Extract full month name\n  )\n\n# Step 2: Summarize the data to count accidents per province, year, and month\naccidents_summary &lt;- rta_sf_mbr_split %&gt;%\n  group_by(province_en, year, month) %&gt;%\n  summarise(total_accidents = n(), .groups = \"drop\")\n\n# Step 3: Create the heatmap\nggplot(accidents_summary, aes(x = month, y = province_en, fill = total_accidents)) +\n  geom_tile(color = \"white\") +  # Use geom_tile to create the heatmap\n  facet_wrap(~year, ncol = 1) +  # Separate the heatmap by year\n  scale_fill_gradient(low = \"lightblue\", high = \"red\", na.value = \"white\", name = \"Number of Accidents\") +\n  labs(title = \"Accident Heatmap by Province and Month\",\n       x = \"Month\",\n       y = \"Province\",\n       fill = \"Total Accidents\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\n# Filter to BMR\nprovinces_to_keep &lt;- c(\"Nakhon Pathom\", \"Pathum Thani\", \"Nonthaburi\", \n                       \"Samut Prakan\", \"Samut Sakhon\", \"Bangkok\")\n\n# Filter the dataset for the 6 provinces\nrta_sf_mbr_filtered &lt;- rta_sf_mbr_split %&gt;%\n  filter(province_en %in% provinces_to_keep)\n\n# Group by date and province, then count the number of accidents per day for each province\naccident_trends_by_province &lt;- rta_sf_mbr_filtered %&gt;%\n  group_by(date = as.Date(date), province_en) %&gt;%\n  summarise(daily_accidents = n(), .groups = \"drop\")\n\n# Plot the time series chart for each province\nggplot(data = accident_trends_by_province, aes(x = date, y = daily_accidents, color = province_en)) +\n  geom_line() +\n  labs(title = \"Time Series of Accident Trends by Province\",\n       x = \"Year\", y = \"Number of Accidents\") +\n  theme_minimal() +\n  facet_wrap(~ province_en, ncol = 2, scales = \"free_y\")\n\n\n\n\n\n\n\n\n\nObservation\nBased on a cursory view, it is apparent that Bangkok has the highest number of RTA from 2019 - 2023 and the numbers are relatively consistent and concentrated except in Q1 2019 & Q4 2023 when it spiked above 15 accidents. Conversely, Samut Sakhon has the lowest number of RTA where in end of 2022, there is close to 0 number of accidents. It is also worthwhile noting that Samut Prakan has an interesting outlook of the RTA data. At certain time frame, it had spikes that indicates that the number of RTA rose above 7 and Samut Prakan also have pockets of “dead spots” which tells us the number of accidents were close to 0.\nInference\nBangkok is notably the most densely populated province in Thailand alongside the influx of tourists. This may explain the consistently high number of RTA in the province.\nAdditionally, Nonthaburi , too, have a high number of RTA, This may due to it being the 2nd most densely populated province (Wiki).\nAs Samut Sakhon is an industrial dominant and formerly an agriculture/fisheries producing province (Wiki), there might not be huge population of people residing or commuting around the province. Hence, it may be why Samut Sakhon has the lowest number of RTA between the 6 provinces of the MBR.\nTASK 2:To conduct detailed spatial analysis of road traffic accidents using appropriate Network Spatial Point Patterns Analysis methods.\n\nGoal: Analyze the spatial patterns of accidents using Network Spatial Point Patterns Analysis (NSPPA).\nMethods: NSPPA techniques to analyze how accidents are distributed along the road network. This involves:\n\nKernel density estimation (KDE) on the road network.\nNearest neighbor analysis to see clustering on road segments.\nNetwork K-function or Ripley’s K-function to analyze the spatial distribution along the network.\n\n\n\n\n\n\n\n\nCode Chunk\ntmap_mode('plot')\ntm_shape(rta_sf)+\n  tm_dots(alpha=0.4,\n          size=0.05)\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\nwrite_rds('view', \"data/rds/view.rds\")\n\n\n\n\nCode Chunk\nwrite_rds('plot', \"data/rds/plot.rds\")\n\n\n\n\n\n\nCode Chunk\nrta_sp &lt;- as_Spatial(rta_sf_mbr_split)\nroadlines_sp &lt;- as_Spatial(roadlines)\nthaiadmin_sp &lt;- as_Spatial(thaiadmin32647)\n\n\n\n\nCode Chunk\nwrite_rds('rta_sp', \"data/rds/rta_sp.rds\")\nwrite_rds('roadlines_sp', \"data/rds/roadlines_sp.rds\")\nwrite_rds('thaiadmin_sp', \"data/rds/thaiadmin_sp.rds\")\n\n\nConverting generic sp format into spatstat’s ppp format\n\n\nCode Chunk\nrta_ppp &lt;- as.ppp(rta_sf)\nrta_ppp\n\n\nMarked planar point pattern: 81376 points\nmarks are numeric, of storage type  'double'\nwindow: rectangle = [-16183187, 1200243] x [-936, 4918525] units\n\n\n\n\nCode Chunk\nkde_rta &lt;- density(rta_ppp,\n                   sigma=bw.diggle,\n                   edge=TRUE,\n                   kernel=\"gaussian\") \n\n\n\n\nCode Chunk\nplot(kde_rta)\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\nkde_rta_adaptive &lt;- adaptive.density(rta_ppp, method=\"kernel\")\nplot(kde_rta_adaptive)\n\n\n\n\n\n\n\n\n\n“Murphy’s law will kick in during the very last minute” (Kam, 2024).\nAnd I attest it was absolutely true."
  },
  {
    "objectID": "HandsOnExercise/HandsOn4/HandsOn4.html",
    "href": "HandsOnExercise/HandsOn4/HandsOn4.html",
    "title": "Hands-On Exercise 4",
    "section": "",
    "text": "In this hands-on exercise, you will learn how to compute spatial weights using R. By the end to this hands-on exercise, you will be able to:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute spatial weights using appropriate functions of spdep package, and\ncalculate spatially lagged variables using appropriate functions of spdep package.\n\n\n\n\nTwo data sets will be used in this hands-on exercise, they are:\n\nHunan county boundary layer. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv: This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n\nBefore we get started, we need to ensure that spdep, sf, tmap and tidyverse packages of R are currently installed in your R.\n\n\nCode Chunk\npacman::p_load(sf, spdep, tmap, tidyverse, knitr, spNetwork, dplyr)\n\n\n\n\n\n\nIn this section, you will learn how to bring a geospatial data and its associated attribute table into R environment. The geospatial data is in ESRI shapefile format and the attribute table is in csv fomat.\n\n\nThe code chunk below uses st_read() of sf package to import Hunan shapefile into R. The imported shapefile will be simple features Object of sf.\n\n\nCode Chunk\nhunan &lt;- st_read(dsn = \"data/geospatial\",\n                    layer=\"Hunan\")\n\n\nReading layer `Hunan' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/HandsOnExercise/HandsOn4/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\nNext, we will import Hunan_2012.csv into R by using read_csv() of readr package. The output is R dataframe class.\n\n\nCode Chunk\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n\n\n\nThe code chunk below will be used to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\n\n\nCode Chunk\nhunan &lt;- left_join(hunan, hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\n\n\n\n\n\nNow, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\n\nCode Chunk\nst_crs(hunan)\n\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\n\n\nCode Chunk\nbasemap &lt;- tm_shape(hunan) +\n  tm_polygons() +\n  tm_text(\"NAME_3\", size=0.5)\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\ntmap_arrange(basemap, gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\nIn this section, you will learn how to use poly2nb() of spdep package to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries. If you look at the documentation you will see that you can pass a “queen” argument that takes TRUE or FALSE as options. If you do not specify this argument the default is set to TRUE, that is, if you don’t specify queen = FALSE this function will return a list of first order neighbours using the Queen criteria.\n\n\nThe code chunk below is used to compute Queen contiguity weight matrix.\n\n\nCode Chunk\nwm_q &lt;- poly2nb(hunan, queen=TRUE)\nsummary(wm_q)\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one heighbours.\nFor each polygon in our polygon object, wm_q lists all neighboring polygons. For example, to see the neighbors for the first polygon in the object, type:\n\n\nCode Chunk\nwm_q\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\n\nPolygon 1 has 5 neighbors. The numbers represent the polygon IDs as stored in hunan SpatialPolygonsDataFrame class.\nWe can retrive the county name of Polygon ID=1 by using the code chunk below:\n\n\nCode Chunk\nhunan$County\n\n\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n\nThe output reveals that Polygon ID=1 is Anxiang county.\nTo reveal the county names of the five neighboring polygons, the code chunk will be used:\n\n\nCode Chunk\nhunan$NAME_3[c(2,3,4,57,85)]\n\n\n[1] \"Hanshou\" \"Jinshi\"  \"Li\"      \"Nan\"     \"Taoyuan\"\n\n\nWe can retrieve the GDPPC of these five countries by using the code chunk below.\n\n\nCode Chunk\nnb1 &lt;- wm_q[[1]]\nnb1 &lt;- hunan$GDPPC[nb1]\nnb1\n\n\n[1] 20981 34592 24473 21311 22879\n\n\nThe printed output above shows that the GDPPC of the five nearest neighbours based on Queen’s method are 20981, 34592, 24473, 21311 and 22879 respectively.\nYou can display the complete weight matrix by using str().\n\n\nCode Chunk\nstr(wm_q)\n\n\nList of 88\n $ : int [1:5] 2 3 4 57 85\n $ : int [1:5] 1 57 58 78 85\n $ : int [1:4] 1 4 5 85\n $ : int [1:4] 1 3 5 6\n $ : int [1:4] 3 4 6 85\n $ : int [1:5] 4 5 69 75 85\n $ : int [1:4] 67 71 74 84\n $ : int [1:7] 9 46 47 56 78 80 86\n $ : int [1:6] 8 66 68 78 84 86\n $ : int [1:8] 16 17 19 20 22 70 72 73\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:3] 11 15 17\n $ : int [1:4] 13 14 17 83\n $ : int [1:5] 10 17 22 72 83\n $ : int [1:7] 10 11 14 15 16 72 83\n $ : int [1:5] 20 22 23 77 83\n $ : int [1:6] 10 20 21 73 74 86\n $ : int [1:7] 10 18 19 21 22 23 82\n $ : int [1:5] 19 20 35 82 86\n $ : int [1:5] 10 16 18 20 83\n $ : int [1:7] 18 20 38 41 77 79 82\n $ : int [1:5] 25 28 31 32 54\n $ : int [1:5] 24 28 31 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:3] 26 29 42\n $ : int [1:5] 24 25 33 49 54\n $ : int [1:3] 27 37 42\n $ : int 33\n $ : int [1:8] 24 25 32 36 39 40 56 81\n $ : int [1:8] 24 31 50 54 55 56 75 85\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 47 80 82 86\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:4] 29 42 43 44\n $ : int [1:4] 23 44 77 79\n $ : int [1:5] 31 40 42 43 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:7] 26 27 29 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:4] 37 38 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:3] 8 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:5] 32 48 52 54 55\n $ : int [1:3] 48 49 52\n $ : int [1:5] 48 49 50 51 54\n $ : int [1:3] 48 55 75\n $ : int [1:6] 24 28 32 49 50 52\n $ : int [1:5] 32 48 50 53 75\n $ : int [1:7] 8 31 32 36 78 80 85\n $ : int [1:6] 1 2 58 64 76 85\n $ : int [1:5] 2 57 68 76 78\n $ : int [1:4] 60 61 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:7] 12 59 60 62 63 77 87\n $ : int [1:3] 61 77 87\n $ : int [1:4] 12 61 77 83\n $ : int [1:2] 57 76\n $ : int 76\n $ : int [1:5] 9 67 68 76 84\n $ : int [1:4] 7 66 76 84\n $ : int [1:5] 9 58 66 76 78\n $ : int [1:3] 6 75 85\n $ : int [1:3] 10 72 73\n $ : int [1:3] 7 73 74\n $ : int [1:5] 10 11 16 17 70\n $ : int [1:5] 10 19 70 71 74\n $ : int [1:6] 7 19 71 73 84 86\n $ : int [1:6] 6 32 53 55 69 85\n $ : int [1:7] 57 58 64 65 66 67 68\n $ : int [1:7] 18 23 38 61 62 63 83\n $ : int [1:7] 2 8 9 56 58 68 85\n $ : int [1:7] 23 38 40 41 43 44 45\n $ : int [1:8] 8 34 35 36 41 45 47 56\n $ : int [1:6] 25 26 31 33 39 42\n $ : int [1:5] 20 21 23 35 41\n $ : int [1:9] 12 13 15 16 17 18 22 63 77\n $ : int [1:6] 7 9 66 67 74 86\n $ : int [1:11] 1 2 3 5 6 32 56 57 69 75 ...\n $ : int [1:9] 8 9 19 21 35 46 47 74 84\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language poly2nb(pl = hunan, queen = TRUE)\n - attr(*, \"type\")= chr \"queen\"\n - attr(*, \"sym\")= logi TRUE\n\n\nBe warned: The output might cut across several pages. Save the trees if you are going to print out the report.\n\n\n\nThe code chunk below is used to compute Rook contiguity weight matrix.\n\n\nCode Chunk\nwm_r &lt;- poly2nb(hunan, queen=FALSE)\nsummary(wm_r)\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 440 \nPercentage nonzero weights: 5.681818 \nAverage number of links: 5 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 2  2 12 20 21 14 11  3  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 10 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connect area unit has 10 neighbours. There are two area units with only one heighbours.\n\n\n\nA connectivity graph takes a point and displays a line to each neighboring point. We are working with polygons at the moment, so we will need to get points in order to make our connectivity graphs. The most typically method for this will be polygon centroids. We will calculate these in the sf package before moving onto the graphs. Getting Latitude and Longitude of Polygon Centroids\nWe will need points to associate with each polygon before we can make our connectivity graph. It will be a little more complicated than just running st_centroid on the sf object: us.bound. We need the coordinates in a separate data frame for this to work. To do this we will use a mapping function. The mapping function applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column of us.bound. Our function will be st_centroid. We will be using map_dbl variation of map from the purrr package. For more documentation, check out map documentation\nTo get our longitude values we map the st_centroid function over the geometry column of us.bound and access the longitude value through double bracket notation [[]] and 1. This allows us to get only the longitude, which is the first value in each centroid.\n\n\nCode Chunk\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\n\nWe do the same for latitude with one key difference. We access the second value per each centroid with [[2]].\n\n\nCode Chunk\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\n\nNow that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.\n\n\nCode Chunk\ncoords &lt;- cbind(longitude, latitude)\n\n\nWe check the first few observations to see if things are formatted correctly.\n\n\nCode Chunk\nhead(coords)\n\n\n     longitude latitude\n[1,]  112.1531 29.44362\n[2,]  112.0372 28.86489\n[3,]  111.8917 29.47107\n[4,]  111.7031 29.74499\n[5,]  111.6138 29.49258\n[6,]  111.0341 29.79863\n\n\n\n\n\n\nCode Chunk\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_q, coords, pch=19, cex=0.6, add=TRUE, col=\"red\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\npar(mfrow=c(1,2))\nplot(hunan$geometry, borders=\"lightgrey\", main=\"Queen Contiguity\")\nplot(wm_q, coords, pch=19, cex=0.6, add=TRUE, col=\"red\")\nplot(hunan$geometry, border=\"lightgrey\", main=\"Rook Contiguity\")\nplot(wm_r, coords, pch=19, cex=0.6, add=TRUE, col=\"red\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this section, you will learn how to derive distance-based weight matrices by using dnearneigh() of spdep package.\nThe function identifies neighbours of region points by Euclidean distance with a distance band with lower d1= and upper d2= bounds controlled by the bounds= argument. If unprojected coordinates are used and either specified in the coordinates object x or with x as a two column matrix and longlat=TRUE, great circle distances in km will be calculated assuming the WGS84 reference ellipsoid.\n\n\nFirstly, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\n\nCode Chunk\n#coords &lt;- coordinates(hunan)\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n\n\nNow, we will compute the distance weight matrix by using dnearneigh() as shown in the code chunk below.\n\n\nCode Chunk\nwm_d62 &lt;- dnearneigh(coords, 0, 62, longlat=TRUE)\nwm_d62\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\nQuiz: What is the meaning of “Average number of links: 3.681818” shown above?\nNext, we will use str() to display the content of wm_d62 weight matrix.\n\n\nCode Chunk\nstr(wm_d62)\n\n\nList of 88\n $ : int [1:5] 3 4 5 57 64\n $ : int [1:4] 57 58 78 85\n $ : int [1:4] 1 4 5 57\n $ : int [1:3] 1 3 5\n $ : int [1:4] 1 3 4 85\n $ : int 69\n $ : int [1:2] 67 84\n $ : int [1:4] 9 46 47 78\n $ : int [1:4] 8 46 68 84\n $ : int [1:4] 16 22 70 72\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:2] 11 17\n $ : int 13\n $ : int [1:4] 10 17 22 83\n $ : int [1:3] 11 14 16\n $ : int [1:3] 20 22 63\n $ : int [1:5] 20 21 73 74 82\n $ : int [1:5] 18 19 21 22 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:4] 10 16 18 20\n $ : int [1:3] 41 77 82\n $ : int [1:4] 25 28 31 54\n $ : int [1:4] 24 28 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:2] 26 29\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:2] 27 37\n $ : int 33\n $ : int [1:2] 24 36\n $ : int 50\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:5] 31 34 45 56 80\n $ : int [1:2] 29 42\n $ : int [1:3] 44 77 79\n $ : int [1:4] 40 42 43 81\n $ : int [1:3] 39 45 79\n $ : int [1:5] 23 35 45 79 82\n $ : int [1:5] 26 37 39 43 81\n $ : int [1:3] 39 42 44\n $ : int [1:2] 38 43\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:5] 8 9 35 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:4] 48 49 50 52\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:2] 48 55\n $ : int [1:5] 24 28 49 50 52\n $ : int [1:4] 48 50 53 75\n $ : int 36\n $ : int [1:5] 1 2 3 58 64\n $ : int [1:5] 2 57 64 66 68\n $ : int [1:3] 60 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:5] 12 60 62 63 87\n $ : int [1:4] 61 63 77 87\n $ : int [1:5] 12 18 61 62 83\n $ : int [1:4] 1 57 58 76\n $ : int 76\n $ : int [1:5] 58 67 68 76 84\n $ : int [1:2] 7 66\n $ : int [1:4] 9 58 66 84\n $ : int [1:2] 6 75\n $ : int [1:3] 10 72 73\n $ : int [1:2] 73 74\n $ : int [1:3] 10 11 70\n $ : int [1:4] 19 70 71 74\n $ : int [1:5] 19 21 71 73 86\n $ : int [1:2] 55 69\n $ : int [1:3] 64 65 66\n $ : int [1:3] 23 38 62\n $ : int [1:2] 2 8\n $ : int [1:4] 38 40 41 45\n $ : int [1:5] 34 35 36 45 47\n $ : int [1:5] 25 26 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:4] 12 13 16 63\n $ : int [1:4] 7 9 66 68\n $ : int [1:2] 2 5\n $ : int [1:4] 21 46 47 74\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language dnearneigh(x = coords, d1 = 0, d2 = 62, longlat = TRUE)\n - attr(*, \"dnn\")= num [1:2] 0 62\n - attr(*, \"bounds\")= chr [1:2] \"GE\" \"LE\"\n - attr(*, \"nbtype\")= chr \"distance\"\n - attr(*, \"sym\")= logi TRUE\n\n\nAnother way to display the structure of the weight matrix is to combine table() and card() of spdep.\n\n\nCode Chunk\ntable(hunan$County, card(wm_d62))\n\n\n               \n                1 2 3 4 5 6\n  Anhua         1 0 0 0 0 0\n  Anren         0 0 0 1 0 0\n  Anxiang       0 0 0 0 1 0\n  Baojing       0 0 0 0 1 0\n  Chaling       0 0 1 0 0 0\n  Changning     0 0 1 0 0 0\n  Changsha      0 0 0 1 0 0\n  Chengbu       0 1 0 0 0 0\n  Chenxi        0 0 0 1 0 0\n  Cili          0 1 0 0 0 0\n  Dao           0 0 0 1 0 0\n  Dongan        0 0 1 0 0 0\n  Dongkou       0 0 0 1 0 0\n  Fenghuang     0 0 0 1 0 0\n  Guidong       0 0 1 0 0 0\n  Guiyang       0 0 0 1 0 0\n  Guzhang       0 0 0 0 0 1\n  Hanshou       0 0 0 1 0 0\n  Hengdong      0 0 0 0 1 0\n  Hengnan       0 0 0 0 1 0\n  Hengshan      0 0 0 0 0 1\n  Hengyang      0 0 0 0 0 1\n  Hongjiang     0 0 0 0 1 0\n  Huarong       0 0 0 1 0 0\n  Huayuan       0 0 0 1 0 0\n  Huitong       0 0 0 1 0 0\n  Jiahe         0 0 0 0 1 0\n  Jianghua      0 0 1 0 0 0\n  Jiangyong     0 1 0 0 0 0\n  Jingzhou      0 1 0 0 0 0\n  Jinshi        0 0 0 1 0 0\n  Jishou        0 0 0 0 0 1\n  Lanshan       0 0 0 1 0 0\n  Leiyang       0 0 0 1 0 0\n  Lengshuijiang 0 0 1 0 0 0\n  Li            0 0 1 0 0 0\n  Lianyuan      0 0 0 0 1 0\n  Liling        0 1 0 0 0 0\n  Linli         0 0 0 1 0 0\n  Linwu         0 0 0 1 0 0\n  Linxiang      1 0 0 0 0 0\n  Liuyang       0 1 0 0 0 0\n  Longhui       0 0 1 0 0 0\n  Longshan      0 1 0 0 0 0\n  Luxi          0 0 0 0 1 0\n  Mayang        0 0 0 0 0 1\n  Miluo         0 0 0 0 1 0\n  Nan           0 0 0 0 1 0\n  Ningxiang     0 0 0 1 0 0\n  Ningyuan      0 0 0 0 1 0\n  Pingjiang     0 1 0 0 0 0\n  Qidong        0 0 1 0 0 0\n  Qiyang        0 0 1 0 0 0\n  Rucheng       0 1 0 0 0 0\n  Sangzhi       0 1 0 0 0 0\n  Shaodong      0 0 0 0 1 0\n  Shaoshan      0 0 0 0 1 0\n  Shaoyang      0 0 0 1 0 0\n  Shimen        1 0 0 0 0 0\n  Shuangfeng    0 0 0 0 0 1\n  Shuangpai     0 0 0 1 0 0\n  Suining       0 0 0 0 1 0\n  Taojiang      0 1 0 0 0 0\n  Taoyuan       0 1 0 0 0 0\n  Tongdao       0 1 0 0 0 0\n  Wangcheng     0 0 0 1 0 0\n  Wugang        0 0 1 0 0 0\n  Xiangtan      0 0 0 1 0 0\n  Xiangxiang    0 0 0 0 1 0\n  Xiangyin      0 0 0 1 0 0\n  Xinhua        0 0 0 0 1 0\n  Xinhuang      1 0 0 0 0 0\n  Xinning       0 1 0 0 0 0\n  Xinshao       0 0 0 0 0 1\n  Xintian       0 0 0 0 1 0\n  Xupu          0 1 0 0 0 0\n  Yanling       0 0 1 0 0 0\n  Yizhang       1 0 0 0 0 0\n  Yongshun      0 0 0 1 0 0\n  Yongxing      0 0 0 1 0 0\n  You           0 0 0 1 0 0\n  Yuanjiang     0 0 0 0 1 0\n  Yuanling      1 0 0 0 0 0\n  Yueyang       0 0 1 0 0 0\n  Zhijiang      0 0 0 0 1 0\n  Zhongfang     0 0 0 1 0 0\n  Zhuzhou       0 0 0 0 1 0\n  Zixing        0 0 1 0 0 0\n\n\n\n\nCode Chunk\nn_comp &lt;- n.comp.nb(wm_d62)\n(n_comp$nc)\n\n\n[1] 1\n\n\n\n\nCode Chunk\ntable(n_comp$comp.id)\n\n\n\n 1 \n88 \n\n\n\n\nNext, we will plot the distance weight matrix by using the code chunk below.\n\n\nCode Chunk\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_d62, coords, add=TRUE)\nplot(k1, coords, add=TRUE, col=\"red\", length=0.08)\n\n\n\n\n\n\n\n\n\nThe red lines show the links of 1st nearest neighbours and the black lines show the links of neighbours within the cut-off distance of 62km.\nAlternatively, we can plot both of them next to each other by using the code chunk below.\n\n\nCode Chunk\npar(mfrow=c(1,2))\nplot(hunan$geometry, border =\"lightgrey\", main=\"1st Nearest Neighbours\")\nplot(k1, coords, add=TRUE, col\"red\", length=0.08)\nplot(hunan$geometry, border=\"lightgrey\", main=\"Distance Link\")\nplot(wd_d62, coords, add=TRUE, pch=19, cex=0.6)\n\n\n\n\n\n\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\n\n\nCode Chunk\nknn6 &lt;- knn2nb(knearneigh(coords, k=6))\n\n\nSimilarly, we can display the content of the matrix by using str().\n\n\nCode Chunk\nstr(knn6)\n\n\nList of 88\n $ : int [1:6] 2 3 4 5 57 64\n $ : int [1:6] 1 3 57 58 78 85\n $ : int [1:6] 1 2 4 5 57 85\n $ : int [1:6] 1 3 5 6 69 85\n $ : int [1:6] 1 3 4 6 69 85\n $ : int [1:6] 3 4 5 69 75 85\n $ : int [1:6] 9 66 67 71 74 84\n $ : int [1:6] 9 46 47 78 80 86\n $ : int [1:6] 8 46 66 68 84 86\n $ : int [1:6] 16 19 22 70 72 73\n $ : int [1:6] 10 14 16 17 70 72\n $ : int [1:6] 13 15 60 61 63 83\n $ : int [1:6] 12 15 60 61 63 83\n $ : int [1:6] 11 15 16 17 72 83\n $ : int [1:6] 12 13 14 17 60 83\n $ : int [1:6] 10 11 17 22 72 83\n $ : int [1:6] 10 11 14 16 72 83\n $ : int [1:6] 20 22 23 63 77 83\n $ : int [1:6] 10 20 21 73 74 82\n $ : int [1:6] 18 19 21 22 23 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:6] 10 16 18 19 20 83\n $ : int [1:6] 18 20 41 77 79 82\n $ : int [1:6] 25 28 31 52 54 81\n $ : int [1:6] 24 28 31 33 54 81\n $ : int [1:6] 25 27 29 33 42 81\n $ : int [1:6] 26 29 30 37 42 81\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:6] 26 27 37 42 43 81\n $ : int [1:6] 26 27 28 33 49 81\n $ : int [1:6] 24 25 36 39 40 54\n $ : int [1:6] 24 31 50 54 55 56\n $ : int [1:6] 25 26 28 30 49 81\n $ : int [1:6] 36 40 41 45 56 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:6] 26 27 29 42 43 44\n $ : int [1:6] 23 43 44 62 77 79\n $ : int [1:6] 25 40 42 43 44 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:6] 26 27 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:6] 37 38 39 42 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:6] 8 9 35 47 78 86\n $ : int [1:6] 8 21 35 46 80 86\n $ : int [1:6] 49 50 51 52 53 55\n $ : int [1:6] 28 33 48 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:6] 28 48 49 50 52 54\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:6] 48 50 51 52 55 75\n $ : int [1:6] 24 28 49 50 51 52\n $ : int [1:6] 32 48 50 52 53 75\n $ : int [1:6] 32 34 36 78 80 85\n $ : int [1:6] 1 2 3 58 64 68\n $ : int [1:6] 2 57 64 66 68 78\n $ : int [1:6] 12 13 60 61 87 88\n $ : int [1:6] 12 13 59 61 63 87\n $ : int [1:6] 12 13 60 62 63 87\n $ : int [1:6] 12 38 61 63 77 87\n $ : int [1:6] 12 18 60 61 62 83\n $ : int [1:6] 1 3 57 58 68 76\n $ : int [1:6] 58 64 66 67 68 76\n $ : int [1:6] 9 58 67 68 76 84\n $ : int [1:6] 7 65 66 68 76 84\n $ : int [1:6] 9 57 58 66 78 84\n $ : int [1:6] 4 5 6 32 75 85\n $ : int [1:6] 10 16 19 22 72 73\n $ : int [1:6] 7 19 73 74 84 86\n $ : int [1:6] 10 11 14 16 17 70\n $ : int [1:6] 10 19 21 70 71 74\n $ : int [1:6] 19 21 71 73 84 86\n $ : int [1:6] 6 32 50 53 55 69\n $ : int [1:6] 58 64 65 66 67 68\n $ : int [1:6] 18 23 38 61 62 63\n $ : int [1:6] 2 8 9 46 58 68\n $ : int [1:6] 38 40 41 43 44 45\n $ : int [1:6] 34 35 36 41 45 47\n $ : int [1:6] 25 26 28 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:6] 12 13 15 16 22 63\n $ : int [1:6] 7 9 66 68 71 74\n $ : int [1:6] 2 3 4 5 56 69\n $ : int [1:6] 8 9 21 46 47 74\n $ : int [1:6] 59 60 61 62 63 88\n $ : int [1:6] 59 60 61 62 63 87\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language knearneigh(x = coords, k = 6)\n - attr(*, \"sym\")= logi FALSE\n - attr(*, \"type\")= chr \"knn\"\n - attr(*, \"knn-k\")= num 6\n - attr(*, \"class\")= chr \"nb\"\n\n\nNotice that each county has six neighbours, no less no more!\n\n\nWe can plot the weight matrix using the code chunk below.\n\n\nCode Chunk\nplot(hunan$geometry, border=\"lightgrey\")\nplot(knn6, coords, pch=19, cex=0.6, add=TRUE, col-\"red\")\n\n\n\n\n\n\n\nIn this section, you will learn how to derive a spatial weight matrix based on Inversed Distance method.\nFirst, we will compute the distances between areas by using nbdists() of spdep.\n\n\nCode Chunk\ndist &lt;- nbdists(wm_q, coords, longlat = TRUE)\nids &lt;- lapply(dist, function(x) 1/(x))\nids\n\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\n[[2]]\n[1] 0.01535405 0.01764308 0.01925924 0.02323898 0.01719350\n\n[[3]]\n[1] 0.03916350 0.02822040 0.03695795 0.01395765\n\n[[4]]\n[1] 0.01820896 0.02822040 0.03414741 0.01539065\n\n[[5]]\n[1] 0.03695795 0.03414741 0.01524598 0.01618354\n\n[[6]]\n[1] 0.015390649 0.015245977 0.021748129 0.011883901 0.009810297\n\n[[7]]\n[1] 0.01708612 0.01473997 0.01150924 0.01872915\n\n[[8]]\n[1] 0.02022144 0.03453056 0.02529256 0.01036340 0.02284457 0.01500600 0.01515314\n\n[[9]]\n[1] 0.02022144 0.01574888 0.02109502 0.01508028 0.02902705 0.01502980\n\n[[10]]\n[1] 0.02281552 0.01387777 0.01538326 0.01346650 0.02100510 0.02631658 0.01874863\n[8] 0.01500046\n\n[[11]]\n[1] 0.01882869 0.02243492 0.02247473\n\n[[12]]\n[1] 0.02779227 0.02419652 0.02333385 0.02986130 0.02335429\n\n[[13]]\n[1] 0.02779227 0.02650020 0.02670323 0.01714243\n\n[[14]]\n[1] 0.01882869 0.01233868 0.02098555\n\n[[15]]\n[1] 0.02650020 0.01233868 0.01096284 0.01562226\n\n[[16]]\n[1] 0.02281552 0.02466962 0.02765018 0.01476814 0.01671430\n\n[[17]]\n[1] 0.01387777 0.02243492 0.02098555 0.01096284 0.02466962 0.01593341 0.01437996\n\n[[18]]\n[1] 0.02039779 0.02032767 0.01481665 0.01473691 0.01459380\n\n[[19]]\n[1] 0.01538326 0.01926323 0.02668415 0.02140253 0.01613589 0.01412874\n\n[[20]]\n[1] 0.01346650 0.02039779 0.01926323 0.01723025 0.02153130 0.01469240 0.02327034\n\n[[21]]\n[1] 0.02668415 0.01723025 0.01766299 0.02644986 0.02163800\n\n[[22]]\n[1] 0.02100510 0.02765018 0.02032767 0.02153130 0.01489296\n\n[[23]]\n[1] 0.01481665 0.01469240 0.01401432 0.02246233 0.01880425 0.01530458 0.01849605\n\n[[24]]\n[1] 0.02354598 0.01837201 0.02607264 0.01220154 0.02514180\n\n[[25]]\n[1] 0.02354598 0.02188032 0.01577283 0.01949232 0.02947957\n\n[[26]]\n[1] 0.02155798 0.01745522 0.02212108 0.02220532\n\n[[27]]\n[1] 0.02155798 0.02490625 0.01562326\n\n[[28]]\n[1] 0.01837201 0.02188032 0.02229549 0.03076171 0.02039506\n\n[[29]]\n[1] 0.02490625 0.01686587 0.01395022\n\n[[30]]\n[1] 0.02090587\n\n[[31]]\n[1] 0.02607264 0.01577283 0.01219005 0.01724850 0.01229012 0.01609781 0.01139438\n[8] 0.01150130\n\n[[32]]\n[1] 0.01220154 0.01219005 0.01712515 0.01340413 0.01280928 0.01198216 0.01053374\n[8] 0.01065655\n\n[[33]]\n[1] 0.01949232 0.01745522 0.02229549 0.02090587 0.01979045\n\n[[34]]\n[1] 0.03113041 0.03589551 0.02882915\n\n[[35]]\n[1] 0.01766299 0.02185795 0.02616766 0.02111721 0.02108253 0.01509020\n\n[[36]]\n[1] 0.01724850 0.03113041 0.01571707 0.01860991 0.02073549 0.01680129\n\n[[37]]\n[1] 0.01686587 0.02234793 0.01510990 0.01550676\n\n[[38]]\n[1] 0.01401432 0.02407426 0.02276151 0.01719415\n\n[[39]]\n[1] 0.01229012 0.02172543 0.01711924 0.02629732 0.01896385\n\n[[40]]\n[1] 0.01609781 0.01571707 0.02172543 0.01506473 0.01987922 0.01894207\n\n[[41]]\n[1] 0.02246233 0.02185795 0.02205991 0.01912542 0.01601083 0.01742892\n\n[[42]]\n[1] 0.02212108 0.01562326 0.01395022 0.02234793 0.01711924 0.01836831 0.01683518\n\n[[43]]\n[1] 0.01510990 0.02629732 0.01506473 0.01836831 0.03112027 0.01530782\n\n[[44]]\n[1] 0.01550676 0.02407426 0.03112027 0.01486508\n\n[[45]]\n[1] 0.03589551 0.01860991 0.01987922 0.02205991 0.02107101 0.01982700\n\n[[46]]\n[1] 0.03453056 0.04033752 0.02689769\n\n[[47]]\n[1] 0.02529256 0.02616766 0.04033752 0.01949145 0.02181458\n\n[[48]]\n[1] 0.02313819 0.03370576 0.02289485 0.01630057 0.01818085\n\n[[49]]\n[1] 0.03076171 0.02138091 0.02394529 0.01990000\n\n[[50]]\n[1] 0.01712515 0.02313819 0.02551427 0.02051530 0.02187179\n\n[[51]]\n[1] 0.03370576 0.02138091 0.02873854\n\n[[52]]\n[1] 0.02289485 0.02394529 0.02551427 0.02873854 0.03516672\n\n[[53]]\n[1] 0.01630057 0.01979945 0.01253977\n\n[[54]]\n[1] 0.02514180 0.02039506 0.01340413 0.01990000 0.02051530 0.03516672\n\n[[55]]\n[1] 0.01280928 0.01818085 0.02187179 0.01979945 0.01882298\n\n[[56]]\n[1] 0.01036340 0.01139438 0.01198216 0.02073549 0.01214479 0.01362855 0.01341697\n\n[[57]]\n[1] 0.028079221 0.017643082 0.031423501 0.029114131 0.013520292 0.009903702\n\n[[58]]\n[1] 0.01925924 0.03142350 0.02722997 0.01434859 0.01567192\n\n[[59]]\n[1] 0.01696711 0.01265572 0.01667105 0.01785036\n\n[[60]]\n[1] 0.02419652 0.02670323 0.01696711 0.02343040\n\n[[61]]\n[1] 0.02333385 0.01265572 0.02343040 0.02514093 0.02790764 0.01219751 0.02362452\n\n[[62]]\n[1] 0.02514093 0.02002219 0.02110260\n\n[[63]]\n[1] 0.02986130 0.02790764 0.01407043 0.01805987\n\n[[64]]\n[1] 0.02911413 0.01689892\n\n[[65]]\n[1] 0.02471705\n\n[[66]]\n[1] 0.01574888 0.01726461 0.03068853 0.01954805 0.01810569\n\n[[67]]\n[1] 0.01708612 0.01726461 0.01349843 0.01361172\n\n[[68]]\n[1] 0.02109502 0.02722997 0.03068853 0.01406357 0.01546511\n\n[[69]]\n[1] 0.02174813 0.01645838 0.01419926\n\n[[70]]\n[1] 0.02631658 0.01963168 0.02278487\n\n[[71]]\n[1] 0.01473997 0.01838483 0.03197403\n\n[[72]]\n[1] 0.01874863 0.02247473 0.01476814 0.01593341 0.01963168\n\n[[73]]\n[1] 0.01500046 0.02140253 0.02278487 0.01838483 0.01652709\n\n[[74]]\n[1] 0.01150924 0.01613589 0.03197403 0.01652709 0.01342099 0.02864567\n\n[[75]]\n[1] 0.011883901 0.010533736 0.012539774 0.018822977 0.016458383 0.008217581\n\n[[76]]\n[1] 0.01352029 0.01434859 0.01689892 0.02471705 0.01954805 0.01349843 0.01406357\n\n[[77]]\n[1] 0.014736909 0.018804247 0.022761507 0.012197506 0.020022195 0.014070428\n[7] 0.008440896\n\n[[78]]\n[1] 0.02323898 0.02284457 0.01508028 0.01214479 0.01567192 0.01546511 0.01140779\n\n[[79]]\n[1] 0.01530458 0.01719415 0.01894207 0.01912542 0.01530782 0.01486508 0.02107101\n\n[[80]]\n[1] 0.01500600 0.02882915 0.02111721 0.01680129 0.01601083 0.01982700 0.01949145\n[8] 0.01362855\n\n[[81]]\n[1] 0.02947957 0.02220532 0.01150130 0.01979045 0.01896385 0.01683518\n\n[[82]]\n[1] 0.02327034 0.02644986 0.01849605 0.02108253 0.01742892\n\n[[83]]\n[1] 0.023354289 0.017142433 0.015622258 0.016714303 0.014379961 0.014593799\n[7] 0.014892965 0.018059871 0.008440896\n\n[[84]]\n[1] 0.01872915 0.02902705 0.01810569 0.01361172 0.01342099 0.01297994\n\n[[85]]\n [1] 0.011451133 0.017193502 0.013957649 0.016183544 0.009810297 0.010656545\n [7] 0.013416965 0.009903702 0.014199260 0.008217581 0.011407794\n\n[[86]]\n[1] 0.01515314 0.01502980 0.01412874 0.02163800 0.01509020 0.02689769 0.02181458\n[8] 0.02864567 0.01297994\n\n[[87]]\n[1] 0.01667105 0.02362452 0.02110260 0.02058034\n\n[[88]]\n[1] 0.01785036 0.02058034\n\n\n\n\n\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors’ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we’ll stick with the style=“W” option for simplicity’s sake but note that other more robust options are available, notably style=“B”.\n\n\nCode Chunk\nrswm_q &lt;- nb2listw(wm_q, style=\"W\", zero.policy = TRUE)\nrswm_q\n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\nThe zero.policy=TRUE option allows for lists of non-neighbors. This should be used with caution since the user may not be aware of missing neighbors in their dataset however, a zero.policy of FALSE would return an error.\nTo see the weight of the first polygon’s eight neighbors type:\n\n\nCode Chunk\nrswm_q$weights[10]\n\n\n[[1]]\n[1] 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125\n\n\nEach neighbor is assigned a 0.125 of the total weight. This means that when R computes the average neighboring income values, each neighbor’s income will be multiplied by 0.125 before being tallied.\nUsing the same method, we can also derive a row standardised distance weight matrix by using the code chunk below.\n\n\nCode Chunk\nrswm_ids &lt;- nb2listw(wm_q, glist=ids, style=\"B\", zero.policy=TRUE)\nrswm_ids\n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn       S0        S1     S2\nB 88 7744 8.786867 0.3776535 3.8137\n\n\n\n\nCode Chunk\nrswm_ids$weights[1]\n\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\n\n\n\nCode Chunk\nsummary(unlist(rswm_ids$weights))\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n0.008218 0.015088 0.018739 0.019614 0.022823 0.040338 \n\n\n\n\n\nIn this section, you will learn how to create four different spatial lagged variables, they are:\n\nspatial lag with row-standardized weights,\nspatial lag as a sum of neighbouring values,\nspatial window average, and\nspatial window sum.\n\n\n\nFinally, we’ll compute the average neighbor GDPPC value for each polygon. These values are often referred to as spatially lagged values.\n\n\nCode Chunk\nGDPPC.lag &lt;- lag.listw(rswm_q, hunan$GDPPC)\nGDPPC.lag\n\n\n [1] 24847.20 22724.80 24143.25 27737.50 27270.25 21248.80 43747.00 33582.71\n [9] 45651.17 32027.62 32671.00 20810.00 25711.50 30672.33 33457.75 31689.20\n[17] 20269.00 23901.60 25126.17 21903.43 22718.60 25918.80 20307.00 20023.80\n[25] 16576.80 18667.00 14394.67 19848.80 15516.33 20518.00 17572.00 15200.12\n[33] 18413.80 14419.33 24094.50 22019.83 12923.50 14756.00 13869.80 12296.67\n[41] 15775.17 14382.86 11566.33 13199.50 23412.00 39541.00 36186.60 16559.60\n[49] 20772.50 19471.20 19827.33 15466.80 12925.67 18577.17 14943.00 24913.00\n[57] 25093.00 24428.80 17003.00 21143.75 20435.00 17131.33 24569.75 23835.50\n[65] 26360.00 47383.40 55157.75 37058.00 21546.67 23348.67 42323.67 28938.60\n[73] 25880.80 47345.67 18711.33 29087.29 20748.29 35933.71 15439.71 29787.50\n[81] 18145.00 21617.00 29203.89 41363.67 22259.09 44939.56 16902.00 16930.00\n\n\nRecalled in the previous section, we retrieved the GDPPC of these five countries by using the code chunk below.\n\n\nCode Chunk\nnb1 &lt;- wm_q[[1]]\nnb1 &lt;- hunan$GDPPC[nb1]\nnb1\n\n\n[1] 20981 34592 24473 21311 22879\n\n\nWe can append the spatially lag GDPPC values onto hunan sf data frame by using the code chunk below.\n\n\nCode Chunk\nlag.list &lt;- list(hunan$NAME_3, lag.listw(rswm_q, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag.list)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag GDPPC\")\nhunan &lt;- left_join(hunan,lag.res)\n\n\nThe following table shows the average neighboring income values (stored in the Inc.lag object) for each county.\n\n\nCode Chunk\nhead(hunan)\n\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3  County GDPPC lag GDPPC\n1 Changde 21098 Anxiang      County Anxiang 23667  24847.20\n2 Changde 21100 Hanshou      County Hanshou 20981  22724.80\n3 Changde 21101  Jinshi County City  Jinshi 34592  24143.25\n4 Changde 21102      Li      County      Li 24473  27737.50\n5 Changde 21103   Linli      County   Linli 25554  27270.25\n6 Changde 21104  Shimen      County  Shimen 27137  21248.80\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n4 POLYGON ((111.3731 29.94649...\n5 POLYGON ((111.6324 29.76288...\n6 POLYGON ((110.8825 30.11675...\n\n\nNext, we will plot both the GDPPC and spatial lag GDPPC for comparison using the code chunk below.\n\n\nCode Chunk\ngdppc &lt;- qtm(hunan, \"GDPPC\")\nlag_gdppc &lt;- qtm(hunan, \"lag GDPPC\")\ntmap_arrange(gdppc, lag_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\nWe can calculate spatial lag as a sum of neighboring values by assigning binary weights. This requires us to go back to our neighbors list, then apply a function that will assign binary weights, then we use glist = in the nb2listw function to explicitly assign these weights.\nWe start by applying a function that will assign a value of 1 per each neighbor. This is done with lapply, which we have been using to manipulate the neighbors structure throughout the past notebooks. Basically it applies a function across each value in the neighbors structure.\n\n\nCode Chunk\nb_weights &lt;- lapply(wm_q, function(x) 0*x + 1)\nb_weights2 &lt;- nb2listw(wm_q, \n                       glist = b_weights, \n                       style = \"B\")\nprint(b_weights2)\n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1    S2\nB 88 7744 448 896 10224\n\n\nWith the proper weights assigned, we can use lag.listw to compute a lag variable from our weight and GDPPC.\n\n\nCode Chunk\nlag_sum &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag_sum)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag_sum GDPPC\")\n\n\nFirst, let us examine the result by using the code chunk below.\n\n\nCode Chunk\nlag_sum\n\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 124236 113624  96573 110950 109081 106244 174988 235079 273907 256221\n[11]  98013 104050 102846  92017 133831 158446 141883 119508 150757 153324\n[21] 113593 129594 142149 100119  82884  74668  43184  99244  46549  20518\n[31] 140576 121601  92069  43258 144567 132119  51694  59024  69349  73780\n[41]  94651 100680  69398  52798 140472 118623 180933  82798  83090  97356\n[51]  59482  77334  38777 111463  74715 174391 150558 122144  68012  84575\n[61] 143045  51394  98279  47671  26360 236917 220631 185290  64640  70046\n[71] 126971 144693 129404 284074 112268 203611 145238 251536 108078 238300\n[81] 108870 108085 262835 248182 244850 404456  67608  33860\n\n\nNext, we will append the lag_sum GDPPC field into hunan sf data frame by using the code chunk below.\n\n\nCode Chunk\nhunan &lt;- left_join(hunan, lag.res)\n\n\nNow, We can plot both the GDPPC and Spatial Lag Sum GDPPC for comparison using the code chunk below.\n\n\nCode Chunk\ngdppc &lt;- qtm(hunan, \"GDPPC\")\nlag_sum_gdppc &lt;- qtm(hunan, \"lag_sum GDPPC\")\ntmap_arrange(gdppc, lag_sum_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\nThe spatial window average uses row-standardized weights and includes the diagonal element. To do this in R, we need to go back to the neighbors structure and add the diagonal element before assigning weights.\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\n\nCode Chunk\nwm_qs &lt;- include.self(wm_q)\n\n\nNotice that the Number of nonzero links, Percentage nonzero weights and Average number of links are 536, 6.921488 and 6.090909 respectively as compared to wm_q of 448, 5.785124 and 5.090909\nLet us take a good look at the neighbour list of area [1] by using the code chunk below.\n\n\nCode Chunk\nwm_qs[[1]]\n\n\n[1]  1  2  3  4 57 85\n\n\nNotice that now [1] has six neighbours instead of five.\nNow we obtain weights with nb2listw()\n\n\nCode Chunk\nwm_qs &lt;- nb2listw(wm_qs)\nwm_qs\n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 30.90265 357.5308\n\n\nAgain, we use nb2listw() and glist() to explicitly assign weight values.\nLastly, we just need to create the lag variable from our weight structure and GDPPC variable.\n\n\nCode Chunk\nlag_w_avg_gpdpc &lt;- lag.listw(wm_qs,\n                              hunan$GDPPC)\nlag_w_avg_gpdpc\n\n\n [1] 24650.50 22434.17 26233.00 27084.60 26927.00 22230.17 47621.20 37160.12\n [9] 49224.71 29886.89 26627.50 22690.17 25366.40 25825.75 30329.00 32682.83\n[17] 25948.62 23987.67 25463.14 21904.38 23127.50 25949.83 20018.75 19524.17\n[25] 18955.00 17800.40 15883.00 18831.33 14832.50 17965.00 17159.89 16199.44\n[33] 18764.50 26878.75 23188.86 20788.14 12365.20 15985.00 13764.83 11907.43\n[41] 17128.14 14593.62 11644.29 12706.00 21712.29 43548.25 35049.00 16226.83\n[49] 19294.40 18156.00 19954.75 18145.17 12132.75 18419.29 14050.83 23619.75\n[57] 24552.71 24733.67 16762.60 20932.60 19467.75 18334.00 22541.00 26028.00\n[65] 29128.50 46569.00 47576.60 36545.50 20838.50 22531.00 42115.50 27619.00\n[73] 27611.33 44523.29 18127.43 28746.38 20734.50 33880.62 14716.38 28516.22\n[81] 18086.14 21244.50 29568.80 48119.71 22310.75 43151.60 17133.40 17009.33\n\n\nNext, we will convert the lag variable listw object into a data.frame by using as.data.frame().\n\n\nCode Chunk\nlag.list.wm_qs &lt;- list(hunan$NAME_3, lag.listw(wm_qs, hunan$GDPPC))\nlag_wm_qs.res &lt;- as.data.frame(lag.list.wm_qs)\ncolnames(lag_wm_qs.res) &lt;- c(\"NAME_3\", \"lag_window_avg GDPPC\")\n\n\nNote: The third command line on the code chunk above renames the field names of lag_wm_q1.res object into NAME_3 and lag_window_avg GDPPC respectively.\nNext, the code chunk below will be used to append lag_window_avg GDPPC values onto hunan sf data.frame by using left_join() of dplyr package.\n\n\nCode Chunk\nhunan &lt;- left_join(hunan, lag_wm_qs.res)\n\n\nTo compare the values of lag GDPPC and Spatial window average, kable() of Knitr package is used to prepare a table using the code chunk below.\n\n\nCode Chunk\nhunan %&gt;%\n  select(\"County\", \n         \"lag GDPPC\", \n         \"lag_window_avg GDPPC\") %&gt;%\n  kable()\n\n\nLastly, qtm() of tmap package is used to plot the lag_gdppc and w_ave_gdppc maps next to each other for quick comparison.\n\n\nCode Chunk\nw_avg_gdppc &lt;- qtm(hunan, \"lag_window_avg GDPPC\")\ntmap_arrange(lag_gdppc, w_avg_gdppc, asp=1, ncol=2)\n\n\n\n\n\nThe spatial window sum is the counter part of the window average, but without using row-standardized weights.\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\n\nCode Chunk\nwm_qs &lt;- include.self(wm_q)\nwm_qs\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\n\nNext, we will assign binary weights to the neighbour structure that includes the diagonal element.\n\n\nCode Chunk\nb_weights &lt;- lapply(wm_qs, function(x) 0*x +1)\nb_weights[1]\n\n\n[[1]]\n[1] 1 1 1 1 1 1\n\n\nNotice that now [1] has six neighbours instead of five.\nAgain, we use nb2listw() and glist() to explicitly assign weight values.\n\n\nCode Chunk\nb_weights2 &lt;- nb2listw(wm_qs,\n                        glist = b_weights,\n                        style = \"B\")\nb_weights2\n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 536 1072 14160\n\n\nWith our new weight structure, we can compute the lag variable with lag.listw().\n\n\nCode Chunk\nw_sum_gdppc &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nw_sum(gdppc)\n\n\nNext, we will convert the lag variable listw object into a data.frame by using as.data.frame().\n\n\nCode Chunk\nw_sum_gdppc.res &lt;- as.data.frame(w_sum_gdppc)\ncolnames(w_sum_gdppc.res) &lt;- c(\"NAMES_3\", \"w_sum GDPPC\")\n\n\nNote: The second command line on the code chunk above renames the field names of w_sum_gdppc.res object into NAME_3 and w_sum GDPPC respectively.\nNext, the code chunk below will be used to append w_sum GDPPC values onto hunan sf data.frame by using left_join() of dplyr package.\n\n\nCode Chunk\nhunan &lt;- left_join(hunan, w_sum_gdppc.res)\n\n\nTo compare the values of lag GDPPC and Spatial window average, kable() of Knitr package is used to prepare a table using the code chunk below.\n\n\nCode Chunk\nhunan %&gt;%\n  select(\"County\", \"lag_sum GDPPC\", \"w_sum GDPPC\") %&gt;%\n  kable()\n\n\nLastly, qtm() of tmap package is used to plot the lag_sum GDPPC and w_sum_gdppc maps next to each other for quick comparison.\n\n\nCode Chunk\nw_sum_gdppc &lt;- qtm(hunan, \"w_sum GDPPC\")\ntmap_arrange(lag_sum_gdppc, w_sum_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\nCreating Neighbours using sf objects"
  },
  {
    "objectID": "HandsOnExercise/HandsOn4/HandsOn4.html#overview",
    "href": "HandsOnExercise/HandsOn4/HandsOn4.html#overview",
    "title": "Hands-On Exercise 4",
    "section": "",
    "text": "In this hands-on exercise, you will learn how to compute spatial weights using R. By the end to this hands-on exercise, you will be able to:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute spatial weights using appropriate functions of spdep package, and\ncalculate spatially lagged variables using appropriate functions of spdep package."
  },
  {
    "objectID": "HandsOnExercise/HandsOn4/HandsOn4.html#the-study-area-and-data",
    "href": "HandsOnExercise/HandsOn4/HandsOn4.html#the-study-area-and-data",
    "title": "Hands-On Exercise 4",
    "section": "",
    "text": "Two data sets will be used in this hands-on exercise, they are:\n\nHunan county boundary layer. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv: This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n\nBefore we get started, we need to ensure that spdep, sf, tmap and tidyverse packages of R are currently installed in your R.\n\n\nCode Chunk\npacman::p_load(sf, spdep, tmap, tidyverse, knitr, spNetwork, dplyr)"
  },
  {
    "objectID": "HandsOnExercise/HandsOn4/HandsOn4.html#getting-the-data-into-r-environment",
    "href": "HandsOnExercise/HandsOn4/HandsOn4.html#getting-the-data-into-r-environment",
    "title": "Hands-On Exercise 4",
    "section": "",
    "text": "In this section, you will learn how to bring a geospatial data and its associated attribute table into R environment. The geospatial data is in ESRI shapefile format and the attribute table is in csv fomat.\n\n\nThe code chunk below uses st_read() of sf package to import Hunan shapefile into R. The imported shapefile will be simple features Object of sf.\n\n\nCode Chunk\nhunan &lt;- st_read(dsn = \"data/geospatial\",\n                    layer=\"Hunan\")\n\n\nReading layer `Hunan' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/HandsOnExercise/HandsOn4/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\nNext, we will import Hunan_2012.csv into R by using read_csv() of readr package. The output is R dataframe class.\n\n\nCode Chunk\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n\n\n\nThe code chunk below will be used to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\n\n\nCode Chunk\nhunan &lt;- left_join(hunan, hunan2012) %&gt;%\n  select(1:4, 7, 15)"
  },
  {
    "objectID": "HandsOnExercise/HandsOn4/HandsOn4.html#visualising-regional-development-indicator",
    "href": "HandsOnExercise/HandsOn4/HandsOn4.html#visualising-regional-development-indicator",
    "title": "Hands-On Exercise 4",
    "section": "",
    "text": "Now, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\n\nCode Chunk\nst_crs(hunan)\n\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\n\n\nCode Chunk\nbasemap &lt;- tm_shape(hunan) +\n  tm_polygons() +\n  tm_text(\"NAME_3\", size=0.5)\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\ntmap_arrange(basemap, gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "HandsOnExercise/HandsOn4/HandsOn4.html#computing-contiguity-spatial-weights",
    "href": "HandsOnExercise/HandsOn4/HandsOn4.html#computing-contiguity-spatial-weights",
    "title": "Hands-On Exercise 4",
    "section": "",
    "text": "In this section, you will learn how to use poly2nb() of spdep package to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries. If you look at the documentation you will see that you can pass a “queen” argument that takes TRUE or FALSE as options. If you do not specify this argument the default is set to TRUE, that is, if you don’t specify queen = FALSE this function will return a list of first order neighbours using the Queen criteria.\n\n\nThe code chunk below is used to compute Queen contiguity weight matrix.\n\n\nCode Chunk\nwm_q &lt;- poly2nb(hunan, queen=TRUE)\nsummary(wm_q)\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one heighbours.\nFor each polygon in our polygon object, wm_q lists all neighboring polygons. For example, to see the neighbors for the first polygon in the object, type:\n\n\nCode Chunk\nwm_q\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\n\nPolygon 1 has 5 neighbors. The numbers represent the polygon IDs as stored in hunan SpatialPolygonsDataFrame class.\nWe can retrive the county name of Polygon ID=1 by using the code chunk below:\n\n\nCode Chunk\nhunan$County\n\n\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n\nThe output reveals that Polygon ID=1 is Anxiang county.\nTo reveal the county names of the five neighboring polygons, the code chunk will be used:\n\n\nCode Chunk\nhunan$NAME_3[c(2,3,4,57,85)]\n\n\n[1] \"Hanshou\" \"Jinshi\"  \"Li\"      \"Nan\"     \"Taoyuan\"\n\n\nWe can retrieve the GDPPC of these five countries by using the code chunk below.\n\n\nCode Chunk\nnb1 &lt;- wm_q[[1]]\nnb1 &lt;- hunan$GDPPC[nb1]\nnb1\n\n\n[1] 20981 34592 24473 21311 22879\n\n\nThe printed output above shows that the GDPPC of the five nearest neighbours based on Queen’s method are 20981, 34592, 24473, 21311 and 22879 respectively.\nYou can display the complete weight matrix by using str().\n\n\nCode Chunk\nstr(wm_q)\n\n\nList of 88\n $ : int [1:5] 2 3 4 57 85\n $ : int [1:5] 1 57 58 78 85\n $ : int [1:4] 1 4 5 85\n $ : int [1:4] 1 3 5 6\n $ : int [1:4] 3 4 6 85\n $ : int [1:5] 4 5 69 75 85\n $ : int [1:4] 67 71 74 84\n $ : int [1:7] 9 46 47 56 78 80 86\n $ : int [1:6] 8 66 68 78 84 86\n $ : int [1:8] 16 17 19 20 22 70 72 73\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:3] 11 15 17\n $ : int [1:4] 13 14 17 83\n $ : int [1:5] 10 17 22 72 83\n $ : int [1:7] 10 11 14 15 16 72 83\n $ : int [1:5] 20 22 23 77 83\n $ : int [1:6] 10 20 21 73 74 86\n $ : int [1:7] 10 18 19 21 22 23 82\n $ : int [1:5] 19 20 35 82 86\n $ : int [1:5] 10 16 18 20 83\n $ : int [1:7] 18 20 38 41 77 79 82\n $ : int [1:5] 25 28 31 32 54\n $ : int [1:5] 24 28 31 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:3] 26 29 42\n $ : int [1:5] 24 25 33 49 54\n $ : int [1:3] 27 37 42\n $ : int 33\n $ : int [1:8] 24 25 32 36 39 40 56 81\n $ : int [1:8] 24 31 50 54 55 56 75 85\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 47 80 82 86\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:4] 29 42 43 44\n $ : int [1:4] 23 44 77 79\n $ : int [1:5] 31 40 42 43 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:7] 26 27 29 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:4] 37 38 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:3] 8 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:5] 32 48 52 54 55\n $ : int [1:3] 48 49 52\n $ : int [1:5] 48 49 50 51 54\n $ : int [1:3] 48 55 75\n $ : int [1:6] 24 28 32 49 50 52\n $ : int [1:5] 32 48 50 53 75\n $ : int [1:7] 8 31 32 36 78 80 85\n $ : int [1:6] 1 2 58 64 76 85\n $ : int [1:5] 2 57 68 76 78\n $ : int [1:4] 60 61 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:7] 12 59 60 62 63 77 87\n $ : int [1:3] 61 77 87\n $ : int [1:4] 12 61 77 83\n $ : int [1:2] 57 76\n $ : int 76\n $ : int [1:5] 9 67 68 76 84\n $ : int [1:4] 7 66 76 84\n $ : int [1:5] 9 58 66 76 78\n $ : int [1:3] 6 75 85\n $ : int [1:3] 10 72 73\n $ : int [1:3] 7 73 74\n $ : int [1:5] 10 11 16 17 70\n $ : int [1:5] 10 19 70 71 74\n $ : int [1:6] 7 19 71 73 84 86\n $ : int [1:6] 6 32 53 55 69 85\n $ : int [1:7] 57 58 64 65 66 67 68\n $ : int [1:7] 18 23 38 61 62 63 83\n $ : int [1:7] 2 8 9 56 58 68 85\n $ : int [1:7] 23 38 40 41 43 44 45\n $ : int [1:8] 8 34 35 36 41 45 47 56\n $ : int [1:6] 25 26 31 33 39 42\n $ : int [1:5] 20 21 23 35 41\n $ : int [1:9] 12 13 15 16 17 18 22 63 77\n $ : int [1:6] 7 9 66 67 74 86\n $ : int [1:11] 1 2 3 5 6 32 56 57 69 75 ...\n $ : int [1:9] 8 9 19 21 35 46 47 74 84\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language poly2nb(pl = hunan, queen = TRUE)\n - attr(*, \"type\")= chr \"queen\"\n - attr(*, \"sym\")= logi TRUE\n\n\nBe warned: The output might cut across several pages. Save the trees if you are going to print out the report.\n\n\n\nThe code chunk below is used to compute Rook contiguity weight matrix.\n\n\nCode Chunk\nwm_r &lt;- poly2nb(hunan, queen=FALSE)\nsummary(wm_r)\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 440 \nPercentage nonzero weights: 5.681818 \nAverage number of links: 5 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 2  2 12 20 21 14 11  3  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 10 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connect area unit has 10 neighbours. There are two area units with only one heighbours.\n\n\n\nA connectivity graph takes a point and displays a line to each neighboring point. We are working with polygons at the moment, so we will need to get points in order to make our connectivity graphs. The most typically method for this will be polygon centroids. We will calculate these in the sf package before moving onto the graphs. Getting Latitude and Longitude of Polygon Centroids\nWe will need points to associate with each polygon before we can make our connectivity graph. It will be a little more complicated than just running st_centroid on the sf object: us.bound. We need the coordinates in a separate data frame for this to work. To do this we will use a mapping function. The mapping function applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column of us.bound. Our function will be st_centroid. We will be using map_dbl variation of map from the purrr package. For more documentation, check out map documentation\nTo get our longitude values we map the st_centroid function over the geometry column of us.bound and access the longitude value through double bracket notation [[]] and 1. This allows us to get only the longitude, which is the first value in each centroid.\n\n\nCode Chunk\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\n\nWe do the same for latitude with one key difference. We access the second value per each centroid with [[2]].\n\n\nCode Chunk\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\n\nNow that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.\n\n\nCode Chunk\ncoords &lt;- cbind(longitude, latitude)\n\n\nWe check the first few observations to see if things are formatted correctly.\n\n\nCode Chunk\nhead(coords)\n\n\n     longitude latitude\n[1,]  112.1531 29.44362\n[2,]  112.0372 28.86489\n[3,]  111.8917 29.47107\n[4,]  111.7031 29.74499\n[5,]  111.6138 29.49258\n[6,]  111.0341 29.79863\n\n\n\n\n\n\nCode Chunk\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_q, coords, pch=19, cex=0.6, add=TRUE, col=\"red\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\npar(mfrow=c(1,2))\nplot(hunan$geometry, borders=\"lightgrey\", main=\"Queen Contiguity\")\nplot(wm_q, coords, pch=19, cex=0.6, add=TRUE, col=\"red\")\nplot(hunan$geometry, border=\"lightgrey\", main=\"Rook Contiguity\")\nplot(wm_r, coords, pch=19, cex=0.6, add=TRUE, col=\"red\")"
  },
  {
    "objectID": "HandsOnExercise/HandsOn4/HandsOn4.html#computing-distance-based-neighbours",
    "href": "HandsOnExercise/HandsOn4/HandsOn4.html#computing-distance-based-neighbours",
    "title": "Hands-On Exercise 4",
    "section": "",
    "text": "In this section, you will learn how to derive distance-based weight matrices by using dnearneigh() of spdep package.\nThe function identifies neighbours of region points by Euclidean distance with a distance band with lower d1= and upper d2= bounds controlled by the bounds= argument. If unprojected coordinates are used and either specified in the coordinates object x or with x as a two column matrix and longlat=TRUE, great circle distances in km will be calculated assuming the WGS84 reference ellipsoid.\n\n\nFirstly, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\n\nCode Chunk\n#coords &lt;- coordinates(hunan)\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n\n\nNow, we will compute the distance weight matrix by using dnearneigh() as shown in the code chunk below.\n\n\nCode Chunk\nwm_d62 &lt;- dnearneigh(coords, 0, 62, longlat=TRUE)\nwm_d62\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\nQuiz: What is the meaning of “Average number of links: 3.681818” shown above?\nNext, we will use str() to display the content of wm_d62 weight matrix.\n\n\nCode Chunk\nstr(wm_d62)\n\n\nList of 88\n $ : int [1:5] 3 4 5 57 64\n $ : int [1:4] 57 58 78 85\n $ : int [1:4] 1 4 5 57\n $ : int [1:3] 1 3 5\n $ : int [1:4] 1 3 4 85\n $ : int 69\n $ : int [1:2] 67 84\n $ : int [1:4] 9 46 47 78\n $ : int [1:4] 8 46 68 84\n $ : int [1:4] 16 22 70 72\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:2] 11 17\n $ : int 13\n $ : int [1:4] 10 17 22 83\n $ : int [1:3] 11 14 16\n $ : int [1:3] 20 22 63\n $ : int [1:5] 20 21 73 74 82\n $ : int [1:5] 18 19 21 22 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:4] 10 16 18 20\n $ : int [1:3] 41 77 82\n $ : int [1:4] 25 28 31 54\n $ : int [1:4] 24 28 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:2] 26 29\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:2] 27 37\n $ : int 33\n $ : int [1:2] 24 36\n $ : int 50\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:5] 31 34 45 56 80\n $ : int [1:2] 29 42\n $ : int [1:3] 44 77 79\n $ : int [1:4] 40 42 43 81\n $ : int [1:3] 39 45 79\n $ : int [1:5] 23 35 45 79 82\n $ : int [1:5] 26 37 39 43 81\n $ : int [1:3] 39 42 44\n $ : int [1:2] 38 43\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:5] 8 9 35 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:4] 48 49 50 52\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:2] 48 55\n $ : int [1:5] 24 28 49 50 52\n $ : int [1:4] 48 50 53 75\n $ : int 36\n $ : int [1:5] 1 2 3 58 64\n $ : int [1:5] 2 57 64 66 68\n $ : int [1:3] 60 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:5] 12 60 62 63 87\n $ : int [1:4] 61 63 77 87\n $ : int [1:5] 12 18 61 62 83\n $ : int [1:4] 1 57 58 76\n $ : int 76\n $ : int [1:5] 58 67 68 76 84\n $ : int [1:2] 7 66\n $ : int [1:4] 9 58 66 84\n $ : int [1:2] 6 75\n $ : int [1:3] 10 72 73\n $ : int [1:2] 73 74\n $ : int [1:3] 10 11 70\n $ : int [1:4] 19 70 71 74\n $ : int [1:5] 19 21 71 73 86\n $ : int [1:2] 55 69\n $ : int [1:3] 64 65 66\n $ : int [1:3] 23 38 62\n $ : int [1:2] 2 8\n $ : int [1:4] 38 40 41 45\n $ : int [1:5] 34 35 36 45 47\n $ : int [1:5] 25 26 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:4] 12 13 16 63\n $ : int [1:4] 7 9 66 68\n $ : int [1:2] 2 5\n $ : int [1:4] 21 46 47 74\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language dnearneigh(x = coords, d1 = 0, d2 = 62, longlat = TRUE)\n - attr(*, \"dnn\")= num [1:2] 0 62\n - attr(*, \"bounds\")= chr [1:2] \"GE\" \"LE\"\n - attr(*, \"nbtype\")= chr \"distance\"\n - attr(*, \"sym\")= logi TRUE\n\n\nAnother way to display the structure of the weight matrix is to combine table() and card() of spdep.\n\n\nCode Chunk\ntable(hunan$County, card(wm_d62))\n\n\n               \n                1 2 3 4 5 6\n  Anhua         1 0 0 0 0 0\n  Anren         0 0 0 1 0 0\n  Anxiang       0 0 0 0 1 0\n  Baojing       0 0 0 0 1 0\n  Chaling       0 0 1 0 0 0\n  Changning     0 0 1 0 0 0\n  Changsha      0 0 0 1 0 0\n  Chengbu       0 1 0 0 0 0\n  Chenxi        0 0 0 1 0 0\n  Cili          0 1 0 0 0 0\n  Dao           0 0 0 1 0 0\n  Dongan        0 0 1 0 0 0\n  Dongkou       0 0 0 1 0 0\n  Fenghuang     0 0 0 1 0 0\n  Guidong       0 0 1 0 0 0\n  Guiyang       0 0 0 1 0 0\n  Guzhang       0 0 0 0 0 1\n  Hanshou       0 0 0 1 0 0\n  Hengdong      0 0 0 0 1 0\n  Hengnan       0 0 0 0 1 0\n  Hengshan      0 0 0 0 0 1\n  Hengyang      0 0 0 0 0 1\n  Hongjiang     0 0 0 0 1 0\n  Huarong       0 0 0 1 0 0\n  Huayuan       0 0 0 1 0 0\n  Huitong       0 0 0 1 0 0\n  Jiahe         0 0 0 0 1 0\n  Jianghua      0 0 1 0 0 0\n  Jiangyong     0 1 0 0 0 0\n  Jingzhou      0 1 0 0 0 0\n  Jinshi        0 0 0 1 0 0\n  Jishou        0 0 0 0 0 1\n  Lanshan       0 0 0 1 0 0\n  Leiyang       0 0 0 1 0 0\n  Lengshuijiang 0 0 1 0 0 0\n  Li            0 0 1 0 0 0\n  Lianyuan      0 0 0 0 1 0\n  Liling        0 1 0 0 0 0\n  Linli         0 0 0 1 0 0\n  Linwu         0 0 0 1 0 0\n  Linxiang      1 0 0 0 0 0\n  Liuyang       0 1 0 0 0 0\n  Longhui       0 0 1 0 0 0\n  Longshan      0 1 0 0 0 0\n  Luxi          0 0 0 0 1 0\n  Mayang        0 0 0 0 0 1\n  Miluo         0 0 0 0 1 0\n  Nan           0 0 0 0 1 0\n  Ningxiang     0 0 0 1 0 0\n  Ningyuan      0 0 0 0 1 0\n  Pingjiang     0 1 0 0 0 0\n  Qidong        0 0 1 0 0 0\n  Qiyang        0 0 1 0 0 0\n  Rucheng       0 1 0 0 0 0\n  Sangzhi       0 1 0 0 0 0\n  Shaodong      0 0 0 0 1 0\n  Shaoshan      0 0 0 0 1 0\n  Shaoyang      0 0 0 1 0 0\n  Shimen        1 0 0 0 0 0\n  Shuangfeng    0 0 0 0 0 1\n  Shuangpai     0 0 0 1 0 0\n  Suining       0 0 0 0 1 0\n  Taojiang      0 1 0 0 0 0\n  Taoyuan       0 1 0 0 0 0\n  Tongdao       0 1 0 0 0 0\n  Wangcheng     0 0 0 1 0 0\n  Wugang        0 0 1 0 0 0\n  Xiangtan      0 0 0 1 0 0\n  Xiangxiang    0 0 0 0 1 0\n  Xiangyin      0 0 0 1 0 0\n  Xinhua        0 0 0 0 1 0\n  Xinhuang      1 0 0 0 0 0\n  Xinning       0 1 0 0 0 0\n  Xinshao       0 0 0 0 0 1\n  Xintian       0 0 0 0 1 0\n  Xupu          0 1 0 0 0 0\n  Yanling       0 0 1 0 0 0\n  Yizhang       1 0 0 0 0 0\n  Yongshun      0 0 0 1 0 0\n  Yongxing      0 0 0 1 0 0\n  You           0 0 0 1 0 0\n  Yuanjiang     0 0 0 0 1 0\n  Yuanling      1 0 0 0 0 0\n  Yueyang       0 0 1 0 0 0\n  Zhijiang      0 0 0 0 1 0\n  Zhongfang     0 0 0 1 0 0\n  Zhuzhou       0 0 0 0 1 0\n  Zixing        0 0 1 0 0 0\n\n\n\n\nCode Chunk\nn_comp &lt;- n.comp.nb(wm_d62)\n(n_comp$nc)\n\n\n[1] 1\n\n\n\n\nCode Chunk\ntable(n_comp$comp.id)\n\n\n\n 1 \n88 \n\n\n\n\nNext, we will plot the distance weight matrix by using the code chunk below.\n\n\nCode Chunk\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_d62, coords, add=TRUE)\nplot(k1, coords, add=TRUE, col=\"red\", length=0.08)\n\n\n\n\n\n\n\n\n\nThe red lines show the links of 1st nearest neighbours and the black lines show the links of neighbours within the cut-off distance of 62km.\nAlternatively, we can plot both of them next to each other by using the code chunk below.\n\n\nCode Chunk\npar(mfrow=c(1,2))\nplot(hunan$geometry, border =\"lightgrey\", main=\"1st Nearest Neighbours\")\nplot(k1, coords, add=TRUE, col\"red\", length=0.08)\nplot(hunan$geometry, border=\"lightgrey\", main=\"Distance Link\")\nplot(wd_d62, coords, add=TRUE, pch=19, cex=0.6)\n\n\n\n\n\n\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\n\n\nCode Chunk\nknn6 &lt;- knn2nb(knearneigh(coords, k=6))\n\n\nSimilarly, we can display the content of the matrix by using str().\n\n\nCode Chunk\nstr(knn6)\n\n\nList of 88\n $ : int [1:6] 2 3 4 5 57 64\n $ : int [1:6] 1 3 57 58 78 85\n $ : int [1:6] 1 2 4 5 57 85\n $ : int [1:6] 1 3 5 6 69 85\n $ : int [1:6] 1 3 4 6 69 85\n $ : int [1:6] 3 4 5 69 75 85\n $ : int [1:6] 9 66 67 71 74 84\n $ : int [1:6] 9 46 47 78 80 86\n $ : int [1:6] 8 46 66 68 84 86\n $ : int [1:6] 16 19 22 70 72 73\n $ : int [1:6] 10 14 16 17 70 72\n $ : int [1:6] 13 15 60 61 63 83\n $ : int [1:6] 12 15 60 61 63 83\n $ : int [1:6] 11 15 16 17 72 83\n $ : int [1:6] 12 13 14 17 60 83\n $ : int [1:6] 10 11 17 22 72 83\n $ : int [1:6] 10 11 14 16 72 83\n $ : int [1:6] 20 22 23 63 77 83\n $ : int [1:6] 10 20 21 73 74 82\n $ : int [1:6] 18 19 21 22 23 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:6] 10 16 18 19 20 83\n $ : int [1:6] 18 20 41 77 79 82\n $ : int [1:6] 25 28 31 52 54 81\n $ : int [1:6] 24 28 31 33 54 81\n $ : int [1:6] 25 27 29 33 42 81\n $ : int [1:6] 26 29 30 37 42 81\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:6] 26 27 37 42 43 81\n $ : int [1:6] 26 27 28 33 49 81\n $ : int [1:6] 24 25 36 39 40 54\n $ : int [1:6] 24 31 50 54 55 56\n $ : int [1:6] 25 26 28 30 49 81\n $ : int [1:6] 36 40 41 45 56 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:6] 26 27 29 42 43 44\n $ : int [1:6] 23 43 44 62 77 79\n $ : int [1:6] 25 40 42 43 44 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:6] 26 27 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:6] 37 38 39 42 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:6] 8 9 35 47 78 86\n $ : int [1:6] 8 21 35 46 80 86\n $ : int [1:6] 49 50 51 52 53 55\n $ : int [1:6] 28 33 48 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:6] 28 48 49 50 52 54\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:6] 48 50 51 52 55 75\n $ : int [1:6] 24 28 49 50 51 52\n $ : int [1:6] 32 48 50 52 53 75\n $ : int [1:6] 32 34 36 78 80 85\n $ : int [1:6] 1 2 3 58 64 68\n $ : int [1:6] 2 57 64 66 68 78\n $ : int [1:6] 12 13 60 61 87 88\n $ : int [1:6] 12 13 59 61 63 87\n $ : int [1:6] 12 13 60 62 63 87\n $ : int [1:6] 12 38 61 63 77 87\n $ : int [1:6] 12 18 60 61 62 83\n $ : int [1:6] 1 3 57 58 68 76\n $ : int [1:6] 58 64 66 67 68 76\n $ : int [1:6] 9 58 67 68 76 84\n $ : int [1:6] 7 65 66 68 76 84\n $ : int [1:6] 9 57 58 66 78 84\n $ : int [1:6] 4 5 6 32 75 85\n $ : int [1:6] 10 16 19 22 72 73\n $ : int [1:6] 7 19 73 74 84 86\n $ : int [1:6] 10 11 14 16 17 70\n $ : int [1:6] 10 19 21 70 71 74\n $ : int [1:6] 19 21 71 73 84 86\n $ : int [1:6] 6 32 50 53 55 69\n $ : int [1:6] 58 64 65 66 67 68\n $ : int [1:6] 18 23 38 61 62 63\n $ : int [1:6] 2 8 9 46 58 68\n $ : int [1:6] 38 40 41 43 44 45\n $ : int [1:6] 34 35 36 41 45 47\n $ : int [1:6] 25 26 28 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:6] 12 13 15 16 22 63\n $ : int [1:6] 7 9 66 68 71 74\n $ : int [1:6] 2 3 4 5 56 69\n $ : int [1:6] 8 9 21 46 47 74\n $ : int [1:6] 59 60 61 62 63 88\n $ : int [1:6] 59 60 61 62 63 87\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language knearneigh(x = coords, k = 6)\n - attr(*, \"sym\")= logi FALSE\n - attr(*, \"type\")= chr \"knn\"\n - attr(*, \"knn-k\")= num 6\n - attr(*, \"class\")= chr \"nb\"\n\n\nNotice that each county has six neighbours, no less no more!\n\n\nWe can plot the weight matrix using the code chunk below.\n\n\nCode Chunk\nplot(hunan$geometry, border=\"lightgrey\")\nplot(knn6, coords, pch=19, cex=0.6, add=TRUE, col-\"red\")"
  },
  {
    "objectID": "HandsOnExercise/HandsOn4/HandsOn4.html#weights-based-on-idw",
    "href": "HandsOnExercise/HandsOn4/HandsOn4.html#weights-based-on-idw",
    "title": "Hands-On Exercise 4",
    "section": "",
    "text": "In this section, you will learn how to derive a spatial weight matrix based on Inversed Distance method.\nFirst, we will compute the distances between areas by using nbdists() of spdep.\n\n\nCode Chunk\ndist &lt;- nbdists(wm_q, coords, longlat = TRUE)\nids &lt;- lapply(dist, function(x) 1/(x))\nids\n\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\n[[2]]\n[1] 0.01535405 0.01764308 0.01925924 0.02323898 0.01719350\n\n[[3]]\n[1] 0.03916350 0.02822040 0.03695795 0.01395765\n\n[[4]]\n[1] 0.01820896 0.02822040 0.03414741 0.01539065\n\n[[5]]\n[1] 0.03695795 0.03414741 0.01524598 0.01618354\n\n[[6]]\n[1] 0.015390649 0.015245977 0.021748129 0.011883901 0.009810297\n\n[[7]]\n[1] 0.01708612 0.01473997 0.01150924 0.01872915\n\n[[8]]\n[1] 0.02022144 0.03453056 0.02529256 0.01036340 0.02284457 0.01500600 0.01515314\n\n[[9]]\n[1] 0.02022144 0.01574888 0.02109502 0.01508028 0.02902705 0.01502980\n\n[[10]]\n[1] 0.02281552 0.01387777 0.01538326 0.01346650 0.02100510 0.02631658 0.01874863\n[8] 0.01500046\n\n[[11]]\n[1] 0.01882869 0.02243492 0.02247473\n\n[[12]]\n[1] 0.02779227 0.02419652 0.02333385 0.02986130 0.02335429\n\n[[13]]\n[1] 0.02779227 0.02650020 0.02670323 0.01714243\n\n[[14]]\n[1] 0.01882869 0.01233868 0.02098555\n\n[[15]]\n[1] 0.02650020 0.01233868 0.01096284 0.01562226\n\n[[16]]\n[1] 0.02281552 0.02466962 0.02765018 0.01476814 0.01671430\n\n[[17]]\n[1] 0.01387777 0.02243492 0.02098555 0.01096284 0.02466962 0.01593341 0.01437996\n\n[[18]]\n[1] 0.02039779 0.02032767 0.01481665 0.01473691 0.01459380\n\n[[19]]\n[1] 0.01538326 0.01926323 0.02668415 0.02140253 0.01613589 0.01412874\n\n[[20]]\n[1] 0.01346650 0.02039779 0.01926323 0.01723025 0.02153130 0.01469240 0.02327034\n\n[[21]]\n[1] 0.02668415 0.01723025 0.01766299 0.02644986 0.02163800\n\n[[22]]\n[1] 0.02100510 0.02765018 0.02032767 0.02153130 0.01489296\n\n[[23]]\n[1] 0.01481665 0.01469240 0.01401432 0.02246233 0.01880425 0.01530458 0.01849605\n\n[[24]]\n[1] 0.02354598 0.01837201 0.02607264 0.01220154 0.02514180\n\n[[25]]\n[1] 0.02354598 0.02188032 0.01577283 0.01949232 0.02947957\n\n[[26]]\n[1] 0.02155798 0.01745522 0.02212108 0.02220532\n\n[[27]]\n[1] 0.02155798 0.02490625 0.01562326\n\n[[28]]\n[1] 0.01837201 0.02188032 0.02229549 0.03076171 0.02039506\n\n[[29]]\n[1] 0.02490625 0.01686587 0.01395022\n\n[[30]]\n[1] 0.02090587\n\n[[31]]\n[1] 0.02607264 0.01577283 0.01219005 0.01724850 0.01229012 0.01609781 0.01139438\n[8] 0.01150130\n\n[[32]]\n[1] 0.01220154 0.01219005 0.01712515 0.01340413 0.01280928 0.01198216 0.01053374\n[8] 0.01065655\n\n[[33]]\n[1] 0.01949232 0.01745522 0.02229549 0.02090587 0.01979045\n\n[[34]]\n[1] 0.03113041 0.03589551 0.02882915\n\n[[35]]\n[1] 0.01766299 0.02185795 0.02616766 0.02111721 0.02108253 0.01509020\n\n[[36]]\n[1] 0.01724850 0.03113041 0.01571707 0.01860991 0.02073549 0.01680129\n\n[[37]]\n[1] 0.01686587 0.02234793 0.01510990 0.01550676\n\n[[38]]\n[1] 0.01401432 0.02407426 0.02276151 0.01719415\n\n[[39]]\n[1] 0.01229012 0.02172543 0.01711924 0.02629732 0.01896385\n\n[[40]]\n[1] 0.01609781 0.01571707 0.02172543 0.01506473 0.01987922 0.01894207\n\n[[41]]\n[1] 0.02246233 0.02185795 0.02205991 0.01912542 0.01601083 0.01742892\n\n[[42]]\n[1] 0.02212108 0.01562326 0.01395022 0.02234793 0.01711924 0.01836831 0.01683518\n\n[[43]]\n[1] 0.01510990 0.02629732 0.01506473 0.01836831 0.03112027 0.01530782\n\n[[44]]\n[1] 0.01550676 0.02407426 0.03112027 0.01486508\n\n[[45]]\n[1] 0.03589551 0.01860991 0.01987922 0.02205991 0.02107101 0.01982700\n\n[[46]]\n[1] 0.03453056 0.04033752 0.02689769\n\n[[47]]\n[1] 0.02529256 0.02616766 0.04033752 0.01949145 0.02181458\n\n[[48]]\n[1] 0.02313819 0.03370576 0.02289485 0.01630057 0.01818085\n\n[[49]]\n[1] 0.03076171 0.02138091 0.02394529 0.01990000\n\n[[50]]\n[1] 0.01712515 0.02313819 0.02551427 0.02051530 0.02187179\n\n[[51]]\n[1] 0.03370576 0.02138091 0.02873854\n\n[[52]]\n[1] 0.02289485 0.02394529 0.02551427 0.02873854 0.03516672\n\n[[53]]\n[1] 0.01630057 0.01979945 0.01253977\n\n[[54]]\n[1] 0.02514180 0.02039506 0.01340413 0.01990000 0.02051530 0.03516672\n\n[[55]]\n[1] 0.01280928 0.01818085 0.02187179 0.01979945 0.01882298\n\n[[56]]\n[1] 0.01036340 0.01139438 0.01198216 0.02073549 0.01214479 0.01362855 0.01341697\n\n[[57]]\n[1] 0.028079221 0.017643082 0.031423501 0.029114131 0.013520292 0.009903702\n\n[[58]]\n[1] 0.01925924 0.03142350 0.02722997 0.01434859 0.01567192\n\n[[59]]\n[1] 0.01696711 0.01265572 0.01667105 0.01785036\n\n[[60]]\n[1] 0.02419652 0.02670323 0.01696711 0.02343040\n\n[[61]]\n[1] 0.02333385 0.01265572 0.02343040 0.02514093 0.02790764 0.01219751 0.02362452\n\n[[62]]\n[1] 0.02514093 0.02002219 0.02110260\n\n[[63]]\n[1] 0.02986130 0.02790764 0.01407043 0.01805987\n\n[[64]]\n[1] 0.02911413 0.01689892\n\n[[65]]\n[1] 0.02471705\n\n[[66]]\n[1] 0.01574888 0.01726461 0.03068853 0.01954805 0.01810569\n\n[[67]]\n[1] 0.01708612 0.01726461 0.01349843 0.01361172\n\n[[68]]\n[1] 0.02109502 0.02722997 0.03068853 0.01406357 0.01546511\n\n[[69]]\n[1] 0.02174813 0.01645838 0.01419926\n\n[[70]]\n[1] 0.02631658 0.01963168 0.02278487\n\n[[71]]\n[1] 0.01473997 0.01838483 0.03197403\n\n[[72]]\n[1] 0.01874863 0.02247473 0.01476814 0.01593341 0.01963168\n\n[[73]]\n[1] 0.01500046 0.02140253 0.02278487 0.01838483 0.01652709\n\n[[74]]\n[1] 0.01150924 0.01613589 0.03197403 0.01652709 0.01342099 0.02864567\n\n[[75]]\n[1] 0.011883901 0.010533736 0.012539774 0.018822977 0.016458383 0.008217581\n\n[[76]]\n[1] 0.01352029 0.01434859 0.01689892 0.02471705 0.01954805 0.01349843 0.01406357\n\n[[77]]\n[1] 0.014736909 0.018804247 0.022761507 0.012197506 0.020022195 0.014070428\n[7] 0.008440896\n\n[[78]]\n[1] 0.02323898 0.02284457 0.01508028 0.01214479 0.01567192 0.01546511 0.01140779\n\n[[79]]\n[1] 0.01530458 0.01719415 0.01894207 0.01912542 0.01530782 0.01486508 0.02107101\n\n[[80]]\n[1] 0.01500600 0.02882915 0.02111721 0.01680129 0.01601083 0.01982700 0.01949145\n[8] 0.01362855\n\n[[81]]\n[1] 0.02947957 0.02220532 0.01150130 0.01979045 0.01896385 0.01683518\n\n[[82]]\n[1] 0.02327034 0.02644986 0.01849605 0.02108253 0.01742892\n\n[[83]]\n[1] 0.023354289 0.017142433 0.015622258 0.016714303 0.014379961 0.014593799\n[7] 0.014892965 0.018059871 0.008440896\n\n[[84]]\n[1] 0.01872915 0.02902705 0.01810569 0.01361172 0.01342099 0.01297994\n\n[[85]]\n [1] 0.011451133 0.017193502 0.013957649 0.016183544 0.009810297 0.010656545\n [7] 0.013416965 0.009903702 0.014199260 0.008217581 0.011407794\n\n[[86]]\n[1] 0.01515314 0.01502980 0.01412874 0.02163800 0.01509020 0.02689769 0.02181458\n[8] 0.02864567 0.01297994\n\n[[87]]\n[1] 0.01667105 0.02362452 0.02110260 0.02058034\n\n[[88]]\n[1] 0.01785036 0.02058034"
  },
  {
    "objectID": "HandsOnExercise/HandsOn4/HandsOn4.html#row-standardised-weights-matrix",
    "href": "HandsOnExercise/HandsOn4/HandsOn4.html#row-standardised-weights-matrix",
    "title": "Hands-On Exercise 4",
    "section": "",
    "text": "Next, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors’ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we’ll stick with the style=“W” option for simplicity’s sake but note that other more robust options are available, notably style=“B”.\n\n\nCode Chunk\nrswm_q &lt;- nb2listw(wm_q, style=\"W\", zero.policy = TRUE)\nrswm_q\n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\nThe zero.policy=TRUE option allows for lists of non-neighbors. This should be used with caution since the user may not be aware of missing neighbors in their dataset however, a zero.policy of FALSE would return an error.\nTo see the weight of the first polygon’s eight neighbors type:\n\n\nCode Chunk\nrswm_q$weights[10]\n\n\n[[1]]\n[1] 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125\n\n\nEach neighbor is assigned a 0.125 of the total weight. This means that when R computes the average neighboring income values, each neighbor’s income will be multiplied by 0.125 before being tallied.\nUsing the same method, we can also derive a row standardised distance weight matrix by using the code chunk below.\n\n\nCode Chunk\nrswm_ids &lt;- nb2listw(wm_q, glist=ids, style=\"B\", zero.policy=TRUE)\nrswm_ids\n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn       S0        S1     S2\nB 88 7744 8.786867 0.3776535 3.8137\n\n\n\n\nCode Chunk\nrswm_ids$weights[1]\n\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\n\n\n\nCode Chunk\nsummary(unlist(rswm_ids$weights))\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n0.008218 0.015088 0.018739 0.019614 0.022823 0.040338"
  },
  {
    "objectID": "HandsOnExercise/HandsOn4/HandsOn4.html#application-of-spatial-weight-matrix",
    "href": "HandsOnExercise/HandsOn4/HandsOn4.html#application-of-spatial-weight-matrix",
    "title": "Hands-On Exercise 4",
    "section": "",
    "text": "In this section, you will learn how to create four different spatial lagged variables, they are:\n\nspatial lag with row-standardized weights,\nspatial lag as a sum of neighbouring values,\nspatial window average, and\nspatial window sum.\n\n\n\nFinally, we’ll compute the average neighbor GDPPC value for each polygon. These values are often referred to as spatially lagged values.\n\n\nCode Chunk\nGDPPC.lag &lt;- lag.listw(rswm_q, hunan$GDPPC)\nGDPPC.lag\n\n\n [1] 24847.20 22724.80 24143.25 27737.50 27270.25 21248.80 43747.00 33582.71\n [9] 45651.17 32027.62 32671.00 20810.00 25711.50 30672.33 33457.75 31689.20\n[17] 20269.00 23901.60 25126.17 21903.43 22718.60 25918.80 20307.00 20023.80\n[25] 16576.80 18667.00 14394.67 19848.80 15516.33 20518.00 17572.00 15200.12\n[33] 18413.80 14419.33 24094.50 22019.83 12923.50 14756.00 13869.80 12296.67\n[41] 15775.17 14382.86 11566.33 13199.50 23412.00 39541.00 36186.60 16559.60\n[49] 20772.50 19471.20 19827.33 15466.80 12925.67 18577.17 14943.00 24913.00\n[57] 25093.00 24428.80 17003.00 21143.75 20435.00 17131.33 24569.75 23835.50\n[65] 26360.00 47383.40 55157.75 37058.00 21546.67 23348.67 42323.67 28938.60\n[73] 25880.80 47345.67 18711.33 29087.29 20748.29 35933.71 15439.71 29787.50\n[81] 18145.00 21617.00 29203.89 41363.67 22259.09 44939.56 16902.00 16930.00\n\n\nRecalled in the previous section, we retrieved the GDPPC of these five countries by using the code chunk below.\n\n\nCode Chunk\nnb1 &lt;- wm_q[[1]]\nnb1 &lt;- hunan$GDPPC[nb1]\nnb1\n\n\n[1] 20981 34592 24473 21311 22879\n\n\nWe can append the spatially lag GDPPC values onto hunan sf data frame by using the code chunk below.\n\n\nCode Chunk\nlag.list &lt;- list(hunan$NAME_3, lag.listw(rswm_q, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag.list)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag GDPPC\")\nhunan &lt;- left_join(hunan,lag.res)\n\n\nThe following table shows the average neighboring income values (stored in the Inc.lag object) for each county.\n\n\nCode Chunk\nhead(hunan)\n\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3  County GDPPC lag GDPPC\n1 Changde 21098 Anxiang      County Anxiang 23667  24847.20\n2 Changde 21100 Hanshou      County Hanshou 20981  22724.80\n3 Changde 21101  Jinshi County City  Jinshi 34592  24143.25\n4 Changde 21102      Li      County      Li 24473  27737.50\n5 Changde 21103   Linli      County   Linli 25554  27270.25\n6 Changde 21104  Shimen      County  Shimen 27137  21248.80\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n4 POLYGON ((111.3731 29.94649...\n5 POLYGON ((111.6324 29.76288...\n6 POLYGON ((110.8825 30.11675...\n\n\nNext, we will plot both the GDPPC and spatial lag GDPPC for comparison using the code chunk below.\n\n\nCode Chunk\ngdppc &lt;- qtm(hunan, \"GDPPC\")\nlag_gdppc &lt;- qtm(hunan, \"lag GDPPC\")\ntmap_arrange(gdppc, lag_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\nWe can calculate spatial lag as a sum of neighboring values by assigning binary weights. This requires us to go back to our neighbors list, then apply a function that will assign binary weights, then we use glist = in the nb2listw function to explicitly assign these weights.\nWe start by applying a function that will assign a value of 1 per each neighbor. This is done with lapply, which we have been using to manipulate the neighbors structure throughout the past notebooks. Basically it applies a function across each value in the neighbors structure.\n\n\nCode Chunk\nb_weights &lt;- lapply(wm_q, function(x) 0*x + 1)\nb_weights2 &lt;- nb2listw(wm_q, \n                       glist = b_weights, \n                       style = \"B\")\nprint(b_weights2)\n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1    S2\nB 88 7744 448 896 10224\n\n\nWith the proper weights assigned, we can use lag.listw to compute a lag variable from our weight and GDPPC.\n\n\nCode Chunk\nlag_sum &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag_sum)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag_sum GDPPC\")\n\n\nFirst, let us examine the result by using the code chunk below.\n\n\nCode Chunk\nlag_sum\n\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 124236 113624  96573 110950 109081 106244 174988 235079 273907 256221\n[11]  98013 104050 102846  92017 133831 158446 141883 119508 150757 153324\n[21] 113593 129594 142149 100119  82884  74668  43184  99244  46549  20518\n[31] 140576 121601  92069  43258 144567 132119  51694  59024  69349  73780\n[41]  94651 100680  69398  52798 140472 118623 180933  82798  83090  97356\n[51]  59482  77334  38777 111463  74715 174391 150558 122144  68012  84575\n[61] 143045  51394  98279  47671  26360 236917 220631 185290  64640  70046\n[71] 126971 144693 129404 284074 112268 203611 145238 251536 108078 238300\n[81] 108870 108085 262835 248182 244850 404456  67608  33860\n\n\nNext, we will append the lag_sum GDPPC field into hunan sf data frame by using the code chunk below.\n\n\nCode Chunk\nhunan &lt;- left_join(hunan, lag.res)\n\n\nNow, We can plot both the GDPPC and Spatial Lag Sum GDPPC for comparison using the code chunk below.\n\n\nCode Chunk\ngdppc &lt;- qtm(hunan, \"GDPPC\")\nlag_sum_gdppc &lt;- qtm(hunan, \"lag_sum GDPPC\")\ntmap_arrange(gdppc, lag_sum_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\nThe spatial window average uses row-standardized weights and includes the diagonal element. To do this in R, we need to go back to the neighbors structure and add the diagonal element before assigning weights.\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\n\nCode Chunk\nwm_qs &lt;- include.self(wm_q)\n\n\nNotice that the Number of nonzero links, Percentage nonzero weights and Average number of links are 536, 6.921488 and 6.090909 respectively as compared to wm_q of 448, 5.785124 and 5.090909\nLet us take a good look at the neighbour list of area [1] by using the code chunk below.\n\n\nCode Chunk\nwm_qs[[1]]\n\n\n[1]  1  2  3  4 57 85\n\n\nNotice that now [1] has six neighbours instead of five.\nNow we obtain weights with nb2listw()\n\n\nCode Chunk\nwm_qs &lt;- nb2listw(wm_qs)\nwm_qs\n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 30.90265 357.5308\n\n\nAgain, we use nb2listw() and glist() to explicitly assign weight values.\nLastly, we just need to create the lag variable from our weight structure and GDPPC variable.\n\n\nCode Chunk\nlag_w_avg_gpdpc &lt;- lag.listw(wm_qs,\n                              hunan$GDPPC)\nlag_w_avg_gpdpc\n\n\n [1] 24650.50 22434.17 26233.00 27084.60 26927.00 22230.17 47621.20 37160.12\n [9] 49224.71 29886.89 26627.50 22690.17 25366.40 25825.75 30329.00 32682.83\n[17] 25948.62 23987.67 25463.14 21904.38 23127.50 25949.83 20018.75 19524.17\n[25] 18955.00 17800.40 15883.00 18831.33 14832.50 17965.00 17159.89 16199.44\n[33] 18764.50 26878.75 23188.86 20788.14 12365.20 15985.00 13764.83 11907.43\n[41] 17128.14 14593.62 11644.29 12706.00 21712.29 43548.25 35049.00 16226.83\n[49] 19294.40 18156.00 19954.75 18145.17 12132.75 18419.29 14050.83 23619.75\n[57] 24552.71 24733.67 16762.60 20932.60 19467.75 18334.00 22541.00 26028.00\n[65] 29128.50 46569.00 47576.60 36545.50 20838.50 22531.00 42115.50 27619.00\n[73] 27611.33 44523.29 18127.43 28746.38 20734.50 33880.62 14716.38 28516.22\n[81] 18086.14 21244.50 29568.80 48119.71 22310.75 43151.60 17133.40 17009.33\n\n\nNext, we will convert the lag variable listw object into a data.frame by using as.data.frame().\n\n\nCode Chunk\nlag.list.wm_qs &lt;- list(hunan$NAME_3, lag.listw(wm_qs, hunan$GDPPC))\nlag_wm_qs.res &lt;- as.data.frame(lag.list.wm_qs)\ncolnames(lag_wm_qs.res) &lt;- c(\"NAME_3\", \"lag_window_avg GDPPC\")\n\n\nNote: The third command line on the code chunk above renames the field names of lag_wm_q1.res object into NAME_3 and lag_window_avg GDPPC respectively.\nNext, the code chunk below will be used to append lag_window_avg GDPPC values onto hunan sf data.frame by using left_join() of dplyr package.\n\n\nCode Chunk\nhunan &lt;- left_join(hunan, lag_wm_qs.res)\n\n\nTo compare the values of lag GDPPC and Spatial window average, kable() of Knitr package is used to prepare a table using the code chunk below.\n\n\nCode Chunk\nhunan %&gt;%\n  select(\"County\", \n         \"lag GDPPC\", \n         \"lag_window_avg GDPPC\") %&gt;%\n  kable()\n\n\nLastly, qtm() of tmap package is used to plot the lag_gdppc and w_ave_gdppc maps next to each other for quick comparison.\n\n\nCode Chunk\nw_avg_gdppc &lt;- qtm(hunan, \"lag_window_avg GDPPC\")\ntmap_arrange(lag_gdppc, w_avg_gdppc, asp=1, ncol=2)\n\n\n\n\n\nThe spatial window sum is the counter part of the window average, but without using row-standardized weights.\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\n\nCode Chunk\nwm_qs &lt;- include.self(wm_q)\nwm_qs\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\n\nNext, we will assign binary weights to the neighbour structure that includes the diagonal element.\n\n\nCode Chunk\nb_weights &lt;- lapply(wm_qs, function(x) 0*x +1)\nb_weights[1]\n\n\n[[1]]\n[1] 1 1 1 1 1 1\n\n\nNotice that now [1] has six neighbours instead of five.\nAgain, we use nb2listw() and glist() to explicitly assign weight values.\n\n\nCode Chunk\nb_weights2 &lt;- nb2listw(wm_qs,\n                        glist = b_weights,\n                        style = \"B\")\nb_weights2\n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 536 1072 14160\n\n\nWith our new weight structure, we can compute the lag variable with lag.listw().\n\n\nCode Chunk\nw_sum_gdppc &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nw_sum(gdppc)\n\n\nNext, we will convert the lag variable listw object into a data.frame by using as.data.frame().\n\n\nCode Chunk\nw_sum_gdppc.res &lt;- as.data.frame(w_sum_gdppc)\ncolnames(w_sum_gdppc.res) &lt;- c(\"NAMES_3\", \"w_sum GDPPC\")\n\n\nNote: The second command line on the code chunk above renames the field names of w_sum_gdppc.res object into NAME_3 and w_sum GDPPC respectively.\nNext, the code chunk below will be used to append w_sum GDPPC values onto hunan sf data.frame by using left_join() of dplyr package.\n\n\nCode Chunk\nhunan &lt;- left_join(hunan, w_sum_gdppc.res)\n\n\nTo compare the values of lag GDPPC and Spatial window average, kable() of Knitr package is used to prepare a table using the code chunk below.\n\n\nCode Chunk\nhunan %&gt;%\n  select(\"County\", \"lag_sum GDPPC\", \"w_sum GDPPC\") %&gt;%\n  kable()\n\n\nLastly, qtm() of tmap package is used to plot the lag_sum GDPPC and w_sum_gdppc maps next to each other for quick comparison.\n\n\nCode Chunk\nw_sum_gdppc &lt;- qtm(hunan, \"w_sum GDPPC\")\ntmap_arrange(lag_sum_gdppc, w_sum_gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "HandsOnExercise/HandsOn4/HandsOn4.html#references",
    "href": "HandsOnExercise/HandsOn4/HandsOn4.html#references",
    "title": "Hands-On Exercise 4",
    "section": "",
    "text": "Creating Neighbours using sf objects"
  },
  {
    "objectID": "TakeHomeExercise/TakeHome1/TakeHome1.html#setting-the-scene",
    "href": "TakeHomeExercise/TakeHome1/TakeHome1.html#setting-the-scene",
    "title": "Take-Home Exercise 1",
    "section": "",
    "text": "According to World Health Organisation (WHO), road traffic accidents cause the death of approximately 1.19 million people each year leave between 20 and 50 million people with non-fatal injuries. More than half of all road traffic deaths occur among vulnerable road users, such as pedestrians, cyclists and motorcyclists.\nRoad traffic injuries are the leading cause of death for children and young adults aged 5–29. Yet two thirds of road traffic fatalities occur among people of working age (18–59 years). Nine in 10 fatalities on the roads occur in low- and middle-income countries, even though these countries have around 60% of the world’s vehicles.\nIn addition to the human suffering caused by road traffic injuries, they also incur a heavy economic burden on victims and their families, both through treatment costs for the injured and through loss of productivity of those killed or disabled. More broadly, road traffic injuries have a serious impact on national economies, costing countries 3% of their annual gross domestic product.\nThailand’s roads are the deadliest in Southeast Asia and among the worst in the world, according to the World Health Organisation. About 20,000 people die in road accidents each year, or about 56 deaths a day (WHO).\nBetween 2014 and 2021, Thailand experienced a notable increase in accident frequencies. Specifically, 19% of all accidents in Thailand occurred on the national highways, which constituted the primary public thoroughfares connecting various regions, provinces, districts, and significant locations within a comprehensive network. Within the broader context of accidents across the country, there existed a considerable 66% likelihood of encountering accident-prone zones, often termed ‘black spots,’ distributed as follows: 66% on straight road segments, 13% at curves, 6% at median points of cross-shaped intersections, 5% at T-shaped intersections and Y-shaped intersections, 3% at cross-shaped intersections, 2% on bridges, and 2% on steep slopes, respectively."
  },
  {
    "objectID": "TakeHomeExercise/TakeHome1/TakeHome1.html#objectives",
    "href": "TakeHomeExercise/TakeHome1/TakeHome1.html#objectives",
    "title": "Take-Home Exercise 1",
    "section": "",
    "text": "By and large, road traffic accidents can be attributed by two major factors, namely: behavioural and environmental factors. Behavioural factors in driving are considered to be major causes of traffic accidents either in direct or indirect manner (Lewin, 1982). These factors can be further grouped into two as, driver behavior (also called driver/driving style) and driver performance, in other words, driver/driving skills (Elander, West, & French, 1993). Environmental factors, on the other hand, includes but not limited to weather condition such as poor visibility during heavy rain or foggy and road conditions such as sharp bend road, slippery slope road, and blind spot.\nPrevious studies have demonstrated the significant potential of Spatial Point Patterns Analysis (SPPA) in exploring and identifying factors influencing road traffic accidents. However, these studies often focus solely on either behavioral or environmental factors, with limited consideration of temporal factors such as season, day of the week, or time of day.\nIn view of this, you are tasked to discover factors affecting road traffic accidents in the Bangkok Metropolitan Region BMR by employing both spatial spatio-temporal point patterns analysis methods.\nThe specific objectives of this take-home exercise are as follows:\n\nTo visualize the spatio-temporal dynamics of road traffic accidents in BMR using appropriate statistical graphics and geovisualization methods.\nTo conduct detailed spatial analysis of road traffic accidents using appropriate Network Spatial Point Patterns Analysis methods.\nTo conduct detailed spatio-temporal analysis of road traffic accidents using appropriate Temporal Network Spatial Point Patterns Analysis methods."
  },
  {
    "objectID": "TakeHomeExercise/TakeHome1/TakeHome1.html#data-preparation",
    "href": "TakeHomeExercise/TakeHome1/TakeHome1.html#data-preparation",
    "title": "Take-Home Exercise 1",
    "section": "",
    "text": "For the purpose of this exercise, three basic data sets must be used:\n\nThailand Road Accident [2019-2022] on Kaggle\nThailand Roads (OpenStreetMap Export) on HDX.\nThailand - Subnational Administrative Boundaries on HDX.\n\n\n\nCode Chunk\npacman::p_load(tidyverse, sf, spatstat, ggplot2, ggmap, tmap, dplyr, lubridate, raster)\n\n\n\nPackages\n\n\nPackages\nFunction\n\n\n\n\nsf\nTo import, manage, and hande geospatial data\n\n\ntidyverse\nFor non-spatial data wrangling\n\n\nsfdep\nTo compute spatial weights, global and local spatial autocorrelation statistics\n\n\nspatstat\nFor analysing spatial points\n\n\nggplot2\nFor data divisualisation\n\n\nggmap\nRetrieve raster map tiles from online mapping services\n\n\ntmap\nCreating thematic maps\n\n\nlubridate\nFor robust date-time usage\n\n\nleaflet\nFor interactive maps\n\n\nknitr\nFor dynamic report generation\n\n\nraster\n\n\n\n\n\nRoad Traffic Accidents (RTA)Road NetworkAdministrative BoundariesImporting Aspatial & Geospatial Data\n\n\nThailand Road Traffic Accident Data [2019-2022] (from Kaggle): This contains the road accident records, including spatial (longitude/latitude) and temporal information (date and time).\n\n\nCode Chunk\nrta_sf &lt;- read_csv(\"data/aspatial/archive/thai_road_accident_2019_2022.csv\") %&gt;%\n  filter(!is.na(longitude) & longitude !='',\n        !is.na(latitude) & latitude != '') %&gt;%\n  st_as_sf(coords = c(\"longitude\", 'latitude'),\n           crs=4326) %&gt;% #WGS 84 - USE BY ALL GPS\n  st_transform(crs=32647) %&gt;% #EPSG \n  mutate(Month_num = month(incident_datetime)) %&gt;%\n  mutate(Month_fac = month(incident_datetime, #fac is factor\n                       label = TRUE, \n                       abbr = TRUE)) %&gt;% #CAN CHANGE TO MON TUES \n  mutate(dayofweek = day(incident_datetime))\n#PROJECTED PCS UTM IS 326, 47 NORTH \n\n\nSaving this geometry with corrected projection for plotting use.\n\n\nCode Chunk\nwrite_rds(rta_sf, \"data/rds/rta_sf.rds\")\n\n\n\n\n\nThailand Roads (OpenStreetMap Export) (from HDX): This will provide the road network to conduct network-based analysis.\n\n\n\nCode Chunk\nroadlines &lt;- st_read(dsn = \"data/geospatial/roadlines/\",\n                      layer = \"hotosm_tha_roads_lines_shp\")\n\n\nReading layer `hotosm_tha_roads_lines_shp' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/TakeHomeExercise/TakeHome1/data/geospatial/roadlines' \n  using driver `ESRI Shapefile'\nSimple feature collection with 2792590 features and 14 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 97.34457 ymin: 5.643645 xmax: 105.6528 ymax: 20.47168\nCRS:           NA\n\n\n\n\nThailand Subnational Administrative Boundaries (from HDX): These boundaries will help in restricting the analysis to the Bangkok Metropolitan Region 9BMR) and may also serve for regional analysis.\n\n\nCode Chunk\nthaiadmin &lt;- st_read(dsn = \"data/geospatial/tha_adm_rtsd_itos_20210121_shp/\",\n                      layer = \"tha_admbnda_adm1_rtsd_20220121\")\n\n\nReading layer `tha_admbnda_adm1_rtsd_20220121' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/TakeHomeExercise/TakeHome1/data/geospatial/tha_adm_rtsd_itos_20210121_shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "TakeHomeExercise/TakeHome1/TakeHome1.html#data-preparation-1",
    "href": "TakeHomeExercise/TakeHome1/TakeHome1.html#data-preparation-1",
    "title": "Take-Home Exercise 1",
    "section": "",
    "text": "Using the function glimpse() in of dplyr in the tidyverse family, we are able to see the data type of each variable.\n\n\nCode Chunk\nglimpse(thaiadmin)\n\n\nRows: 77\nColumns: 17\n$ Shape_Leng &lt;dbl&gt; 2.417227, 1.695100, 1.251111, 1.884945, 3.041716, 1.739908,…\n$ Shape_Area &lt;dbl&gt; 0.13133873, 0.07926199, 0.05323766, 0.12698345, 0.21393797,…\n$ ADM1_EN    &lt;chr&gt; \"Bangkok\", \"Samut Prakan\", \"Nonthaburi\", \"Pathum Thani\", \"P…\n$ ADM1_TH    &lt;chr&gt; \"กรุงเทพมหานคร\", \"สมุทรปราการ\", \"นนทบุรี\", \"ปทุมธานี\", \"พระนครศรีอ…\n$ ADM1_PCODE &lt;chr&gt; \"TH10\", \"TH11\", \"TH12\", \"TH13\", \"TH14\", \"TH15\", \"TH16\", \"TH…\n$ ADM1_REF   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM1ALT1EN &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM1ALT2EN &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM1ALT1TH &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM1ALT2TH &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM0_EN    &lt;chr&gt; \"Thailand\", \"Thailand\", \"Thailand\", \"Thailand\", \"Thailand\",…\n$ ADM0_TH    &lt;chr&gt; \"ประเทศไทย\", \"ประเทศไทย\", \"ประเทศไทย\", \"ประเทศไทย\", \"ประเทศ…\n$ ADM0_PCODE &lt;chr&gt; \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\",…\n$ date       &lt;date&gt; 2019-02-18, 2019-02-18, 2019-02-18, 2019-02-18, 2019-02-18…\n$ validOn    &lt;date&gt; 2022-01-22, 2022-01-22, 2022-01-22, 2022-01-22, 2022-01-22…\n$ validTo    &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ geometry   &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((100.6139 13..., MULTIPOLYGON (…\n\n\n\n\nCode Chunk\nplot(thaiadmin)\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\nplot(st_geometry(thaiadmin))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\nst_crs(thaiadmin)\n\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\n\n\nCode Chunk\nthaiadmin32647 &lt;- st_transform(thaiadmin, crs = 32647)\n\n\n\n\nCode Chunk\nst_crs(thaiadmin32647)\n\n\nCoordinate Reference System:\n  User input: EPSG:32647 \n  wkt:\nPROJCRS[\"WGS 84 / UTM zone 47N\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"UTM zone 47N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",99,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"Between 96°E and 102°E, northern hemisphere between equator and 84°N, onshore and offshore. China. Indonesia. Laos. Malaysia - West Malaysia. Mongolia. Myanmar (Burma). Russian Federation. Thailand.\"],\n        BBOX[0,96,84,102]],\n    ID[\"EPSG\",32647]]\n\n\nCurrently, the EPSG code in thaiadmin is 32647.\n\n\n\n\n\nCode Chunk\nthaiadmin32647$\"acc_code\" &lt;- lengths(st_intersects(thaiadmin32647, rta_sf))\n\n\n\n\nCode Chunk\nsummary(thaiadmin32647$'acc_code')\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    140     503     822    1057    1165    6090 \n\n\nn_distinct() of the dyplr package counts the number of incidents.\n\n\nCode Chunk\nn_distinct(rta_sf$acc_code)\n\n\n[1] 81376"
  },
  {
    "objectID": "TakeHomeExercise/TakeHome1/TakeHome1.html#data-wrangling",
    "href": "TakeHomeExercise/TakeHome1/TakeHome1.html#data-wrangling",
    "title": "Take-Home Exercise 1",
    "section": "",
    "text": "The output reflected that there are no duplicates in the data.\n\n\nCode Chunk\nduplicate &lt;- rta_sf %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\nduplicate\n\n\nSimple feature collection with 0 features and 19 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nProjected CRS: WGS 84 / UTM zone 47N\n# A tibble: 0 × 20\n# ℹ 20 variables: acc_code &lt;dbl&gt;, incident_datetime &lt;dttm&gt;,\n#   report_datetime &lt;dttm&gt;, province_th &lt;chr&gt;, province_en &lt;chr&gt;, agency &lt;chr&gt;,\n#   route &lt;chr&gt;, vehicle_type &lt;chr&gt;, presumed_cause &lt;chr&gt;, accident_type &lt;chr&gt;,\n#   number_of_vehicles_involved &lt;dbl&gt;, number_of_fatalities &lt;dbl&gt;,\n#   number_of_injuries &lt;dbl&gt;, weather_condition &lt;chr&gt;, road_description &lt;chr&gt;,\n#   slope_description &lt;chr&gt;, geometry &lt;GEOMETRY [m]&gt;, Month_num &lt;dbl&gt;,\n#   Month_fac &lt;ord&gt;, dayofweek &lt;int&gt;\n\n\n\n\n\n“dplyr::select” was used as an error message “Error: unable to find an inherited method for function ‘select’ for signature ‘x = “sf”’”.\n\n\nCode Chunk\nrta_sf_mbr &lt;- rta_sf %&gt;%\n  dplyr::select(-province_th, -route) %&gt;%\n  filter(province_en %in% c(\"Nakhon Pathom\", \"Pathum Thani\", \"Nonthaburi\", \"Samut Prakan\", \"Samut Sakhon\", \"Bangkok\"))\n\n\nThe two variables were dropped as it were in Thai language. Additionally, the scope has been narrowed to only 6 provinces that are in the BMR.\n\n\n\nThe incident_datetime and report_datetime column were separated for ease of manipulation.\n\n\nCode Chunk\nrta_sf_mbr_split &lt;- rta_sf_mbr %&gt;%\n  separate(incident_datetime, into = c(\"incident_date\", \"incident_time\"), sep = \" \") %&gt;%\n  separate(report_datetime, into = c(\"report_date\", \"report_time\"), sep = \" \")\n\n\nIn ensuring that the data is in POSIXct format, ymd_hms was used.\n\n\nCode Chunk\nrta_sf_mbr_split &lt;- rta_sf_mbr %&gt;%\n  mutate(\n    datetime_parsed = ymd_hms(incident_datetime),\n    date = ymd_hms(datetime_parsed),\n    time = format(datetime_parsed, \"%H:%M:%S\")\n  )\n\n\nPulse Check: To check all of the current columns.\n\n\nCode Chunk\ncolnames(rta_sf_mbr_split)\n\n\n [1] \"acc_code\"                    \"incident_datetime\"          \n [3] \"report_datetime\"             \"province_en\"                \n [5] \"agency\"                      \"vehicle_type\"               \n [7] \"presumed_cause\"              \"accident_type\"              \n [9] \"number_of_vehicles_involved\" \"number_of_fatalities\"       \n[11] \"number_of_injuries\"          \"weather_condition\"          \n[13] \"road_description\"            \"slope_description\"          \n[15] \"geometry\"                    \"Month_num\"                  \n[17] \"Month_fac\"                   \"dayofweek\"                  \n[19] \"datetime_parsed\"             \"date\"                       \n[21] \"time\"                       \n\n\n\n\n\n\nThe variables that were not used in the analysis were dropped and it was filtered to the BMR.\n\n\nCode Chunk\nthaiadmin_bmr &lt;- thaiadmin %&gt;%\n  dplyr::select(-ADM1_TH, -ADM0_TH, -ADM1_REF, -ADM1ALT1EN, -ADM1ALT2EN, -ADM1ALT1TH,\n         -ADM1ALT2TH, -ADM0_PCODE, -validTo, -validOn) %&gt;%\n  filter(ADM1_EN %in% c(\"Nakhon Pathom\", \"Pathum Thani\", \"Nonthaburi\", \"Samut Prakan\", \"Samut Sakhon\", \"Bangkok\"))\n\n\n\n\nWhen analysing spatial point patterns, it is a good practice to confine the analysis with a geographical area like Singapore boundary. In spatstat, an object called owin is specially designed to represent this polygonal region.\nThe code chunk below is used to covert sg SpatialPolygon object into owin object of spatstat.\n\n\nCode Chunk\nbmr_owin &lt;- as.owin(thaiadmin32647)\n\n\n\n\nCode Chunk\nplot(bmr_owin)\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\ncoords &lt;- st_coordinates(rta_sf_mbr_split)\n# Create the ppp object with the same window as bmr_owin\nthaiadmin_ppp &lt;- as.ppp(coords, W = bmr_owin)\n\n# View the result\nplot(thaiadmin_ppp)\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\nsummary(thaiadmin_ppp)\n\n\nPlanar point pattern:  12986 points\nAverage intensity 2.516079e-08 points per square unit\n\n*Pattern contains duplicated points*\n\nCoordinates are given to 10 decimal places\n\nWindow: polygonal boundary\n728 separate polygons (no holes)\n             vertices        area relative.area\npolygon 1          53 2.53355e+03      4.91e-09\npolygon 2        1523 6.23850e+06      1.21e-05\npolygon 3          42 2.49113e+04      4.83e-08\npolygon 4          62 2.23046e+04      4.32e-08\npolygon 5          56 4.39488e+05      8.52e-07\npolygon 6          46 7.65029e+03      1.48e-08\npolygon 7          29 9.27916e+03      1.80e-08\npolygon 8          59 8.02014e+03      1.55e-08\npolygon 9         300 3.40852e+05      6.60e-07\npolygon 10        112 6.79950e+05      1.32e-06\npolygon 11         42 3.37968e+04      6.55e-08\npolygon 12        219 5.29394e+06      1.03e-05\npolygon 13         50 2.05900e+05      3.99e-07\npolygon 14        268 6.79882e+06      1.32e-05\npolygon 15         34 1.55825e+05      3.02e-07\npolygon 16         51 3.93134e+04      7.62e-08\npolygon 17         78 1.75837e+05      3.41e-07\npolygon 18        101 5.42686e+05      1.05e-06\npolygon 19        256 4.74527e+05      9.19e-07\npolygon 20         58 9.19444e+03      1.78e-08\npolygon 21         59 2.87919e+05      5.58e-07\npolygon 22         60 6.27905e+04      1.22e-07\npolygon 23        485 1.43133e+07      2.77e-05\npolygon 24        117 1.38538e+05      2.68e-07\npolygon 25        104 5.70710e+04      1.11e-07\npolygon 26        195 4.59998e+05      8.91e-07\npolygon 27         73 2.41793e+04      4.68e-08\npolygon 28        210 9.62852e+05      1.87e-06\npolygon 29         97 1.59662e+05      3.09e-07\npolygon 30        178 8.27111e+05      1.60e-06\npolygon 31        156 1.18428e+05      2.29e-07\npolygon 32         47 1.13996e+04      2.21e-08\npolygon 33         95 6.90830e+04      1.34e-07\npolygon 34         83 3.93177e+05      7.62e-07\npolygon 35         51 2.10677e+04      4.08e-08\npolygon 36         49 6.59279e+04      1.28e-07\npolygon 37         58 5.17956e+03      1.00e-08\npolygon 38         78 2.71442e+04      5.26e-08\npolygon 39        175 6.79048e+05      1.32e-06\npolygon 40        108 7.31483e+04      1.42e-07\npolygon 41        350 4.09442e+06      7.93e-06\npolygon 42        996 5.02741e+06      9.74e-06\npolygon 43         37 4.46504e+03      8.65e-09\npolygon 44         54 3.50271e+05      6.79e-07\npolygon 45        120 1.24977e+05      2.42e-07\npolygon 46        140 1.09669e+05      2.12e-07\npolygon 47         50 1.03162e+05      2.00e-07\npolygon 48         46 1.22206e+04      2.37e-08\npolygon 49         61 2.95012e+04      5.72e-08\npolygon 50        189 1.49213e+06      2.89e-06\npolygon 51         67 6.80527e+05      1.32e-06\npolygon 52        104 1.24896e+05      2.42e-07\npolygon 53         70 2.76957e+04      5.37e-08\npolygon 54         38 3.01574e+05      5.84e-07\npolygon 55         53 1.26348e+04      2.45e-08\npolygon 56         73 6.96475e+04      1.35e-07\npolygon 57         97 1.16021e+05      2.25e-07\npolygon 58        288 6.13520e+05      1.19e-06\npolygon 59        125 9.76483e+04      1.89e-07\npolygon 60        290 4.90893e+05      9.51e-07\npolygon 61         86 2.96223e+03      5.74e-09\npolygon 62        107 7.10532e+04      1.38e-07\npolygon 63         42 2.42729e+03      4.70e-09\npolygon 64         82 9.69627e+03      1.88e-08\npolygon 65        116 4.07177e+03      7.89e-09\npolygon 66        284 4.24081e+05      8.22e-07\npolygon 67        167 5.62031e+04      1.09e-07\npolygon 68        120 2.36548e+05      4.58e-07\npolygon 69       4581 2.10438e+08      4.08e-04\npolygon 70         86 5.63982e+04      1.09e-07\npolygon 71         64 8.12083e+03      1.57e-08\npolygon 72         62 6.58777e+04      1.28e-07\npolygon 73         52 5.27989e+04      1.02e-07\npolygon 74        152 2.86297e+03      5.55e-09\npolygon 75         54 9.21124e+04      1.78e-07\npolygon 76         52 3.61962e+04      7.01e-08\npolygon 77         69 1.02026e+05      1.98e-07\npolygon 78         49 4.19303e+04      8.12e-08\npolygon 79         37 1.40027e+04      2.71e-08\npolygon 80        226 3.76927e+05      7.30e-07\npolygon 81         86 2.75512e+05      5.34e-07\npolygon 82          8 7.96410e+03      1.54e-08\npolygon 83         38 7.22431e+04      1.40e-07\npolygon 84         48 3.33832e+04      6.47e-08\npolygon 85         46 2.94345e+04      5.70e-08\npolygon 86         33 6.35164e+03      1.23e-08\npolygon 87        344 4.03961e+06      7.83e-06\npolygon 88        441 7.31057e+05      1.42e-06\npolygon 89        249 4.78911e+05      9.28e-07\npolygon 90         52 5.01728e+03      9.72e-09\npolygon 91        116 3.62860e+05      7.03e-07\npolygon 92        125 7.50948e+04      1.45e-07\npolygon 93        123 8.15184e+04      1.58e-07\npolygon 94        108 2.78359e+06      5.39e-06\npolygon 95         56 2.11673e+05      4.10e-07\npolygon 96        364 1.86320e+06      3.61e-06\npolygon 97        161 5.21457e+05      1.01e-06\npolygon 98         34 6.70203e+03      1.30e-08\npolygon 99         32 7.97567e+03      1.55e-08\npolygon 100       130 1.97501e+06      3.83e-06\npolygon 101      1416 1.22482e+07      2.37e-05\npolygon 102        57 9.17313e+04      1.78e-07\npolygon 103        60 8.59325e+03      1.66e-08\npolygon 104        89 6.72186e+04      1.30e-07\npolygon 105        79 3.91725e+04      7.59e-08\npolygon 106        58 1.80916e+04      3.51e-08\npolygon 107        49 1.25443e+04      2.43e-08\npolygon 108       632 2.95702e+06      5.73e-06\npolygon 109        52 4.08946e+04      7.92e-08\npolygon 110       130 1.28922e+05      2.50e-07\npolygon 111        54 3.41374e+04      6.61e-08\npolygon 112        88 3.10409e+04      6.01e-08\npolygon 113       155 1.31462e+05      2.55e-07\npolygon 114        57 1.35510e+04      2.63e-08\npolygon 115        58 1.74498e+04      3.38e-08\npolygon 116       203 4.73856e+04      9.18e-08\npolygon 117       114 1.28611e+05      2.49e-07\npolygon 118       105 4.43573e+05      8.59e-07\npolygon 119       255 3.36132e+04      6.51e-08\npolygon 120       190 3.98132e+04      7.71e-08\npolygon 121      2874 1.10463e+08      2.14e-04\npolygon 122       143 5.49569e+04      1.06e-07\npolygon 123       186 2.29056e+05      4.44e-07\npolygon 124       154 1.53180e+06      2.97e-06\npolygon 125       440 2.10619e+04      4.08e-08\npolygon 126       302 4.71123e+04      9.13e-08\npolygon 127        56 1.17346e+05      2.27e-07\npolygon 128        31 3.09800e+04      6.00e-08\npolygon 129       130 4.69193e+04      9.09e-08\npolygon 130       271 1.22079e+06      2.37e-06\npolygon 131        81 2.68329e+04      5.20e-08\npolygon 132        26 2.60408e+03      5.05e-09\npolygon 133       400 1.88826e+06      3.66e-06\npolygon 134        46 1.37856e+04      2.67e-08\npolygon 135        26 7.52813e+04      1.46e-07\npolygon 136        11 1.09345e+04      2.12e-08\npolygon 137        12 1.21607e+04      2.36e-08\npolygon 138       107 1.61052e+05      3.12e-07\npolygon 139        39 1.02309e+05      1.98e-07\npolygon 140        13 2.16292e+04      4.19e-08\npolygon 141        31 8.74137e+04      1.69e-07\npolygon 142        10 1.61048e+04      3.12e-08\npolygon 143        12 5.99257e+03      1.16e-08\npolygon 144        21 8.49665e+04      1.65e-07\npolygon 145       227 1.97629e+06      3.83e-06\npolygon 146        56 3.94580e+04      7.65e-08\npolygon 147        54 4.76934e+04      9.24e-08\npolygon 148       293 8.85374e+05      1.72e-06\npolygon 149        71 5.28210e+04      1.02e-07\npolygon 150        37 1.12415e+04      2.18e-08\npolygon 151        76 1.68825e+05      3.27e-07\npolygon 152        59 8.12766e+04      1.57e-07\npolygon 153        80 8.32872e+04      1.61e-07\npolygon 154        60 2.66550e+04      5.16e-08\npolygon 155       116 1.79496e+05      3.48e-07\npolygon 156       197 7.99359e+04      1.55e-07\npolygon 157        58 9.33090e+04      1.81e-07\npolygon 158        44 9.01237e+03      1.75e-08\npolygon 159       152 8.10480e+04      1.57e-07\npolygon 160        95 7.92271e+05      1.54e-06\npolygon 161       104 4.56811e+04      8.85e-08\npolygon 162        53 7.07697e+04      1.37e-07\npolygon 163        35 3.21371e+04      6.23e-08\npolygon 164        30 2.47659e+03      4.80e-09\npolygon 165        76 3.78024e+04      7.32e-08\npolygon 166        27 4.82358e+04      9.35e-08\npolygon 167        23 7.26466e+03      1.41e-08\npolygon 168        52 3.88426e+03      7.53e-09\npolygon 169        40 7.02522e+04      1.36e-07\npolygon 170        30 5.40608e+04      1.05e-07\npolygon 171        99 2.16592e+05      4.20e-07\npolygon 172        83 2.81683e+04      5.46e-08\npolygon 173        40 7.75391e+03      1.50e-08\npolygon 174       157 1.83378e+07      3.55e-05\npolygon 175       167 2.71430e+05      5.26e-07\npolygon 176        86 2.35305e+04      4.56e-08\npolygon 177        14 5.15806e+03      9.99e-09\npolygon 178        62 4.49328e+04      8.71e-08\npolygon 179        27 6.19557e+04      1.20e-07\npolygon 180       133 5.92845e+05      1.15e-06\npolygon 181        86 6.04232e+05      1.17e-06\npolygon 182        72 1.31340e+05      2.54e-07\npolygon 183       138 1.16705e+06      2.26e-06\npolygon 184       245 3.45624e+06      6.70e-06\npolygon 185       109 4.18371e+05      8.11e-07\npolygon 186       472 2.39033e+07      4.63e-05\npolygon 187      1779 1.22523e+08      2.37e-04\npolygon 188        60 1.00340e+05      1.94e-07\npolygon 189        20 4.44892e+04      8.62e-08\npolygon 190        13 6.19586e+03      1.20e-08\npolygon 191       219 2.40052e+05      4.65e-07\npolygon 192        61 3.03300e+05      5.88e-07\npolygon 193        97 7.00283e+04      1.36e-07\npolygon 194      1062 1.69975e+07      3.29e-05\npolygon 195       145 3.01654e+05      5.84e-07\npolygon 196        91 5.83270e+04      1.13e-07\npolygon 197       105 4.14904e+04      8.04e-08\npolygon 198        93 3.63466e+04      7.04e-08\npolygon 199        39 1.01887e+04      1.97e-08\npolygon 200       130 2.63366e+05      5.10e-07\npolygon 201        77 5.34847e+04      1.04e-07\npolygon 202       100 1.23871e+05      2.40e-07\npolygon 203       202 1.95658e+05      3.79e-07\npolygon 204        52 2.04966e+05      3.97e-07\npolygon 205        25 2.58576e+04      5.01e-08\npolygon 206       417 1.51241e+06      2.93e-06\npolygon 207        51 5.71169e+03      1.11e-08\npolygon 208        53 1.52662e+04      2.96e-08\npolygon 209       119 2.77607e+04      5.38e-08\npolygon 210       116 2.76633e+05      5.36e-07\npolygon 211       253 7.77594e+05      1.51e-06\npolygon 212        65 5.53954e+04      1.07e-07\npolygon 213        62 2.17422e+04      4.21e-08\npolygon 214        42 2.64518e+04      5.13e-08\npolygon 215       113 5.21417e+04      1.01e-07\npolygon 216        11 2.24699e+03      4.35e-09\npolygon 217       233 4.05953e+05      7.87e-07\npolygon 218        74 3.72021e+04      7.21e-08\npolygon 219        61 2.21703e+04      4.30e-08\npolygon 220        43 3.47592e+04      6.73e-08\npolygon 221        43 7.09465e+03      1.37e-08\npolygon 222        63 8.21115e+04      1.59e-07\npolygon 223        68 7.42576e+04      1.44e-07\npolygon 224        52 6.13990e+04      1.19e-07\npolygon 225        57 6.14708e+04      1.19e-07\npolygon 226       112 3.32886e+05      6.45e-07\npolygon 227       142 1.09433e+06      2.12e-06\npolygon 228        50 7.74627e+04      1.50e-07\npolygon 229        34 5.71566e+03      1.11e-08\npolygon 230        47 3.28406e+04      6.36e-08\npolygon 231       243 1.32610e+06      2.57e-06\npolygon 232        34 2.24836e+03      4.36e-09\npolygon 233       157 1.42622e+05      2.76e-07\npolygon 234       179 5.63039e+06      1.09e-05\npolygon 235        39 6.77867e+04      1.31e-07\npolygon 236        42 8.65515e+02      1.68e-09\npolygon 237       205 6.69319e+05      1.30e-06\npolygon 238        54 2.77612e+03      5.38e-09\npolygon 239        90 6.76635e+04      1.31e-07\npolygon 240        67 7.75670e+04      1.50e-07\npolygon 241       143 1.74168e+05      3.37e-07\npolygon 242        40 1.60737e+05      3.11e-07\npolygon 243      2552 2.35426e+08      4.56e-04\npolygon 244        70 3.94070e+04      7.64e-08\npolygon 245       208 1.63232e+06      3.16e-06\npolygon 246        83 2.82963e+05      5.48e-07\npolygon 247       175 1.36324e+06      2.64e-06\npolygon 248        65 6.64185e+05      1.29e-06\npolygon 249       138 6.64843e+04      1.29e-07\npolygon 250        31 3.78277e+05      7.33e-07\npolygon 251        32 5.35809e+03      1.04e-08\npolygon 252       147 9.30384e+05      1.80e-06\npolygon 253        40 1.12524e+05      2.18e-07\npolygon 254        25 2.10691e+03      4.08e-09\npolygon 255       629 1.85471e+07      3.59e-05\npolygon 256        42 1.21267e+04      2.35e-08\npolygon 257        58 5.54077e+04      1.07e-07\npolygon 258       218 3.28794e+05      6.37e-07\npolygon 259        71 7.82792e+04      1.52e-07\npolygon 260        67 4.87781e+05      9.45e-07\npolygon 261        57 2.08458e+05      4.04e-07\npolygon 262       226 4.85861e+06      9.41e-06\npolygon 263        88 8.14478e+04      1.58e-07\npolygon 264        56 2.16487e+06      4.19e-06\npolygon 265        79 1.29659e+05      2.51e-07\npolygon 266       132 1.79898e+05      3.49e-07\npolygon 267        33 1.02731e+04      1.99e-08\npolygon 268       142 1.54527e+06      2.99e-06\npolygon 269       729 1.48280e+07      2.87e-05\npolygon 270       133 1.05890e+05      2.05e-07\npolygon 271        83 2.34362e+05      4.54e-07\npolygon 272        14 1.07965e+04      2.09e-08\npolygon 273       168 4.15326e+05      8.05e-07\npolygon 274        36 2.35428e+04      4.56e-08\npolygon 275        23 1.15661e+04      2.24e-08\npolygon 276        45 5.15325e+04      9.98e-08\npolygon 277        53 6.94478e+05      1.35e-06\npolygon 278        59 5.77518e+04      1.12e-07\npolygon 279       190 3.61314e+05      7.00e-07\npolygon 280       976 8.68567e+06      1.68e-05\npolygon 281        29 2.06981e+04      4.01e-08\npolygon 282        22 1.58427e+04      3.07e-08\npolygon 283        93 2.83389e+06      5.49e-06\npolygon 284        96 1.01050e+05      1.96e-07\npolygon 285       213 9.57989e+06      1.86e-05\npolygon 286       133 8.18763e+05      1.59e-06\npolygon 287       187 6.84325e+05      1.33e-06\npolygon 288        46 1.34985e+05      2.62e-07\npolygon 289        28 3.30205e+04      6.40e-08\npolygon 290        32 2.18078e+04      4.23e-08\npolygon 291       113 2.19925e+05      4.26e-07\npolygon 292        84 6.51097e+04      1.26e-07\npolygon 293       312 5.67054e+05      1.10e-06\npolygon 294        33 1.32885e+04      2.57e-08\npolygon 295        88 3.10315e+05      6.01e-07\npolygon 296        69 3.45741e+05      6.70e-07\npolygon 297       109 7.10583e+05      1.38e-06\npolygon 298       496 1.97740e+07      3.83e-05\npolygon 299        76 7.29808e+05      1.41e-06\npolygon 300        32 2.14783e+04      4.16e-08\npolygon 301        76 5.01893e+04      9.72e-08\npolygon 302        49 6.44866e+04      1.25e-07\npolygon 303        38 2.44360e+04      4.73e-08\npolygon 304        34 1.44985e+04      2.81e-08\npolygon 305        47 9.61114e+04      1.86e-07\npolygon 306       282 9.25103e+06      1.79e-05\npolygon 307        31 5.93875e+04      1.15e-07\npolygon 308       750 1.01758e+08      1.97e-04\npolygon 309        36 2.17645e+04      4.22e-08\npolygon 310        20 3.07083e+04      5.95e-08\npolygon 311        56 1.37639e+05      2.67e-07\npolygon 312        54 2.17419e+05      4.21e-07\npolygon 313        76 2.35627e+06      4.57e-06\npolygon 314        37 1.48907e+05      2.89e-07\npolygon 315        31 2.74226e+04      5.31e-08\npolygon 316        18 1.59275e+04      3.09e-08\npolygon 317        46 9.19515e+04      1.78e-07\npolygon 318       545 6.85244e+07      1.33e-04\npolygon 319        47 2.53689e+05      4.92e-07\npolygon 320        46 1.16386e+05      2.26e-07\npolygon 321        37 1.09409e+04      2.12e-08\npolygon 322        63 1.04438e+06      2.02e-06\npolygon 323       177 6.53342e+05      1.27e-06\npolygon 324       696 3.67112e+06      7.11e-06\npolygon 325       204 3.91698e+05      7.59e-07\npolygon 326       415 6.92778e+05      1.34e-06\npolygon 327        61 7.78251e+04      1.51e-07\npolygon 328        37 3.82324e+04      7.41e-08\npolygon 329       106 6.70020e+05      1.30e-06\npolygon 330       183 7.86389e+05      1.52e-06\npolygon 331        17 5.99620e+03      1.16e-08\npolygon 332        36 5.59015e+04      1.08e-07\npolygon 333        24 5.14841e+04      9.98e-08\npolygon 334        73 6.15783e+04      1.19e-07\npolygon 335        67 5.42200e+04      1.05e-07\npolygon 336        40 9.26211e+04      1.79e-07\npolygon 337        28 2.40995e+05      4.67e-07\npolygon 338        72 1.12406e+04      2.18e-08\npolygon 339        37 1.95637e+03      3.79e-09\npolygon 340        58 2.73734e+05      5.30e-07\npolygon 341       105 2.50438e+05      4.85e-07\npolygon 342        96 3.31618e+06      6.43e-06\npolygon 343        23 2.20023e+04      4.26e-08\npolygon 344        46 5.56469e+04      1.08e-07\npolygon 345       146 2.19599e+06      4.25e-06\npolygon 346       858 2.06853e+06      4.01e-06\npolygon 347       119 4.60709e+05      8.93e-07\npolygon 348        40 5.63947e+03      1.09e-08\npolygon 349        34 2.67960e+04      5.19e-08\npolygon 350       144 5.21384e+04      1.01e-07\npolygon 351        23 1.52352e+04      2.95e-08\npolygon 352        50 1.43738e+04      2.78e-08\npolygon 353        29 9.78437e+04      1.90e-07\npolygon 354        70 1.55289e+05      3.01e-07\npolygon 355        50 2.49115e+04      4.83e-08\npolygon 356        69 3.49514e+04      6.77e-08\npolygon 357        18 1.63675e+04      3.17e-08\npolygon 358        61 1.93303e+04      3.75e-08\npolygon 359        68 3.55635e+04      6.89e-08\npolygon 360       121 4.20225e+05      8.14e-07\npolygon 361        55 4.37920e+03      8.48e-09\npolygon 362        79 3.72043e+04      7.21e-08\npolygon 363        48 7.44105e+03      1.44e-08\npolygon 364        53 1.05232e+04      2.04e-08\npolygon 365        53 1.78218e+04      3.45e-08\npolygon 366        59 4.19901e+04      8.14e-08\npolygon 367       359 6.41671e+05      1.24e-06\npolygon 368       185 7.41242e+04      1.44e-07\npolygon 369        48 5.59843e+03      1.08e-08\npolygon 370       103 7.90648e+04      1.53e-07\npolygon 371        75 5.70544e+04      1.11e-07\npolygon 372        49 1.36485e+03      2.64e-09\npolygon 373        14 5.72691e+03      1.11e-08\npolygon 374        55 2.69781e+04      5.23e-08\npolygon 375        32 3.31282e+03      6.42e-09\npolygon 376       223 2.07929e+05      4.03e-07\npolygon 377        70 5.49744e+04      1.07e-07\npolygon 378        52 6.89398e+04      1.34e-07\npolygon 379        63 7.92309e+03      1.54e-08\npolygon 380        21 1.19439e+03      2.31e-09\npolygon 381        60 1.45064e+04      2.81e-08\npolygon 382        27 1.58312e+03      3.07e-09\npolygon 383        42 8.58139e+04      1.66e-07\npolygon 384        17 6.68861e+03      1.30e-08\npolygon 385        15 2.10492e+03      4.08e-09\npolygon 386        24 5.18389e+03      1.00e-08\npolygon 387       174 2.03651e+05      3.95e-07\npolygon 388        30 1.10485e+04      2.14e-08\npolygon 389       273 7.80652e+05      1.51e-06\npolygon 390       189 3.83543e+05      7.43e-07\npolygon 391       112 1.70948e+05      3.31e-07\npolygon 392       158 1.50097e+05      2.91e-07\npolygon 393        15 2.33364e+03      4.52e-09\npolygon 394        35 1.25320e+03      2.43e-09\npolygon 395        31 4.97341e+04      9.64e-08\npolygon 396        67 6.77975e+04      1.31e-07\npolygon 397        26 5.68397e+04      1.10e-07\npolygon 398        88 8.06683e+04      1.56e-07\npolygon 399        27 5.35406e+04      1.04e-07\npolygon 400       647 2.23542e+06      4.33e-06\npolygon 401      7658 5.21946e+08      1.01e-03\npolygon 402       168 3.93488e+05      7.62e-07\npolygon 403        44 6.48599e+04      1.26e-07\npolygon 404        37 2.30143e+04      4.46e-08\npolygon 405        18 2.87844e+03      5.58e-09\npolygon 406        70 2.27072e+05      4.40e-07\npolygon 407        58 4.98893e+03      9.67e-09\npolygon 408        81 2.28360e+04      4.42e-08\npolygon 409        22 4.83019e+03      9.36e-09\npolygon 410        37 5.19766e+03      1.01e-08\npolygon 411        17 6.41580e+02      1.24e-09\npolygon 412      1042 3.66538e+07      7.10e-05\npolygon 413        37 9.83199e+05      1.90e-06\npolygon 414        97 4.08270e+04      7.91e-08\npolygon 415        26 1.97750e+04      3.83e-08\npolygon 416        22 3.43179e+03      6.65e-09\npolygon 417        37 2.62906e+03      5.09e-09\npolygon 418        88 7.04665e+04      1.37e-07\npolygon 419        19 5.05037e+04      9.79e-08\npolygon 420        35 9.15793e+04      1.77e-07\npolygon 421        15 3.98830e+04      7.73e-08\npolygon 422        66 4.32073e+05      8.37e-07\npolygon 423       151 3.94490e+06      7.64e-06\npolygon 424        18 6.74356e+04      1.31e-07\npolygon 425        36 8.12520e+04      1.57e-07\npolygon 426        28 2.19188e+04      4.25e-08\npolygon 427        33 1.46144e+03      2.83e-09\npolygon 428        29 6.08303e+03      1.18e-08\npolygon 429         9 5.53715e+03      1.07e-08\npolygon 430        57 5.48791e+05      1.06e-06\npolygon 431       132 7.72824e+04      1.50e-07\npolygon 432        26 8.00521e+03      1.55e-08\npolygon 433        25 1.11667e+04      2.16e-08\npolygon 434        55 1.67508e+05      3.25e-07\npolygon 435        31 1.15254e+04      2.23e-08\npolygon 436        24 7.19219e+04      1.39e-07\npolygon 437        76 2.37555e+05      4.60e-07\npolygon 438        69 1.35858e+04      2.63e-08\npolygon 439        65 4.23839e+04      8.21e-08\npolygon 440        36 1.14927e+05      2.23e-07\npolygon 441      2287 9.12824e+07      1.77e-04\npolygon 442        95 4.42889e+04      8.58e-08\npolygon 443       109 1.03446e+05      2.00e-07\npolygon 444        77 1.59703e+05      3.09e-07\npolygon 445       227 1.99692e+05      3.87e-07\npolygon 446        49 1.07047e+05      2.07e-07\npolygon 447       146 5.99463e+04      1.16e-07\npolygon 448       159 2.65301e+05      5.14e-07\npolygon 449        65 1.19625e+04      2.32e-08\npolygon 450        71 1.52377e+04      2.95e-08\npolygon 451        23 1.31938e+04      2.56e-08\npolygon 452        93 3.58871e+04      6.95e-08\npolygon 453       278 4.58306e+05      8.88e-07\npolygon 454       113 3.79180e+04      7.35e-08\npolygon 455       159 4.96168e+05      9.61e-07\npolygon 456        96 2.93744e+06      5.69e-06\npolygon 457        45 2.67684e+04      5.19e-08\npolygon 458        49 1.08557e+04      2.10e-08\npolygon 459        19 6.24117e+03      1.21e-08\npolygon 460        16 2.96742e+03      5.75e-09\npolygon 461        12 7.15038e+03      1.39e-08\npolygon 462        79 1.51253e+05      2.93e-07\npolygon 463        22 1.01888e+04      1.97e-08\npolygon 464        14 1.57671e+04      3.05e-08\npolygon 465        23 1.80620e+04      3.50e-08\npolygon 466        47 3.75673e+04      7.28e-08\npolygon 467       160 2.73650e+05      5.30e-07\npolygon 468        68 1.23958e+05      2.40e-07\npolygon 469        37 4.24857e+04      8.23e-08\npolygon 470        38 1.04510e+04      2.02e-08\npolygon 471        50 1.47986e+04      2.87e-08\npolygon 472        27 1.22110e+05      2.37e-07\npolygon 473        67 1.78475e+04      3.46e-08\npolygon 474        66 2.85792e+04      5.54e-08\npolygon 475       126 4.96218e+05      9.61e-07\npolygon 476       105 4.42466e+04      8.57e-08\npolygon 477       114 6.63473e+05      1.29e-06\npolygon 478        41 1.33234e+04      2.58e-08\npolygon 479        65 1.59051e+04      3.08e-08\npolygon 480       113 2.05741e+05      3.99e-07\npolygon 481       267 5.51487e+05      1.07e-06\npolygon 482        39 1.14095e+04      2.21e-08\npolygon 483        56 8.94248e+03      1.73e-08\npolygon 484        74 1.72258e+04      3.34e-08\npolygon 485       107 2.28611e+04      4.43e-08\npolygon 486        62 1.90286e+04      3.69e-08\npolygon 487        98 4.19191e+04      8.12e-08\npolygon 488        65 1.80129e+04      3.49e-08\npolygon 489       342 3.86996e+06      7.50e-06\npolygon 490        43 2.19587e+04      4.25e-08\npolygon 491       107 3.13084e+04      6.07e-08\npolygon 492       320 1.58487e+07      3.07e-05\npolygon 493        58 9.88911e+03      1.92e-08\npolygon 494        79 2.49221e+04      4.83e-08\npolygon 495        89 2.40401e+06      4.66e-06\npolygon 496        19 2.92454e+04      5.67e-08\npolygon 497        35 1.37126e+04      2.66e-08\npolygon 498        82 8.40732e+05      1.63e-06\npolygon 499       155 1.87816e+05      3.64e-07\npolygon 500        36 5.77116e+04      1.12e-07\npolygon 501        30 4.80004e+04      9.30e-08\npolygon 502        52 1.48710e+05      2.88e-07\npolygon 503       315 1.42816e+07      2.77e-05\npolygon 504        40 9.87368e+03      1.91e-08\npolygon 505        50 2.38974e+05      4.63e-07\npolygon 506        54 1.58622e+03      3.07e-09\npolygon 507        27 2.28487e+05      4.43e-07\npolygon 508        30 5.07905e+04      9.84e-08\npolygon 509       601 2.03002e+07      3.93e-05\npolygon 510        53 3.78138e+05      7.33e-07\npolygon 511        34 2.43995e+05      4.73e-07\npolygon 512       148 2.57773e+06      4.99e-06\npolygon 513        95 1.56768e+05      3.04e-07\npolygon 514        24 4.73557e+03      9.18e-09\npolygon 515        96 4.15899e+05      8.06e-07\npolygon 516       124 3.99783e+05      7.75e-07\npolygon 517        56 1.70067e+05      3.30e-07\npolygon 518        28 1.53720e+04      2.98e-08\npolygon 519        55 1.33170e+06      2.58e-06\npolygon 520       118 3.29217e+05      6.38e-07\npolygon 521        29 6.20959e+04      1.20e-07\npolygon 522        33 2.15361e+05      4.17e-07\npolygon 523        58 4.16817e+05      8.08e-07\npolygon 524        69 3.35637e+04      6.50e-08\npolygon 525       343 4.66814e+06      9.04e-06\npolygon 526        42 1.16465e+05      2.26e-07\npolygon 527        11 5.06855e+03      9.82e-09\npolygon 528         9 5.15857e+03      9.99e-09\npolygon 529         9 5.35523e+03      1.04e-08\npolygon 530         9 3.83108e+03      7.42e-09\npolygon 531         9 3.58652e+03      6.95e-09\npolygon 532       507 1.40987e+07      2.73e-05\npolygon 533       176 4.33569e+04      8.40e-08\npolygon 534       145 9.41676e+05      1.82e-06\npolygon 535        50 8.12779e+05      1.57e-06\npolygon 536       132 1.36344e+05      2.64e-07\npolygon 537        64 6.09134e+05      1.18e-06\npolygon 538        58 4.28221e+03      8.30e-09\npolygon 539        86 6.10087e+05      1.18e-06\npolygon 540        33 8.51220e+04      1.65e-07\npolygon 541        36 1.29299e+05      2.51e-07\npolygon 542        36 3.09126e+05      5.99e-07\npolygon 543        78 2.41537e+05      4.68e-07\npolygon 544       391 2.05521e+06      3.98e-06\npolygon 545        55 7.68289e+04      1.49e-07\npolygon 546       149 3.98786e+04      7.73e-08\npolygon 547        53 3.68511e+04      7.14e-08\npolygon 548        68 9.62080e+03      1.86e-08\npolygon 549       111 6.66904e+05      1.29e-06\npolygon 550        41 6.36567e+03      1.23e-08\npolygon 551        23 3.28407e+04      6.36e-08\npolygon 552        24 2.01316e+04      3.90e-08\npolygon 553        64 1.78360e+04      3.46e-08\npolygon 554        46 2.35588e+05      4.56e-07\npolygon 555        28 1.43876e+04      2.79e-08\npolygon 556        42 5.42452e+04      1.05e-07\npolygon 557       103 9.55404e+05      1.85e-06\npolygon 558       409 1.59832e+06      3.10e-06\npolygon 559        26 1.43374e+04      2.78e-08\npolygon 560        23 1.56599e+04      3.03e-08\npolygon 561        56 1.07634e+05      2.09e-07\npolygon 562        29 2.08616e+04      4.04e-08\npolygon 563        23 8.70746e+04      1.69e-07\npolygon 564        22 2.44345e+04      4.73e-08\npolygon 565        29 2.12011e+05      4.11e-07\npolygon 566        26 4.30850e+04      8.35e-08\npolygon 567      3076 1.54644e+08      3.00e-04\npolygon 568        33 3.30314e+04      6.40e-08\npolygon 569        45 1.36371e+04      2.64e-08\npolygon 570        42 9.78281e+03      1.90e-08\npolygon 571        42 1.95779e+04      3.79e-08\npolygon 572        77 1.36570e+04      2.65e-08\npolygon 573        56 4.17267e+04      8.08e-08\npolygon 574        42 7.73459e+04      1.50e-07\npolygon 575       366 1.55299e+06      3.01e-06\npolygon 576       186 3.88337e+05      7.52e-07\npolygon 577       265 4.75199e+06      9.21e-06\npolygon 578        97 1.36906e+05      2.65e-07\npolygon 579       212 7.21442e+05      1.40e-06\npolygon 580        37 1.94568e+05      3.77e-07\npolygon 581        46 2.01957e+04      3.91e-08\npolygon 582        48 2.22218e+05      4.31e-07\npolygon 583        97 1.20950e+05      2.34e-07\npolygon 584        42 1.54741e+04      3.00e-08\npolygon 585        39 1.73207e+05      3.36e-07\npolygon 586        28 3.91697e+04      7.59e-08\npolygon 587        22 1.78201e+03      3.45e-09\npolygon 588        33 2.45257e+03      4.75e-09\npolygon 589       277 1.51027e+06      2.93e-06\npolygon 590       168 1.47433e+06      2.86e-06\npolygon 591        38 2.26133e+05      4.38e-07\npolygon 592       144 4.08278e+05      7.91e-07\npolygon 593        68 3.78086e+05      7.33e-07\npolygon 594        72 2.32205e+04      4.50e-08\npolygon 595       121 5.13480e+06      9.95e-06\npolygon 596        58 2.13844e+04      4.14e-08\npolygon 597        81 4.79942e+04      9.30e-08\npolygon 598       137 3.33007e+04      6.45e-08\npolygon 599       473 7.49755e+06      1.45e-05\npolygon 600        39 1.67970e+05      3.25e-07\npolygon 601       365 1.69664e+06      3.29e-06\npolygon 602        40 1.23577e+05      2.39e-07\npolygon 603       134 2.23991e+04      4.34e-08\npolygon 604       452 3.35999e+07      6.51e-05\npolygon 605        44 2.67716e+04      5.19e-08\npolygon 606        64 4.78819e+04      9.28e-08\npolygon 607        53 3.69509e+03      7.16e-09\npolygon 608       142 1.11369e+05      2.16e-07\npolygon 609       125 2.17926e+06      4.22e-06\npolygon 610        19 3.20952e+04      6.22e-08\npolygon 611       255 2.24402e+06      4.35e-06\npolygon 612        37 1.97936e+05      3.84e-07\npolygon 613        64 4.30987e+03      8.35e-09\npolygon 614       119 1.14005e+04      2.21e-08\npolygon 615        57 9.63602e+04      1.87e-07\npolygon 616        24 2.27351e+05      4.41e-07\npolygon 617       420 1.18945e+07      2.30e-05\npolygon 618       135 3.00968e+05      5.83e-07\npolygon 619        76 9.77677e+03      1.89e-08\npolygon 620        98 3.97489e+05      7.70e-07\npolygon 621        90 4.44495e+05      8.61e-07\npolygon 622        30 4.92474e+03      9.54e-09\npolygon 623        20 7.02674e+04      1.36e-07\npolygon 624        13 9.92488e+03      1.92e-08\npolygon 625        10 1.09966e+04      2.13e-08\npolygon 626       105 8.67378e+05      1.68e-06\npolygon 627         8 5.19214e+03      1.01e-08\npolygon 628        12 5.26167e+03      1.02e-08\npolygon 629        58 2.33547e+05      4.53e-07\npolygon 630        86 1.84186e+05      3.57e-07\npolygon 631        26 3.08047e+04      5.97e-08\npolygon 632        15 1.59462e+04      3.09e-08\npolygon 633        15 1.40602e+04      2.72e-08\npolygon 634        24 6.15344e+02      1.19e-09\npolygon 635        28 7.59425e+04      1.47e-07\npolygon 636        46 4.21602e+05      8.17e-07\npolygon 637       973 4.53393e+06      8.78e-06\npolygon 638       236 3.69682e+04      7.16e-08\npolygon 639       123 2.05167e+04      3.98e-08\npolygon 640        16 1.78479e+04      3.46e-08\npolygon 641        46 2.64240e+04      5.12e-08\npolygon 642        21 1.58657e+03      3.07e-09\npolygon 643       130 1.46585e+06      2.84e-06\npolygon 644        54 2.63840e+05      5.11e-07\npolygon 645       569 8.23323e+05      1.60e-06\npolygon 646        15 2.47074e+04      4.79e-08\npolygon 647        21 1.30712e+04      2.53e-08\npolygon 648        21 3.12432e+04      6.05e-08\npolygon 649        46 1.23221e+05      2.39e-07\npolygon 650        83 1.21574e+06      2.36e-06\npolygon 651        12 5.86805e+03      1.14e-08\npolygon 652       101 3.42115e+05      6.63e-07\npolygon 653      1497 1.51898e+08      2.94e-04\npolygon 654        57 1.39542e+05      2.70e-07\npolygon 655        27 1.20628e+05      2.34e-07\npolygon 656       204 5.88042e+06      1.14e-05\npolygon 657       141 8.66534e+05      1.68e-06\npolygon 658        38 6.23377e+05      1.21e-06\npolygon 659        48 2.47230e+05      4.79e-07\npolygon 660       179 1.30495e+06      2.53e-06\npolygon 661        36 1.14201e+05      2.21e-07\npolygon 662        15 1.45050e+04      2.81e-08\npolygon 663       143 8.24087e+05      1.60e-06\npolygon 664        18 4.19441e+03      8.13e-09\npolygon 665        40 1.93887e+04      3.76e-08\npolygon 666       105 2.81658e+05      5.46e-07\npolygon 667        51 2.19842e+05      4.26e-07\npolygon 668       505 6.33277e+05      1.23e-06\npolygon 669       106 1.03407e+05      2.00e-07\npolygon 670        53 2.72546e+04      5.28e-08\npolygon 671        37 6.11314e+04      1.18e-07\npolygon 672       841 1.83412e+06      3.55e-06\npolygon 673        36 1.25970e+05      2.44e-07\npolygon 674        34 5.79091e+03      1.12e-08\npolygon 675        37 3.76989e+04      7.30e-08\npolygon 676        16 1.25536e+04      2.43e-08\npolygon 677        78 8.69543e+04      1.68e-07\npolygon 678        16 3.04913e+04      5.91e-08\npolygon 679        34 4.63023e+04      8.97e-08\npolygon 680       139 7.82568e+04      1.52e-07\npolygon 681        42 7.95697e+04      1.54e-07\npolygon 682        49 1.18881e+05      2.30e-07\npolygon 683        58 9.55949e+04      1.85e-07\npolygon 684        21 2.08564e+04      4.04e-08\npolygon 685        65 5.29567e+05      1.03e-06\npolygon 686        41 1.73404e+04      3.36e-08\npolygon 687        65 1.69682e+04      3.29e-08\npolygon 688        13 1.46245e+04      2.83e-08\npolygon 689      1066 2.88894e+07      5.60e-05\npolygon 690        18 1.26335e+04      2.45e-08\npolygon 691       125 1.17609e+06      2.28e-06\npolygon 692       291 2.89175e+07      5.60e-05\npolygon 693        39 3.25381e+04      6.30e-08\npolygon 694        77 9.05555e+05      1.75e-06\npolygon 695       133 1.39549e+05      2.70e-07\npolygon 696        30 1.39928e+04      2.71e-08\npolygon 697        31 2.58780e+04      5.01e-08\npolygon 698        19 4.77690e+04      9.26e-08\npolygon 699        18 1.19413e+04      2.31e-08\npolygon 700        27 7.06965e+04      1.37e-07\npolygon 701        94 2.93367e+05      5.68e-07\npolygon 702       357 4.19642e+06      8.13e-06\npolygon 703        50 1.72523e+04      3.34e-08\npolygon 704         9 3.97401e+03      7.70e-09\npolygon 705        55 1.36144e+04      2.64e-08\npolygon 706       109 3.14625e+05      6.10e-07\npolygon 707        23 2.00716e+04      3.89e-08\npolygon 708       123 3.01503e+05      5.84e-07\npolygon 709        33 9.99966e+03      1.94e-08\npolygon 710        39 4.26341e+04      8.26e-08\npolygon 711        21 2.44897e+04      4.74e-08\npolygon 712        14 7.89863e+03      1.53e-08\npolygon 713        44 2.38081e+04      4.61e-08\npolygon 714        35 3.59738e+04      6.97e-08\npolygon 715        66 8.56579e+04      1.66e-07\npolygon 716        25 1.69954e+04      3.29e-08\npolygon 717        16 1.02405e+04      1.98e-08\npolygon 718        23 4.72763e+04      9.16e-08\npolygon 719        30 1.71468e+04      3.32e-08\npolygon 720       206 4.10359e+06      7.95e-06\npolygon 721        35 1.63422e+04      3.17e-08\npolygon 722        26 6.61320e+04      1.28e-07\npolygon 723        24 3.00670e+04      5.83e-08\npolygon 724        60 5.43431e+05      1.05e-06\npolygon 725        52 1.33290e+05      2.58e-07\npolygon 726        51 1.57518e+05      3.05e-07\npolygon 727        46 2.22838e+05      4.32e-07\npolygon 728    177339 5.13717e+11      9.95e-01\nenclosing rectangle: [325178.8, 1213655.7] x [620860.6, 2263241] units\n                     (888500 x 1642000 units)\nWindow area = 5.1612e+11 square units\nFraction of frame area: 0.354\n\n\ncolnames was used to list all of the columns in the df.\n\n\nCode Chunk\ncolnames(thaiadmin32647)\n\n\n [1] \"Shape_Leng\" \"Shape_Area\" \"ADM1_EN\"    \"ADM1_TH\"    \"ADM1_PCODE\"\n [6] \"ADM1_REF\"   \"ADM1ALT1EN\" \"ADM1ALT2EN\" \"ADM1ALT1TH\" \"ADM1ALT2TH\"\n[11] \"ADM0_EN\"    \"ADM0_TH\"    \"ADM0_PCODE\" \"date\"       \"validOn\"   \n[16] \"validTo\"    \"geometry\"   \"acc_code\"  \n\n\n\n\n\n\nNoticed that the geometry type is in multi-line string, not suitable for maniuplation.\n\n\nCode Chunk\nst_geometry(roadlines)\n\n\nGeometry set for 2792590 features \nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 97.34457 ymin: 5.643645 xmax: 105.6528 ymax: 20.47168\nCRS:           NA\nFirst 5 geometries:\n\n\n\n\n\n\nCode Chunk\nroadlines &lt;- st_cast(roadlines, \"LINESTRING\")\n\n\nHead() function allows to view the first 5 rows of the data.\n\n\nCode Chunk\nhead(roadlines, n=5)\n\n\nSimple feature collection with 5 features and 14 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 100.7831 ymin: 13.73046 xmax: 100.7913 ymax: 13.74091\nCRS:           NA\n            name               name_en        highway  surface smoothness width\n1     ถนนฉลองกรุง    Chalong Krung Road      secondary    paved       &lt;NA&gt;  &lt;NA&gt;\n2 ซอยฉลองกรุง 1/1 Soi Chalong Krung 1/1    residential     &lt;NA&gt;       &lt;NA&gt;  &lt;NA&gt;\n3           &lt;NA&gt;                  &lt;NA&gt; secondary_link     &lt;NA&gt;       &lt;NA&gt;  &lt;NA&gt;\n4           &lt;NA&gt;                  &lt;NA&gt;        service     &lt;NA&gt;       &lt;NA&gt;  &lt;NA&gt;\n5     ถนนฉลองกรุง    Chalong Krung Road      secondary concrete       &lt;NA&gt;  &lt;NA&gt;\n  lanes oneway bridge layer source        name_th     osm_id  osm_type\n1  &lt;NA&gt;    yes   &lt;NA&gt;  &lt;NA&gt;   &lt;NA&gt;     ถนนฉลองกรุง 1125681229 ways_line\n2  &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;   &lt;NA&gt; ซอยฉลองกรุง 1/1  594401607 ways_line\n3  &lt;NA&gt;    yes   &lt;NA&gt;  &lt;NA&gt;   &lt;NA&gt;           &lt;NA&gt;  472283206 ways_line\n4  &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;   &lt;NA&gt;           &lt;NA&gt;  594401608 ways_line\n5     2    yes    yes     1   Bing     ถนนฉลองกรุง  116847248 ways_line\n                        geometry\n1 LINESTRING (100.7913 13.740...\n2 LINESTRING (100.7882 13.736...\n3 LINESTRING (100.7844 13.732...\n4 LINESTRING (100.7873 13.735...\n5 LINESTRING (100.7831 13.730...\n\n\n\n\n\n\n\n\nCode Chunk\nroadlines &lt;- st_set_crs(roadlines, 4326)\nroadlines_transformed &lt;- st_transform(roadlines, crs = 32647)\n\n\n\n\nCode Chunk\nmerged_data &lt;- st_join(roadlines_transformed, thaiadmin32647, join = st_intersects)\n\n\n\n\nCode Chunk\nwrite_rds(merged_data, \"data/rds/merged_data.rds\")\n\n\nEXTRACT ROADLINES UNDER BMR then put it accidents data to DR\nTASK 1:To visualize the spatio-temporal dynamics of road traffic accidents in BMR using appropriate statistical graphics and geovisualization methods.\n\nGoal: Create visuals to show how road accidents are distributed over space and time.\nMethods: Use statistical graphics and geovisualization techniques such as:\n\nHeatmaps for spatial density of accidents.\nTime series charts for accident trends over time.\nAnimated maps to show changes in accident locations over time (spatio-temporal dynamics).\n\n\nFrom this heatmap, we can see Bangkok notoriously topped the charts in the number of RTAs every year followed by Samut Prakan.\n\n\nCode Chunk\n# Step 1: Extract the year from the date column\nrta_sf_mbr_split &lt;- rta_sf_mbr_split %&gt;%\n  mutate(year = year(date))  # Extract year using lubridate's year() function\n\n# Step 2: Summarize the data to count accidents per province and year\naccidents_summary &lt;- rta_sf_mbr_split %&gt;%\n  group_by(province_en, year) %&gt;%\n  summarise(total_accidents = n(), .groups = \"drop\")\n\n# Step 3: Create the heatmap with number of accidents in each tile\nggplot(accidents_summary, aes(x = year, y = province_en, fill = total_accidents)) +\n  geom_tile(color = \"white\") +  # Use geom_tile to create the heatmap\n  geom_text(aes(label = total_accidents), color = \"black\", size = 3) +  # Add the number of accidents in each tile\n  scale_fill_gradient(low = \"lightblue\", high = \"red\", na.value = \"white\", name = \"Number of Accidents\") +\n  labs(title = \"Accident Heatmap by Province and Year\",\n       x = \"Year\",\n       y = \"Province\",\n       fill = \"Total Accidents\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability\n\n\n\n\n\n\n\n\n\nA heatmap segregated of months was populated below to gain a better understanding of the accidents in each month.\n\n\nCode Chunk\n# Step 1: Extract the year and month name from the date column\nrta_sf_mbr_split &lt;- rta_sf_mbr_split %&gt;%\n  mutate(\n    year = year(date),                # Extract year\n    month = month(date, label = TRUE, abbr = FALSE)  # Extract full month name\n  )\n\n# Step 2: Summarize the data to count accidents per province, year, and month\naccidents_summary &lt;- rta_sf_mbr_split %&gt;%\n  group_by(province_en, year, month) %&gt;%\n  summarise(total_accidents = n(), .groups = \"drop\")\n\n# Step 3: Create the heatmap\nggplot(accidents_summary, aes(x = month, y = province_en, fill = total_accidents)) +\n  geom_tile(color = \"white\") +  # Use geom_tile to create the heatmap\n  facet_wrap(~year, ncol = 1) +  # Separate the heatmap by year\n  scale_fill_gradient(low = \"lightblue\", high = \"red\", na.value = \"white\", name = \"Number of Accidents\") +\n  labs(title = \"Accident Heatmap by Province and Month\",\n       x = \"Month\",\n       y = \"Province\",\n       fill = \"Total Accidents\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\n# Filter to BMR\nprovinces_to_keep &lt;- c(\"Nakhon Pathom\", \"Pathum Thani\", \"Nonthaburi\", \n                       \"Samut Prakan\", \"Samut Sakhon\", \"Bangkok\")\n\n# Filter the dataset for the 6 provinces\nrta_sf_mbr_filtered &lt;- rta_sf_mbr_split %&gt;%\n  filter(province_en %in% provinces_to_keep)\n\n# Group by date and province, then count the number of accidents per day for each province\naccident_trends_by_province &lt;- rta_sf_mbr_filtered %&gt;%\n  group_by(date = as.Date(date), province_en) %&gt;%\n  summarise(daily_accidents = n(), .groups = \"drop\")\n\n# Plot the time series chart for each province\nggplot(data = accident_trends_by_province, aes(x = date, y = daily_accidents, color = province_en)) +\n  geom_line() +\n  labs(title = \"Time Series of Accident Trends by Province\",\n       x = \"Year\", y = \"Number of Accidents\") +\n  theme_minimal() +\n  facet_wrap(~ province_en, ncol = 2, scales = \"free_y\")\n\n\n\n\n\n\n\n\n\nObservation\nBased on a cursory view, it is apparent that Bangkok has the highest number of RTA from 2019 - 2023 and the numbers are relatively consistent and concentrated except in Q1 2019 & Q4 2023 when it spiked above 15 accidents. Conversely, Samut Sakhon has the lowest number of RTA where in end of 2022, there is close to 0 number of accidents. It is also worthwhile noting that Samut Prakan has an interesting outlook of the RTA data. At certain time frame, it had spikes that indicates that the number of RTA rose above 7 and Samut Prakan also have pockets of “dead spots” which tells us the number of accidents were close to 0.\nInference\nBangkok is notably the most densely populated province in Thailand alongside the influx of tourists. This may explain the consistently high number of RTA in the province.\nAdditionally, Nonthaburi , too, have a high number of RTA, This may due to it being the 2nd most densely populated province (Wiki).\nAs Samut Sakhon is an industrial dominant and formerly an agriculture/fisheries producing province (Wiki), there might not be huge population of people residing or commuting around the province. Hence, it may be why Samut Sakhon has the lowest number of RTA between the 6 provinces of the MBR.\nTASK 2:To conduct detailed spatial analysis of road traffic accidents using appropriate Network Spatial Point Patterns Analysis methods.\n\nGoal: Analyze the spatial patterns of accidents using Network Spatial Point Patterns Analysis (NSPPA).\nMethods: NSPPA techniques to analyze how accidents are distributed along the road network. This involves:\n\nKernel density estimation (KDE) on the road network.\nNearest neighbor analysis to see clustering on road segments.\nNetwork K-function or Ripley’s K-function to analyze the spatial distribution along the network."
  },
  {
    "objectID": "TakeHomeExercise/TakeHome1/TakeHome1.html#spatial-data-wrangling",
    "href": "TakeHomeExercise/TakeHome1/TakeHome1.html#spatial-data-wrangling",
    "title": "Take-Home Exercise 1",
    "section": "",
    "text": "Code Chunk\ntmap_mode('plot')\ntm_shape(rta_sf)+\n  tm_dots(alpha=0.4,\n          size=0.05)\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\nwrite_rds('view', \"data/rds/view.rds\")\n\n\n\n\nCode Chunk\nwrite_rds('plot', \"data/rds/plot.rds\")\n\n\n\n\n\n\nCode Chunk\nrta_sp &lt;- as_Spatial(rta_sf_mbr_split)\nroadlines_sp &lt;- as_Spatial(roadlines)\nthaiadmin_sp &lt;- as_Spatial(thaiadmin32647)\n\n\n\n\nCode Chunk\nwrite_rds('rta_sp', \"data/rds/rta_sp.rds\")\nwrite_rds('roadlines_sp', \"data/rds/roadlines_sp.rds\")\nwrite_rds('thaiadmin_sp', \"data/rds/thaiadmin_sp.rds\")\n\n\nConverting generic sp format into spatstat’s ppp format\n\n\nCode Chunk\nrta_ppp &lt;- as.ppp(rta_sf)\nrta_ppp\n\n\nMarked planar point pattern: 81376 points\nmarks are numeric, of storage type  'double'\nwindow: rectangle = [-16183187, 1200243] x [-936, 4918525] units\n\n\n\n\nCode Chunk\nkde_rta &lt;- density(rta_ppp,\n                   sigma=bw.diggle,\n                   edge=TRUE,\n                   kernel=\"gaussian\") \n\n\n\n\nCode Chunk\nplot(kde_rta)\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\nkde_rta_adaptive &lt;- adaptive.density(rta_ppp, method=\"kernel\")\nplot(kde_rta_adaptive)\n\n\n\n\n\n\n\n\n\n“Murphy’s law will kick in during the very last minute” (Kam, 2024).\nAnd I attest it was absolutely true."
  },
  {
    "objectID": "HandsOnExercise/HandsOn5a/HandsOn5a.html",
    "href": "HandsOnExercise/HandsOn5a/HandsOn5a.html",
    "title": "Hands-On Exercise 5a",
    "section": "",
    "text": "In this hands-on exercise, you will learn how to compute Global Measures of Spatial Autocorrelation (GMSA) by using spdep package. By the end to this hands-on exercise, you will be able to:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute Global Spatial Autocorrelation (GSA) statistics by using appropriate functions of spdep package,\n\nplot Moran scatterplot,\ncompute and plot spatial correlogram using appropriate function of spdep package.\n\nprovide statistically correct interpretation of GSA statistics.\n\n\n\n\n\n\nIn spatial policy, one of the main development objective of the local government and planners is to ensure equal distribution of development in the province. Our task in this study, hence, is to apply appropriate spatial statistical methods to discover if development are even distributed geographically. If the answer is No. Then, our next question will be “is there sign of spatial clustering?”. And, if the answer for this question is yes, then our next question will be “where are these clusters?”\nIn this case study, we are interested to examine the spatial pattern of a selected development indicator (i.e. GDP per capita) of Hunan Provice, People Republic of China.\n\n\n\nTwo data sets will be used in this hands-on exercise, they are:\n\nHunan province administrative boundary layer at county level. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv: This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n\n\nBefore we get started, we need to ensure that spdep, sf, tmap and tidyverse packages of R are currently installed in your R.\n\nsf is use for importing and handling geospatial data in R,\ntidyverse is mainly use for wrangling attribute data in R,\nspdep will be used to compute spatial weights, global and local spatial autocorrelation statistics, and\ntmap will be used to prepare cartographic quality chropleth map.\n\nThe code chunk below is used to perform the following tasks:\n\ncreating a package list containing the necessary R packages,\nchecking if the R packages in the package list have been installed in R,\n\nif they have yet to be installed, RStudio will installed the missing packages,\n\nlaunching the packages into R environment.\n\n\n\nCode Chunk\npacman::p_load(sf, spdep, tmap, tidyverse, dplyr)\n\n\n\n\n\n\nIn this section, you will learn how to bring a geospatial data and its associated attribute table into R environment. The geospatial data is in ESRI shapefile format and the attribute table is in csv fomat.\n\n\nThe code chunk below uses st_read() of sf package to import Hunan shapefile into R. The imported shapefile will be simple features Object of sf.\n\n\nCode Chunk\nhunan &lt;- st_read(\"data/geospatial\",\n                  layer = \"Hunan\")\n\n\n\n\nCode Chunk\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n\n\n\nThe code chunk below will be used to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\n\n\nCode Chunk\nst_crs(hunan)\n\n\n\n\nCode Chunk\nhunan &lt;- left_join(hunan, hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\n\n\n\n\nNow, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\n\nCode Chunk\nequal &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal interval classification\")\n\nquantile &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal quantile classification\")\n\ntmap_arrange(equal, \n             quantile, \n             asp=1, \n             ncol=2)\n\n\n\n\n\n\nIn this section, you will learn how to compute global spatial autocorrelation statistics and to perform spatial complete randomness test for global spatial autocorrelation.\n\n\nBefore we can compute the global spatial autocorrelation statistics, we need to construct a spatial weights of the study area. The spatial weights is used to define the neighbourhood relationships between the geographical units (i.e. county) in the study area.\nIn the code chunk below, poly2nb() of spdep package is used to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries. If you look at the documentation you will see that you can pass a “queen” argument that takes TRUE or FALSE as options. If you do not specify this argument the default is set to TRUE, that is, if you don’t specify queen = FALSE this function will return a list of first order neighbours using the Queen criteria.\nMore specifically, the code chunk below is used to compute Queen contiguity weight matrix.\n\n\nCode Chunk\nwm_q &lt;- poly2nb(hunan,\n                  queen=TRUE)\nsummary(wm_q)\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one neighbours.\n\n\n\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors’ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we’ll stick with the style=“W” option for simplicity’s sake but note that other more robust options are available, notably style=“B”.\n\n\nCode Chunk\nrswm_q &lt;- nb2list(wm_q,\n                    style=\"W\",\n                    zero.policy=TRUE)\nrswm_q\n\n\nLearning from above code chunk:\n\nThe input of nb2listw() must be an object of class nb. The syntax of the function has two major arguments, namely style and zero.poly.\nstyle can take values “W”, “B”, “C”, “U”, “minmax” and “S”. B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).\nIf zero policy is set to TRUE, weights vectors of zero length are inserted for regions without neighbour in the neighbours list. These will in turn generate lag values of zero, equivalent to the sum of products of the zero row t(rep(0, length=length(neighbours))) %*% x, for arbitrary numerical vector x of length length(neighbours). The spatially lagged value of x for the zero-neighbour region will then be zero, which may (or may not) be a sensible choice.\n\n\n\n\n\nIn this section, you will learn how to perform Moran’s I statistics testing by using moran.test() of spdep.\n\n\nThe code chunk below performs Moran’s I statistical testing using moran.test() of spdep.\n\n\nCode Chunk\nmoran.test(hunan$GDPPC,\n            listw=rswm_q,\n            zero.policy = TRUE,\n            na.action = na.omit)\n\n\n\n\n\nThe code chunk below performs permutation test for Moran’s I statistic by using moran.mc() of spdep. A total of 1000 simulation will be performed.\n\n\nCode Chunk\nset.seed(1234)\nbperm = moran.mc(hunan$GDPPC,\n                 list1 = rswm_q,\n                 nsim = 999,\n                 zero.policy = TRUE,\n                 na.action = na.omit)\nbperm\n\n\n\n\n\nIt is always a good practice for us the examine the simulated Moran’s I test statistics in greater detail. This can be achieved by plotting the distribution of the statistical values as a histogram by using the code chunk below.\nIn the code chunk below hist() and abline() of R Graphics are used.\n\n\nCode Chunk\nmean(bperm$res[1:999])\n\n\n\n\nCode Chunk\nvar(bperm$res[1:999])\n\n\n\n\nCode Chunk\nsummary(bperm$res[1:999])\n\n\n\n\nCode Chunk\nhist(bperm$res,\n      freq=TRUE,\n      breaks = 20,\n      xlab = \"Simulated Moran's I\")\nabline(v=0,\n       col = \"red\")\n\n\n\n\n\n\nIn this section, you will learn how to perform Geary’s C statistics testing by using appropriate functions of spdep package.\n\n\nThe code chunk below performs Geary’s C test for spatial autocorrelation by using geary.test() of spdep.\n\n\nCode Chunk\ngeary.test(hunan$GDPPC, listw = rswm_q)\n\n\n\n\n\nThe code chunk below performs permutation test for Geary’s C statistic by using geary.mc() of spdep.\n\n\nCode Chunk\nset.seed(1234)\nbperm=geary.mc(hunan$GDPPC,\n                listw=rswm_q,\n                nsim=999)\n\nbperm\n\n\n\n\n\nNext, we will plot a histogram to reveal the distribution of the simulated values by using the code chunk below.\n\n\nCode Chunk\nmean(bperm$res[1:999])\n\n\n\n\nCode Chunk\nvar(bperm$res[1:999])\n\n\n\n\nCode Chunk\nsummary(bperm$res[1:999])\n\n\n\n\nCode Chunk\nhist(bperm$res, freq=TRUE, breaks=20, xlab=\"Simulated Geary c\")\nabline(v=1, col=\"red\")\n\n\n\n\n\n\nSpatial correlograms are great to examine patterns of spatial autocorrelation in your data or model residuals. They show how correlated are pairs of spatial observations when you increase the distance (lag) between them - they are plots of some index of autocorrelation (Moran’s I or Geary’s c) against distance.Although correlograms are not as fundamental as variograms (a keystone concept of geostatistics), they are very useful as an exploratory and descriptive tool. For this purpose they actually provide richer information than variograms.\n\n\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Moran’s I. The plot() of base Graph is then used to plot the output.\n\n\nCode Chunk\nMI_corr &lt;- sp.correlogram(wm_q,\n                          hunan$GDPPC,\n                          order = 6,\n                          method = \"I\",\n                          style = \"W\")\nplot(MI_corr)\n\n\nBy plotting the output might not allow us to provide complete interpretation. This is because not all autocorrelation values are statistically significant. Hence, it is important for us to examine the full analysis report by printing out the analysis results as in the code chunk below.\n\n\nCode Chunk\nprint(MI_corr)\n\n\n\n\n\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Geary’s C. The plot() of base Graph is then used to plot the output.\n\n\nCode Chunk\nGC_corr &lt;- sp.correlogram(wm_q,\n                          hunan$GDPPC,\n                          order = 6,\n                          method = \"C\",\n                          style=\"W\")\nplot(GC_corr)\n\n\nSimilar to the previous step, we will print out the analysis report by using the code chunk below.\n\n\nCode Chunk\nprint(GC_corr)"
  },
  {
    "objectID": "HandsOnExercise/HandsOn5a/HandsOn5a.html#overview",
    "href": "HandsOnExercise/HandsOn5a/HandsOn5a.html#overview",
    "title": "Hands-On Exercise 5a",
    "section": "",
    "text": "In this hands-on exercise, you will learn how to compute Global Measures of Spatial Autocorrelation (GMSA) by using spdep package. By the end to this hands-on exercise, you will be able to:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute Global Spatial Autocorrelation (GSA) statistics by using appropriate functions of spdep package,\n\nplot Moran scatterplot,\ncompute and plot spatial correlogram using appropriate function of spdep package.\n\nprovide statistically correct interpretation of GSA statistics."
  },
  {
    "objectID": "HandsOnExercise/HandsOn5a/HandsOn5a.html#getting-started",
    "href": "HandsOnExercise/HandsOn5a/HandsOn5a.html#getting-started",
    "title": "Hands-On Exercise 5a",
    "section": "",
    "text": "In spatial policy, one of the main development objective of the local government and planners is to ensure equal distribution of development in the province. Our task in this study, hence, is to apply appropriate spatial statistical methods to discover if development are even distributed geographically. If the answer is No. Then, our next question will be “is there sign of spatial clustering?”. And, if the answer for this question is yes, then our next question will be “where are these clusters?”\nIn this case study, we are interested to examine the spatial pattern of a selected development indicator (i.e. GDP per capita) of Hunan Provice, People Republic of China.\n\n\n\nTwo data sets will be used in this hands-on exercise, they are:\n\nHunan province administrative boundary layer at county level. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv: This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n\n\nBefore we get started, we need to ensure that spdep, sf, tmap and tidyverse packages of R are currently installed in your R.\n\nsf is use for importing and handling geospatial data in R,\ntidyverse is mainly use for wrangling attribute data in R,\nspdep will be used to compute spatial weights, global and local spatial autocorrelation statistics, and\ntmap will be used to prepare cartographic quality chropleth map.\n\nThe code chunk below is used to perform the following tasks:\n\ncreating a package list containing the necessary R packages,\nchecking if the R packages in the package list have been installed in R,\n\nif they have yet to be installed, RStudio will installed the missing packages,\n\nlaunching the packages into R environment.\n\n\n\nCode Chunk\npacman::p_load(sf, spdep, tmap, tidyverse, dplyr)"
  },
  {
    "objectID": "HandsOnExercise/HandsOn5a/HandsOn5a.html#getting-the-data-into-r-environment",
    "href": "HandsOnExercise/HandsOn5a/HandsOn5a.html#getting-the-data-into-r-environment",
    "title": "Hands-On Exercise 5a",
    "section": "",
    "text": "In this section, you will learn how to bring a geospatial data and its associated attribute table into R environment. The geospatial data is in ESRI shapefile format and the attribute table is in csv fomat.\n\n\nThe code chunk below uses st_read() of sf package to import Hunan shapefile into R. The imported shapefile will be simple features Object of sf.\n\n\nCode Chunk\nhunan &lt;- st_read(\"data/geospatial\",\n                  layer = \"Hunan\")\n\n\n\n\nCode Chunk\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n\n\n\nThe code chunk below will be used to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\n\n\nCode Chunk\nst_crs(hunan)\n\n\n\n\nCode Chunk\nhunan &lt;- left_join(hunan, hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\n\n\n\n\nNow, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\n\nCode Chunk\nequal &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal interval classification\")\n\nquantile &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal quantile classification\")\n\ntmap_arrange(equal, \n             quantile, \n             asp=1, \n             ncol=2)"
  },
  {
    "objectID": "HandsOnExercise/HandsOn5a/HandsOn5a.html#global-measures-of-spatial-autocorrelation",
    "href": "HandsOnExercise/HandsOn5a/HandsOn5a.html#global-measures-of-spatial-autocorrelation",
    "title": "Hands-On Exercise 5a",
    "section": "",
    "text": "In this section, you will learn how to compute global spatial autocorrelation statistics and to perform spatial complete randomness test for global spatial autocorrelation.\n\n\nBefore we can compute the global spatial autocorrelation statistics, we need to construct a spatial weights of the study area. The spatial weights is used to define the neighbourhood relationships between the geographical units (i.e. county) in the study area.\nIn the code chunk below, poly2nb() of spdep package is used to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries. If you look at the documentation you will see that you can pass a “queen” argument that takes TRUE or FALSE as options. If you do not specify this argument the default is set to TRUE, that is, if you don’t specify queen = FALSE this function will return a list of first order neighbours using the Queen criteria.\nMore specifically, the code chunk below is used to compute Queen contiguity weight matrix.\n\n\nCode Chunk\nwm_q &lt;- poly2nb(hunan,\n                  queen=TRUE)\nsummary(wm_q)\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one neighbours.\n\n\n\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors’ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we’ll stick with the style=“W” option for simplicity’s sake but note that other more robust options are available, notably style=“B”.\n\n\nCode Chunk\nrswm_q &lt;- nb2list(wm_q,\n                    style=\"W\",\n                    zero.policy=TRUE)\nrswm_q\n\n\nLearning from above code chunk:\n\nThe input of nb2listw() must be an object of class nb. The syntax of the function has two major arguments, namely style and zero.poly.\nstyle can take values “W”, “B”, “C”, “U”, “minmax” and “S”. B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).\nIf zero policy is set to TRUE, weights vectors of zero length are inserted for regions without neighbour in the neighbours list. These will in turn generate lag values of zero, equivalent to the sum of products of the zero row t(rep(0, length=length(neighbours))) %*% x, for arbitrary numerical vector x of length length(neighbours). The spatially lagged value of x for the zero-neighbour region will then be zero, which may (or may not) be a sensible choice."
  },
  {
    "objectID": "HandsOnExercise/HandsOn5a/HandsOn5a.html#global-measures-of-spatial-autocorrelation-morans-i",
    "href": "HandsOnExercise/HandsOn5a/HandsOn5a.html#global-measures-of-spatial-autocorrelation-morans-i",
    "title": "Hands-On Exercise 5a",
    "section": "",
    "text": "In this section, you will learn how to perform Moran’s I statistics testing by using moran.test() of spdep.\n\n\nThe code chunk below performs Moran’s I statistical testing using moran.test() of spdep.\n\n\nCode Chunk\nmoran.test(hunan$GDPPC,\n            listw=rswm_q,\n            zero.policy = TRUE,\n            na.action = na.omit)\n\n\n\n\n\nThe code chunk below performs permutation test for Moran’s I statistic by using moran.mc() of spdep. A total of 1000 simulation will be performed.\n\n\nCode Chunk\nset.seed(1234)\nbperm = moran.mc(hunan$GDPPC,\n                 list1 = rswm_q,\n                 nsim = 999,\n                 zero.policy = TRUE,\n                 na.action = na.omit)\nbperm\n\n\n\n\n\nIt is always a good practice for us the examine the simulated Moran’s I test statistics in greater detail. This can be achieved by plotting the distribution of the statistical values as a histogram by using the code chunk below.\nIn the code chunk below hist() and abline() of R Graphics are used.\n\n\nCode Chunk\nmean(bperm$res[1:999])\n\n\n\n\nCode Chunk\nvar(bperm$res[1:999])\n\n\n\n\nCode Chunk\nsummary(bperm$res[1:999])\n\n\n\n\nCode Chunk\nhist(bperm$res,\n      freq=TRUE,\n      breaks = 20,\n      xlab = \"Simulated Moran's I\")\nabline(v=0,\n       col = \"red\")"
  },
  {
    "objectID": "HandsOnExercise/HandsOn5a/HandsOn5a.html#global-measures-of-spatial-autocorrelation-gearys-c",
    "href": "HandsOnExercise/HandsOn5a/HandsOn5a.html#global-measures-of-spatial-autocorrelation-gearys-c",
    "title": "Hands-On Exercise 5a",
    "section": "",
    "text": "In this section, you will learn how to perform Geary’s C statistics testing by using appropriate functions of spdep package.\n\n\nThe code chunk below performs Geary’s C test for spatial autocorrelation by using geary.test() of spdep.\n\n\nCode Chunk\ngeary.test(hunan$GDPPC, listw = rswm_q)\n\n\n\n\n\nThe code chunk below performs permutation test for Geary’s C statistic by using geary.mc() of spdep.\n\n\nCode Chunk\nset.seed(1234)\nbperm=geary.mc(hunan$GDPPC,\n                listw=rswm_q,\n                nsim=999)\n\nbperm\n\n\n\n\n\nNext, we will plot a histogram to reveal the distribution of the simulated values by using the code chunk below.\n\n\nCode Chunk\nmean(bperm$res[1:999])\n\n\n\n\nCode Chunk\nvar(bperm$res[1:999])\n\n\n\n\nCode Chunk\nsummary(bperm$res[1:999])\n\n\n\n\nCode Chunk\nhist(bperm$res, freq=TRUE, breaks=20, xlab=\"Simulated Geary c\")\nabline(v=1, col=\"red\")"
  },
  {
    "objectID": "HandsOnExercise/HandsOn5a/HandsOn5a.html#spatial-correlogram",
    "href": "HandsOnExercise/HandsOn5a/HandsOn5a.html#spatial-correlogram",
    "title": "Hands-On Exercise 5a",
    "section": "",
    "text": "Spatial correlograms are great to examine patterns of spatial autocorrelation in your data or model residuals. They show how correlated are pairs of spatial observations when you increase the distance (lag) between them - they are plots of some index of autocorrelation (Moran’s I or Geary’s c) against distance.Although correlograms are not as fundamental as variograms (a keystone concept of geostatistics), they are very useful as an exploratory and descriptive tool. For this purpose they actually provide richer information than variograms.\n\n\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Moran’s I. The plot() of base Graph is then used to plot the output.\n\n\nCode Chunk\nMI_corr &lt;- sp.correlogram(wm_q,\n                          hunan$GDPPC,\n                          order = 6,\n                          method = \"I\",\n                          style = \"W\")\nplot(MI_corr)\n\n\nBy plotting the output might not allow us to provide complete interpretation. This is because not all autocorrelation values are statistically significant. Hence, it is important for us to examine the full analysis report by printing out the analysis results as in the code chunk below.\n\n\nCode Chunk\nprint(MI_corr)\n\n\n\n\n\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Geary’s C. The plot() of base Graph is then used to plot the output.\n\n\nCode Chunk\nGC_corr &lt;- sp.correlogram(wm_q,\n                          hunan$GDPPC,\n                          order = 6,\n                          method = \"C\",\n                          style=\"W\")\nplot(GC_corr)\n\n\nSimilar to the previous step, we will print out the analysis report by using the code chunk below.\n\n\nCode Chunk\nprint(GC_corr)"
  },
  {
    "objectID": "HandsOnExercise/HandsOn5b/HandsOn5b.html",
    "href": "HandsOnExercise/HandsOn5b/HandsOn5b.html",
    "title": "Hands-On Exercise 5b",
    "section": "",
    "text": "Local Measures of Spatial Autocorrelation (LMSA) focus on the relationships between each observation and its surroundings, rather than providing a single summary of these relationships across the map. In this sense, they are not summary statistics but scores that allow us to learn more about the spatial structure in our data. The general intuition behind the metrics however is similar to that of global ones. Some of them are even mathematically connected, where the global version can be decomposed into a collection of local ones. One such example are Local Indicators of Spatial Association (LISA). Beside LISA, Getis-Ord’s Gi-statistics will be introduce as an alternative LMSA statistics that present complementary information or allow us to obtain similar insights for geographically referenced data.\nIn this hands-on exercise, you will learn how to compute Local Measures of Spatial Autocorrelation (LMSA) by using spdep package. By the end to this hands-on exercise, you will be able to:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute Local Indicator of Spatial Association (LISA) statistics for detecting clusters and outliers by using appropriate functions spdep package;\ncompute Getis-Ord’s Gi-statistics for detecting hot spot or/and cold spot area by using appropriate functions of spdep package; and\nto visualise the analysis output by using tmap package.\n\n\n\n\n\n\nIn spatial policy, one of the main development objective of the local govenment and planners is to ensure equal distribution of development in the province. Our task in this study, hence, is to apply appropriate spatial statistical methods to discover if development are even distributed geographically. If the answer is No. Then, our next question will be “is there sign of spatial clustering?”. And, if the answer for this question is yes, then our next question will be “where are these clusters?”\nIn this case study, we are interested to examine the spatial pattern of a selected development indicator (i.e. GDP per capita) of Hunan Provice, People Republic of China.(https://en.wikipedia.org/wiki/Hunan)\n\n\n\nTwo data sets will be used in this hands-on exercise, they are:\n\nHunan province administrative boundary layer at county level. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv: This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n\n\nBefore we get started, we need to ensure that spdep, sf, tmap and tidyverse packages of R are currently installed in your R.\n\nsf is use for importing and handling geospatial data in R,\ntidyverse is mainly use for wrangling attribute data in R,\nspdep will be used to compute spatial weights, global and local spatial autocorrelation statistics, and\ntmap will be used to prepare cartographic quality chropleth map.\n\nThe code chunk below is used to perform the following tasks:\n\ncreating a package list containing the necessary R packages,\nchecking if the R packages in the package list have been installed in R,\n\nif they have yet to be installed, RStudio will installed the missing packages,\n\nlaunching the packages into R environment.\n\n\n\nCode Chunk\npacman::p_load(sf, spdep, tmap, tidyverse)\n\n\n\n\n\n\nIn this section, you will learn how to bring a geospatial data and its associated attribute table into R environment. The geospatial data is in ESRI shapefile format and the attribute table is in csv fomat.\n\n\nThe code chunk below uses st_read() of sf package to import Hunan shapefile into R. The imported shapefile will be simple features Object of sf.\n\n\nCode Chunk\nhunan &lt;- st_read(dsn = \"data/geospatial\",\n                  layer = \"Hunan\")\n\n\n\n\n\nNext, we will import Hunan_2012.csv into R by using read_csv() of readr package. The output is R data frame class.\n\n\nCode Chunk\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n\n\n\nThe code chunk below will be used to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\n\n\nCode Chunk\nhunan &lt;- left_join(hunan, hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\n\n\n\n\nNow, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\n\nCode Chunk\nequal &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n           n=5,\n          style = \"equal\") +\n  tm_borders(alpha-0.5) +\n  tm_layout(main.title=\"Equal Interval Classication\")\n\nquantile &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n           n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha=0.5) +\n  tm_layout(main.title = \"Equal Quantitle Classification\")\n\ntmap_arrange(equal,\n             quantitle,\n             asp=1,\n             ncol=2)\n\n\n\n\n\n\nLocal Indicators of Spatial Association or LISA are statistics that evaluate the existence of clusters and/or outliers in the spatial arrangement of a given variable. For instance if we are studying distribution of GDP per capita of Hunan Provice, People Republic of China, local clusters in GDP per capita mean that there are counties that have higher or lower rates than is to be expected by chance alone; that is, the values occurring are above or below those of a random distribution in space.\nIn this section, you will learn how to apply appropriate Local Indicators for Spatial Association (LISA), especially local Moran’I to detect cluster and/or outlier from GDP per capita 2012 of Hunan Province, PRC.\n\n\nBefore we can compute the local spatial autocorrelation statistics, we need to construct a spatial weights of the study area. The spatial weights is used to define the neighbourhood relationships between the geographical units (i.e. county) in the study area.\nIn the code chunk below, poly2nb() of spdep package is used to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries. If you look at the documentation you will see that you can pass a “queen” argument that takes TRUE or FALSE as options. If you do not specify this argument the default is set to TRUE, that is, if you don’t specify queen = FALSE this function will return a list of first order neighbours using the Queen criteria.\nMore specifically, the code chunk below is used to compute Queen contiguity weight matrix.\n\n\nCode Chunk\nwm_q &lt;- poly2nb(hunan,\n                  queen=TRUE)\n\nsummary(wm_q)\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one neighbours.\n\n\n\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors’ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we’ll stick with the style=“W” option for simplicity’s sake but note that other more robust options are available, notably style=“B”.\n\n\nCode Chunk\nrswm_q &lt;- nb2listw(wm_q,\n                   style=\"W\",\n                   zero.policy = TRUE)\nrswm_q\n\n\nLearning Code Chunk Below:\n\nThe input of nb2listw() must be an object of class nb. The syntax of the function has two major arguments, namely style and zero.poly.\nstyle can take values “W”, “B”, “C”, “U”, “minmax” and “S”. B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).\nIf zero policy is set to TRUE, weights vectors of zero length are inserted for regions without neighbour in the neighbours list. These will in turn generate lag values of zero, equivalent to the sum of products of the zero row t(rep(0, length=length(neighbours))) %*% x, for arbitrary numerical vector x of length length(neighbours). The spatially lagged value of x for the zero-neighbour region will then be zero, which may (or may not) be a sensible choice.\n\n\n\n\nTo compute local Moran’s I, the localmoran() function of spdep will be used. It computes Ii values, given a set of zi values and a listw object providing neighbour weighting information for the polygon associated with the zi values.\nThe code chunks below are used to compute local Moran’s I of GDPPC2012 at the county level.\n\n\nCode Chunk\nfips &lt;- order(hunan$County)\nlocalMI &lt;- localmoran(hunan$GDPPC, rswm_q)\nhead(localMI)\n\n\nlocalmoran() function returns a matrix of values whose columns are:\n\nIi: the local Moran’s I statistics\nE.Ii: the expectation of local moran statistic under the randomisation hypothesis\nVar.Ii: the variance of local moran statistic under the randomisation hypothesis\nZ.Ii:the standard deviate of local moran statistic\nPr(): the p-value of local moran statistic\n\nThe code chunk below list the content of the local Moran matrix derived by using printCoefmat().\n\n\nCode Chunk\nprintCoefmat(data.frame(\n  localMI[fips,], \n  row.names=hunan$County[fips]),\n  check.names=FALSE)\n\n\n\n\nBefore mapping the local Moran’s I map, it is wise to append the local Moran’s I dataframe (i.e. localMI) onto hunan SpatialPolygonDataFrame. The code chunks below can be used to perform the task. The out SpatialPolygonDataFrame is called hunan.localMI.\n\n\nCode Chunk\nhunan.localMI &lt;- cbind(hunan,localMI) %&gt;%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\n\n\n\n\nUsing choropleth mapping functions of tmap package, we can plot the local Moran’s I values by using the code chinks below.\n\n\nCode Chunk\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nThe choropleth shows there is evidence for both positive and negative Ii values. However, it is useful to consider the p-values for each of these values, as consider above.\nThe code chunks below produce a choropleth map of Moran’s I p-values by using functions of tmap package.\n\n\nCode Chunk\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\",\n          breaks=c(-Inf,0.001, 0.01, 0.05, 0.1, Inf),\n          palette = \"-Blues\",\n          title = \"Local Moran's I p-values\") +\n  tm_borders(alpha =0.5)\n\n\n\n\n\nFor effective interpretation, it is better to plot both the local Moran’s I values map and its corresponding p-values map next to each other.\nThe code chunk below will be used to create such visualisation.\n\n\nCode Chunk\nlocalMI.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\", \n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\npvalue.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)\n\n\n\n\n\n\n\nThe LISA Cluster Map shows the significant locations color coded by type of spatial autocorrelation. The first step before we can generate the LISA cluster map is to plot the Moran scatterplot.\n\n\nThe Moran scatterplot is an illustration of the relationship between the values of the chosen attribute at each location and the average value of the same attribute at neighboring locations.\nThe code chunk below plots the Moran scatterplot of GDPPC 2012 by using moran.plot() of spdep.\n\n\nCode Chunk\nnci &lt;- moran.plot(hunan$GDPPC, rswm_q,\n                    labels = as.character(hunan$County),\n                    xlab-\"GDPPC 2012\",\n                    ylab = \"Spatially Lag GDPPC 2012\")\n\n\nNotice that the plot is split in 4 quadrants. The top right corner belongs to areas that have high GDPPC and are surrounded by other areas that have the average level of GDPPC. This are the high-high locations in the lesson slide.\n\n\n\nFirst we will use scale() to centers and scales the variable. Here centering is done by subtracting the mean (omitting NAs) the corresponding columns, and scaling is done by dividing the (centered) variable by their standard deviations.\n\n\nCode Chunk\nhunan$Z.GDPPC &lt;- scale(hunan$GDPPC) %&gt;%\n  as.vector\n\n\nThe as.vector() added to the end is to make sure that the data type we get out of this is a vector, that map neatly into out dataframe.\nNow, we are ready to plot the Moran scatterplot again by using the code chunk below.\n\n\nCode Chunk\nnci2 &lt;- moran.plot(hunan$Z.GDPPC, rswm_q,\n                   labels=as.character(hunan$County),\n                   xlab=\"z-GDPPC 2012\",\n                   ylab = \"Spatially Lag z-GDPPC 2012\")\n\n\n\n\n\nThe code chunks below show the steps to prepare a LISA cluster map.\n\n\nCode Chunk\nquadrant &lt;- vector(mode=\"numeric\", length = nrow(localMI))\n\n\nNext, derives the spatially lagged variable of interest (i.e. GDPPC) and centers the spatially lagged variable around its mean.\n\n\nCode Chunk\nhunan$lag_GDPPC &lt;- lag.listw(rswm_q, hunan$GDPPC)\nDV &lt;- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)\n\n\nThis is follow by centering the local Moran’s around the mean.\n\n\nCode Chunk\nLM_I &lt;- localMI[,1] - mean(localMI[,1])\n\n\nNext, we will set a statistical significance level for the local Moran.\n\n\nCode Chunk\nsignif &lt;- 0.05\n\n\nThese four command lines define the low-low (1), low-high (2), high-low (3) and high-high (4) categories.\n\n\nCode Chunk\nquadrant[DV &lt;0 & LM_I&gt;0] &lt;- 1\nquadrant[DV &gt;0 & LM_I&lt;0] &lt;- 2\nquadrant[DV &lt;0 & LM_I&lt;0] &lt;- 3  \nquadrant[DV &gt;0 & LM_I&gt;0] &lt;- 4\n\n\nLastly, places non-significant Moran in the category 0.\n\n\nCode Chunk\nquadrant[localMI[,5]&gt;signif] &lt;- 0\n\n\nIn fact, we can combined all the steps into one single code chunk as shown below:\n\n\nCode Chunk\nquadrant &lt;- vector(mode=\"numeric\",length=nrow(localMI))\nhunan$lag_GDPPC &lt;- lag.listw(rswm_q, hunan$GDPPC)\nDV &lt;- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)     \nLM_I &lt;- localMI[,1]   \nsignif &lt;- 0.05       \nquadrant[DV &lt;0 & LM_I&gt;0] &lt;- 1\nquadrant[DV &gt;0 & LM_I&lt;0] &lt;- 2\nquadrant[DV &lt;0 & LM_I&lt;0] &lt;- 3  \nquadrant[DV &gt;0 & LM_I&gt;0] &lt;- 4    \nquadrant[localMI[,5]&gt;signif] &lt;- 0\n\n\n\n\n\nNow, we can build the LISA map by using the code chunks below.\n\n\nCode Chunk\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\n\nFor effective interpretation, it is better to plot both the local Moran’s I values map and its corresponding p-values map next to each other.\nThe code chunk below will be used to create such visualisation.\n\n\nCode Chunk\ngdppc &lt;- qtm(hunan, \"GDPPC\")\n\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nLISAmap &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\ntmap_arrange(gdppc, LISAmap, \n             asp=1, ncol=2)\n\n\n\n\n\n\nBeside detecting cluster and outliers, localised spatial statistics can be also used to detect hot spot and/or cold spot areas.\nThe term ‘hot spot’ has been used generically across disciplines to describe a region or value that is higher relative to its surroundings (Lepers et al 2005, Aben et al 2012, Isobe et al 2015).\n\n\nAn alternative spatial statistics to detect spatial anomalies is the Getis and Ord’s G-statistics (Getis and Ord, 1972; Ord and Getis, 1995). It looks at neighbours within a defined proximity to identify where either high or low values clutser spatially. Here, statistically significant hot-spots are recognised as areas of high values where other areas within a neighbourhood range also share high values too.\nThe analysis consists of three steps:\n\nDeriving spatial weight matrix\nComputing Gi statistics\nMapping Gi statistics\n\n\n\n\nFirst, we need to define a new set of neighbours. Whist the spatial autocorrelation considered units which shared borders, for Getis-Ord we are defining neighbours based on distance.\nThere are two type of distance-based proximity matrix, they are:\n\nfixed distance weight matrix; and\nadaptive distance weight matrix.\n\n\n\nWe will need points to associate with each polygon before we can make our connectivity graph. It will be a little more complicated than just running st_centroid() on the sf object: us.bound. We need the coordinates in a separate data frame for this to work. To do this we will use a mapping function. The mapping function applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column of us.bound. Our function will be st_centroid(). We will be using map_dbl variation of map from the purrr package. For more documentation, check out map documentation\nTo get our longitude values we map the st_centroid() function over the geometry column of us.bound and access the longitude value through double bracket notation [[]] and 1. This allows us to get only the longitude, which is the first value in each centroid.\n\n\nCode Chunk\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[1])\n\n\nWe do the same for latitude with one key difference. We access the second value per each centroid with [[2]].\n\n\nCode Chunk\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\n\nNow that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.\n\n\nCode Chunk\ncoords &lt;- cbind(longitude, latitude)\n\n\n\n\n\nFirstly, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\n\nCode Chunk\n#coords &lt;- coordinates(hunan)\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n\n\nNow, we will compute the distance weight matrix by using dnearneigh() as shown in the code chunk below.\n\n\nCode Chunk\nwm_d62 &lt;- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object\n\n\nCode Chunk\nwm62_lw &lt;- nb2listw(wm_d62, style = 'B')\nsummary(wm62_lw)\n\n\nThe output spatial weights object is called wm62_lw.\n\n\n\n\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\n\n\nCode Chunk\nknn &lt;- knn2nb(knearneigh(coords, k=8))\nknn\n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object.\n\n\nCode Chunk\nknn_lw &lt;- nb2listw(knn, style = 'B')\nsummary(knn_lw)\n\n\n\n\n\n\n\n\n\n\nCode Chunk\nfips &lt;- order(hunan$County)\ngi.fixed &lt;- localG(hunan$GDPPC, wm62_lw)\ngi.fixed\n\n\nThe output of localG() is a vector of G or Gstar values, with attributes “gstari” set to TRUE or FALSE, “call” set to the function call, and class “localG”.\nThe Gi statistics is represented as a Z-score. Greater values represent a greater intensity of clustering and the direction (positive or negative) indicates high or low clusters.\nNext, we will join the Gi values to their corresponding hunan sf data frame by using the code chunk below.\n\n\nCode Chunk\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.fixed)) %&gt;%\n  rename(gstat_fixed = as.matrix.gi.fixed.)\n\n\nIn fact, the code chunk above performs three tasks. First, it convert the output vector (i.e. gi.fixed) into r matrix object by using as.matrix(). Next, cbind() is used to join hunan@data and gi.fixed matrix to produce a new SpatialPolygonDataFrame called hunan.gi. Lastly, the field name of the gi values is renamed to gstat_fixed by using rename().\n\n\n\nThe code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\n\nCode Chunk\ngdppc &lt;- qtm(hunan, \"GDPPC\")\n\nGimap &lt;-tm_shape(hunan.gi) +\n  tm_fill(col = \"gstat_fixed\", \n          style = \"pretty\",\n          palette=\"-RdBu\",\n          title = \"local Gi\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, Gimap, asp=1, ncol=2)\n\n\n\n\n\nThe code chunk below are used to compute the Gi values for GDPPC2012 by using an adaptive distance weight matrix (i.e knb_lw).\n\n\nCode Chunk\nfips &lt;- order(hunan$County)\ngi.adaptive &lt;- localG(hunan$GDPPC, knn_lw)\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.adaptive)) %&gt;%\n  rename(gstat_adaptive = as.matrix.gi.adaptive.)\n\n\n\n\n\nIt is time for us to visualise the locations of hot spot and cold spot areas. The choropleth mapping functions of tmap package will be used to map the Gi values.\nThe code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\n\nCode Chunk\ngdppc&lt;- qtm(hunan, \"GDPPC\")\n\nGimap &lt;- tm_shape(hunan.gi) + \n  tm_fill(col = \"gstat_adaptive\", \n          style = \"pretty\", \n          palette=\"-RdBu\", \n          title = \"local Gi\") + \n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, \n             Gimap, \n             asp=1, \n             ncol=2)"
  },
  {
    "objectID": "HandsOnExercise/HandsOn5b/HandsOn5b.html#overview",
    "href": "HandsOnExercise/HandsOn5b/HandsOn5b.html#overview",
    "title": "Hands-On Exercise 5b",
    "section": "",
    "text": "Local Measures of Spatial Autocorrelation (LMSA) focus on the relationships between each observation and its surroundings, rather than providing a single summary of these relationships across the map. In this sense, they are not summary statistics but scores that allow us to learn more about the spatial structure in our data. The general intuition behind the metrics however is similar to that of global ones. Some of them are even mathematically connected, where the global version can be decomposed into a collection of local ones. One such example are Local Indicators of Spatial Association (LISA). Beside LISA, Getis-Ord’s Gi-statistics will be introduce as an alternative LMSA statistics that present complementary information or allow us to obtain similar insights for geographically referenced data.\nIn this hands-on exercise, you will learn how to compute Local Measures of Spatial Autocorrelation (LMSA) by using spdep package. By the end to this hands-on exercise, you will be able to:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute Local Indicator of Spatial Association (LISA) statistics for detecting clusters and outliers by using appropriate functions spdep package;\ncompute Getis-Ord’s Gi-statistics for detecting hot spot or/and cold spot area by using appropriate functions of spdep package; and\nto visualise the analysis output by using tmap package."
  },
  {
    "objectID": "HandsOnExercise/HandsOn5b/HandsOn5b.html#getting-started",
    "href": "HandsOnExercise/HandsOn5b/HandsOn5b.html#getting-started",
    "title": "Hands-On Exercise 5b",
    "section": "",
    "text": "In spatial policy, one of the main development objective of the local govenment and planners is to ensure equal distribution of development in the province. Our task in this study, hence, is to apply appropriate spatial statistical methods to discover if development are even distributed geographically. If the answer is No. Then, our next question will be “is there sign of spatial clustering?”. And, if the answer for this question is yes, then our next question will be “where are these clusters?”\nIn this case study, we are interested to examine the spatial pattern of a selected development indicator (i.e. GDP per capita) of Hunan Provice, People Republic of China.(https://en.wikipedia.org/wiki/Hunan)\n\n\n\nTwo data sets will be used in this hands-on exercise, they are:\n\nHunan province administrative boundary layer at county level. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv: This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n\n\nBefore we get started, we need to ensure that spdep, sf, tmap and tidyverse packages of R are currently installed in your R.\n\nsf is use for importing and handling geospatial data in R,\ntidyverse is mainly use for wrangling attribute data in R,\nspdep will be used to compute spatial weights, global and local spatial autocorrelation statistics, and\ntmap will be used to prepare cartographic quality chropleth map.\n\nThe code chunk below is used to perform the following tasks:\n\ncreating a package list containing the necessary R packages,\nchecking if the R packages in the package list have been installed in R,\n\nif they have yet to be installed, RStudio will installed the missing packages,\n\nlaunching the packages into R environment.\n\n\n\nCode Chunk\npacman::p_load(sf, spdep, tmap, tidyverse)"
  },
  {
    "objectID": "HandsOnExercise/HandsOn5b/HandsOn5b.html#getting-the-data-into-r-environment",
    "href": "HandsOnExercise/HandsOn5b/HandsOn5b.html#getting-the-data-into-r-environment",
    "title": "Hands-On Exercise 5b",
    "section": "",
    "text": "In this section, you will learn how to bring a geospatial data and its associated attribute table into R environment. The geospatial data is in ESRI shapefile format and the attribute table is in csv fomat.\n\n\nThe code chunk below uses st_read() of sf package to import Hunan shapefile into R. The imported shapefile will be simple features Object of sf.\n\n\nCode Chunk\nhunan &lt;- st_read(dsn = \"data/geospatial\",\n                  layer = \"Hunan\")\n\n\n\n\n\nNext, we will import Hunan_2012.csv into R by using read_csv() of readr package. The output is R data frame class.\n\n\nCode Chunk\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n\n\n\nThe code chunk below will be used to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\n\n\nCode Chunk\nhunan &lt;- left_join(hunan, hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\n\n\n\n\nNow, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\n\nCode Chunk\nequal &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n           n=5,\n          style = \"equal\") +\n  tm_borders(alpha-0.5) +\n  tm_layout(main.title=\"Equal Interval Classication\")\n\nquantile &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n           n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha=0.5) +\n  tm_layout(main.title = \"Equal Quantitle Classification\")\n\ntmap_arrange(equal,\n             quantitle,\n             asp=1,\n             ncol=2)"
  },
  {
    "objectID": "HandsOnExercise/HandsOn5b/HandsOn5b.html#local-indicators-of-spatial-associationlisa",
    "href": "HandsOnExercise/HandsOn5b/HandsOn5b.html#local-indicators-of-spatial-associationlisa",
    "title": "Hands-On Exercise 5b",
    "section": "",
    "text": "Local Indicators of Spatial Association or LISA are statistics that evaluate the existence of clusters and/or outliers in the spatial arrangement of a given variable. For instance if we are studying distribution of GDP per capita of Hunan Provice, People Republic of China, local clusters in GDP per capita mean that there are counties that have higher or lower rates than is to be expected by chance alone; that is, the values occurring are above or below those of a random distribution in space.\nIn this section, you will learn how to apply appropriate Local Indicators for Spatial Association (LISA), especially local Moran’I to detect cluster and/or outlier from GDP per capita 2012 of Hunan Province, PRC.\n\n\nBefore we can compute the local spatial autocorrelation statistics, we need to construct a spatial weights of the study area. The spatial weights is used to define the neighbourhood relationships between the geographical units (i.e. county) in the study area.\nIn the code chunk below, poly2nb() of spdep package is used to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries. If you look at the documentation you will see that you can pass a “queen” argument that takes TRUE or FALSE as options. If you do not specify this argument the default is set to TRUE, that is, if you don’t specify queen = FALSE this function will return a list of first order neighbours using the Queen criteria.\nMore specifically, the code chunk below is used to compute Queen contiguity weight matrix.\n\n\nCode Chunk\nwm_q &lt;- poly2nb(hunan,\n                  queen=TRUE)\n\nsummary(wm_q)\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one neighbours.\n\n\n\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors’ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we’ll stick with the style=“W” option for simplicity’s sake but note that other more robust options are available, notably style=“B”.\n\n\nCode Chunk\nrswm_q &lt;- nb2listw(wm_q,\n                   style=\"W\",\n                   zero.policy = TRUE)\nrswm_q\n\n\nLearning Code Chunk Below:\n\nThe input of nb2listw() must be an object of class nb. The syntax of the function has two major arguments, namely style and zero.poly.\nstyle can take values “W”, “B”, “C”, “U”, “minmax” and “S”. B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).\nIf zero policy is set to TRUE, weights vectors of zero length are inserted for regions without neighbour in the neighbours list. These will in turn generate lag values of zero, equivalent to the sum of products of the zero row t(rep(0, length=length(neighbours))) %*% x, for arbitrary numerical vector x of length length(neighbours). The spatially lagged value of x for the zero-neighbour region will then be zero, which may (or may not) be a sensible choice.\n\n\n\n\nTo compute local Moran’s I, the localmoran() function of spdep will be used. It computes Ii values, given a set of zi values and a listw object providing neighbour weighting information for the polygon associated with the zi values.\nThe code chunks below are used to compute local Moran’s I of GDPPC2012 at the county level.\n\n\nCode Chunk\nfips &lt;- order(hunan$County)\nlocalMI &lt;- localmoran(hunan$GDPPC, rswm_q)\nhead(localMI)\n\n\nlocalmoran() function returns a matrix of values whose columns are:\n\nIi: the local Moran’s I statistics\nE.Ii: the expectation of local moran statistic under the randomisation hypothesis\nVar.Ii: the variance of local moran statistic under the randomisation hypothesis\nZ.Ii:the standard deviate of local moran statistic\nPr(): the p-value of local moran statistic\n\nThe code chunk below list the content of the local Moran matrix derived by using printCoefmat().\n\n\nCode Chunk\nprintCoefmat(data.frame(\n  localMI[fips,], \n  row.names=hunan$County[fips]),\n  check.names=FALSE)\n\n\n\n\nBefore mapping the local Moran’s I map, it is wise to append the local Moran’s I dataframe (i.e. localMI) onto hunan SpatialPolygonDataFrame. The code chunks below can be used to perform the task. The out SpatialPolygonDataFrame is called hunan.localMI.\n\n\nCode Chunk\nhunan.localMI &lt;- cbind(hunan,localMI) %&gt;%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\n\n\n\n\nUsing choropleth mapping functions of tmap package, we can plot the local Moran’s I values by using the code chinks below.\n\n\nCode Chunk\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nThe choropleth shows there is evidence for both positive and negative Ii values. However, it is useful to consider the p-values for each of these values, as consider above.\nThe code chunks below produce a choropleth map of Moran’s I p-values by using functions of tmap package.\n\n\nCode Chunk\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\",\n          breaks=c(-Inf,0.001, 0.01, 0.05, 0.1, Inf),\n          palette = \"-Blues\",\n          title = \"Local Moran's I p-values\") +\n  tm_borders(alpha =0.5)\n\n\n\n\n\nFor effective interpretation, it is better to plot both the local Moran’s I values map and its corresponding p-values map next to each other.\nThe code chunk below will be used to create such visualisation.\n\n\nCode Chunk\nlocalMI.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\", \n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\npvalue.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)"
  },
  {
    "objectID": "HandsOnExercise/HandsOn5b/HandsOn5b.html#creating-a-lisa-cluster-map",
    "href": "HandsOnExercise/HandsOn5b/HandsOn5b.html#creating-a-lisa-cluster-map",
    "title": "Hands-On Exercise 5b",
    "section": "",
    "text": "The LISA Cluster Map shows the significant locations color coded by type of spatial autocorrelation. The first step before we can generate the LISA cluster map is to plot the Moran scatterplot.\n\n\nThe Moran scatterplot is an illustration of the relationship between the values of the chosen attribute at each location and the average value of the same attribute at neighboring locations.\nThe code chunk below plots the Moran scatterplot of GDPPC 2012 by using moran.plot() of spdep.\n\n\nCode Chunk\nnci &lt;- moran.plot(hunan$GDPPC, rswm_q,\n                    labels = as.character(hunan$County),\n                    xlab-\"GDPPC 2012\",\n                    ylab = \"Spatially Lag GDPPC 2012\")\n\n\nNotice that the plot is split in 4 quadrants. The top right corner belongs to areas that have high GDPPC and are surrounded by other areas that have the average level of GDPPC. This are the high-high locations in the lesson slide.\n\n\n\nFirst we will use scale() to centers and scales the variable. Here centering is done by subtracting the mean (omitting NAs) the corresponding columns, and scaling is done by dividing the (centered) variable by their standard deviations.\n\n\nCode Chunk\nhunan$Z.GDPPC &lt;- scale(hunan$GDPPC) %&gt;%\n  as.vector\n\n\nThe as.vector() added to the end is to make sure that the data type we get out of this is a vector, that map neatly into out dataframe.\nNow, we are ready to plot the Moran scatterplot again by using the code chunk below.\n\n\nCode Chunk\nnci2 &lt;- moran.plot(hunan$Z.GDPPC, rswm_q,\n                   labels=as.character(hunan$County),\n                   xlab=\"z-GDPPC 2012\",\n                   ylab = \"Spatially Lag z-GDPPC 2012\")\n\n\n\n\n\nThe code chunks below show the steps to prepare a LISA cluster map.\n\n\nCode Chunk\nquadrant &lt;- vector(mode=\"numeric\", length = nrow(localMI))\n\n\nNext, derives the spatially lagged variable of interest (i.e. GDPPC) and centers the spatially lagged variable around its mean.\n\n\nCode Chunk\nhunan$lag_GDPPC &lt;- lag.listw(rswm_q, hunan$GDPPC)\nDV &lt;- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)\n\n\nThis is follow by centering the local Moran’s around the mean.\n\n\nCode Chunk\nLM_I &lt;- localMI[,1] - mean(localMI[,1])\n\n\nNext, we will set a statistical significance level for the local Moran.\n\n\nCode Chunk\nsignif &lt;- 0.05\n\n\nThese four command lines define the low-low (1), low-high (2), high-low (3) and high-high (4) categories.\n\n\nCode Chunk\nquadrant[DV &lt;0 & LM_I&gt;0] &lt;- 1\nquadrant[DV &gt;0 & LM_I&lt;0] &lt;- 2\nquadrant[DV &lt;0 & LM_I&lt;0] &lt;- 3  \nquadrant[DV &gt;0 & LM_I&gt;0] &lt;- 4\n\n\nLastly, places non-significant Moran in the category 0.\n\n\nCode Chunk\nquadrant[localMI[,5]&gt;signif] &lt;- 0\n\n\nIn fact, we can combined all the steps into one single code chunk as shown below:\n\n\nCode Chunk\nquadrant &lt;- vector(mode=\"numeric\",length=nrow(localMI))\nhunan$lag_GDPPC &lt;- lag.listw(rswm_q, hunan$GDPPC)\nDV &lt;- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)     \nLM_I &lt;- localMI[,1]   \nsignif &lt;- 0.05       \nquadrant[DV &lt;0 & LM_I&gt;0] &lt;- 1\nquadrant[DV &gt;0 & LM_I&lt;0] &lt;- 2\nquadrant[DV &lt;0 & LM_I&lt;0] &lt;- 3  \nquadrant[DV &gt;0 & LM_I&gt;0] &lt;- 4    \nquadrant[localMI[,5]&gt;signif] &lt;- 0\n\n\n\n\n\nNow, we can build the LISA map by using the code chunks below.\n\n\nCode Chunk\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\n\nFor effective interpretation, it is better to plot both the local Moran’s I values map and its corresponding p-values map next to each other.\nThe code chunk below will be used to create such visualisation.\n\n\nCode Chunk\ngdppc &lt;- qtm(hunan, \"GDPPC\")\n\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nLISAmap &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\ntmap_arrange(gdppc, LISAmap, \n             asp=1, ncol=2)"
  },
  {
    "objectID": "HandsOnExercise/HandsOn5b/HandsOn5b.html#hot-spot-and-cold-spot-area-analysis",
    "href": "HandsOnExercise/HandsOn5b/HandsOn5b.html#hot-spot-and-cold-spot-area-analysis",
    "title": "Hands-On Exercise 5b",
    "section": "",
    "text": "Beside detecting cluster and outliers, localised spatial statistics can be also used to detect hot spot and/or cold spot areas.\nThe term ‘hot spot’ has been used generically across disciplines to describe a region or value that is higher relative to its surroundings (Lepers et al 2005, Aben et al 2012, Isobe et al 2015).\n\n\nAn alternative spatial statistics to detect spatial anomalies is the Getis and Ord’s G-statistics (Getis and Ord, 1972; Ord and Getis, 1995). It looks at neighbours within a defined proximity to identify where either high or low values clutser spatially. Here, statistically significant hot-spots are recognised as areas of high values where other areas within a neighbourhood range also share high values too.\nThe analysis consists of three steps:\n\nDeriving spatial weight matrix\nComputing Gi statistics\nMapping Gi statistics\n\n\n\n\nFirst, we need to define a new set of neighbours. Whist the spatial autocorrelation considered units which shared borders, for Getis-Ord we are defining neighbours based on distance.\nThere are two type of distance-based proximity matrix, they are:\n\nfixed distance weight matrix; and\nadaptive distance weight matrix.\n\n\n\nWe will need points to associate with each polygon before we can make our connectivity graph. It will be a little more complicated than just running st_centroid() on the sf object: us.bound. We need the coordinates in a separate data frame for this to work. To do this we will use a mapping function. The mapping function applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column of us.bound. Our function will be st_centroid(). We will be using map_dbl variation of map from the purrr package. For more documentation, check out map documentation\nTo get our longitude values we map the st_centroid() function over the geometry column of us.bound and access the longitude value through double bracket notation [[]] and 1. This allows us to get only the longitude, which is the first value in each centroid.\n\n\nCode Chunk\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[1])\n\n\nWe do the same for latitude with one key difference. We access the second value per each centroid with [[2]].\n\n\nCode Chunk\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\n\nNow that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.\n\n\nCode Chunk\ncoords &lt;- cbind(longitude, latitude)\n\n\n\n\n\nFirstly, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\n\nCode Chunk\n#coords &lt;- coordinates(hunan)\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n\n\nNow, we will compute the distance weight matrix by using dnearneigh() as shown in the code chunk below.\n\n\nCode Chunk\nwm_d62 &lt;- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object\n\n\nCode Chunk\nwm62_lw &lt;- nb2listw(wm_d62, style = 'B')\nsummary(wm62_lw)\n\n\nThe output spatial weights object is called wm62_lw.\n\n\n\n\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\n\n\nCode Chunk\nknn &lt;- knn2nb(knearneigh(coords, k=8))\nknn\n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object.\n\n\nCode Chunk\nknn_lw &lt;- nb2listw(knn, style = 'B')\nsummary(knn_lw)"
  },
  {
    "objectID": "HandsOnExercise/HandsOn5b/HandsOn5b.html#computing-gi-statistics",
    "href": "HandsOnExercise/HandsOn5b/HandsOn5b.html#computing-gi-statistics",
    "title": "Hands-On Exercise 5b",
    "section": "",
    "text": "Code Chunk\nfips &lt;- order(hunan$County)\ngi.fixed &lt;- localG(hunan$GDPPC, wm62_lw)\ngi.fixed\n\n\nThe output of localG() is a vector of G or Gstar values, with attributes “gstari” set to TRUE or FALSE, “call” set to the function call, and class “localG”.\nThe Gi statistics is represented as a Z-score. Greater values represent a greater intensity of clustering and the direction (positive or negative) indicates high or low clusters.\nNext, we will join the Gi values to their corresponding hunan sf data frame by using the code chunk below.\n\n\nCode Chunk\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.fixed)) %&gt;%\n  rename(gstat_fixed = as.matrix.gi.fixed.)\n\n\nIn fact, the code chunk above performs three tasks. First, it convert the output vector (i.e. gi.fixed) into r matrix object by using as.matrix(). Next, cbind() is used to join hunan@data and gi.fixed matrix to produce a new SpatialPolygonDataFrame called hunan.gi. Lastly, the field name of the gi values is renamed to gstat_fixed by using rename().\n\n\n\nThe code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\n\nCode Chunk\ngdppc &lt;- qtm(hunan, \"GDPPC\")\n\nGimap &lt;-tm_shape(hunan.gi) +\n  tm_fill(col = \"gstat_fixed\", \n          style = \"pretty\",\n          palette=\"-RdBu\",\n          title = \"local Gi\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, Gimap, asp=1, ncol=2)\n\n\n\n\n\nThe code chunk below are used to compute the Gi values for GDPPC2012 by using an adaptive distance weight matrix (i.e knb_lw).\n\n\nCode Chunk\nfips &lt;- order(hunan$County)\ngi.adaptive &lt;- localG(hunan$GDPPC, knn_lw)\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.adaptive)) %&gt;%\n  rename(gstat_adaptive = as.matrix.gi.adaptive.)\n\n\n\n\n\nIt is time for us to visualise the locations of hot spot and cold spot areas. The choropleth mapping functions of tmap package will be used to map the Gi values.\nThe code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\n\nCode Chunk\ngdppc&lt;- qtm(hunan, \"GDPPC\")\n\nGimap &lt;- tm_shape(hunan.gi) + \n  tm_fill(col = \"gstat_adaptive\", \n          style = \"pretty\", \n          palette=\"-RdBu\", \n          title = \"local Gi\") + \n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, \n             Gimap, \n             asp=1, \n             ncol=2)"
  },
  {
    "objectID": "TakeHomeExercise/TakeHome2/TakeHome2.html",
    "href": "TakeHomeExercise/TakeHome2/TakeHome2.html",
    "title": "Take-Home Exercise 2",
    "section": "",
    "text": "Tourism is one of Thailand’s largest industries, accounting for some 20% of the gross domestic product (GDP). In 2019, Thailand earned 90 billion US$ from domestic and international tourism, but the COVID-19 pandemic caused revenues to crash to 24 billion US$ in 2020.\nFigure below shows the total revenue receipt from tourism sector from January 2019 until Feb 2023. The figure reveals that the revenue from tourism industry have been recovered gradually since September 2021.\n\n\n\nTo discover:\n\nif the key indicators of tourism economy of Thailand are independent from space and space and time.\nIf the tourism economy is indeed spatial and spatio-temporal dependent, then, you would like to detect where are the clusters and outliers, and the emerging hot spot/cold spot areas.\n\n\n\n\n\n\nCode Chunk\npacman::p_load(tidyverse, sf, spatstat, ggplot2, ggmap, tmap, dplyr, lubridate, raster, gtsummary, tidyr, spdep, knitr, plotly, units)\n\n\n\n\n\nPackages\nFunction\n\n\n\n\nsf\nTo import, manage, and hande geospatial data\n\n\ntidyverse\nFor non-spatial data wrangling\n\n\nsfdep\nTo compute spatial weights, global and local spatial autocorrelation statistics\n\n\nspatstat\nFor analysing spatial points\n\n\nggplot2\nFor data divisualisation\n\n\nggmap\nRetrieve raster map tiles from online mapping services\n\n\ntmap\nCreating thematic maps\n\n\nlubridate\nFor robust date-time usage\n\n\nleaflet\nFor interactive maps\n\n\nknitr\nFor dynamic report generation\n\n\nspdep\n\n\n\n\n\n\n\n\n\nIn this section, st_read() of sf package is used to import ‘tha_adm_rtsd_itos_20210121_shp’ dataset into R enironment. st_transform() is used to transform the coordinate reference system (CRS) as we are assigning the boundary data for Thailand and the CRS is EPSG 32647.\n\n\nCode Chunk\nthaiboundary &lt;- st_read(dsn = \"data/geospatial/tha_adm_rtsd_itos_20210121_shp/\",\n                      layer = \"tha_admbnda_adm1_rtsd_20220121\") %&gt;%\n  st_transform(crs = 32647)\n\n\nReading layer `tha_admbnda_adm1_rtsd_20220121' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/TakeHomeExercise/TakeHome2/data/geospatial/tha_adm_rtsd_itos_20210121_shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\n\n\nCode Chunk\nthaiboundary &lt;- thaiboundary %&gt;%\n  dplyr::select(-ADM1_TH, -ADM1_PCODE, -ADM1_REF, -ADM1ALT1EN, \n                -ADM1ALT1TH, -ADM1ALT2TH, -ADM0_EN, -ADM0_TH,\n                -ADM0_PCODE, -validTo, -validOn, -date, -ADM1ALT2EN)\n\n\nIn ensuring the right assignment of the coordinate reference system to the data, st_crs() is used.\n\n\nCode Chunk\nst_crs(thaiboundary)\n\n\nCoordinate Reference System:\n  User input: EPSG:32647 \n  wkt:\nPROJCRS[\"WGS 84 / UTM zone 47N\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"UTM zone 47N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",99,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"Between 96°E and 102°E, northern hemisphere between equator and 84°N, onshore and offshore. China. Indonesia. Laos. Malaysia - West Malaysia. Mongolia. Myanmar (Burma). Russian Federation. Thailand.\"],\n        BBOX[0,96,84,102]],\n    ID[\"EPSG\",32647]]\n\n\n\n\nCode Chunk\nglimpse(thaiboundary)\n\n\nRows: 77\nColumns: 4\n$ Shape_Leng &lt;dbl&gt; 2.417227, 1.695100, 1.251111, 1.884945, 3.041716, 1.739908,…\n$ Shape_Area &lt;dbl&gt; 0.13133873, 0.07926199, 0.05323766, 0.12698345, 0.21393797,…\n$ ADM1_EN    &lt;chr&gt; \"Bangkok\", \"Samut Prakan\", \"Nonthaburi\", \"Pathum Thani\", \"P…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((674339.8 15..., MULTIPOLYGON (…\n\n\nAfter importing the dataset, we will plot the Thailand map using tmap.\n\n\nCode Chunk\ntmap_mode(\"plot\")\ntmap_options(check.and.fix = TRUE)\ntm_shape(thaiboundary) +\n  tm_fill(col = \"white\") +\n  tm_borders(col = \"black\", lwd = 0.3, alpha = 0.6) +\n  tm_layout(\n    main.title = \"Thailand Administrative Boundary\",\n    main.title.size = 0.8,\n    main.title.color = \"blue\",\n    main.title.position = \"center\",\n    legend.show = TRUE,\n    frame = FALSE\n  )\n\n\n\n\n\n\n\n\n\nCode Chunk\ntmap_mode(\"view\")\n\n\n\n\nCode Chunk\ntm_shape(thaiboundary) +\n  tm_polygons()\n\n\n\n\n\n\nCode Chunk\nview(thaiboundary)\n\n\n\n\n\n\n\nCode Chunk\nrevenue &lt;- read_csv(\"data/aspatial/thailand_domestic_tourism_2019_2023_ver2.csv\")\n\n\nThe data is then transposed. The “date” column is also converted to POSIXct for ease of manipulation at the later stage.\n\n\nCode Chunk\nrevenue_transposed &lt;- revenue %&gt;%\n  mutate(date = as.POSIXct(date)) %&gt;%\n  dplyr::select(-province_thai, -region_thai) %&gt;%\n  pivot_wider(\n    names_from = variable,  # Use unique values in 'events' as column names\n    values_from = value,  # Fill new columns with values from 'value'\n    values_fill = list(value = 0)  # Fill NA with 0\n  )\n\n\n\n\n[1] \"date\"          \"province_thai\" \"province_eng\"  \"region_thai\"  \n[5] \"region_eng\"    \"variable\"      \"value\"        \n\n\n\n\n\n\n\n\nCOVID-19 Pandemic in Thailand\n\n\n\nIn rooting the understanding of the COVID-19 situation in Thailand, it is important to read the Wiki page specifically under the section of Government Response.\n\n\nThe time period of pre-COVID19 was set from 1st January 2019 to 2nd April 2020 as quoted from Wiki “On 3 April, the Civil Aviation Authority of Thailand ordered that there would be a ban on all passengers flights landing in Thailand from the morning of 4 April to the evening of the 6 April..” under the section of Government Response - International Travel Restrictions.\nFollowing the closure of Thailand borders on 4th April 2020, the first series of border opening was 1st July 2021 as evidenced by “On 15 July 2021, it became possible for vaccinated holidaymakers from overseas to visit three islands in the Surat Thani province; Koh Samui, Koh Tao and Koh Phangan. The decision follows the reopening of Phuket, Thailand’s largest island, on Thursday, July 1”. Thus, the timeframe of COVID19 was set from 4th April 2020 - 30th June 2021.\nThe Government of Thailand only lifted the restrictions such as PCR testing for fully-vaccinated travellers on 1st May 2022 as seen “Starting 1 May 2022, fully-vaccinated travellers will NOT require RT-PCR on arrival but still need Thailand Pass. ATK is voluntary only.”\nThus the 2nd part of COVID19 pandemic in Thailand is 1st July 2021 to 30th April 2022. 1st May 2022 marks the reopening of the country borders hence, for this post COVID19 data is set from 1st May 2022 to 31st December 2024.\n\n\nCode Chunk\nrevenue_transposed &lt;- revenue_transposed %&gt;%\n  mutate(\n    Period = case_when(\n      date &gt;= as.Date(\"2019-01-01\") & date &lt;= as.Date(\"2020-04-03\") ~ \"Pre-COVID\",\n      date &gt;= as.Date(\"2020-04-04\") & date &lt;= as.Date(\"2021-06-30\") ~ \"COVID_P1\",\n      date &gt;= as.Date(\"2021-07-01\") & date &lt;= as.Date(\"2022-04-30\") ~ \"COVID_P2\",\n      date &gt;= as.Date(\"2022-05-01\") & date &lt;= as.Date(\"2023-12-31\") ~ \"Post-COVID\",\n      TRUE ~ NA_character_  # For any dates outside these ranges\n    )\n  )\n\n\n\n\n\n\n\nPrior combining the SF & DF, we would want to compare the unique values (namely the provinces) in the columns \"ADM1_EN\" from the thaiboundary spatial data frame and \"province_eng\" from the revenue_transposed data frame. The unique() function helps to identify any differences between the two columns.\n\n\nCode Chunk\n# Get unique values from both columns\nunique_thaiboundary &lt;- unique(thaiboundary$ADM1_EN)\nunique_revenue &lt;- unique(revenue_transposed$province_eng)\n\n# Find differences\ndiff_thaiboundary &lt;- setdiff(unique_thaiboundary, unique_revenue)  # In thaiboundary but not in revenue\ndiff_revenue &lt;- setdiff(unique_revenue, unique_thaiboundary)      # In revenue but not in thaiboundary\n\n# Create a data frame to show differences\ndifference_table &lt;- data.frame(\n  thaiboundary_only = diff_thaiboundary,\n  revenue_only = diff_revenue\n)\n\n# Print the differences\nprint(difference_table)\n\n\n  thaiboundary_only    revenue_only\n1          Lop Buri         Lopburi\n2          Chai Nat         Chainat\n3         Chon Buri        Chonburi\n4      Prachin Buri     Prachinburi\n5          Buri Ram       Phang Nga\n6         Si Sa Ket         Buriram\n7  Nong Bua Lam Phu         Sisaket\n8          Phangnga Nong Bua Lamphu\n\n\nNoticeably, we are able to witness the differences between both columns in the above output, this may result in missing values when combining both dataframes which is critical to our analysis. The differences are mainly due to the presence of white spaces.\nHenceforth, in the below code, we update the values in the \"ADM1_EN\" column of the thaiboundary spatial data frame by creating a mapping of the old values to the new values and then use the dplyr package to perform the replacement.\n\n\nCode Chunk\n# Create a data frame for the mapping\nmapping &lt;- data.frame(\n  old_value = c(\"Lop Buri\", \"Chai Nat\", \"Chon Buri\", \"Prachin Buri\", \"Buri Ram\", \n                \"Si Sa Ket\", \"Nong Bua Lam Phu\", \"Phangnga\"),\n  new_value = c(\"Lopburi\", \"Chainat\", \"Chonburi\", \"Prachinburi\", \"Buriram\", \n                \"Sisaket\", \"Nong Bua Lamphu\", \"Phang Nga\"),\n  stringsAsFactors = FALSE\n)\n\n# Create a named vector for recoding\nrecode_vector &lt;- setNames(mapping$new_value, mapping$old_value)\n\n# Update the ADM1_EN column using recode_vector\nthaiboundary &lt;- thaiboundary %&gt;%\n  mutate(ADM1_EN = recode(ADM1_EN, !!!recode_vector))\n\n\nThereafter, we perform a check again if there are still differences between columns \"ADM1_EN\" from the thaiboundary spatial data frame and \"province_eng\" from the revenue data frame. The results returned none which is reassuring to witness.\n\n\nCode Chunk\n# Get unique values from each column\nunique_adm1_en &lt;- unique(thaiboundary$ADM1_EN)\nunique_province_eng &lt;- unique(revenue$province_eng)\n\n# Compare unique values\ndifferences_adm1_en &lt;- setdiff(unique_adm1_en, unique_province_eng)\ndifferences_province_eng &lt;- setdiff(unique_province_eng, unique_adm1_en)\n\n# Create a data frame to show differences side by side\ncomparison_table_check &lt;- tibble(\n  ADM1_EN_Not_In_Province = differences_adm1_en,\n  Province_Eng_Not_In_ADM1 = differences_province_eng\n)\n\n# Display the comparison table\nprint(comparison_table_check)\n\n\n# A tibble: 0 × 2\n# ℹ 2 variables: ADM1_EN_Not_In_Province &lt;chr&gt;, Province_Eng_Not_In_ADM1 &lt;chr&gt;\n\n\n\n\n\n\n\nCode Chunk\nthaiboundary &lt;- thaiboundary %&gt;%\n  rename(Province = ADM1_EN)\n\n\n\n\nCode Chunk\nrevenue_transposed &lt;- revenue_transposed %&gt;%\n  rename(Province = province_eng)\n\n\n\n\n\nUnwanted columns also then removed for ease of wrangling.\n\n\nCode Chunk\ncombined &lt;- thaiboundary %&gt;%\n  left_join(revenue_transposed, by = \"Province\")\n\n\nThe below code was run to simplify the gemetries through adjusting the tolerance.\n\n\nCode Chunk\ncombined$geometry &lt;- st_simplify(combined$geometry, dTolerance = 0.01)\n\n\n\n\nCode Chunk\n# Set the tmap mode to plot for static output\ntmap_mode('view')\n\n# Define the unique periods\nperiods &lt;- c(\"Pre-COVID\", \"COVID_P1\", \"COVID_P2\", \"Post-COVID\")\n\n# Loop through each period and create a map\nfor (period in periods) {\n  # Subset data for the current period\n  current_data &lt;- combined[combined$Period == period, ]\n\n  # Check if there's any data for the current period\n  if (nrow(current_data) == 0) {\n    next  # Skip to the next iteration if no data is found\n  }\n\n  # Create the basemap\n  basemap &lt;- tm_shape(current_data) +\n    tm_polygons() +\n    tm_text(\"Province\", size = 0.5) +\n    tm_layout(main.title = paste(\"Period:\", period))\n\n  # Create the GDP map\n  revenue &lt;- tm_shape(current_data) +\n    tm_polygons(\"revenue_all\") +\n    tm_layout(main.title = paste(\"Revenue in\", period))\n\n  # Combine the basemap and GDP map\n  combined_map &lt;- tmap_arrange(basemap, revenue, asp = 1, ncol = 2)\n\nprint(combined_map)\n}\n\n\n\n\n\n\n\n\nCode Chunk\ncolnames(combined)\n\n\n [1] \"Shape_Leng\"         \"Shape_Area\"         \"Province\"          \n [4] \"date\"               \"region_eng\"         \"ratio_tourist_stay\"\n [7] \"no_tourist_stay\"    \"no_tourist_all\"     \"no_tourist_thai\"   \n[10] \"no_tourist_foreign\" \"revenue_all\"        \"revenue_thai\"      \n[13] \"revenue_foreign\"    \"Period\"             \"geometry\"          \n\n\n\n\n\n\n\nThe code chunk below uses st_area() of sf package to derive the area of each province. We are creating a new column Area to store the area values.\n\n\nCode Chunk\ncombined$Area &lt;- combined %&gt;%\n  st_area()\n\n\nA bar chart of each COVID period is generated alongside the percentage of the total revenue the province contributed.\n\n\n\n\n\n\nPercentage of the Barchart Not Drawn to Scale\n\n\n\n\n\n\nRefined code\n\n\n\n\n\n\nCode Chunk\ncombined_no_geometry &lt;- combined %&gt;%\n  st_drop_geometry()\n\n# Summarise revenue data for Pre-COVID period only\nrevenue_summary &lt;- combined_no_geometry %&gt;%\n  filter(Period == \"Pre-COVID\") %&gt;%\n  group_by(Province) %&gt;%\n  summarise(total_revenue = sum(revenue_all, na.rm = TRUE)) %&gt;%\n  ungroup()\n\n# Create the pie chart for Pre-COVID period\ncreate_pie_chart &lt;- function(data) {\n  period_data &lt;- data %&gt;%\n    arrange(desc(total_revenue)) %&gt;%\n    mutate(Province = if_else(row_number() &lt;= 10, Province, \"Other\")) %&gt;%\n    group_by(Province) %&gt;%\n    summarise(total_revenue = sum(total_revenue)) %&gt;%\n    ungroup()\n  \n  total_period_revenue &lt;- sum(period_data$total_revenue)\n  \n  period_data &lt;- period_data %&gt;%\n    mutate(percentage = (total_revenue / total_period_revenue) * 100)\n  \n  plot_ly(period_data, labels = ~Province, values = ~percentage, type = 'pie',\n          textposition = 'inside',\n          textinfo = 'label+percent',\n          hoverinfo = 'text',\n          text = ~paste(Province, \n                        \"&lt;br&gt;Revenue: $\", formatC(total_revenue, format=\"f\", big.mark=\",\", digits=0),\n                        \"&lt;br&gt;Percentage: \", formatC(percentage, format=\"f\", digits=1), \"%\"),\n          marker = list(line = list(color = '#FFFFFF', width = 1))) %&gt;%\n    layout(title = list(text =  \"Revenue Distribution by Province - Pre-COVID: 1st Jan 2019 - 3rd March 2020\",\n                        font = list(size = 14)),\n           showlegend = FALSE)\n}\n\n# Create and display the Pre-COVID pie chart\npre_covid_chart &lt;- create_pie_chart(revenue_summary)\npre_covid_chart\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\ncombined_no_geometry &lt;- combined %&gt;%\n  st_drop_geometry()\n\n# Summarise revenue data for Pre-COVID period only\nrevenue_summary &lt;- combined_no_geometry %&gt;%\n  filter(Period == \"COVID_P1\") %&gt;%\n  group_by(Province) %&gt;%\n  summarise(total_revenue = sum(revenue_all, na.rm = TRUE)) %&gt;%\n  ungroup()\n\n# Create the pie chart for Pre-COVID period\ncreate_pie_chart &lt;- function(data) {\n  period_data &lt;- data %&gt;%\n    arrange(desc(total_revenue)) %&gt;%\n    mutate(Province = if_else(row_number() &lt;= 10, Province, \"Other\")) %&gt;%\n    group_by(Province) %&gt;%\n    summarise(total_revenue = sum(total_revenue)) %&gt;%\n    ungroup()\n  \n  total_period_revenue &lt;- sum(period_data$total_revenue)\n  \n  period_data &lt;- period_data %&gt;%\n    mutate(percentage = (total_revenue / total_period_revenue) * 100)\n  \n  plot_ly(period_data, labels = ~Province, values = ~percentage, type = 'pie',\n          textposition = 'inside',\n          textinfo = 'label+percent',\n          hoverinfo = 'text',\n          text = ~paste(Province, \n                        \"&lt;br&gt;Revenue: $\", formatC(total_revenue, format=\"f\", big.mark=\",\", digits=0),\n                        \"&lt;br&gt;Percentage: \", formatC(percentage, format=\"f\", digits=1), \"%\"),\n          marker = list(line = list(color = '#FFFFFF', width = 1))) %&gt;%\n    layout(title = list(text = \"Revenue Distribution by Province - COVID PART I: 4th April 2020 - 30th June 2021\",\n                        font = list(size=14)),\n           showlegend = FALSE)\n}\n\n# Create and display the Pre-COVID pie chart\ncovidp1_chart &lt;- create_pie_chart(revenue_summary)\ncovidp1_chart\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\ncombined_no_geometry &lt;- combined %&gt;%\n  st_drop_geometry()\n\n# Summarise revenue data for Pre-COVID period only\nrevenue_summary &lt;- combined_no_geometry %&gt;%\n  filter(Period == \"COVID_P2\") %&gt;%\n  group_by(Province) %&gt;%\n  summarise(total_revenue = sum(revenue_all, na.rm = TRUE)) %&gt;%\n  ungroup()\n\n# Create the pie chart for Pre-COVID period\ncreate_pie_chart &lt;- function(data) {\n  period_data &lt;- data %&gt;%\n    arrange(desc(total_revenue)) %&gt;%\n    mutate(Province = if_else(row_number() &lt;= 10, Province, \"Other\")) %&gt;%\n    group_by(Province) %&gt;%\n    summarise(total_revenue = sum(total_revenue)) %&gt;%\n    ungroup()\n  \n  total_period_revenue &lt;- sum(period_data$total_revenue)\n  \n  period_data &lt;- period_data %&gt;%\n    mutate(percentage = (total_revenue / total_period_revenue) * 100)\n  \n  plot_ly(period_data, labels = ~Province, values = ~percentage, type = 'pie',\n          textposition = 'inside',\n          textinfo = 'label+percent',\n          hoverinfo = 'text',\n          text = ~paste(Province, \n                        \"&lt;br&gt;Revenue: $\", formatC(total_revenue, format=\"f\", big.mark=\",\", digits=0),\n                        \"&lt;br&gt;Percentage: \", formatC(percentage, format=\"f\", digits=1), \"%\"),\n          marker = list(line = list(color = '#FFFFFF', width = 1))) %&gt;%\n    layout(title = list(text =  \"Revenue Distribution by Province - COVID PART II: 1st July 2021 - 30th April 2022\",\n                        font = list(size = 14)),\n           showlegend = FALSE\n           )\n}\n\n# Create and display the Pre-COVID pie chart\ncovidp2_chart &lt;- create_pie_chart(revenue_summary)\ncovidp2_chart\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\ncombined_no_geometry &lt;- combined %&gt;%\n  st_drop_geometry()\n\n# Summarise revenue data for Pre-COVID period only\nrevenue_summary &lt;- combined_no_geometry %&gt;%\n  filter(Period == \"Post-COVID\") %&gt;%\n  group_by(Province) %&gt;%\n  summarise(total_revenue = sum(revenue_all, na.rm = TRUE)) %&gt;%\n  ungroup()\n\n# Create the pie chart for Pre-COVID period\ncreate_pie_chart &lt;- function(data) {\n  period_data &lt;- data %&gt;%\n    arrange(desc(total_revenue)) %&gt;%\n    mutate(Province = if_else(row_number() &lt;= 10, Province, \"Other\")) %&gt;%\n    group_by(Province) %&gt;%\n    summarise(total_revenue = sum(total_revenue)) %&gt;%\n    ungroup()\n  \n  total_period_revenue &lt;- sum(period_data$total_revenue)\n  \n  period_data &lt;- period_data %&gt;%\n    mutate(percentage = (total_revenue / total_period_revenue) * 100)\n  \n  plot_ly(period_data, labels = ~Province, values = ~percentage, type = 'pie',\n          textposition = 'inside',\n          textinfo = 'label+percent',\n          hoverinfo = 'text',\n          text = ~paste(Province, \n                        \"&lt;br&gt;Revenue: $\", formatC(total_revenue, format=\"f\", big.mark=\",\", digits=0),\n                        \"&lt;br&gt;Percentage: \", formatC(percentage, format=\"f\", digits=1), \"%\"),\n          marker = list(line = list(color = '#FFFFFF', width = 1))) %&gt;%\n    layout(title = \"Revenue Distribution by Province - Post-COVID: 1st May 2022 onwards\",size=0.5, \n           showlegend = FALSE)\n}\n\n# Create and display the Pre-COVID pie chart\npost_covid_chart &lt;- create_pie_chart(revenue_summary)\npost_covid_chart\n\n\n\n\n\n\nPre-COVID: 1st Jan 2019 - 3rd March 2020\n38.3% of the total revenue was generated from Bangkok while Phuket generated 16.91%. Choburi generated 9.88% while Krabi was 4.24%. Chiangmai was 4.14%.\nCOVID Part 1: 4th April 2020 - 30th June 2021\n30.71% of the total revenue was generated from Bangkok while Chiangmai generated 8.8%. Chonburi generated 5.52%. Phuket generated 3.21%.\nCOVID Part 2: 1st July 2021 - 30th April 2022\nCOVID P2, 22.08% of the total revenue during the period was generated from Bangkok while 12.67% comes from Phuket. Chiang Mai & Chonburi generated 8.59% and 9.55% of the total revenue in the total period.\nPost-COVID: 1st May 2022 onwards\nUnderstandably, Bangkok generated 34.58% while Phuket generated 19.62%. Chonburi generated 9.27% and Chiangmai generated 4.94%. Krabi only generated 1.25%\nInteresting Observations:\n\nKrabi was the top 5 provinces of the total revenue of tourism in Pre-COVID. However, post COVID it wasn’t in top 5 and merely generated 1.25% only. This is may due to the “slow international flight resumption at Krabi airport, especially from China, India and South Korea” as taken from The Bangkok Post, November 2023.\n\n\n\nCode Chunk\n# input: the dataframe and the variable name, chart style, title \nchoropleth_plot &lt;- function(rev, varname, style, title) {\n  tm_shape(rev) +\n    tm_fill(varname, \n          n= 5,\n          style = style) +\n    tm_borders(alpha = 0.5) +\n    tm_layout(main.title = title,\n              main.title.size = 1,\n              main.title.position = \"center\",\n              legend.height = 0.45, \n              legend.width = 0.35,\n              frame = TRUE)+ \n    tm_compass(position = c('left','bottom'))\n}\ntmap_mode(\"plot\")\n\n\n\n\n\n\n\n\n\n\n\nIn the code chunk below, poly2nb() of spdep package is used to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries. By default, Queen contiguity is applied.\n\n\nCode Chunk\nwm_q &lt;- poly2nb(thaiboundary, queen=TRUE)\nsummary(wm_q)\n\n\nNeighbour list object:\nNumber of regions: 77 \nNumber of nonzero links: 352 \nPercentage nonzero weights: 5.93692 \nAverage number of links: 4.571429 \n1 region with no links:\n67\n2 disjoint connected subgraphs\nLink number distribution:\n\n 0  1  2  3  4  5  6  7  8  9 \n 1  1  5 17 15 17 10  5  4  2 \n1 least connected region:\n14 with 1 link\n2 most connected regions:\n29 51 with 9 links\n\n\nThe summary report above shows that there are 77 area units in Thailand. The most connected area unit has 9 links.\n\n\n\nIn the code chunk below, poly2nb() of spdep package is used to compute contiguity weight matrices for the study area. We specify queen = FALSE to compute Rook contiguity.\n\n\nCode Chunk\nwm_q &lt;- poly2nb(thaiboundary, queen=FALSE)\nsummary(wm_q)\n\n\nNeighbour list object:\nNumber of regions: 77 \nNumber of nonzero links: 352 \nPercentage nonzero weights: 5.93692 \nAverage number of links: 4.571429 \n1 region with no links:\n67\n2 disjoint connected subgraphs\nLink number distribution:\n\n 0  1  2  3  4  5  6  7  8  9 \n 1  1  5 17 15 17 10  5  4  2 \n1 least connected region:\n14 with 1 link\n2 most connected regions:\n29 51 with 9 links\n\n\nThe summary report above shows that there are 77 area units in Thailand. The most connected area unit has 9 links.\nBoth of them possess the same number of link (4.57).\n\n\n\n\nWe will derive the distance-based weight matrices by using dnearneigh() of spdep package. The function identifies neighbours of region points by Euclidean distance with a distance band with lower and upper bounds controlled by the bounds argument or by Great Circle distance in kilometres if longlat argument is set to TRUE.\n\n\nDetermining the cut-off distance\nTo ensure that each region has at least one neighbour, we need to find out the minimum distance within which all regions have at least oneneighbour. We can do this by following these steps:\n\nGetting the coordinates of polygon centroids. This is required as an input in the next step.\nWe need to associate each polygon with a point and its coordinates need to be in a separate data frame. We will use a mapping function that applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column of thaiboundary. Our function will be st_centroid(). We will be using map_dbl() variation of map from the purrr package. purrr is loaded when we load tidyverse package.\nTo get our longitude values we map the st_centroid() function over the geometry column of thaiboundary and access the longitude value through double bracket notation [[]] and\n\nThis allows us to get only the longitude, which is the first value in each centroid.\n\n\n\n\nCode Chunk\nlongitude &lt;- map_dbl(thaiboundary$geometry, ~st_centroid(.x)[[1]])\n\n\nWe do the same for latitude with one key difference. We access the second value per each centroid with [[2]]\n\n\nCode Chunk\nlatitude &lt;- map_dbl(thaiboundary$geometry, ~st_centroid(.x)[[2]])\n\n\nNow that we have latitude and longitude, I used cbind to put longitude and latitude into the same object. We should check the first few observations to see if things are formatted correctly.\n\n\nCode Chunk\ncoords &lt;- cbind(longitude, latitude)\nhead(coords, 5)\n\n\n     longitude latitude\n[1,]  675514.6  1523087\n[2,]  685033.7  1503755\n[3,]  650477.2  1539777\n[4,]  681656.0  1555581\n[5,]  664627.1  1586462\n\n\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\n\nCode Chunk\nk1 &lt;- knn2nb(knearneigh(coords, k = 1))\nk1dists &lt;- unlist(nbdists(k1, coords))\nsummary(k1dists)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  21548   51966   64530   63281   76823  110987 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 110,987 metres, approximately 111, 000 metres. This will be used as the upper threshold as it gives certainty that all regions will have at least one neighbour.\nComputing the fixed distance weight matrix\nNow, we will compute the distance weight matrix by using dnearneigh() as shown below.\n\n\nCode Chunk\nwm_d111 &lt;- dnearneigh(coords,0,111000)\nwm_d111\n\n\nNeighbour list object:\nNumber of regions: 77 \nNumber of nonzero links: 350 \nPercentage nonzero weights: 5.903188 \nAverage number of links: 4.545455 \n2 disjoint connected subgraphs\n\n\nFrom the above code, we can see that the average number of links is 4.54.\nNext, we use str() to display the content of wm_d111 weight matrix.\n\n\nCode Chunk\nstr(wm_d111)\n\n\nList of 77\n $ : int [1:12] 2 3 4 5 6 10 11 15 17 59 ...\n $ : int [1:10] 1 3 4 5 11 15 17 59 60 61\n $ : int [1:13] 1 2 4 5 6 8 10 17 56 58 ...\n $ : int [1:14] 1 2 3 5 6 8 10 15 16 17 ...\n $ : int [1:13] 1 2 3 4 6 7 8 9 10 17 ...\n $ : int [1:11] 1 3 4 5 7 8 9 10 17 58 ...\n $ : int [1:7] 5 6 8 9 10 17 48\n $ : int [1:10] 3 4 5 6 7 9 10 48 49 58\n $ : int [1:7] 5 6 7 8 48 49 58\n $ : int [1:9] 1 3 4 5 6 7 8 16 17\n $ : int [1:6] 1 2 12 13 15 16\n $ : int [1:3] 11 13 15\n $ : int [1:5] 11 12 14 15 18\n $ : int 13\n $ : int [1:9] 1 2 4 11 12 13 16 17 18\n $ : int [1:6] 4 10 11 15 17 18\n $ : int [1:10] 1 2 3 4 5 6 7 10 15 16\n $ : int [1:3] 13 15 16\n $ : int 20\n $ : int [1:2] 19 21\n $ : int [1:2] 20 22\n $ : int [1:2] 21 23\n $ : int [1:2] 22 26\n $ : int [1:3] 26 34 38\n $ : int [1:2] 29 55\n $ : int [1:4] 23 24 34 38\n $ : int [1:2] 32 36\n $ : int [1:4] 29 30 31 32\n $ : int [1:3] 25 28 33\n $ : int [1:3] 28 32 36\n $ : int 28\n $ : int [1:3] 27 28 30\n $ : int [1:3] 29 34 35\n $ : int [1:5] 24 26 33 35 38\n $ : int [1:4] 33 34 36 38\n $ : int [1:4] 27 30 35 37\n $ : int [1:2] 36 38\n $ : int [1:5] 24 26 34 35 37\n $ : int [1:3] 40 41 47\n $ : int [1:2] 39 41\n $ : int [1:3] 39 40 43\n $ : int [1:3] 43 52 53\n $ : int [1:4] 41 42 44 52\n $ : int [1:2] 43 45\n $ : int [1:2] 44 46\n $ : int 45\n $ : int 39\n $ : int [1:6] 7 8 9 49 50 54\n $ : int [1:6] 8 9 48 50 57 58\n $ : int [1:5] 48 49 51 52 54\n $ : int 50\n $ : int [1:4] 42 43 50 53\n $ : int [1:4] 42 52 54 55\n $ : int [1:4] 48 50 53 55\n $ : int [1:3] 25 53 54\n $ : int [1:5] 3 59 60 61 62\n $ : int [1:2] 49 58\n $ : int [1:9] 3 4 5 6 8 9 49 57 59\n $ : int [1:10] 1 2 3 4 5 6 56 58 60 61\n $ : int [1:9] 1 2 3 4 5 56 59 61 62\n $ : int [1:8] 1 2 3 4 56 59 60 62\n $ : int [1:4] 56 60 61 63\n $ : int 62\n $ : int [1:4] 65 68 73 74\n $ : int [1:5] 64 66 67 68 73\n $ : int [1:3] 65 67 68\n $ : int [1:2] 65 66\n $ : int [1:4] 64 65 66 69\n $ : int [1:2] 68 70\n $ : int 69\n $ : int [1:3] 72 74 75\n $ : int [1:3] 71 73 74\n $ : int [1:4] 64 65 72 74\n $ : int [1:4] 64 71 72 73\n $ : int [1:3] 71 76 77\n $ : int [1:2] 75 77\n $ : int [1:2] 75 76\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:77] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language dnearneigh(x = coords, d1 = 0, d2 = 111000)\n - attr(*, \"dnn\")= num [1:2] 0 111000\n - attr(*, \"bounds\")= chr [1:2] \"GE\" \"LE\"\n - attr(*, \"nbtype\")= chr \"distance\"\n - attr(*, \"sym\")= logi TRUE\n\n\nWe can observe that each region has different number of neighbours.\n\n\nCode Chunk\npar(mfrow = c(1,2))\nplot(thaiboundary$geometry, border = \"lightgrey\",main=\"1st nearest neighbours\" )\nplot(k1, coords, add = TRUE, col = \"red\", length = 0.88, )\n\nplot(thaiboundary$geometry, border = \"lightgrey\", main = \"Distance Link\")\nplot(wm_d111, coords, add = TRUE, pch = 19, cex = 0.6)\n\n\n\n\n\n\n\n\n\nFrom the above output, we can make a few observations:\n\nIt illustrates a characteristic of fixed distance weight matrix–more densely settled areas (usually the urban areas) tend to have more neighbours which is in instance it is in the middle where Bangkok resides and the less densely settled areas (usually the rural provinces) tend to have lesser neighbours\nThe geographical areas of the regions in Thailand are largely varying. In the middle, bottom and top right, we see the neighbour links are dense especially in the middle and less dense in the rest of the regions where white spaces are\nLastly, it is interesting to note that there is a dead space between the densely settled area of Bangkok region and the southern part of Thailand. This may indicate that tourism was not developed at all.\n\n\n\n\nTo overcome the issue of fixed distance weight matrix where there is uneven distribution of neighbours, we can use directly control the numbers of neighbours using k-nearest neighbours, as shown in the code chunk below.\nAs a rule-of-thumb, we will set k = 8 i.e., all regions will have 8 neighbours.\n\n\nCode Chunk\nknn8 &lt;- knn2nb(knearneigh(coords, k=8))\nknn8\n\n\nNeighbour list object:\nNumber of regions: 77 \nNumber of nonzero links: 616 \nPercentage nonzero weights: 10.38961 \nAverage number of links: 8 \nNon-symmetric neighbours list\n\n\nPlotting Adaptive Distance-based Neighbours\n\n\nCode Chunk\npar(mfrow = c(1,2))\nplot(thaiboundary$geometry, border = \"lightgrey\",main=\"8 nearest neighbours\" )\nplot(knn8, coords, add = TRUE, col = \"blue\", length = 0.88, )\n\nplot(thaiboundary$geometry, border = \"lightgrey\", main = \"Distance Link w KNN\")\nplot(knn8, coords, add = TRUE, col = \"red\", pch = 19, cex = 0.6)\n\n\n\n\n\n\n\n\n\n\n\n\n\nSelecting a spatial weight matrix is use is dependent on the geographical area of interest and the focus of the study8.\nIn this study, between contiguity-based and distance-based spatial weight matrices, we lean towards distance-based matrices. Within distance-based matrices, we will select the adaptive distance-based spatial weight matrix for our subsequent analysis.\nThe reasons are summarised here:\n\nThailand has 77 provinces and it is relatively small . Hence, a contiguity-based matrix will have the issue where larger LGAs have more neighbours and smaller LGAs have lesser neighbours. This would likely skew our analysis. Therefore, distance-based methods are preferred.\nAs mentioned earlier, the fixed distance-based method has the disadvantage that some regions would only have 1 neighbour, while on average regions have 23 neighbours. Statistical test for regions with only 1 neighbour may not be valid.\n\nBased on the above, we will select adaptive distance-based spatial weight matrix.\n\n\n\n\n\nCode Chunk\nrswm_knn8 &lt;- nb2listw(knn8,\n                   style = \"W\",\n                   zero.policy = TRUE)\nrswm_knn8\n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 77 \nNumber of nonzero links: 616 \nPercentage nonzero weights: 10.38961 \nAverage number of links: 8 \nNon-symmetric neighbours list\n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 77 5929 77 17.03125 319.9688\n\n\nWe will be using the row-standardised weight matrix for the next part of the analysis.\n\n\n\nThis in sub-section, we will use two methods: Moran’s I and Geary’s C to test the hypothesis the following hypothesis:\n\nH0: Observed spatial patterns of values is equally likely as any other spatial pattern i.e. data is randomly disbursed, no spatial pattern\nH1: Data is more spatially clustered than expected by chance alone.\n\n\n\nWe will perform Moran’s I statistical testing by using moran.test() of spdep. Moran’s I describe how features differ from the values in the study area as a whole. The Moran I statistic ranges from -1 to 1. If the Moran I is:\n\npositive (I&gt;0): Clustered, observations tend to be similar\nnegative (I&lt;0): Disperse, observations tend to be dissimilar\napproximately zero: observations arranged randomly over space"
  },
  {
    "objectID": "TakeHomeExercise/TakeHome2/TakeHome2.html#setting-the-scene",
    "href": "TakeHomeExercise/TakeHome2/TakeHome2.html#setting-the-scene",
    "title": "Take-Home Exercise 2",
    "section": "",
    "text": "Tourism is one of Thailand’s largest industries, accounting for some 20% of the gross domestic product (GDP). In 2019, Thailand earned 90 billion US$ from domestic and international tourism, but the COVID-19 pandemic caused revenues to crash to 24 billion US$ in 2020.\nFigure below shows the total revenue receipt from tourism sector from January 2019 until Feb 2023. The figure reveals that the revenue from tourism industry have been recovered gradually since September 2021."
  },
  {
    "objectID": "TakeHomeExercise/TakeHome2/TakeHome2.html#objectives",
    "href": "TakeHomeExercise/TakeHome2/TakeHome2.html#objectives",
    "title": "Take-Home Exercise 2",
    "section": "",
    "text": "To discover:\n\nif the key indicators of tourism economy of Thailand are independent from space and space and time.\nIf the tourism economy is indeed spatial and spatio-temporal dependent, then, you would like to detect where are the clusters and outliers, and the emerging hot spot/cold spot areas."
  },
  {
    "objectID": "TakeHomeExercise/TakeHome2/TakeHome2.html#importing-packages",
    "href": "TakeHomeExercise/TakeHome2/TakeHome2.html#importing-packages",
    "title": "Take-Home Exercise 2",
    "section": "",
    "text": "Code Chunk\npacman::p_load(tidyverse, sf, spatstat, ggplot2, ggmap, tmap, dplyr, lubridate, raster, gtsummary, tidyr, spdep, knitr, plotly, units)\n\n\n\n\n\nPackages\nFunction\n\n\n\n\nsf\nTo import, manage, and hande geospatial data\n\n\ntidyverse\nFor non-spatial data wrangling\n\n\nsfdep\nTo compute spatial weights, global and local spatial autocorrelation statistics\n\n\nspatstat\nFor analysing spatial points\n\n\nggplot2\nFor data divisualisation\n\n\nggmap\nRetrieve raster map tiles from online mapping services\n\n\ntmap\nCreating thematic maps\n\n\nlubridate\nFor robust date-time usage\n\n\nleaflet\nFor interactive maps\n\n\nknitr\nFor dynamic report generation\n\n\nspdep"
  },
  {
    "objectID": "TakeHomeExercise/TakeHome2/TakeHome2.html#importing-packages-1",
    "href": "TakeHomeExercise/TakeHome2/TakeHome2.html#importing-packages-1",
    "title": "Take-Home Exercise 2",
    "section": "",
    "text": "In this section, st_read() of sf package is used to import ‘tha_adm_rtsd_itos_20210121_shp’ dataset into R enironment. st_transform() is used to transform the coordinate reference system (CRS) as we are assigning the boundary data for Thailand and the CRS is EPSG 32647.\n\n\nCode Chunk\nthaiboundary &lt;- st_read(dsn = \"data/geospatial/tha_adm_rtsd_itos_20210121_shp/\",\n                      layer = \"tha_admbnda_adm1_rtsd_20220121\") %&gt;%\n  st_transform(crs = 32647)\n\n\nReading layer `tha_admbnda_adm1_rtsd_20220121' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/TakeHomeExercise/TakeHome2/data/geospatial/tha_adm_rtsd_itos_20210121_shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\n\n\nCode Chunk\nthaiboundary &lt;- thaiboundary %&gt;%\n  dplyr::select(-ADM1_TH, -ADM1_PCODE, -ADM1_REF, -ADM1ALT1EN, \n                -ADM1ALT1TH, -ADM1ALT2TH, -ADM0_EN, -ADM0_TH,\n                -ADM0_PCODE, -validTo, -validOn, -date, -ADM1ALT2EN)\n\n\nIn ensuring the right assignment of the coordinate reference system to the data, st_crs() is used.\n\n\nCode Chunk\nst_crs(thaiboundary)\n\n\nCoordinate Reference System:\n  User input: EPSG:32647 \n  wkt:\nPROJCRS[\"WGS 84 / UTM zone 47N\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"UTM zone 47N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",99,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"Between 96°E and 102°E, northern hemisphere between equator and 84°N, onshore and offshore. China. Indonesia. Laos. Malaysia - West Malaysia. Mongolia. Myanmar (Burma). Russian Federation. Thailand.\"],\n        BBOX[0,96,84,102]],\n    ID[\"EPSG\",32647]]\n\n\n\n\nCode Chunk\nglimpse(thaiboundary)\n\n\nRows: 77\nColumns: 4\n$ Shape_Leng &lt;dbl&gt; 2.417227, 1.695100, 1.251111, 1.884945, 3.041716, 1.739908,…\n$ Shape_Area &lt;dbl&gt; 0.13133873, 0.07926199, 0.05323766, 0.12698345, 0.21393797,…\n$ ADM1_EN    &lt;chr&gt; \"Bangkok\", \"Samut Prakan\", \"Nonthaburi\", \"Pathum Thani\", \"P…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((674339.8 15..., MULTIPOLYGON (…\n\n\nAfter importing the dataset, we will plot the Thailand map using tmap.\n\n\nCode Chunk\ntmap_mode(\"plot\")\ntmap_options(check.and.fix = TRUE)\ntm_shape(thaiboundary) +\n  tm_fill(col = \"white\") +\n  tm_borders(col = \"black\", lwd = 0.3, alpha = 0.6) +\n  tm_layout(\n    main.title = \"Thailand Administrative Boundary\",\n    main.title.size = 0.8,\n    main.title.color = \"blue\",\n    main.title.position = \"center\",\n    legend.show = TRUE,\n    frame = FALSE\n  )\n\n\n\n\n\n\n\n\n\nCode Chunk\ntmap_mode(\"view\")\n\n\n\n\nCode Chunk\ntm_shape(thaiboundary) +\n  tm_polygons()\n\n\n\n\n\n\nCode Chunk\nview(thaiboundary)\n\n\n\n\n\n\n\nCode Chunk\nrevenue &lt;- read_csv(\"data/aspatial/thailand_domestic_tourism_2019_2023_ver2.csv\")\n\n\nThe data is then transposed. The “date” column is also converted to POSIXct for ease of manipulation at the later stage.\n\n\nCode Chunk\nrevenue_transposed &lt;- revenue %&gt;%\n  mutate(date = as.POSIXct(date)) %&gt;%\n  dplyr::select(-province_thai, -region_thai) %&gt;%\n  pivot_wider(\n    names_from = variable,  # Use unique values in 'events' as column names\n    values_from = value,  # Fill new columns with values from 'value'\n    values_fill = list(value = 0)  # Fill NA with 0\n  )\n\n\n\n\n[1] \"date\"          \"province_thai\" \"province_eng\"  \"region_thai\"  \n[5] \"region_eng\"    \"variable\"      \"value\"        \n\n\n\n\n\n\n\n\nCOVID-19 Pandemic in Thailand\n\n\n\nIn rooting the understanding of the COVID-19 situation in Thailand, it is important to read the Wiki page specifically under the section of Government Response.\n\n\nThe time period of pre-COVID19 was set from 1st January 2019 to 2nd April 2020 as quoted from Wiki “On 3 April, the Civil Aviation Authority of Thailand ordered that there would be a ban on all passengers flights landing in Thailand from the morning of 4 April to the evening of the 6 April..” under the section of Government Response - International Travel Restrictions.\nFollowing the closure of Thailand borders on 4th April 2020, the first series of border opening was 1st July 2021 as evidenced by “On 15 July 2021, it became possible for vaccinated holidaymakers from overseas to visit three islands in the Surat Thani province; Koh Samui, Koh Tao and Koh Phangan. The decision follows the reopening of Phuket, Thailand’s largest island, on Thursday, July 1”. Thus, the timeframe of COVID19 was set from 4th April 2020 - 30th June 2021.\nThe Government of Thailand only lifted the restrictions such as PCR testing for fully-vaccinated travellers on 1st May 2022 as seen “Starting 1 May 2022, fully-vaccinated travellers will NOT require RT-PCR on arrival but still need Thailand Pass. ATK is voluntary only.”\nThus the 2nd part of COVID19 pandemic in Thailand is 1st July 2021 to 30th April 2022. 1st May 2022 marks the reopening of the country borders hence, for this post COVID19 data is set from 1st May 2022 to 31st December 2024.\n\n\nCode Chunk\nrevenue_transposed &lt;- revenue_transposed %&gt;%\n  mutate(\n    Period = case_when(\n      date &gt;= as.Date(\"2019-01-01\") & date &lt;= as.Date(\"2020-04-03\") ~ \"Pre-COVID\",\n      date &gt;= as.Date(\"2020-04-04\") & date &lt;= as.Date(\"2021-06-30\") ~ \"COVID_P1\",\n      date &gt;= as.Date(\"2021-07-01\") & date &lt;= as.Date(\"2022-04-30\") ~ \"COVID_P2\",\n      date &gt;= as.Date(\"2022-05-01\") & date &lt;= as.Date(\"2023-12-31\") ~ \"Post-COVID\",\n      TRUE ~ NA_character_  # For any dates outside these ranges\n    )\n  )\n\n\n\n\n\n\n\nPrior combining the SF & DF, we would want to compare the unique values (namely the provinces) in the columns \"ADM1_EN\" from the thaiboundary spatial data frame and \"province_eng\" from the revenue_transposed data frame. The unique() function helps to identify any differences between the two columns.\n\n\nCode Chunk\n# Get unique values from both columns\nunique_thaiboundary &lt;- unique(thaiboundary$ADM1_EN)\nunique_revenue &lt;- unique(revenue_transposed$province_eng)\n\n# Find differences\ndiff_thaiboundary &lt;- setdiff(unique_thaiboundary, unique_revenue)  # In thaiboundary but not in revenue\ndiff_revenue &lt;- setdiff(unique_revenue, unique_thaiboundary)      # In revenue but not in thaiboundary\n\n# Create a data frame to show differences\ndifference_table &lt;- data.frame(\n  thaiboundary_only = diff_thaiboundary,\n  revenue_only = diff_revenue\n)\n\n# Print the differences\nprint(difference_table)\n\n\n  thaiboundary_only    revenue_only\n1          Lop Buri         Lopburi\n2          Chai Nat         Chainat\n3         Chon Buri        Chonburi\n4      Prachin Buri     Prachinburi\n5          Buri Ram       Phang Nga\n6         Si Sa Ket         Buriram\n7  Nong Bua Lam Phu         Sisaket\n8          Phangnga Nong Bua Lamphu\n\n\nNoticeably, we are able to witness the differences between both columns in the above output, this may result in missing values when combining both dataframes which is critical to our analysis. The differences are mainly due to the presence of white spaces.\nHenceforth, in the below code, we update the values in the \"ADM1_EN\" column of the thaiboundary spatial data frame by creating a mapping of the old values to the new values and then use the dplyr package to perform the replacement.\n\n\nCode Chunk\n# Create a data frame for the mapping\nmapping &lt;- data.frame(\n  old_value = c(\"Lop Buri\", \"Chai Nat\", \"Chon Buri\", \"Prachin Buri\", \"Buri Ram\", \n                \"Si Sa Ket\", \"Nong Bua Lam Phu\", \"Phangnga\"),\n  new_value = c(\"Lopburi\", \"Chainat\", \"Chonburi\", \"Prachinburi\", \"Buriram\", \n                \"Sisaket\", \"Nong Bua Lamphu\", \"Phang Nga\"),\n  stringsAsFactors = FALSE\n)\n\n# Create a named vector for recoding\nrecode_vector &lt;- setNames(mapping$new_value, mapping$old_value)\n\n# Update the ADM1_EN column using recode_vector\nthaiboundary &lt;- thaiboundary %&gt;%\n  mutate(ADM1_EN = recode(ADM1_EN, !!!recode_vector))\n\n\nThereafter, we perform a check again if there are still differences between columns \"ADM1_EN\" from the thaiboundary spatial data frame and \"province_eng\" from the revenue data frame. The results returned none which is reassuring to witness.\n\n\nCode Chunk\n# Get unique values from each column\nunique_adm1_en &lt;- unique(thaiboundary$ADM1_EN)\nunique_province_eng &lt;- unique(revenue$province_eng)\n\n# Compare unique values\ndifferences_adm1_en &lt;- setdiff(unique_adm1_en, unique_province_eng)\ndifferences_province_eng &lt;- setdiff(unique_province_eng, unique_adm1_en)\n\n# Create a data frame to show differences side by side\ncomparison_table_check &lt;- tibble(\n  ADM1_EN_Not_In_Province = differences_adm1_en,\n  Province_Eng_Not_In_ADM1 = differences_province_eng\n)\n\n# Display the comparison table\nprint(comparison_table_check)\n\n\n# A tibble: 0 × 2\n# ℹ 2 variables: ADM1_EN_Not_In_Province &lt;chr&gt;, Province_Eng_Not_In_ADM1 &lt;chr&gt;\n\n\n\n\n\n\n\nCode Chunk\nthaiboundary &lt;- thaiboundary %&gt;%\n  rename(Province = ADM1_EN)\n\n\n\n\nCode Chunk\nrevenue_transposed &lt;- revenue_transposed %&gt;%\n  rename(Province = province_eng)\n\n\n\n\n\nUnwanted columns also then removed for ease of wrangling.\n\n\nCode Chunk\ncombined &lt;- thaiboundary %&gt;%\n  left_join(revenue_transposed, by = \"Province\")\n\n\nThe below code was run to simplify the gemetries through adjusting the tolerance.\n\n\nCode Chunk\ncombined$geometry &lt;- st_simplify(combined$geometry, dTolerance = 0.01)\n\n\n\n\nCode Chunk\n# Set the tmap mode to plot for static output\ntmap_mode('view')\n\n# Define the unique periods\nperiods &lt;- c(\"Pre-COVID\", \"COVID_P1\", \"COVID_P2\", \"Post-COVID\")\n\n# Loop through each period and create a map\nfor (period in periods) {\n  # Subset data for the current period\n  current_data &lt;- combined[combined$Period == period, ]\n\n  # Check if there's any data for the current period\n  if (nrow(current_data) == 0) {\n    next  # Skip to the next iteration if no data is found\n  }\n\n  # Create the basemap\n  basemap &lt;- tm_shape(current_data) +\n    tm_polygons() +\n    tm_text(\"Province\", size = 0.5) +\n    tm_layout(main.title = paste(\"Period:\", period))\n\n  # Create the GDP map\n  revenue &lt;- tm_shape(current_data) +\n    tm_polygons(\"revenue_all\") +\n    tm_layout(main.title = paste(\"Revenue in\", period))\n\n  # Combine the basemap and GDP map\n  combined_map &lt;- tmap_arrange(basemap, revenue, asp = 1, ncol = 2)\n\nprint(combined_map)\n}\n\n\n\n\n\n\n\n\nCode Chunk\ncolnames(combined)\n\n\n [1] \"Shape_Leng\"         \"Shape_Area\"         \"Province\"          \n [4] \"date\"               \"region_eng\"         \"ratio_tourist_stay\"\n [7] \"no_tourist_stay\"    \"no_tourist_all\"     \"no_tourist_thai\"   \n[10] \"no_tourist_foreign\" \"revenue_all\"        \"revenue_thai\"      \n[13] \"revenue_foreign\"    \"Period\"             \"geometry\""
  },
  {
    "objectID": "TakeHomeExercise/TakeHome2/TakeHome2.html#exploratory-data-analysis",
    "href": "TakeHomeExercise/TakeHome2/TakeHome2.html#exploratory-data-analysis",
    "title": "Take-Home Exercise 2",
    "section": "",
    "text": "The code chunk below uses st_area() of sf package to derive the area of each province. We are creating a new column Area to store the area values.\n\n\nCode Chunk\ncombined$Area &lt;- combined %&gt;%\n  st_area()\n\n\nA bar chart of each COVID period is generated alongside the percentage of the total revenue the province contributed.\n\n\n\n\n\n\nPercentage of the Barchart Not Drawn to Scale\n\n\n\n\n\n\nRefined code\n\n\n\n\n\n\nCode Chunk\ncombined_no_geometry &lt;- combined %&gt;%\n  st_drop_geometry()\n\n# Summarise revenue data for Pre-COVID period only\nrevenue_summary &lt;- combined_no_geometry %&gt;%\n  filter(Period == \"Pre-COVID\") %&gt;%\n  group_by(Province) %&gt;%\n  summarise(total_revenue = sum(revenue_all, na.rm = TRUE)) %&gt;%\n  ungroup()\n\n# Create the pie chart for Pre-COVID period\ncreate_pie_chart &lt;- function(data) {\n  period_data &lt;- data %&gt;%\n    arrange(desc(total_revenue)) %&gt;%\n    mutate(Province = if_else(row_number() &lt;= 10, Province, \"Other\")) %&gt;%\n    group_by(Province) %&gt;%\n    summarise(total_revenue = sum(total_revenue)) %&gt;%\n    ungroup()\n  \n  total_period_revenue &lt;- sum(period_data$total_revenue)\n  \n  period_data &lt;- period_data %&gt;%\n    mutate(percentage = (total_revenue / total_period_revenue) * 100)\n  \n  plot_ly(period_data, labels = ~Province, values = ~percentage, type = 'pie',\n          textposition = 'inside',\n          textinfo = 'label+percent',\n          hoverinfo = 'text',\n          text = ~paste(Province, \n                        \"&lt;br&gt;Revenue: $\", formatC(total_revenue, format=\"f\", big.mark=\",\", digits=0),\n                        \"&lt;br&gt;Percentage: \", formatC(percentage, format=\"f\", digits=1), \"%\"),\n          marker = list(line = list(color = '#FFFFFF', width = 1))) %&gt;%\n    layout(title = list(text =  \"Revenue Distribution by Province - Pre-COVID: 1st Jan 2019 - 3rd March 2020\",\n                        font = list(size = 14)),\n           showlegend = FALSE)\n}\n\n# Create and display the Pre-COVID pie chart\npre_covid_chart &lt;- create_pie_chart(revenue_summary)\npre_covid_chart\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\ncombined_no_geometry &lt;- combined %&gt;%\n  st_drop_geometry()\n\n# Summarise revenue data for Pre-COVID period only\nrevenue_summary &lt;- combined_no_geometry %&gt;%\n  filter(Period == \"COVID_P1\") %&gt;%\n  group_by(Province) %&gt;%\n  summarise(total_revenue = sum(revenue_all, na.rm = TRUE)) %&gt;%\n  ungroup()\n\n# Create the pie chart for Pre-COVID period\ncreate_pie_chart &lt;- function(data) {\n  period_data &lt;- data %&gt;%\n    arrange(desc(total_revenue)) %&gt;%\n    mutate(Province = if_else(row_number() &lt;= 10, Province, \"Other\")) %&gt;%\n    group_by(Province) %&gt;%\n    summarise(total_revenue = sum(total_revenue)) %&gt;%\n    ungroup()\n  \n  total_period_revenue &lt;- sum(period_data$total_revenue)\n  \n  period_data &lt;- period_data %&gt;%\n    mutate(percentage = (total_revenue / total_period_revenue) * 100)\n  \n  plot_ly(period_data, labels = ~Province, values = ~percentage, type = 'pie',\n          textposition = 'inside',\n          textinfo = 'label+percent',\n          hoverinfo = 'text',\n          text = ~paste(Province, \n                        \"&lt;br&gt;Revenue: $\", formatC(total_revenue, format=\"f\", big.mark=\",\", digits=0),\n                        \"&lt;br&gt;Percentage: \", formatC(percentage, format=\"f\", digits=1), \"%\"),\n          marker = list(line = list(color = '#FFFFFF', width = 1))) %&gt;%\n    layout(title = list(text = \"Revenue Distribution by Province - COVID PART I: 4th April 2020 - 30th June 2021\",\n                        font = list(size=14)),\n           showlegend = FALSE)\n}\n\n# Create and display the Pre-COVID pie chart\ncovidp1_chart &lt;- create_pie_chart(revenue_summary)\ncovidp1_chart\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\ncombined_no_geometry &lt;- combined %&gt;%\n  st_drop_geometry()\n\n# Summarise revenue data for Pre-COVID period only\nrevenue_summary &lt;- combined_no_geometry %&gt;%\n  filter(Period == \"COVID_P2\") %&gt;%\n  group_by(Province) %&gt;%\n  summarise(total_revenue = sum(revenue_all, na.rm = TRUE)) %&gt;%\n  ungroup()\n\n# Create the pie chart for Pre-COVID period\ncreate_pie_chart &lt;- function(data) {\n  period_data &lt;- data %&gt;%\n    arrange(desc(total_revenue)) %&gt;%\n    mutate(Province = if_else(row_number() &lt;= 10, Province, \"Other\")) %&gt;%\n    group_by(Province) %&gt;%\n    summarise(total_revenue = sum(total_revenue)) %&gt;%\n    ungroup()\n  \n  total_period_revenue &lt;- sum(period_data$total_revenue)\n  \n  period_data &lt;- period_data %&gt;%\n    mutate(percentage = (total_revenue / total_period_revenue) * 100)\n  \n  plot_ly(period_data, labels = ~Province, values = ~percentage, type = 'pie',\n          textposition = 'inside',\n          textinfo = 'label+percent',\n          hoverinfo = 'text',\n          text = ~paste(Province, \n                        \"&lt;br&gt;Revenue: $\", formatC(total_revenue, format=\"f\", big.mark=\",\", digits=0),\n                        \"&lt;br&gt;Percentage: \", formatC(percentage, format=\"f\", digits=1), \"%\"),\n          marker = list(line = list(color = '#FFFFFF', width = 1))) %&gt;%\n    layout(title = list(text =  \"Revenue Distribution by Province - COVID PART II: 1st July 2021 - 30th April 2022\",\n                        font = list(size = 14)),\n           showlegend = FALSE\n           )\n}\n\n# Create and display the Pre-COVID pie chart\ncovidp2_chart &lt;- create_pie_chart(revenue_summary)\ncovidp2_chart\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\ncombined_no_geometry &lt;- combined %&gt;%\n  st_drop_geometry()\n\n# Summarise revenue data for Pre-COVID period only\nrevenue_summary &lt;- combined_no_geometry %&gt;%\n  filter(Period == \"Post-COVID\") %&gt;%\n  group_by(Province) %&gt;%\n  summarise(total_revenue = sum(revenue_all, na.rm = TRUE)) %&gt;%\n  ungroup()\n\n# Create the pie chart for Pre-COVID period\ncreate_pie_chart &lt;- function(data) {\n  period_data &lt;- data %&gt;%\n    arrange(desc(total_revenue)) %&gt;%\n    mutate(Province = if_else(row_number() &lt;= 10, Province, \"Other\")) %&gt;%\n    group_by(Province) %&gt;%\n    summarise(total_revenue = sum(total_revenue)) %&gt;%\n    ungroup()\n  \n  total_period_revenue &lt;- sum(period_data$total_revenue)\n  \n  period_data &lt;- period_data %&gt;%\n    mutate(percentage = (total_revenue / total_period_revenue) * 100)\n  \n  plot_ly(period_data, labels = ~Province, values = ~percentage, type = 'pie',\n          textposition = 'inside',\n          textinfo = 'label+percent',\n          hoverinfo = 'text',\n          text = ~paste(Province, \n                        \"&lt;br&gt;Revenue: $\", formatC(total_revenue, format=\"f\", big.mark=\",\", digits=0),\n                        \"&lt;br&gt;Percentage: \", formatC(percentage, format=\"f\", digits=1), \"%\"),\n          marker = list(line = list(color = '#FFFFFF', width = 1))) %&gt;%\n    layout(title = \"Revenue Distribution by Province - Post-COVID: 1st May 2022 onwards\",size=0.5, \n           showlegend = FALSE)\n}\n\n# Create and display the Pre-COVID pie chart\npost_covid_chart &lt;- create_pie_chart(revenue_summary)\npost_covid_chart\n\n\n\n\n\n\nPre-COVID: 1st Jan 2019 - 3rd March 2020\n38.3% of the total revenue was generated from Bangkok while Phuket generated 16.91%. Choburi generated 9.88% while Krabi was 4.24%. Chiangmai was 4.14%.\nCOVID Part 1: 4th April 2020 - 30th June 2021\n30.71% of the total revenue was generated from Bangkok while Chiangmai generated 8.8%. Chonburi generated 5.52%. Phuket generated 3.21%.\nCOVID Part 2: 1st July 2021 - 30th April 2022\nCOVID P2, 22.08% of the total revenue during the period was generated from Bangkok while 12.67% comes from Phuket. Chiang Mai & Chonburi generated 8.59% and 9.55% of the total revenue in the total period.\nPost-COVID: 1st May 2022 onwards\nUnderstandably, Bangkok generated 34.58% while Phuket generated 19.62%. Chonburi generated 9.27% and Chiangmai generated 4.94%. Krabi only generated 1.25%\nInteresting Observations:\n\nKrabi was the top 5 provinces of the total revenue of tourism in Pre-COVID. However, post COVID it wasn’t in top 5 and merely generated 1.25% only. This is may due to the “slow international flight resumption at Krabi airport, especially from China, India and South Korea” as taken from The Bangkok Post, November 2023.\n\n\n\nCode Chunk\n# input: the dataframe and the variable name, chart style, title \nchoropleth_plot &lt;- function(rev, varname, style, title) {\n  tm_shape(rev) +\n    tm_fill(varname, \n          n= 5,\n          style = style) +\n    tm_borders(alpha = 0.5) +\n    tm_layout(main.title = title,\n              main.title.size = 1,\n              main.title.position = \"center\",\n              legend.height = 0.45, \n              legend.width = 0.35,\n              frame = TRUE)+ \n    tm_compass(position = c('left','bottom'))\n}\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "TakeHomeExercise/TakeHome2/TakeHome2.html#exploratory-spatial-data-analysis",
    "href": "TakeHomeExercise/TakeHome2/TakeHome2.html#exploratory-spatial-data-analysis",
    "title": "Take-Home Exercise 2",
    "section": "",
    "text": "In the code chunk below, poly2nb() of spdep package is used to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries. By default, Queen contiguity is applied.\n\n\nCode Chunk\nwm_q &lt;- poly2nb(thaiboundary, queen=TRUE)\nsummary(wm_q)\n\n\nNeighbour list object:\nNumber of regions: 77 \nNumber of nonzero links: 352 \nPercentage nonzero weights: 5.93692 \nAverage number of links: 4.571429 \n1 region with no links:\n67\n2 disjoint connected subgraphs\nLink number distribution:\n\n 0  1  2  3  4  5  6  7  8  9 \n 1  1  5 17 15 17 10  5  4  2 \n1 least connected region:\n14 with 1 link\n2 most connected regions:\n29 51 with 9 links\n\n\nThe summary report above shows that there are 77 area units in Thailand. The most connected area unit has 9 links.\n\n\n\nIn the code chunk below, poly2nb() of spdep package is used to compute contiguity weight matrices for the study area. We specify queen = FALSE to compute Rook contiguity.\n\n\nCode Chunk\nwm_q &lt;- poly2nb(thaiboundary, queen=FALSE)\nsummary(wm_q)\n\n\nNeighbour list object:\nNumber of regions: 77 \nNumber of nonzero links: 352 \nPercentage nonzero weights: 5.93692 \nAverage number of links: 4.571429 \n1 region with no links:\n67\n2 disjoint connected subgraphs\nLink number distribution:\n\n 0  1  2  3  4  5  6  7  8  9 \n 1  1  5 17 15 17 10  5  4  2 \n1 least connected region:\n14 with 1 link\n2 most connected regions:\n29 51 with 9 links\n\n\nThe summary report above shows that there are 77 area units in Thailand. The most connected area unit has 9 links.\nBoth of them possess the same number of link (4.57).\n\n\n\n\nWe will derive the distance-based weight matrices by using dnearneigh() of spdep package. The function identifies neighbours of region points by Euclidean distance with a distance band with lower and upper bounds controlled by the bounds argument or by Great Circle distance in kilometres if longlat argument is set to TRUE.\n\n\nDetermining the cut-off distance\nTo ensure that each region has at least one neighbour, we need to find out the minimum distance within which all regions have at least oneneighbour. We can do this by following these steps:\n\nGetting the coordinates of polygon centroids. This is required as an input in the next step.\nWe need to associate each polygon with a point and its coordinates need to be in a separate data frame. We will use a mapping function that applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column of thaiboundary. Our function will be st_centroid(). We will be using map_dbl() variation of map from the purrr package. purrr is loaded when we load tidyverse package.\nTo get our longitude values we map the st_centroid() function over the geometry column of thaiboundary and access the longitude value through double bracket notation [[]] and\n\nThis allows us to get only the longitude, which is the first value in each centroid.\n\n\n\n\nCode Chunk\nlongitude &lt;- map_dbl(thaiboundary$geometry, ~st_centroid(.x)[[1]])\n\n\nWe do the same for latitude with one key difference. We access the second value per each centroid with [[2]]\n\n\nCode Chunk\nlatitude &lt;- map_dbl(thaiboundary$geometry, ~st_centroid(.x)[[2]])\n\n\nNow that we have latitude and longitude, I used cbind to put longitude and latitude into the same object. We should check the first few observations to see if things are formatted correctly.\n\n\nCode Chunk\ncoords &lt;- cbind(longitude, latitude)\nhead(coords, 5)\n\n\n     longitude latitude\n[1,]  675514.6  1523087\n[2,]  685033.7  1503755\n[3,]  650477.2  1539777\n[4,]  681656.0  1555581\n[5,]  664627.1  1586462\n\n\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\n\nCode Chunk\nk1 &lt;- knn2nb(knearneigh(coords, k = 1))\nk1dists &lt;- unlist(nbdists(k1, coords))\nsummary(k1dists)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  21548   51966   64530   63281   76823  110987 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 110,987 metres, approximately 111, 000 metres. This will be used as the upper threshold as it gives certainty that all regions will have at least one neighbour.\nComputing the fixed distance weight matrix\nNow, we will compute the distance weight matrix by using dnearneigh() as shown below.\n\n\nCode Chunk\nwm_d111 &lt;- dnearneigh(coords,0,111000)\nwm_d111\n\n\nNeighbour list object:\nNumber of regions: 77 \nNumber of nonzero links: 350 \nPercentage nonzero weights: 5.903188 \nAverage number of links: 4.545455 \n2 disjoint connected subgraphs\n\n\nFrom the above code, we can see that the average number of links is 4.54.\nNext, we use str() to display the content of wm_d111 weight matrix.\n\n\nCode Chunk\nstr(wm_d111)\n\n\nList of 77\n $ : int [1:12] 2 3 4 5 6 10 11 15 17 59 ...\n $ : int [1:10] 1 3 4 5 11 15 17 59 60 61\n $ : int [1:13] 1 2 4 5 6 8 10 17 56 58 ...\n $ : int [1:14] 1 2 3 5 6 8 10 15 16 17 ...\n $ : int [1:13] 1 2 3 4 6 7 8 9 10 17 ...\n $ : int [1:11] 1 3 4 5 7 8 9 10 17 58 ...\n $ : int [1:7] 5 6 8 9 10 17 48\n $ : int [1:10] 3 4 5 6 7 9 10 48 49 58\n $ : int [1:7] 5 6 7 8 48 49 58\n $ : int [1:9] 1 3 4 5 6 7 8 16 17\n $ : int [1:6] 1 2 12 13 15 16\n $ : int [1:3] 11 13 15\n $ : int [1:5] 11 12 14 15 18\n $ : int 13\n $ : int [1:9] 1 2 4 11 12 13 16 17 18\n $ : int [1:6] 4 10 11 15 17 18\n $ : int [1:10] 1 2 3 4 5 6 7 10 15 16\n $ : int [1:3] 13 15 16\n $ : int 20\n $ : int [1:2] 19 21\n $ : int [1:2] 20 22\n $ : int [1:2] 21 23\n $ : int [1:2] 22 26\n $ : int [1:3] 26 34 38\n $ : int [1:2] 29 55\n $ : int [1:4] 23 24 34 38\n $ : int [1:2] 32 36\n $ : int [1:4] 29 30 31 32\n $ : int [1:3] 25 28 33\n $ : int [1:3] 28 32 36\n $ : int 28\n $ : int [1:3] 27 28 30\n $ : int [1:3] 29 34 35\n $ : int [1:5] 24 26 33 35 38\n $ : int [1:4] 33 34 36 38\n $ : int [1:4] 27 30 35 37\n $ : int [1:2] 36 38\n $ : int [1:5] 24 26 34 35 37\n $ : int [1:3] 40 41 47\n $ : int [1:2] 39 41\n $ : int [1:3] 39 40 43\n $ : int [1:3] 43 52 53\n $ : int [1:4] 41 42 44 52\n $ : int [1:2] 43 45\n $ : int [1:2] 44 46\n $ : int 45\n $ : int 39\n $ : int [1:6] 7 8 9 49 50 54\n $ : int [1:6] 8 9 48 50 57 58\n $ : int [1:5] 48 49 51 52 54\n $ : int 50\n $ : int [1:4] 42 43 50 53\n $ : int [1:4] 42 52 54 55\n $ : int [1:4] 48 50 53 55\n $ : int [1:3] 25 53 54\n $ : int [1:5] 3 59 60 61 62\n $ : int [1:2] 49 58\n $ : int [1:9] 3 4 5 6 8 9 49 57 59\n $ : int [1:10] 1 2 3 4 5 6 56 58 60 61\n $ : int [1:9] 1 2 3 4 5 56 59 61 62\n $ : int [1:8] 1 2 3 4 56 59 60 62\n $ : int [1:4] 56 60 61 63\n $ : int 62\n $ : int [1:4] 65 68 73 74\n $ : int [1:5] 64 66 67 68 73\n $ : int [1:3] 65 67 68\n $ : int [1:2] 65 66\n $ : int [1:4] 64 65 66 69\n $ : int [1:2] 68 70\n $ : int 69\n $ : int [1:3] 72 74 75\n $ : int [1:3] 71 73 74\n $ : int [1:4] 64 65 72 74\n $ : int [1:4] 64 71 72 73\n $ : int [1:3] 71 76 77\n $ : int [1:2] 75 77\n $ : int [1:2] 75 76\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:77] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language dnearneigh(x = coords, d1 = 0, d2 = 111000)\n - attr(*, \"dnn\")= num [1:2] 0 111000\n - attr(*, \"bounds\")= chr [1:2] \"GE\" \"LE\"\n - attr(*, \"nbtype\")= chr \"distance\"\n - attr(*, \"sym\")= logi TRUE\n\n\nWe can observe that each region has different number of neighbours.\n\n\nCode Chunk\npar(mfrow = c(1,2))\nplot(thaiboundary$geometry, border = \"lightgrey\",main=\"1st nearest neighbours\" )\nplot(k1, coords, add = TRUE, col = \"red\", length = 0.88, )\n\nplot(thaiboundary$geometry, border = \"lightgrey\", main = \"Distance Link\")\nplot(wm_d111, coords, add = TRUE, pch = 19, cex = 0.6)\n\n\n\n\n\n\n\n\n\nFrom the above output, we can make a few observations:\n\nIt illustrates a characteristic of fixed distance weight matrix–more densely settled areas (usually the urban areas) tend to have more neighbours which is in instance it is in the middle where Bangkok resides and the less densely settled areas (usually the rural provinces) tend to have lesser neighbours\nThe geographical areas of the regions in Thailand are largely varying. In the middle, bottom and top right, we see the neighbour links are dense especially in the middle and less dense in the rest of the regions where white spaces are\nLastly, it is interesting to note that there is a dead space between the densely settled area of Bangkok region and the southern part of Thailand. This may indicate that tourism was not developed at all.\n\n\n\n\nTo overcome the issue of fixed distance weight matrix where there is uneven distribution of neighbours, we can use directly control the numbers of neighbours using k-nearest neighbours, as shown in the code chunk below.\nAs a rule-of-thumb, we will set k = 8 i.e., all regions will have 8 neighbours.\n\n\nCode Chunk\nknn8 &lt;- knn2nb(knearneigh(coords, k=8))\nknn8\n\n\nNeighbour list object:\nNumber of regions: 77 \nNumber of nonzero links: 616 \nPercentage nonzero weights: 10.38961 \nAverage number of links: 8 \nNon-symmetric neighbours list\n\n\nPlotting Adaptive Distance-based Neighbours\n\n\nCode Chunk\npar(mfrow = c(1,2))\nplot(thaiboundary$geometry, border = \"lightgrey\",main=\"8 nearest neighbours\" )\nplot(knn8, coords, add = TRUE, col = \"blue\", length = 0.88, )\n\nplot(thaiboundary$geometry, border = \"lightgrey\", main = \"Distance Link w KNN\")\nplot(knn8, coords, add = TRUE, col = \"red\", pch = 19, cex = 0.6)\n\n\n\n\n\n\n\n\n\n\n\n\n\nSelecting a spatial weight matrix is use is dependent on the geographical area of interest and the focus of the study8.\nIn this study, between contiguity-based and distance-based spatial weight matrices, we lean towards distance-based matrices. Within distance-based matrices, we will select the adaptive distance-based spatial weight matrix for our subsequent analysis.\nThe reasons are summarised here:\n\nThailand has 77 provinces and it is relatively small . Hence, a contiguity-based matrix will have the issue where larger LGAs have more neighbours and smaller LGAs have lesser neighbours. This would likely skew our analysis. Therefore, distance-based methods are preferred.\nAs mentioned earlier, the fixed distance-based method has the disadvantage that some regions would only have 1 neighbour, while on average regions have 23 neighbours. Statistical test for regions with only 1 neighbour may not be valid.\n\nBased on the above, we will select adaptive distance-based spatial weight matrix.\n\n\n\n\n\nCode Chunk\nrswm_knn8 &lt;- nb2listw(knn8,\n                   style = \"W\",\n                   zero.policy = TRUE)\nrswm_knn8\n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 77 \nNumber of nonzero links: 616 \nPercentage nonzero weights: 10.38961 \nAverage number of links: 8 \nNon-symmetric neighbours list\n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 77 5929 77 17.03125 319.9688\n\n\nWe will be using the row-standardised weight matrix for the next part of the analysis.\n\n\n\nThis in sub-section, we will use two methods: Moran’s I and Geary’s C to test the hypothesis the following hypothesis:\n\nH0: Observed spatial patterns of values is equally likely as any other spatial pattern i.e. data is randomly disbursed, no spatial pattern\nH1: Data is more spatially clustered than expected by chance alone.\n\n\n\nWe will perform Moran’s I statistical testing by using moran.test() of spdep. Moran’s I describe how features differ from the values in the study area as a whole. The Moran I statistic ranges from -1 to 1. If the Moran I is:\n\npositive (I&gt;0): Clustered, observations tend to be similar\nnegative (I&lt;0): Disperse, observations tend to be dissimilar\napproximately zero: observations arranged randomly over space"
  },
  {
    "objectID": "InClassExercise/InClass6/InClass6.html",
    "href": "InClassExercise/InClass6/InClass6.html",
    "title": "In-Class Exercise 6",
    "section": "",
    "text": "Code Chunk\npacman::p_load(sf, sfdep, tmap, plotly, tidyverse, kendell)\nCode Chunk\nhunan &lt;- st_read(dsn = \"data/geospatial\",\n                  layer = \"Hunan\")\nCode Chunk\nGDPPC &lt;- read_csv(\"data/aspatial/Hunan_GDPPC.csv\")\nCode Chunk\nGDPPC_st &lt;- spacetime(GDPPC, hunan, #GDPPC is attirbute while hunan is geospatial\n                      .loc_col = \"County\", \n                      .time_col = \"Year\") #cannot use original time and date fiekld\nThe above code has multiple layer due to different timeS.\nHint:Need to convert day to integer OR drop the day. Cannot need to have decimals places. Need to use lubridate to change to POSIXct.\nBelow code is to check if it is a time-space cube.\nCode Chunk\nis_spacetime_cube(GDPPC_st)"
  },
  {
    "objectID": "InClassExercise/InClass6/InClass6.html#inputing-computing-gi",
    "href": "InClassExercise/InClass6/InClass6.html#inputing-computing-gi",
    "title": "In-Class Exercise 6",
    "section": "1 Inputing Computing Gi*",
    "text": "1 Inputing Computing Gi*\nGood in detecting hotspot (incidences) and coldspot (low incidences)\n\n\nCode Chunk\nGDPPC_nb &lt;- GDPPC_st %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(\n    st_contiguity(geometry)),\n    wt = st_inverse_distance(nb,\n                              geometry,\n                              scale = 1,\n                              alpha = 1),\n    .before = 1) %&gt;%\n  set_nbs(\"nb\") %&gt;%\n  set_wts(\"wt\")\n\n\n\n\n\n\n\n\nThings to learn from above code chunk\n\n\n\n\nActivate() of dplyr package is used to activate the geometry context\nmutate() of dplyr package is used to create two new columns nb and wt\nthen we will activate the data context again and copy over the nb and wt columns to each time-slice using set_nbs() and set_wts()\nrow order is very important so do not rearrange the observations after using set_nbs() or set_wts()\n\n\n\nWe can use these news columns to manually calculate the local Gi* for each location. We can do this by grouping by Year and using local_gstar_perm() of sfdep package. After which, we use unnest() to unnest gi_star of the newly created gi_starts data.frame.\n\n\nCode Chunk\ngi_stars &lt;- GDPPC_nb %&gt;%\n  group_by(Year) %&gt;%\n  mutate(gi_star = local_gstar_perm(\n    GDPPC, nb, wt)) %&gt;%\n  tidyr::unnest(gi_star)"
  },
  {
    "objectID": "InClassExercise/InClass6/InClass6.html#mann-kendall-test",
    "href": "InClassExercise/InClass6/InClass6.html#mann-kendall-test",
    "title": "In-Class Exercise 6",
    "section": "2 Mann-Kendall Test",
    "text": "2 Mann-Kendall Test\nA monotonic series or function is one that only increases (or decreases) and never changes direction. As long as the function either stays flats\n\n\nCode Chunk\ncbg &lt;- gi_stars %&gt;%\n  ungroup() %&gt;%\n  filter(County == \"Changsha\") |&gt;\n  select(County, Year, gi_star)\n\n\nNext we plot the result by using ggplot2 functions.\n\n\nCode Chunk\neval: false\nggplot(data = cbg,\n       aes(X = Year,\n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\n\n\n\nCode Chunk\ncbg %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  tidyr::unnest_wider(mk)\n\n\nIn above result, sl is the p-value. With reference to the results, we will reject the hypothesis null and infer that a slight upward trend.\n\n\nCode Chunk\nehsa &lt;- gi_stars %&gt;%\n  group_by(County) %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  tidyr::unnest_wider(mk)\nhead(ehsa)\n\n\nMann-Kendall test data.frame"
  },
  {
    "objectID": "InClassExercise/InClass6/InClass6.html#performing-emerging-hotspot-analysis",
    "href": "InClassExercise/InClass6/InClass6.html#performing-emerging-hotspot-analysis",
    "title": "In-Class Exercise 6",
    "section": "3 Performing Emerging Hotspot Analysis",
    "text": "3 Performing Emerging Hotspot Analysis\nLastly, we will perform EHSA analysis by using emerging_hotspot_analysis() of sfdep package. It takes a spacetime object x (i.e. GDPPC_st), and the quoted name of the variable of interest (i.e. GDPPC) for .var argument. The k argument is used to specify the number of time lags which is set to 1 by default. Lastly, nsim map numbers of simulation is to be performed\n\n\nCode Chunk\nehsa &lt;- emerging_hotspot_analysis(\n  x = GDPPC_st,\n  .var = \"GDPPC\",\n  k = 1,\n  nsim = 99\n)\n\n\n\n\nCode Chunk\nggplot(data = ehsa,\n       aes = (x = classification)) +\n  geom_bar()\n\n\n\n\nCode Chunk\nhunan_ehsa &lt;- hunan %&gt;%\n  left_join(ehsa,\n              by = join_by(County == location))\n\n\n\n\nCode Chunk\nehsa_sig &lt;- hunan_ehsa %&gt;%\n  filter(p_value &lt;0.05)\ntmap_mode(\"plot\")\ntm_shape(hunan_ehsa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(ehsa_sig) +\n  tm_fill(\"classification\") +\n  tm_borders(alpha = 0.4)"
  },
  {
    "objectID": "InClassExercise/InClass5/InClass5.html",
    "href": "InClassExercise/InClass5/InClass5.html",
    "title": "In-Class Exercise 5",
    "section": "",
    "text": "Introducing sfdep.\n\nsfdep creates an sf and tidyverse friendly interface to the package as well as introduces new functionality that is not present in spdep.\nsfdep utilizes list columns extensively to make this interface possible.”"
  },
  {
    "objectID": "InClassExercise/InClass5/InClass5.html#content",
    "href": "InClassExercise/InClass5/InClass5.html#content",
    "title": "In-Class Exercise 5",
    "section": "",
    "text": "Introducing sfdep.\n\nsfdep creates an sf and tidyverse friendly interface to the package as well as introduces new functionality that is not present in spdep.\nsfdep utilizes list columns extensively to make this interface possible.”"
  },
  {
    "objectID": "InClassExercise/InClass5/InClass5.html#getting-started",
    "href": "InClassExercise/InClass5/InClass5.html#getting-started",
    "title": "In-Class Exercise 5",
    "section": "2 Getting started",
    "text": "2 Getting started\n\n2.1 Installing and Loading the R Packages\nFour R packages will be used for this in-class exercise, they are: sf, sfdep, tmap and tidyverse.\n\n\n2.2 The code\n\n\nCode Chunk\npacman::p_load(sf, sfdep, tmap, tidyverse)"
  },
  {
    "objectID": "InClassExercise/InClass5/InClass5.html#the-data",
    "href": "InClassExercise/InClass5/InClass5.html#the-data",
    "title": "In-Class Exercise 5",
    "section": "3 The Data",
    "text": "3 The Data\nFor the purpose of this in-class exercise, the Hunan data sets will be used. There are two data sets in this use case, they are:\n\nHunan, a geospatial data set in ESRI shapefile format, and\nHunan_2012, an attribute data set in csv format.\n\n\nDo It Yourself!The code\n\n\nUsing the steps you learned in previous lesson, import Hunan shapefile into R environment as an sf data frame.\n\n\n\n\n\nCode Chunk\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\n\nReading layer `Hunan' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/InClassExercise/InClass5/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "InClassExercise/InClass5/InClass5.html#importing-attribute-table",
    "href": "InClassExercise/InClass5/InClass5.html#importing-attribute-table",
    "title": "In-Class Exercise 5",
    "section": "4 Importing Attribute Table",
    "text": "4 Importing Attribute Table\n\nDo It Yourself!The code\n\n\nUsing the steps you learned in previous lesson, import Hunan_2012.csv into R environment as an tibble data frame.\n\n\n\n\n\nCode Chunk\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")"
  },
  {
    "objectID": "InClassExercise/InClass5/InClass5.html#combining-both-data-frame-by-using-left-join",
    "href": "InClassExercise/InClass5/InClass5.html#combining-both-data-frame-by-using-left-join",
    "title": "In-Class Exercise 5",
    "section": "5 Combining both data frame by using left join",
    "text": "5 Combining both data frame by using left join\n\nDo It Yourself!The code\n\n\nUsing the steps you learned in previous lesson, combine the Hunan sf data frame and Hunan_2012 data frame. Ensure that the output is an sf data frame.\n\n\n\n\n\nCode Chunk\nhunan_GDPPC &lt;- left_join(hunan, hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor the purpose of this exercise, we only retain column 1 to 4, column 7 and column 15. You should examine the output sf data.frame to learn know what are these fields.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn order to retain the geospatial properties, the left data frame must the sf data.frame (i.e. hunan)"
  },
  {
    "objectID": "InClassExercise/InClass5/InClass5.html#plotting-a-choropleth-map",
    "href": "InClassExercise/InClass5/InClass5.html#plotting-a-choropleth-map",
    "title": "In-Class Exercise 5",
    "section": "6 Plotting a choropleth map",
    "text": "6 Plotting a choropleth map\n\nDo It Yourself!The plotThe code\n\n\nUsing the steps you learned in previous lesson, plot a choropleth map showing the distribution of GDPPC of Hunan Province.\n\n\n\n\nCode Chunk\ntmap_mode(\"plot\")\ntm_shape(hunan_GDPPC) +\n  tm_fill(\"GDPPC\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"GDPPC\") +\n  tm_layout(main.title = \"Distribution of GDP per capita by county, Hunan Province\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\ntmap_mode(\"plot\")\ntm_shape(hunan_GDPPC) +\n  tm_fill(\"GDPPC\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"GDPPC\") +\n  tm_layout(main.title = \"Distribution of GDP per capita by county, Hunan Province\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)"
  },
  {
    "objectID": "InClassExercise/InClass5/InClass5.html#global-measures-of-spatial-association",
    "href": "InClassExercise/InClass5/InClass5.html#global-measures-of-spatial-association",
    "title": "In-Class Exercise 5",
    "section": "7 Global Measures of Spatial Association",
    "text": "7 Global Measures of Spatial Association\n\n7.1 Step 1: Deriving Queen’s contiguity weights: sfdep methods\n\n\n\n\n\nCode Chunk\nwm_q &lt;- hunan_GDPPC %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb,\n                         style = \"W\"),\n         .before = 1) \n\n\n\n\n\nNotice that st_weights() provides tree arguments, they are:\n\nnb: A neighbor list object as created by st_neighbors().\nstyle: Default “W” for row standardized weights. This value can also be “B”, “C”, “U”, “minmax”, and “S”. B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).\nallow_zero: If TRUE, assigns zero as lagged value to zone without neighbors.\n\n\n\n\n\n\n\n7.2 The wm_q\n\n\n\nCode Chunk\nwm_q\n\n\nSimple feature collection with 88 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                               nb\n1                 2, 3, 4, 57, 85\n2               1, 57, 58, 78, 85\n3                     1, 4, 5, 85\n4                      1, 3, 5, 6\n5                     3, 4, 6, 85\n6                4, 5, 69, 75, 85\n7                  67, 71, 74, 84\n8       9, 46, 47, 56, 78, 80, 86\n9           8, 66, 68, 78, 84, 86\n10 16, 17, 19, 20, 22, 70, 72, 73\n                                                                            wt\n1                                                      0.2, 0.2, 0.2, 0.2, 0.2\n2                                                      0.2, 0.2, 0.2, 0.2, 0.2\n3                                                       0.25, 0.25, 0.25, 0.25\n4                                                       0.25, 0.25, 0.25, 0.25\n5                                                       0.25, 0.25, 0.25, 0.25\n6                                                      0.2, 0.2, 0.2, 0.2, 0.2\n7                                                       0.25, 0.25, 0.25, 0.25\n8  0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571\n9             0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667\n10                      0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125\n     NAME_2  ID_3    NAME_3   ENGTYPE_3    County GDPPC\n1   Changde 21098   Anxiang      County   Anxiang 23667\n2   Changde 21100   Hanshou      County   Hanshou 20981\n3   Changde 21101    Jinshi County City    Jinshi 34592\n4   Changde 21102        Li      County        Li 24473\n5   Changde 21103     Linli      County     Linli 25554\n6   Changde 21104    Shimen      County    Shimen 27137\n7  Changsha 21109   Liuyang County City   Liuyang 63118\n8  Changsha 21110 Ningxiang      County Ningxiang 62202\n9  Changsha 21111 Wangcheng      County Wangcheng 70666\n10 Chenzhou 21112     Anren      County     Anren 12761\n                         geometry\n1  POLYGON ((112.0625 29.75523...\n2  POLYGON ((112.2288 29.11684...\n3  POLYGON ((111.8927 29.6013,...\n4  POLYGON ((111.3731 29.94649...\n5  POLYGON ((111.6324 29.76288...\n6  POLYGON ((110.8825 30.11675...\n7  POLYGON ((113.9905 28.5682,...\n8  POLYGON ((112.7181 28.38299...\n9  POLYGON ((112.7914 28.52688...\n10 POLYGON ((113.1757 26.82734...\n\n\n\n\n\n\n7.3 Computing Global Moran’ I\n\n\nIn the code chunk below, global_moran() function is used to compute the Moran’s I value. Different from spdep package, the output is a tibble data.frame.\n\n\n\nCode Chunk\nmoranI &lt;- global_moran(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt)\nglimpse(moranI)\n\n\nList of 2\n $ I: num 0.301\n $ K: num 7.64\n\n\n\n\n\n\n\n\n7.4 Performing Global Moran’sI test\nIn general, Moran’s I test will be performed instead of just computing the Moran’s I statistics. With sfdep package, Moran’s I test can be performed by using global_moran_test() as shown in the code chunk below.\n\nThe outputThe codeTips\n\n\n\n\n\nCode Chunk\nglobal_moran_test(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt)\n\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\n\n\n\n\n\n\nCode Chunk\nglobal_moran_test(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt)\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nThe default for alternative argument is “two.sided”. Other supported arguments are “greater” or “less”. randomization, and\nBy default the randomization argument is TRUE. If FALSE, under the assumption of normality.\n\n\n\n\n\n\n\n\n\n7.5 Performing Global Moran’I permutation test\nIn practice, Monte carlo simulation should be used to perform the statistical test. For sfdep, it is supported by globel_moran_perm()\n\nStep 1Step 2The report\n\n\nIt is always a good practice to use set.seed() before performing simulation. This is to ensure that the computation is reproducible.\n\n\n\nCode Chunk\nset.seed(1234)\n\n\n\n\n\nNext, global_moran_perm() is used to perform Monte Carlo simulation.\n\n\n\nCode Chunk\nglobal_moran_perm(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt,\n                  nsim = 99)\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.30075, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n\n\nThe statistical report on previous tab shows that the p-value is smaller than alpha value of 0.05. Hence, we have enough statistical evidence to reject the null hypothesis that the spatial distribution of GPD per capita are resemble random distribution (i.e. independent from spatial). Because the Moran’s I statistics is greater than 0. We can infer that the spatial distribution shows sign of clustering.\n\n\n\n\n\n\nReminder\n\n\n\nThe numbers of simulation is alway equal to nsim + 1. This mean in nsim = 99. This mean 100 simulation will be performed."
  },
  {
    "objectID": "InClassExercise/InClass5/InClass5.html#lisa-map",
    "href": "InClassExercise/InClass5/InClass5.html#lisa-map",
    "title": "In-Class Exercise 5",
    "section": "8 LISA map",
    "text": "8 LISA map\n\n\nLISA map is a categorical map showing outliers and clusters. There are two types of outliers namely: High-Low and Low-High outliers. Likewise, there are two type of clusters namely: High-High and Low-Low cluaters. In fact, LISA map is an interpreted map by combining local Moran’s I of geographical areas and their respective p-values."
  },
  {
    "objectID": "InClassExercise/InClass5/InClass5.html#computing-local-morans-i",
    "href": "InClassExercise/InClass5/InClass5.html#computing-local-morans-i",
    "title": "In-Class Exercise 5",
    "section": "9 Computing local Moran’s I",
    "text": "9 Computing local Moran’s I\nIn this section, you will learn how to compute Local Moran’s I of GDPPC at county level by using local_moran() of sfdep package.\n\nThe codeThe output\n\n\n\n\n\nCode Chunk\nlisa &lt;- wm_q %&gt;% \n  mutate(local_moran = local_moran(\n    GDPPC, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\n\n\n\n\nThe output of local_moran() is a sf data.frame containing the columns ii, eii, var_ii, z_ii, p_ii, p_ii_sim, and p_folded_sim.\n\nii: local moran statistic\neii: expectation of local moran statistic; for localmoran_permthe permutation sample means\nvar_ii: variance of local moran statistic; for localmoran_permthe permutation sample standard deviations\nz_ii: standard deviate of local moran statistic; for localmoran_perm based on permutation sample means and standard deviations p_ii: p-value of local moran statistic using pnorm(); for localmoran_perm using standard deviatse based on permutation sample means and standard deviations p_ii_sim: For localmoran_perm(), rank() and punif() of observed statistic rank for [0, 1] p-values using alternative= -p_folded_sim: the simulation folded [0, 0.5] range ranked p-value (based on https://github.com/pysal/esda/blob/4a63e0b5df1e754b17b5f1205b cadcbecc5e061/esda/crand.py#L211-L213)\nskewness: For localmoran_perm, the output of e1071::skewness() for the permutation samples underlying the standard deviates\nkurtosis: For localmoran_perm, the output of e1071::kurtosis() for the permutation samples underlying the standard deviates.\n\n\n\n\n\n\n\n9.1 Visualising local Moran’s I\n\n\nIn this code chunk below, tmap functions are used prepare a choropleth map by using value in the ii field.\n\n\n\nCode Chunk\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(\n    main.title = \"local Moran's I of GDPPC\",\n    main.title.size = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.2 Visualising p-value of local Moran’s I\n\n\nIn the code chunk below, tmap functions are used prepare a choropleth map by using value in the p_ii_sim field.\n\n\n\nCode Chunk\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_fill(\"p_ii_sim\") + \n  tm_borders(alpha = 0.5) +\n   tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 2)\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nFor p-values, the appropriate classification should be 0.001, 0.01, 0.05 and not significant instead of using default classification scheme.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.3 Visualising local Moran’s I and p-value\nFor effective comparison, it will be better for us to plot both maps next to each other.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\ntmap_mode(\"plot\")\nmap1 &lt;- tm_shape(lisa) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"local Moran's I of GDPPC\",\n            main.title.size = 0.8)\n\nmap2 &lt;- tm_shape(lisa) +\n  tm_fill(\"p_ii\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)\n\n\n\n\n\n\n\n\n\n9.4 Plotting LISA map\n\n\nIn lisa sf data.frame, we can find three fields contain the LISA categories. They are mean, median and pysal. In general, classification in mean will be used as shown in the code chunk below.\n\n\n\nCode Chunk\nlisa_sig &lt;- lisa  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.4)"
  },
  {
    "objectID": "InClassExercise/InClass5/InClass5.html#hot-spot-and-cold-spot-area-analysis-hcsa",
    "href": "InClassExercise/InClass5/InClass5.html#hot-spot-and-cold-spot-area-analysis-hcsa",
    "title": "In-Class Exercise 5",
    "section": "10 Hot Spot and Cold Spot Area Analysis (HCSA)",
    "text": "10 Hot Spot and Cold Spot Area Analysis (HCSA)\n\n\nHCSA uses spatial weights to identify locations of statistically significant hot spots and cold spots in an spatially weighted attribute that are in proximity to one another based on a calculated distance. The analysis groups features when similar high (hot) or low (cold) values are found in a cluster. The polygon features usually represent administration boundaries or a custom grid structure.\n\n\n\n\n\n\n10.1 Computing local Gi* statistics\nAs usual, we will need to derive a spatial weight matrix before we can compute local Gi* statistics. Code chunk below will be used to derive a spatial weight matrix by using sfdep functions and tidyverse approach.\n\n\n\nCode Chunk\nwm_idw &lt;- hunan_GDPPC %&gt;%\n  mutate(nb = include_self(\n    st_contiguity(geometry)),\n    wts = st_inverse_distance(nb, \n                              geometry, \n                              scale = 1,\n                              alpha = 1),\n         .before = 1)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nGi* and local Gi* are distance-based spatial statistics. Hence, distance methods instead of contiguity methods should be used to derive the spatial weight matrix.\nSince we are going to compute Gi* statistics, include_self()is used.\n\n\n\n\n\n\n10.2 Computing local Gi* statistics\nNow, we will compute the local Gi* by using the code chunk below.\n\n\n\nCode Chunk\nHCSA &lt;- wm_idw %&gt;% \n  mutate(local_Gi = local_gstar_perm(\n    GDPPC, nb, wts, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_Gi)\nHCSA\n\n\nSimple feature collection with 88 features and 18 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n# A tibble: 88 × 19\n    gi_star cluster     e_gi  var_gi std_dev p_value p_sim p_folded_sim skewness\n      &lt;dbl&gt; &lt;fct&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1  0.261   Low     0.00126  1.07e-7  0.283  7.78e-1  0.66         0.33    0.783\n 2 -0.276   Low     0.000969 4.76e-8 -0.123  9.02e-1  0.98         0.49    0.713\n 3  0.00573 High    0.00156  2.53e-7 -0.0571 9.54e-1  0.78         0.39    0.972\n 4  0.528   High    0.00155  2.97e-7  0.321  7.48e-1  0.56         0.28    0.942\n 5  0.466   High    0.00137  2.76e-7  0.386  7.00e-1  0.52         0.26    1.32 \n 6 -0.445   High    0.000992 7.08e-8 -0.588  5.57e-1  0.68         0.34    0.692\n 7  2.99    High    0.000700 4.05e-8  3.13   1.74e-3  0.04         0.02    0.975\n 8  2.04    High    0.00152  1.58e-7  1.77   7.59e-2  0.16         0.08    1.26 \n 9  4.42    High    0.00130  1.18e-7  4.22   2.39e-5  0.02         0.01    1.20 \n10  1.21    Low     0.00175  1.25e-7  1.49   1.36e-1  0.18         0.09    0.408\n# ℹ 78 more rows\n# ℹ 10 more variables: kurtosis &lt;dbl&gt;, nb &lt;nb&gt;, wts &lt;list&gt;, NAME_2 &lt;chr&gt;,\n#   ID_3 &lt;int&gt;, NAME_3 &lt;chr&gt;, ENGTYPE_3 &lt;chr&gt;, County &lt;chr&gt;, GDPPC &lt;dbl&gt;,\n#   geometry &lt;POLYGON [°]&gt;\n\n\n\n\n\n\n10.3 Visualising Gi*\n\n\nIn the code chunk below, tmap functions are used to plot the local Gi* (i.e. gi_star) at the province level.\n\n\n\nCode Chunk\ntmap_mode(\"plot\")\ntm_shape(HCSA) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.4 Visualising p-value of HCSA\n\n\nIn the code chunk below, tmap functions are used to plot the p-values of local Gi* (i.e. p_sim) at the province level.\n\n\n\nCode Chunk\ntmap_mode(\"plot\")\ntm_shape(HCSA) +\n  tm_fill(\"p_sim\") + \n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.5 Visuaising local HCSA\nFor effective comparison, you can plot both maps next to each other as shown below.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\ntmap_mode(\"plot\")\nmap1 &lt;- tm_shape(HCSA) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"Gi* of GDPPC\",\n            main.title.size = 0.8)\n\nmap2 &lt;- tm_shape(HCSA) +\n  tm_fill(\"p_value\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of Gi*\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)\n\n\n\n\n\n\n\n\n\n10.6 Visualising hot spot and cold spot areas\n\n\nNow, we are ready to plot the significant (i.e. p-values less than 0.05) hot spot and cold spot areas by using appropriate tmap functions as shown below.\n\n\n\nCode Chunk\nHCSA_sig &lt;- HCSA  %&gt;%\n  filter(p_sim &lt; 0.05)\ntmap_mode(\"plot\")\ntm_shape(HCSA) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(HCSA_sig) +\n  tm_fill(\"cluster\") + \n  tm_borders(alpha = 0.4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFigure above reveals that there is one hot spot area and two cold spot areas. Interestingly, the hot spot areas coincide with the High-high cluster identifies by using local Moran’s I method in the earlier sub-section."
  },
  {
    "objectID": "HandsOnExercise/HandsOn6/HandsOn6.html",
    "href": "HandsOnExercise/HandsOn6/HandsOn6.html",
    "title": "Hands-On Exercise 6",
    "section": "",
    "text": "In this hands-on exercise, you will gain hands-on experience on how to delineate homogeneous region by using geographically referenced multivariate data. There are two major analysis, namely:\n\nhierarchical cluster analysis; and\nspatially constrained cluster analysis.\n\n\n\nBy the end of this hands-on exercise, you will able:\n\nto convert GIS polygon data into R’s simple feature data.frame by using appropriate functions of sf package of R;\nto convert simple feature data.frame into R’s SpatialPolygonDataFrame object by using appropriate sf of package of R;\nto perform custer analysis by using hclust() of Base R;\nto perform spatially constrained cluster analysis using skater() of Base R; and\nto visualise the analysis output by using ggplot2 and tmap package."
  },
  {
    "objectID": "HandsOnExercise/HandsOn6/HandsOn6.html#overview",
    "href": "HandsOnExercise/HandsOn6/HandsOn6.html#overview",
    "title": "Hands-On Exercise 6",
    "section": "",
    "text": "In this hands-on exercise, you will gain hands-on experience on how to delineate homogeneous region by using geographically referenced multivariate data. There are two major analysis, namely:\n\nhierarchical cluster analysis; and\nspatially constrained cluster analysis.\n\n\n\nBy the end of this hands-on exercise, you will able:\n\nto convert GIS polygon data into R’s simple feature data.frame by using appropriate functions of sf package of R;\nto convert simple feature data.frame into R’s SpatialPolygonDataFrame object by using appropriate sf of package of R;\nto perform custer analysis by using hclust() of Base R;\nto perform spatially constrained cluster analysis using skater() of Base R; and\nto visualise the analysis output by using ggplot2 and tmap package."
  },
  {
    "objectID": "HandsOnExercise/HandsOn6/HandsOn6.html#getting-started",
    "href": "HandsOnExercise/HandsOn6/HandsOn6.html#getting-started",
    "title": "Hands-On Exercise 6",
    "section": "2 Getting Started",
    "text": "2 Getting Started\n\n2.1 The analytical question\nIn geobusiness and spatial policy, it is a common practice to delineate the market or planning area into homogeneous regions by using multivariate data. In this hands-on exercise, we are interested to delineate Shan State, Myanmar into homogeneous regions by using multiple Information and Communication technology (ICT) measures, namely: Radio, Television, Land line phone, Mobile phone, Computer, and Internet at home."
  },
  {
    "objectID": "HandsOnExercise/HandsOn6/HandsOn6.html#the-data",
    "href": "HandsOnExercise/HandsOn6/HandsOn6.html#the-data",
    "title": "Hands-On Exercise 6",
    "section": "3 The data",
    "text": "3 The data\nTwo data sets will be used in this study. They are:\n\nMyanmar Township Boundary Data (i.e. myanmar_township_boundaries) : This is a GIS data in ESRI shapefile format. It consists of township boundary information of Myanmar. The spatial data are captured in polygon features.\nShan-ICT.csv: This is an extract of The 2014 Myanmar Population and Housing Census Myanmar at the township level.\n\nBoth data sets are download from Myanmar Information Management Unit (MIMU)\n\n3.1 Installing and loading R packages\nBefore we get started, it is important for us to install the necessary R packages into R and launch these R packages into R environment.\nThe R packages needed for this exercise are as follows:\n\nSpatial data handling\n\nsf, rgdal and spdep\n\nAttribute data handling\n\ntidyverse, especially readr, ggplot2 and dplyr\n\nChoropleth mapping\n\ntmap\n\nMultivariate data visualisation and analysis\n\ncoorplot, ggpubr, and heatmaply\n\nCluster analysis\n\ncluster\nClustGeo\n\n\nThe code chunks below installs and launches these R packages into R environment.\n\n\nCode Chunk\npacman::p_load(spdep, tmap, sf, ClustGeo, \n               ggpubr, cluster, factoextra, NbClust,\n               heatmaply, corrplot, psych, tidyverse, GGally)\n\n\nNote: With tidyverse, we do not have to install readr, ggplot2 and dplyr packages separately. In fact, tidyverse also installs other very useful R packages such as tidyr."
  },
  {
    "objectID": "HandsOnExercise/HandsOn6/HandsOn6.html#data-import-and-prepatation",
    "href": "HandsOnExercise/HandsOn6/HandsOn6.html#data-import-and-prepatation",
    "title": "Hands-On Exercise 6",
    "section": "4 Data Import and Prepatation",
    "text": "4 Data Import and Prepatation\n\n4.1 Importing geospatial data into R environment\nIn this section, you will import Myanmar Township Boundary GIS data and its associated attrbiute table into R environment.\nThe Myanmar Township Boundary GIS data is in ESRI shapefile format. It will be imported into R environment by using the st_read() function of sf.\nThe code chunks used are shown below:\n\n\nCode Chunk\nshan_sf &lt;- st_read(dsn = \"data/geospatial\", \n                   layer = \"myanmar_township_boundaries\") %&gt;%\n  filter(ST %in% c(\"Shan (East)\", \"Shan (North)\", \"Shan (South)\")) %&gt;%\n  select(c(2:7))\n\n\nReading layer `myanmar_township_boundaries' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/HandsOnExercise/HandsOn6/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 330 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.17275 ymin: 9.671252 xmax: 101.1699 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\n\nThe imported township boundary object is called shan_sf. It is saved in simple feature data.frame format. We can view the content of the newly created shan_sf simple features data.frame by using the code chunk below.\n\n\nCode Chunk\nshan_sf\n\n\nSimple feature collection with 55 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 96.15107 ymin: 19.29932 xmax: 101.1699 ymax: 24.15907\nGeodetic CRS:  WGS 84\nFirst 10 features:\n             ST ST_PCODE       DT   DT_PCODE        TS  TS_PCODE\n1  Shan (North)   MMR015  Mongmit MMR015D008   Mongmit MMR015017\n2  Shan (South)   MMR014 Taunggyi MMR014D001   Pindaya MMR014006\n3  Shan (South)   MMR014 Taunggyi MMR014D001   Ywangan MMR014007\n4  Shan (South)   MMR014 Taunggyi MMR014D001  Pinlaung MMR014009\n5  Shan (North)   MMR015  Mongmit MMR015D008    Mabein MMR015018\n6  Shan (South)   MMR014 Taunggyi MMR014D001     Kalaw MMR014005\n7  Shan (South)   MMR014 Taunggyi MMR014D001     Pekon MMR014010\n8  Shan (South)   MMR014 Taunggyi MMR014D001  Lawksawk MMR014008\n9  Shan (North)   MMR015  Kyaukme MMR015D003 Nawnghkio MMR015013\n10 Shan (North)   MMR015  Kyaukme MMR015D003   Kyaukme MMR015012\n                         geometry\n1  MULTIPOLYGON (((96.96001 23...\n2  MULTIPOLYGON (((96.7731 21....\n3  MULTIPOLYGON (((96.78483 21...\n4  MULTIPOLYGON (((96.49518 20...\n5  MULTIPOLYGON (((96.66306 24...\n6  MULTIPOLYGON (((96.49518 20...\n7  MULTIPOLYGON (((97.14738 19...\n8  MULTIPOLYGON (((96.94981 22...\n9  MULTIPOLYGON (((96.75648 22...\n10 MULTIPOLYGON (((96.95498 22...\n\n\nNotice that sf.data.frame is conformed to Hardy Wickham’s tidy framework.\nSince shan_sf is conformed to tidy framework, we can also glimpse() to reveal the data type of it’s fields.\n\n\nCode Chunk\nglimpse(shan_sf)\n\n\nRows: 55\nColumns: 7\n$ ST       &lt;chr&gt; \"Shan (North)\", \"Shan (South)\", \"Shan (South)\", \"Shan (South)…\n$ ST_PCODE &lt;chr&gt; \"MMR015\", \"MMR014\", \"MMR014\", \"MMR014\", \"MMR015\", \"MMR014\", \"…\n$ DT       &lt;chr&gt; \"Mongmit\", \"Taunggyi\", \"Taunggyi\", \"Taunggyi\", \"Mongmit\", \"Ta…\n$ DT_PCODE &lt;chr&gt; \"MMR015D008\", \"MMR014D001\", \"MMR014D001\", \"MMR014D001\", \"MMR0…\n$ TS       &lt;chr&gt; \"Mongmit\", \"Pindaya\", \"Ywangan\", \"Pinlaung\", \"Mabein\", \"Kalaw…\n$ TS_PCODE &lt;chr&gt; \"MMR015017\", \"MMR014006\", \"MMR014007\", \"MMR014009\", \"MMR01501…\n$ geometry &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((96.96001 23..., MULTIPOLYGON (((…\n\n\n\n\n4.2 Importing aspatial data into R environment\nThe csv file will be import using read_csv function of readr package.\nThe code chunks used are shown below:\n\n\nCode Chunk\nict &lt;- read_csv (\"data/aspatial/Shan-ICT.csv\")\n\n\nThe imported InfoComm variables are extracted from The 2014 Myanmar Population and Housing Census Myanmar. The attribute data set is called ict. It is saved in R’s * tibble data.frame* format.\nThe code chunk below reveal the summary statistics of ict data.frame.\n\n\nCode Chunk\nsummary(ict)\n\n\n District Pcode     District Name      Township Pcode     Township Name     \n Length:55          Length:55          Length:55          Length:55         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n Total households     Radio         Television    Land line phone \n Min.   : 3318    Min.   :  115   Min.   :  728   Min.   :  20.0  \n 1st Qu.: 8711    1st Qu.: 1260   1st Qu.: 3744   1st Qu.: 266.5  \n Median :13685    Median : 2497   Median : 6117   Median : 695.0  \n Mean   :18369    Mean   : 4487   Mean   :10183   Mean   : 929.9  \n 3rd Qu.:23471    3rd Qu.: 6192   3rd Qu.:13906   3rd Qu.:1082.5  \n Max.   :82604    Max.   :30176   Max.   :62388   Max.   :6736.0  \n  Mobile phone      Computer      Internet at home\n Min.   :  150   Min.   :  20.0   Min.   :   8.0  \n 1st Qu.: 2037   1st Qu.: 121.0   1st Qu.:  88.0  \n Median : 3559   Median : 244.0   Median : 316.0  \n Mean   : 6470   Mean   : 575.5   Mean   : 760.2  \n 3rd Qu.: 7177   3rd Qu.: 507.0   3rd Qu.: 630.5  \n Max.   :48461   Max.   :6705.0   Max.   :9746.0  \n\n\nThere are a total of eleven fields and 55 observation in the tibble data.frame.\n\n\n4.3 Derive new variables using dplyr package\nThe unit of measurement of the values are number of household. Using these values directly will be bias by the underlying total number of households. In general, the townships with relatively higher total number of households will also have higher number of households owning radio, TV, etc.\nIn order to overcome this problem, we will derive the penetration rate of each ICT variable by using the code chunk below.\n\n\nCode Chunk\nict_derived &lt;- ict %&gt;%\n  mutate(`RADIO_PR` = `Radio`/`Total households`*1000) %&gt;%\n  mutate(`TV_PR` = `Television`/`Total households`*1000) %&gt;%\n  mutate(`LLPHONE_PR` = `Land line phone`/`Total households`*1000) %&gt;%\n  mutate(`MPHONE_PR` = `Mobile phone`/`Total households`*1000) %&gt;%\n  mutate(`COMPUTER_PR` = `Computer`/`Total households`*1000) %&gt;%\n  mutate(`INTERNET_PR` = `Internet at home`/`Total households`*1000) %&gt;%\n  rename(`DT_PCODE` =`District Pcode`,`DT`=`District Name`,\n         `TS_PCODE`=`Township Pcode`, `TS`=`Township Name`,\n         `TT_HOUSEHOLDS`=`Total households`,\n         `RADIO`=`Radio`, `TV`=`Television`, \n         `LLPHONE`=`Land line phone`, `MPHONE`=`Mobile phone`,\n         `COMPUTER`=`Computer`, `INTERNET`=`Internet at home`) \n\n\nLet us review the summary statistics of the newly derived penetration rates using the code chunk below.\n\n\nCode Chunk\nsummary(ict_derived)\n\n\n   DT_PCODE              DT              TS_PCODE              TS           \n Length:55          Length:55          Length:55          Length:55         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n TT_HOUSEHOLDS       RADIO             TV           LLPHONE      \n Min.   : 3318   Min.   :  115   Min.   :  728   Min.   :  20.0  \n 1st Qu.: 8711   1st Qu.: 1260   1st Qu.: 3744   1st Qu.: 266.5  \n Median :13685   Median : 2497   Median : 6117   Median : 695.0  \n Mean   :18369   Mean   : 4487   Mean   :10183   Mean   : 929.9  \n 3rd Qu.:23471   3rd Qu.: 6192   3rd Qu.:13906   3rd Qu.:1082.5  \n Max.   :82604   Max.   :30176   Max.   :62388   Max.   :6736.0  \n     MPHONE         COMPUTER         INTERNET         RADIO_PR     \n Min.   :  150   Min.   :  20.0   Min.   :   8.0   Min.   : 21.05  \n 1st Qu.: 2037   1st Qu.: 121.0   1st Qu.:  88.0   1st Qu.:138.95  \n Median : 3559   Median : 244.0   Median : 316.0   Median :210.95  \n Mean   : 6470   Mean   : 575.5   Mean   : 760.2   Mean   :215.68  \n 3rd Qu.: 7177   3rd Qu.: 507.0   3rd Qu.: 630.5   3rd Qu.:268.07  \n Max.   :48461   Max.   :6705.0   Max.   :9746.0   Max.   :484.52  \n     TV_PR         LLPHONE_PR       MPHONE_PR       COMPUTER_PR    \n Min.   :116.0   Min.   :  2.78   Min.   : 36.42   Min.   : 3.278  \n 1st Qu.:450.2   1st Qu.: 22.84   1st Qu.:190.14   1st Qu.:11.832  \n Median :517.2   Median : 37.59   Median :305.27   Median :18.970  \n Mean   :509.5   Mean   : 51.09   Mean   :314.05   Mean   :24.393  \n 3rd Qu.:606.4   3rd Qu.: 69.72   3rd Qu.:428.43   3rd Qu.:29.897  \n Max.   :842.5   Max.   :181.49   Max.   :735.43   Max.   :92.402  \n  INTERNET_PR     \n Min.   :  1.041  \n 1st Qu.:  8.617  \n Median : 22.829  \n Mean   : 30.644  \n 3rd Qu.: 41.281  \n Max.   :117.985  \n\n\nNotice that six new fields have been added into the data.frame. They are RADIO_PR, TV_PR, LLPHONE_PR, MPHONE_PR, COMPUTER_PR, and INTERNET_PR."
  },
  {
    "objectID": "HandsOnExercise/HandsOn6/HandsOn6.html#exploratory-data-analysis-eda",
    "href": "HandsOnExercise/HandsOn6/HandsOn6.html#exploratory-data-analysis-eda",
    "title": "Hands-On Exercise 6",
    "section": "5 Exploratory Data Analysis (EDA)",
    "text": "5 Exploratory Data Analysis (EDA)\n\n5.1 EDA using statistical graphics\nWe can plot the distribution of the variables (i.e. Number of households with radio) by using appropriate Exploratory Data Analysis (EDA) as shown in the code chunk below.\nHistogram is useful to identify the overall distribution of the data values (i.e. left skew, right skew or normal distribution)\n\n\nCode Chunk\nggplot(data=ict_derived, \n       aes(x=`RADIO`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\n\n\n\n\n\n\n\n\nBoxplot is useful to detect if there are outliers.\n\n\nCode Chunk\nggplot(data=ict_derived, \n       aes(x=`RADIO`)) +\n  geom_boxplot(color=\"black\", \n               fill=\"light blue\")\n\n\n\n\n\n\n\n\n\nNext, we will also plotting the distribution of the newly derived variables (i.e. Radio penetration rate) by using the code chunk below.\n\n\nCode Chunk\nggplot(data=ict_derived, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\nggplot(data=ict_derived, \n       aes(x=`RADIO_PR`)) +\n  geom_boxplot(color=\"black\", \n               fill=\"light blue\")\n\n\n\n\n\n\n\n\n\nWhat can you observed from the distributions reveal in the histogram and boxplot.\nIn the figure below, multiple histograms are plotted to reveal the distribution of the selected variables in the ict_derived data.frame.\n\n\n\n\n\n\n\n\n\nThe code chunks below are used to create the data visualisation. They consist of two main parts. First, we will create the individual histograms using the code chunk below.\n\n\nCode Chunk\nradio &lt;- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\ntv &lt;- ggplot(data=ict_derived, \n             aes(x= `TV_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\nllphone &lt;- ggplot(data=ict_derived, \n             aes(x= `LLPHONE_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\nmphone &lt;- ggplot(data=ict_derived, \n             aes(x= `MPHONE_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\ncomputer &lt;- ggplot(data=ict_derived, \n             aes(x= `COMPUTER_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\ninternet &lt;- ggplot(data=ict_derived, \n             aes(x= `INTERNET_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\n\nNext, the ggarrange() function of ggpubr package is used to group these histograms together.\n\n\nCode Chunk\nggarrange(radio, tv, llphone, mphone, computer, internet, \n          ncol = 3, \n          nrow = 2)\n\n\n\n\n5.2 EDA using choropleth map\n\n5.2.1 Joining geospatial data with aspatial data\nBefore we can prepare the choropleth map, we need to combine both the geospatial data object (i.e. shan_sf) and aspatial data.frame object (i.e. ict_derived) into one. This will be performed by using the left_join function of dplyr package. The shan_sf simple feature data.frame will be used as the base data object and the ict_derived data.frame will be used as the join table.\nThe code chunks below is used to perform the task. The unique identifier used to join both data objects is TS_PCODE.\n\n\nCode Chunk\nshan_sf &lt;- left_join(shan_sf, \n                     ict_derived, by=c(\"TS_PCODE\"=\"TS_PCODE\"))\n  \nwrite_rds(shan_sf, \"data/rds/shan_sf.rds\")\n\n\nThe message above shows that TS_CODE field is the common field used to perform the left-join.\nIt is important to note that there is no new output data been created. Instead, the data fields from ict_derived data frame are now updated into the data frame of shan_sf.\n\n\nCode Chunk\nshan_sf &lt;- read_rds(\"data/rds/shan_sf.rds\")\n\n\n\n\n5.2.2 Preparing a choropleth map\nTo have a quick look at the distribution of Radio penetration rate of Shan State at township level, a choropleth map will be prepared.\nThe code chunks below are used to prepare the choroplethby using the qtm() function of tmap package.\n\n\nCode Chunk\nqtm(shan_sf, \"RADIO_PR\")\n\n\n\n\n\n\n\n\n\nIn order to reveal the distribution shown in the choropleth map above are bias to the underlying total number of households at the townships, we will create two choropleth maps, one for the total number of households (i.e. TT_HOUSEHOLDS.map) and one for the total number of household with Radio (RADIO.map) by using the code chunk below.\n\n\nCode Chunk\nTT_HOUSEHOLDS.map &lt;- tm_shape(shan_sf) + \n  tm_fill(col = \"TT_HOUSEHOLDS\",\n          n = 5,\n          style = \"jenks\", \n          title = \"Total households\") + \n  tm_borders(alpha = 0.5) \n\nRADIO.map &lt;- tm_shape(shan_sf) + \n  tm_fill(col = \"RADIO\",\n          n = 5,\n          style = \"jenks\",\n          title = \"Number Radio \") + \n  tm_borders(alpha = 0.5) \n\ntmap_arrange(TT_HOUSEHOLDS.map, RADIO.map,\n             asp=NA, ncol=2)\n\n\n\n\n\n\n\n\n\nNotice that the choropleth maps above clearly show that townships with relatively larger number ot households are also showing relatively higher number of radio ownership.\nNow let us plot the choropleth maps showing the dsitribution of total number of households and Radio penetration rate by using the code chunk below.\n\n\nCode Chunk\ntm_shape(shan_sf) +\n    tm_polygons(c(\"TT_HOUSEHOLDS\", \"RADIO_PR\"),\n                style=\"jenks\") +\n    tm_facets(sync = TRUE, ncol = 2) +\n  tm_legend(legend.position = c(\"right\", \"bottom\"))+\n  tm_layout(outer.margins=0, asp=0)\n\n\n\n\n\n\n\n\n\nCan you identify the differences?"
  },
  {
    "objectID": "HandsOnExercise/HandsOn6/HandsOn6.html#correlation-analysis",
    "href": "HandsOnExercise/HandsOn6/HandsOn6.html#correlation-analysis",
    "title": "Hands-On Exercise 6",
    "section": "6 Correlation Analysis",
    "text": "6 Correlation Analysis\nBefore we perform cluster analysis, it is important for us to ensure that the cluster variables are not highly correlated.\nIn this section, you will learn how to use corrplot.mixed() function of corrplot package to visualise and analyse the correlation of the input variables.\n\n\nCode Chunk\ncluster_vars.cor = cor(ict_derived[,12:17])\ncorrplot.mixed(cluster_vars.cor,\n         lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\n\nThe correlation plot above shows that COMPUTER_PR and INTERNET_PR are highly correlated. This suggest that only one of them should be used in the cluster analysis instead of both."
  },
  {
    "objectID": "HandsOnExercise/HandsOn6/HandsOn6.html#hierarchy-cluster-analysis",
    "href": "HandsOnExercise/HandsOn6/HandsOn6.html#hierarchy-cluster-analysis",
    "title": "Hands-On Exercise 6",
    "section": "7 Hierarchy Cluster Analysis",
    "text": "7 Hierarchy Cluster Analysis\nIn this section, you will learn how to perform hierarchical cluster analysis. The analysis consists of four major steps:\n\n7.1 Extracting clustering variables\nThe code chunk below will be used to extract the clustering variables from the shan_sf simple feature object into data.frame.\n\n\nCode Chunk\ncluster_vars &lt;- shan_sf %&gt;%\n  st_set_geometry(NULL) %&gt;%\n  select(\"TS.x\", \"RADIO_PR\", \"TV_PR\", \"LLPHONE_PR\", \"MPHONE_PR\", \"COMPUTER_PR\")\nhead(cluster_vars,10)\n\n\n        TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\n1    Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\n2    Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\n3    Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\n4   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\n5     Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\n6      Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\n7      Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\n8   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\n9  Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\n10   Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\nNotice that the final clustering variables list does not include variable INTERNET_PR because it is highly correlated with variable COMPUTER_PR.\nNext, we need to change the rows by township name instead of row number by using the code chunk below\n\n\nCode Chunk\nrow.names(cluster_vars) &lt;- cluster_vars$\"TS.x\"\nhead(cluster_vars,10)\n\n\n               TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\nMongmit     Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\nPindaya     Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\nYwangan     Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\nPinlaung   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\nMabein       Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\nKalaw         Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\nPekon         Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\nLawksawk   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\nNawnghkio Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\nKyaukme     Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\nNotice that the row number has been replaced into the township name.\nNow, we will delete the TS.x field by using the code chunk below.\n\n\nCode Chunk\nshan_ict &lt;- select(cluster_vars, c(2:6))\nhead(shan_ict, 10)\n\n\n          RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\nMongmit   286.1852 554.1313   35.30618  260.6944    12.15939\nPindaya   417.4647 505.1300   19.83584  162.3917    12.88190\nYwangan   484.5215 260.5734   11.93591  120.2856     4.41465\nPinlaung  231.6499 541.7189   28.54454  249.4903    13.76255\nMabein    449.4903 708.6423   72.75255  392.6089    16.45042\nKalaw     280.7624 611.6204   42.06478  408.7951    29.63160\nPekon     318.6118 535.8494   39.83270  214.8476    18.97032\nLawksawk  387.1017 630.0035   31.51366  320.5686    21.76677\nNawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\nKyaukme   210.9548 601.1773   39.58267  372.4930    30.94709\n\n\n\n\nCode Chunk\nwrite_rds(shan_ict, \"data/rds/shan_ict.rds\")\n\n\n\n\nCode Chunk\nshan_ict &lt;- read_rds(\"data/rds/shan_ict.rds\")\n\n\n\n\n7.2 Data Standardisation\nIn general, multiple variables will be used in cluster analysis. It is not unusual their values range are different. In order to avoid the cluster analysis result is baised to clustering variables with large values, it is useful to standardise the input variables before performing cluster analysis.\n\n\n7.3 Min-Max standardisation\nIn the code chunk below, normalize() of heatmaply package is used to stadardisation the clustering variables by using Min-Max method. The summary() is then used to display the summary statistics of the standardised clustering variables.\n\n\nCode Chunk\nshan_ict.std &lt;- normalize(shan_ict)\nsummary(shan_ict.std)\n\n\n    RADIO_PR          TV_PR          LLPHONE_PR       MPHONE_PR     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.2544   1st Qu.:0.4600   1st Qu.:0.1123   1st Qu.:0.2199  \n Median :0.4097   Median :0.5523   Median :0.1948   Median :0.3846  \n Mean   :0.4199   Mean   :0.5416   Mean   :0.2703   Mean   :0.3972  \n 3rd Qu.:0.5330   3rd Qu.:0.6750   3rd Qu.:0.3746   3rd Qu.:0.5608  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n  COMPUTER_PR     \n Min.   :0.00000  \n 1st Qu.:0.09598  \n Median :0.17607  \n Mean   :0.23692  \n 3rd Qu.:0.29868  \n Max.   :1.00000  \n\n\nNotice that the values range of the Min-max standardised clustering variables are 0-1 now.\n\n\n7.4 Z-score standardisation\nZ-score standardisation can be performed easily by using scale() of Base R. The code chunk below will be used to stadardisation the clustering variables by using Z-score method.\n\n\nCode Chunk\nshan_ict.z &lt;- scale(shan_ict)\ndescribe(shan_ict.z)\n\n\n            vars  n mean sd median trimmed  mad   min  max range  skew kurtosis\nRADIO_PR       1 55    0  1  -0.04   -0.06 0.94 -1.85 2.55  4.40  0.48    -0.27\nTV_PR          2 55    0  1   0.05    0.04 0.78 -2.47 2.09  4.56 -0.38    -0.23\nLLPHONE_PR     3 55    0  1  -0.33   -0.15 0.68 -1.19 3.20  4.39  1.37     1.49\nMPHONE_PR      4 55    0  1  -0.05   -0.06 1.01 -1.58 2.40  3.98  0.48    -0.34\nCOMPUTER_PR    5 55    0  1  -0.26   -0.18 0.64 -1.03 3.31  4.34  1.80     2.96\n              se\nRADIO_PR    0.13\nTV_PR       0.13\nLLPHONE_PR  0.13\nMPHONE_PR   0.13\nCOMPUTER_PR 0.13\n\n\nNotice the mean and standard deviation of the Z-score standardised clustering variables are 0 and 1 respectively.\nNote: describe() of psych package is used here instead of summary() of Base R because the earlier provides standard deviation.\nWarning: Z-score standardisation method should only be used if we would assume all variables come from some normal distribution.\n\n\n7.5 Visualising the standardised clustering variables\nBeside reviewing the summary statistics of the standardised clustering variables, it is also a good practice to visualise their distribution graphical.\nThe code chunk below plot the scaled Radio_PR field.\n\n\nCode Chunk\nr &lt;- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  ggtitle(\"Raw values without standardisation\")\n\nshan_ict_s_df &lt;- as.data.frame(shan_ict.std)\ns &lt;- ggplot(data=shan_ict_s_df, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  ggtitle(\"Min-Max Standardisation\")\n\nshan_ict_z_df &lt;- as.data.frame(shan_ict.z)\nz &lt;- ggplot(data=shan_ict_z_df, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  ggtitle(\"Z-score Standardisation\")\n\nggarrange(r, s, z,\n          ncol = 3,\n          nrow = 1)\n\n\n\n\n\n\n\n\n\n\nWhat statistical conclusion can you draw from the histograms above?\n\n\n\nCode Chunk\nr &lt;- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"light blue\") +\n  ggtitle(\"Raw values without standardisation\")\n\nshan_ict_s_df &lt;- as.data.frame(shan_ict.std)\ns &lt;- ggplot(data=shan_ict_s_df, \n       aes(x=`RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"light blue\") +\n  ggtitle(\"Min-Max Standardisation\")\n\nshan_ict_z_df &lt;- as.data.frame(shan_ict.z)\nz &lt;- ggplot(data=shan_ict_z_df, \n       aes(x=`RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"light blue\") +\n  ggtitle(\"Z-score Standardisation\")\n\nggarrange(r, s, z,\n          ncol = 3,\n          nrow = 1)\n\n\n\n\n\n\n\n\n\n\n\n7.6 Computing proximity matrix\nIn R, many packages provide functions to calculate distance matrix. We will compute the proximity matrix by using dist() of R.\ndist() supports six distance proximity calculations, they are: euclidean, maximum, manhattan, canberra, binary and minkowski. The default is euclidean proximity matrix.\nThe code chunk below is used to compute the proximity matrix using euclidean method.\n\n\nCode Chunk\nproxmat &lt;- dist(shan_ict, method = 'euclidean')\n\n\nThe code chunk below can then be used to list the content of proxmat for visual inspection.\n\n\nCode Chunk\nproxmat\n\n\n\n\n7.7 Computing hierarchical clustering\nIn R, there are several packages provide hierarchical clustering function. In this hands-on exercise, hclust() of R stats will be used.\nhclust() employed agglomeration method to compute the cluster. Eight clustering algorithms are supported, they are: ward.D, ward.D2, single, complete, average(UPGMA), mcquitty(WPGMA), median(WPGMC) and centroid(UPGMC).\nThe code chunk below performs hierarchical cluster analysis using ward.D method. The hierarchical clustering output is stored in an object of class hclust which describes the tree produced by the clustering process.\n\n\nCode Chunk\nhclust_ward &lt;- hclust(proxmat, method = 'ward.D')\n\n\nWe can then plot the tree by using plot() of R Graphics as shown in the code chunk below.\n\n\nCode Chunk\nplot(hclust_ward, cex = 0.6)\n\n\n\n\n\n\n\n\n\n\n\n7.8 Selecting the optimal clustering algorithm\nOne of the challenge in performing hierarchical clustering is to identify stronger clustering structures. The issue can be solved by using use agnes() function of cluster package. It functions like hclus(), however, with the agnes() function you can also get the agglomerative coefficient, which measures the amount of clustering structure found (values closer to 1 suggest strong clustering structure).\nThe code chunk below will be used to compute the agglomerative coefficients of all hierarchical clustering algorithms.\n\n\nCode Chunk\nm &lt;- c( \"average\", \"single\", \"complete\", \"ward\")\nnames(m) &lt;- c( \"average\", \"single\", \"complete\", \"ward\")\n\nac &lt;- function(x) {\n  agnes(shan_ict, method = x)$ac\n}\n\nmap_dbl(m, ac)\n\n\n  average    single  complete      ward \n0.8131144 0.6628705 0.8950702 0.9427730 \n\n\nWith reference to the output above, we can see that Ward’s method provides the strongest clustering structure among the four methods assessed. Hence, in the subsequent analysis, only Ward’s method will be used.\n\n\n7.9 Determining Optimal Clusters\nAnother technical challenge face by data analyst in performing clustering analysis is to determine the optimal clusters to retain.\nThere are three commonly used methods to determine the optimal clusters, they are:\n\nElbow Method\nAverage Silhouette Method\nGap Statistic Method\n\n\n7.9.1 Gap Statistic Method\nThe gap statistic compares the total within intra-cluster variation for different values of k with their expected values under null reference distribution of the data. The estimate of the optimal clusters will be value that maximize the gap statistic (i.e., that yields the largest gap statistic). This means that the clustering structure is far away from the random uniform distribution of points.\nTo compute the gap statistic, clusGap() of cluster package will be used.\n\n\nCode Chunk\nset.seed(12345)\ngap_stat &lt;- clusGap(shan_ict, \n                    FUN = hcut, \n                    nstart = 25, \n                    K.max = 10, \n                    B = 50)\n# Print the result\nprint(gap_stat, method = \"firstmax\")\n\n\nClustering Gap statistic [\"clusGap\"] from call:\nclusGap(x = shan_ict, FUNcluster = hcut, K.max = 10, B = 50, nstart = 25)\nB=50 simulated reference sets, k = 1..10; spaceH0=\"scaledPCA\"\n --&gt; Number of clusters (method 'firstmax'): 1\n          logW   E.logW       gap     SE.sim\n [1,] 8.407129 8.680794 0.2736651 0.04460994\n [2,] 8.130029 8.350712 0.2206824 0.03880130\n [3,] 7.992265 8.202550 0.2102844 0.03362652\n [4,] 7.862224 8.080655 0.2184311 0.03784781\n [5,] 7.756461 7.978022 0.2215615 0.03897071\n [6,] 7.665594 7.887777 0.2221833 0.03973087\n [7,] 7.590919 7.806333 0.2154145 0.04054939\n [8,] 7.526680 7.731619 0.2049390 0.04198644\n [9,] 7.458024 7.660795 0.2027705 0.04421874\n[10,] 7.377412 7.593858 0.2164465 0.04540947\n\n\nAlso note that the hcut function used is from factoextra package.\nNext, we can visualise the plot by using fviz_gap_stat() of factoextra package.\n\n\nCode Chunk\nfviz_gap_stat(gap_stat)\n\n\n\n\n\n\n\n\n\nWith reference to the gap statistic graph above, the recommended number of cluster to retain is 1. However, it is not logical to retain only one cluster. By examine the gap statistic graph, the 6-cluster gives the largest gap statistic and should be the next best cluster to pick.\nNote: In addition to these commonly used approaches, the NbClust package, published by Charrad et al., 2014, provides 30 indices for determining the relevant number of clusters and proposes to users the best clustering scheme from the different results obtained by varying all combinations of number of clusters, distance measures, and clustering methods.\n\n\n\n7.10 Interpreting the dendrograms\nIn the dendrogram displayed above, each leaf corresponds to one observation. As we move up the tree, observations that are similar to each other are combined into branches, which are themselves fused at a higher height.\nThe height of the fusion, provided on the vertical axis, indicates the (dis)similarity between two observations. The higher the height of the fusion, the less similar the observations are. Note that, conclusions about the proximity of two observations can be drawn only based on the height where branches containing those two observations first are fused. We cannot use the proximity of two observations along the horizontal axis as a criteria of their similarity.\nIt’s also possible to draw the dendrogram with a border around the selected clusters by using rect.hclust() of R stats. The argument border is used to specify the border colors for the rectangles.\n\n\nCode Chunk\nplot(hclust_ward, cex = 0.6)\nrect.hclust(hclust_ward, \n            k = 6, \n            border = 2:5)\n\n\n\n\n\n\n\n\n\n\n\n7.11 Visually-driven hierarchical clustering analysis\nIn this section, we will learn how to perform visually-driven hiearchical clustering analysis by using heatmaply package.\nWith heatmaply, we are able to build both highly interactive cluster heatmap or static cluster heatmap.\n\n7.11.1 Transforming the data frame into a matrix\nThe data was loaded into a data frame, but it has to be a data matrix to make your heatmap.\nThe code chunk below will be used to transform shan_ict data frame into a data matrix.\n\n\nCode Chunk\nshan_ict_mat &lt;- data.matrix(shan_ict)\n\n\n\n\n7.11.2 Plotting interactive cluster heatmap using heatmaply()\nIn the code chunk below, the heatmaply() of heatmaply package is used to build an interactive cluster heatmap.\n\n\nCode Chunk\nheatmaply(normalize(shan_ict_mat),\n          Colv=NA,\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\",\n          seriate = \"OLO\",\n          colors = Blues,\n          k_row = 6,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"Geographic Segmentation of Shan State by ICT indicators\",\n          xlab = \"ICT Indicators\",\n          ylab = \"Townships of Shan State\"\n          )\n\n\n\n\n\n\n\n\n\n7.12 Mapping the clusters formed\nWith closed examination of the dendragram above, we have decided to retain six clusters.\ncutree() of R Base will be used in the code chunk below to derive a 6-cluster model.\n\n\nCode Chunk\ngroups &lt;- as.factor(cutree(hclust_ward, k=6))\n\n\nThe output is called groups. It is a list object.\nIn order to visualise the clusters, the groups object need to be appended onto shan_sf simple feature object.\nThe code chunk below form the join in three steps:\n\nthe groups list object will be converted into a matrix;\ncbind() is used to append groups matrix onto shan_sf to produce an output simple feature object called shan_sf_cluster; and\nrename of dplyr package is used to rename as.matrix.groups field as CLUSTER.\n\n\n\nCode Chunk\nshan_sf_cluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER`=`as.matrix.groups.`)\n\n\nNext, qtm() of tmap package is used to plot the choropleth map showing the cluster formed.\n\n\nCode Chunk\nqtm(shan_sf_cluster, \"CLUSTER\")\n\n\n\n\n\n\n\n\n\nThe choropleth map above reveals the clusters are very fragmented. The is one of the major limitation when non-spatial clustering algorithm such as hierarchical cluster analysis method is used."
  },
  {
    "objectID": "HandsOnExercise/HandsOn6/HandsOn6.html#spatially-constrained-clustering-skater-approach",
    "href": "HandsOnExercise/HandsOn6/HandsOn6.html#spatially-constrained-clustering-skater-approach",
    "title": "Hands-On Exercise 6",
    "section": "8 Spatially Constrained Clustering: SKATER approach",
    "text": "8 Spatially Constrained Clustering: SKATER approach\nIn this section, you will learn how to derive spatially constrained cluster by using skater() method of spdep package.\n\n8.1 Converting into SpatialPolygonsDataFrame\nFirst, we need to convert shan_sf into SpatialPolygonsDataFrame. This is because SKATER function only support sp objects such as SpatialPolygonDataFrame.\nThe code chunk below uses as_Spatial() of sf package to convert shan_sf into a SpatialPolygonDataFrame called shan_sp.\n\n\nCode Chunk\nshan_sp &lt;- as_Spatial(shan_sf)\n\n\n\n\n8.2 Computing Neighbour List\nNext, poly2nd() of spdep package will be used to compute the neighbours list from polygon list.\n\n\nCode Chunk\nshan.nb &lt;- poly2nb(shan_sp)\nsummary(shan.nb)\n\n\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\n\nWe can plot the neighbours list on shan_sp by using the code chunk below. Since we now can plot the community area boundaries as well, we plot this graph on top of the map. The first plot command gives the boundaries. This is followed by the plot of the neighbor list object, with coordinates applied to the original SpatialPolygonDataFrame (Shan state township boundaries) to extract the centroids of the polygons. These are used as the nodes for the graph representation. We also set the color to blue and specify add=TRUE to plot the network on top of the boundaries.\n\n\nCode Chunk\ncoords &lt;- st_coordinates(\n  st_centroid(st_geometry(shan_sf)))\n\n\n\n\nCode Chunk\nplot(st_geometry(shan_sf), \n     border=grey(.5))\nplot(shan.nb,\n     coords, \n     col=\"blue\", \n     add=TRUE)\n\n\n\n\n\n\n\n\n\nNote that if you plot the network first and then the boundaries, some of the areas will be clipped. This is because the plotting area is determined by the characteristics of the first plot. In this example, because the boundary map extends further than the graph, we plot it first.\n\n\n8.3 Computing minimum spanning tree\n\n8.3.1 Calculating edge costs\nNext, nbcosts() of spdep package is used to compute the cost of each edge. It is the distance between it nodes. This function compute this distance using a data.frame with observations vector in each node.\nThe code chunk below is used to compute the cost of each edge.\n\n\nCode Chunk\nlcosts &lt;- nbcosts(shan.nb, shan_ict)\n\n\nFor each observation, this gives the pairwise dissimilarity between its values on the five variables and the values for the neighbouring observation (from the neighbour list). Basically, this is the notion of a generalised weight for a spatial weights matrix.\nNext, We will incorporate these costs into a weights object in the same way as we did in the calculation of inverse of distance weights. In other words, we convert the neighbour list to a list weights object by specifying the just computed lcosts as the weights.\nIn order to achieve this, nb2listw() of spdep package is used as shown in the code chunk below.\nNote that we specify the style as B to make sure the cost values are not row-standardised.\n\n\nCode Chunk\nshan.w &lt;- nb2listw(shan.nb, \n                   lcosts, \n                   style=\"B\")\nsummary(shan.w)\n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\nWeights style: B \nWeights constants summary:\n   n   nn       S0       S1        S2\nB 55 3025 76267.65 58260785 522016004\n\n\n\n\n\n8.4 Computing minimum spanning tree\nThe minimum spanning tree is computed by mean of the mstree() of spdep package as shown in the code chunk below.\n\n\nCode Chunk\nshan.mst &lt;- mstree(shan.w)\n\n\nAfter computing the MST, we can check its class and dimension by using the code chunk below.\n\n\nCode Chunk\nclass(shan.mst)\n\n\n[1] \"mst\"    \"matrix\"\n\n\nCode Chunk\ndim(shan.mst)\n\n\n[1] 54  3\n\n\nNote that the dimension is 54 and not 55. This is because the minimum spanning tree consists on n-1 edges (links) in order to traverse all the nodes.\nWe can display the content of shan.mst by using head() as shown in the code chunk below.\n\n\nCode Chunk\nhead(shan.mst)\n\n\n     [,1] [,2]      [,3]\n[1,]   54   48  47.79331\n[2,]   54   17 109.08506\n[3,]   54   45 127.42203\n[4,]   45   52 146.78891\n[5,]   52   13 110.55197\n[6,]   13   28  92.79567\n\n\nThe plot method for the MST include a way to show the observation numbers of the nodes in addition to the edge. As before, we plot this together with the township boundaries. We can see how the initial neighbour list is simplified to just one edge connecting each of the nodes, while passing through all the nodes.\n\n\nCode Chunk\nplot(st_geometry(shan_sf), \n                 border=gray(.5))\nplot.mst(shan.mst, \n         coords, \n         col=\"blue\", \n         cex.lab=0.7, \n         cex.circles=0.005, \n         add=TRUE)\n\n\n\n\n\n\n\n\n\n\n\n8.5 Computing spatially constrained clusters using SKATER method\nThe code chunk below compute the spatially constrained cluster using skater() of spdep package.\n\n\nCode Chunk\nclust6 &lt;- spdep::skater(edges = shan.mst[,1:2], \n                 data = shan_ict, \n                 method = \"euclidean\", \n                 ncuts = 5)\n\n\nThe skater() takes three mandatory arguments: - the first two columns of the MST matrix (i.e. not the cost), - the data matrix (to update the costs as units are being grouped), and - the number of cuts. Note: It is set to one less than the number of clusters. So, the value specified is not the number of clusters, but the number of cuts in the graph, one less than the number of clusters.\nThe result of the skater() is an object of class skater. We can examine its contents by using the code chunk below.\n\n\nCode Chunk\nstr(clust6)\n\n\nList of 8\n $ groups      : num [1:55] 3 3 6 3 3 3 3 3 3 3 ...\n $ edges.groups:List of 6\n  ..$ :List of 3\n  .. ..$ node: num [1:22] 13 48 54 55 45 37 34 16 25 52 ...\n  .. ..$ edge: num [1:21, 1:3] 48 55 54 37 34 16 45 25 13 13 ...\n  .. ..$ ssw : num 3423\n  ..$ :List of 3\n  .. ..$ node: num [1:18] 47 27 53 38 42 15 41 51 43 32 ...\n  .. ..$ edge: num [1:17, 1:3] 53 15 42 38 41 51 15 27 15 43 ...\n  .. ..$ ssw : num 3759\n  ..$ :List of 3\n  .. ..$ node: num [1:11] 2 6 8 1 36 4 10 9 46 5 ...\n  .. ..$ edge: num [1:10, 1:3] 6 1 8 36 4 6 8 10 10 9 ...\n  .. ..$ ssw : num 1458\n  ..$ :List of 3\n  .. ..$ node: num [1:2] 44 20\n  .. ..$ edge: num [1, 1:3] 44 20 95\n  .. ..$ ssw : num 95\n  ..$ :List of 3\n  .. ..$ node: num 23\n  .. ..$ edge: num[0 , 1:3] \n  .. ..$ ssw : num 0\n  ..$ :List of 3\n  .. ..$ node: num 3\n  .. ..$ edge: num[0 , 1:3] \n  .. ..$ ssw : num 0\n $ not.prune   : NULL\n $ candidates  : int [1:6] 1 2 3 4 5 6\n $ ssto        : num 12613\n $ ssw         : num [1:6] 12613 10977 9962 9540 9123 ...\n $ crit        : num [1:2] 1 Inf\n $ vec.crit    : num [1:55] 1 1 1 1 1 1 1 1 1 1 ...\n - attr(*, \"class\")= chr \"skater\"\n\n\nThe most interesting component of this list structure is the groups vector containing the labels of the cluster to which each observation belongs (as before, the label itself is arbitary). This is followed by a detailed summary for each of the clusters in the edges.groups list. Sum of squares measures are given as ssto for the total and ssw to show the effect of each of the cuts on the overall criterion.\nWe can check the cluster assignment by using the conde chunk below.\n\n\nCode Chunk\nccs6 &lt;- clust6$groups\nccs6\n\n\n [1] 3 3 6 3 3 3 3 3 3 3 2 1 1 1 2 1 1 1 2 4 1 2 5 1 1 1 2 1 2 2 1 2 2 1 1 3 1 2\n[39] 2 2 2 2 2 4 1 3 2 1 1 1 2 1 2 1 1\n\n\nWe can find out how many observations are in each cluster by means of the table command. Parenthetially, we can also find this as the dimension of each vector in the lists contained in edges.groups. For example, the first list has node with dimension 12, which is also the number of observations in the first cluster.\n\n\nCode Chunk\ntable(ccs6)\n\n\nccs6\n 1  2  3  4  5  6 \n22 18 11  2  1  1 \n\n\nLastly, we can also plot the pruned tree that shows the five clusters on top of the townshop area.\n\n\nCode Chunk\nplot(st_geometry(shan_sf), \n     border=gray(.5))\nplot(clust6, \n     coords, \n     cex.lab=.7,\n     groups.colors=c(\"red\",\"green\",\"blue\", \"brown\", \"pink\"),\n     cex.circles=0.005, \n     add=TRUE)\n\n\n\n\n\n\n\n\n\n\n\n8.6 Visualising the clusters in choropleth map\nThe code chunk below is used to plot the newly derived clusters by using SKATER method.\n\n\nCode Chunk\ngroups_mat &lt;- as.matrix(clust6$groups)\nshan_sf_spatialcluster &lt;- cbind(shan_sf_cluster, as.factor(groups_mat)) %&gt;%\n  rename(`SP_CLUSTER`=`as.factor.groups_mat.`)\nqtm(shan_sf_spatialcluster, \"SP_CLUSTER\")\n\n\n\n\n\n\n\n\n\nFor easy comparison, it will be better to place both the hierarchical clustering and spatially constrained hierarchical clustering maps next to each other.\n\n\nCode Chunk\nhclust.map &lt;- qtm(shan_sf_cluster,\n                  \"CLUSTER\") + \n  tm_borders(alpha = 0.5) \n\nshclust.map &lt;- qtm(shan_sf_spatialcluster,\n                   \"SP_CLUSTER\") + \n  tm_borders(alpha = 0.5) \n\ntmap_arrange(hclust.map, shclust.map,\n             asp=NA, ncol=2)"
  },
  {
    "objectID": "HandsOnExercise/HandsOn6/HandsOn6.html#spatially-constrained-clustering-clustgeo-method",
    "href": "HandsOnExercise/HandsOn6/HandsOn6.html#spatially-constrained-clustering-clustgeo-method",
    "title": "Hands-On Exercise 6",
    "section": "9 Spatially Constrained Clustering: ClustGeo Method",
    "text": "9 Spatially Constrained Clustering: ClustGeo Method\nIn this section, you will gain hands-on experience on using functions provided by ClustGeo package to perform non-spatially constrained hierarchical cluster analysis and spatially constrained cluster analysis.\n\n9.1 A short note about ClustGeo package\nClustGeo package is an R package specially designed to support the need of performing spatially constrained cluster analysis. More specifically, it provides a Ward-like hierarchical clustering algorithm called hclustgeo() including spatial/geographical constraints.\nIn the nutshell, the algorithm uses two dissimilarity matrices D0 and D1 along with a mixing parameter alpha, whereby the value of alpha must be a real number between [0, 1]. D0 can be non-Euclidean and the weights of the observations can be non-uniform. It gives the dissimilarities in the attribute/clustering variable space. D1, on the other hand, gives the dissimilarities in the constraint space. The criterion minimised at each stage is a convex combination of the homogeneity criterion calculated with D0 and the homogeneity criterion calculated with D1.\nThe idea is then to determine a value of alpha which increases the spatial contiguity without deteriorating too much the quality of the solution based on the variables of interest. This need is supported by a function called choicealpha().\n\n\n9.2 Ward-like hierarchical clustering: ClustGeo\nClustGeo package provides function called hclustgeo() to perform a typical Ward-like hierarchical clustering just like hclust() you learned in previous section.\nTo perform non-spatially constrained hierarchical clustering, we only need to provide the function a dissimilarity matrix as shown in the code chunk below.\n\n\nCode Chunk\nnongeo_cluster &lt;- hclustgeo(proxmat)\nplot(nongeo_cluster, cex = 0.5)\nrect.hclust(nongeo_cluster, \n            k = 6, \n            border = 2:5)\n\n\n\n\n\n\n\n\n\nNote that the dissimilarity matrix must be an object of class dist, i.e. an object obtained with the function dist(). For sample code chunk, please refer to 5.7.6 Computing proximity matrix\n\n9.2.1 Mapping the clusters formed\nSimilarly, we can plot the clusters on a categorical area shaded map by using the steps we learned in 5.7.12 Mapping the clusters formed.\n\n\nCode Chunk\ngroups &lt;- as.factor(cutree(nongeo_cluster, k=6))\n\n\n\n\nCode Chunk\nshan_sf_ngeo_cluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER` = `as.matrix.groups.`)\n\n\n\n\nCode Chunk\nqtm(shan_sf_ngeo_cluster, \"CLUSTER\")\n\n\n\n\n\n\n\n\n\n\n\n\n9.3 Spatially Constrained Hierarchical Clustering\nBefore we can performed spatially constrained hierarchical clustering, a spatial distance matrix will be derived by using st_distance() of sf package.\n\n\nCode Chunk\ndist &lt;- st_distance(shan_sf, shan_sf)\ndistmat &lt;- as.dist(dist)\n\n\nNotice that as.dist() is used to convert the data frame into matrix.\nNext, choicealpha() will be used to determine a suitable value for the mixing parameter alpha as shown in the code chunk below.\n\n\nCode Chunk\ncr &lt;- choicealpha(proxmat, distmat, range.alpha = seq(0, 1, 0.1), K=6, graph = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith reference to the graphs above, alpha = 0.2 will be used as shown in the code chunk below.\n\n\nCode Chunk\nclustG &lt;- hclustgeo(proxmat, distmat, alpha = 0.2)\n\n\nNext, cutree() is used to derive the cluster objecct.\n\n\nCode Chunk\ngroups &lt;- as.factor(cutree(clustG, k=6))\n\n\nWe will then join back the group list with shan_sf polygon feature data frame by using the code chunk below.\n\n\nCode Chunk\nshan_sf_Gcluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER` = `as.matrix.groups.`)\n\n\nWe can now plot the map of the newly delineated spatially constrained clusters.\n\n\nCode Chunk\nqtm(shan_sf_Gcluster, \"CLUSTER\")"
  },
  {
    "objectID": "HandsOnExercise/HandsOn6/HandsOn6.html#visual-interpretation-of-clusters",
    "href": "HandsOnExercise/HandsOn6/HandsOn6.html#visual-interpretation-of-clusters",
    "title": "Hands-On Exercise 6",
    "section": "10 Visual Interpretation of Clusters",
    "text": "10 Visual Interpretation of Clusters\n\n10.1 Visualising individual clustering variable\nCode chunk below is used to reveal the distribution of a clustering variable (i.e RADIO_PR) by cluster.\n\n\nCode Chunk\nggplot(data = shan_sf_ngeo_cluster,\n       aes(x = CLUSTER, y = RADIO_PR)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nThe boxplot reveals Cluster 3 displays the highest mean Radio Ownership Per Thousand Household. This is followed by Cluster 2, 1, 4, 6 and 5.\n\n\n10.2 Multivariate Visualisation\nPast studies shown that parallel coordinate plot can be used to reveal clustering variables by cluster very effectively. In the code chunk below, ggparcoord() of GGally package\n\n\nCode Chunk\nggparcoord(data = shan_sf_ngeo_cluster, \n           columns = c(17:21), \n           scale = \"globalminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of ICT Variables by Cluster\") +\n  facet_grid(~ CLUSTER) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\n\n\n\n\n\nThe parallel coordinate plot above reveals that households in Cluster 4 townships tend to own the highest number of TV and mobile-phone. On the other hand, households in Cluster 5 tends to own the lowest of all the five ICT.\nNote that the scale argument of ggparcoor() provide several methods to scale the clustering variables. They are:\n\nstd: univariately, subtract mean and divide by standard deviation.\nrobust: univariately, subtract median and divide by median absolute deviation.\nuniminmax: univariately, scale so the minimum of the variable is zero, and the maximum is one.\nglobalminmax: no scaling is done; the range of the graphs is defined by the global minimum and the global maximum.\ncenter: use uniminmax to standardize vertical height, then center each variable at a value specified by the scaleSummary param.\ncenterObs: use uniminmax to standardize vertical height, then center each variable at the value of the observation specified by the centerObsID param\n\nThere is no one best scaling method to use. You should explore them and select the one that best meet your analysis need.\nLast but not least, we can also compute the summary statistics such as mean, median, sd, etc to complement the visual interpretation.\nIn the code chunk below, group_by() and summarise() of dplyr are used to derive mean values of the clustering variables.\n\n\nCode Chunk\nshan_sf_ngeo_cluster %&gt;% \n  st_set_geometry(NULL) %&gt;%\n  group_by(CLUSTER) %&gt;%\n  summarise(mean_RADIO_PR = mean(RADIO_PR),\n            mean_TV_PR = mean(TV_PR),\n            mean_LLPHONE_PR = mean(LLPHONE_PR),\n            mean_MPHONE_PR = mean(MPHONE_PR),\n            mean_COMPUTER_PR = mean(COMPUTER_PR))\n\n\n# A tibble: 6 × 6\n  CLUSTER mean_RADIO_PR mean_TV_PR mean_LLPHONE_PR mean_MPHONE_PR\n  &lt;chr&gt;           &lt;dbl&gt;      &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;\n1 1               221.        521.            44.2           246.\n2 2               237.        402.            23.9           134.\n3 3               300.        611.            52.2           392.\n4 4               196.        744.            99.0           651.\n5 5               124.        224.            38.0           132.\n6 6                98.6       499.            74.5           468.\n# ℹ 1 more variable: mean_COMPUTER_PR &lt;dbl&gt;"
  },
  {
    "objectID": "HandsOnExercise/HandsOn7/HandsOn7.html",
    "href": "HandsOnExercise/HandsOn7/HandsOn7.html",
    "title": "Hands-On Exercise 7",
    "section": "",
    "text": "Geographically weighted regression (GWR) is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable). In this hands-on exercise, you will learn how to build hedonic pricing models by using GWR methods. The dependent variable is the resale prices of condominium in 2015. The independent variables are divided into either structural and locational."
  },
  {
    "objectID": "HandsOnExercise/HandsOn7/HandsOn7.html#overview",
    "href": "HandsOnExercise/HandsOn7/HandsOn7.html#overview",
    "title": "Hands-On Exercise 7",
    "section": "",
    "text": "Geographically weighted regression (GWR) is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable). In this hands-on exercise, you will learn how to build hedonic pricing models by using GWR methods. The dependent variable is the resale prices of condominium in 2015. The independent variables are divided into either structural and locational."
  },
  {
    "objectID": "HandsOnExercise/HandsOn7/HandsOn7.html#the-data",
    "href": "HandsOnExercise/HandsOn7/HandsOn7.html#the-data",
    "title": "Hands-On Exercise 7",
    "section": "2 The Data",
    "text": "2 The Data\nTwo data sets will be used in this model building exercise, they are:\n\nURA Master Plan subzone boundary in shapefile format (i.e. MP14_SUBZONE_WEB_PL)\ncondo_resale_2015 in csv format (i.e. condo_resale_2015.csv)"
  },
  {
    "objectID": "HandsOnExercise/HandsOn7/HandsOn7.html#getting-started",
    "href": "HandsOnExercise/HandsOn7/HandsOn7.html#getting-started",
    "title": "Hands-On Exercise 7",
    "section": "3 Getting Started",
    "text": "3 Getting Started\nBefore we get started, it is important for us to install the necessary R packages into R and launch these R packages into R environment.\nThe R packages needed for this exercise are as follows:\n\nR package for building OLS and performing diagnostics tests\n\nolsrr\n\nR package for calibrating geographical weighted family of models\n\nGWmodel\n\nR package for multivariate data visualisation and analysis\n\ncorrplot\n\nSpatial data handling\n\nsf\n\nAttribute data handling\n\ntidyverse, especially readr, ggplot2 and dplyr\n\nChoropleth mapping\n\ntmap\n\n\nThe code chunks below installs and launches these R packages into R environment.\n\n\nCode Chunk\npacman::p_load(olsrr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, tidyverse, gtsummary)"
  },
  {
    "objectID": "HandsOnExercise/HandsOn7/HandsOn7.html#a-short-note-about-gwmodel",
    "href": "HandsOnExercise/HandsOn7/HandsOn7.html#a-short-note-about-gwmodel",
    "title": "Hands-On Exercise 7",
    "section": "4 A short note about GWmodel",
    "text": "4 A short note about GWmodel\nGWmodel package provides a collection of localised spatial statistical methods, namely: GW summary statistics, GW principal components analysis, GW discriminant analysis and various forms of GW regression; some of which are provided in basic and robust (outlier resistant) forms. Commonly, outputs or parameters of the GWmodel are mapped to provide a useful exploratory tool, which can often precede (and direct) a more traditional or sophisticated statistical analysis."
  },
  {
    "objectID": "HandsOnExercise/HandsOn7/HandsOn7.html#geospatial-data-wrangling",
    "href": "HandsOnExercise/HandsOn7/HandsOn7.html#geospatial-data-wrangling",
    "title": "Hands-On Exercise 7",
    "section": "5 Geospatial Data Wrangling",
    "text": "5 Geospatial Data Wrangling\n\n5.1 Importing geospatial data\nThe geospatial data used in this hands-on exercise is called MP14_SUBZONE_WEB_PL. It is in ESRI shapefile format. The shapefile consists of URA Master Plan 2014’s planning subzone boundaries. Polygon features are used to represent these geographic boundaries. The GIS data is in svy21 projected coordinates systems.\nThe code chunk below is used to import MP_SUBZONE_WEB_PL shapefile by using st_read() of sf packages.\n\n\nCode Chunk\nmpsz = st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/HandsOnExercise/HandsOn7/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nThe report above shows that the R object used to contain the imported MP14_SUBZONE_WEB_PL shapefile is called mpsz and it is a simple feature object. The geometry type is multipolygon. it is also important to note that mpsz simple feature object does not have EPSG information.\n\n\n5.2 Updating CRS information\nThe code chunk below updates the newly imported mpsz with the correct ESPG code (i.e. 3414)\n\n\nCode Chunk\nmpsz_svy21 &lt;- st_transform(mpsz, 3414)\n\n\nAfter transforming the projection metadata, you can varify the projection of the newly transformed mpsz_svy21 by using st_crs() of sf package.\nThe code chunk below will be used to varify the newly transformed mpsz_svy21.\n\n\nCode Chunk\nst_crs(mpsz_svy21)\n\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nNotice that the EPSG: is indicated as 3414 now.\nNext, you will reveal the extent of mpsz_svy21 by using st_bbox() of sf package.\n\n\nCode Chunk\nst_bbox(mpsz_svy21) #view extent\n\n\n     xmin      ymin      xmax      ymax \n 2667.538 15748.721 56396.440 50256.334"
  },
  {
    "objectID": "HandsOnExercise/HandsOn7/HandsOn7.html#aspatial-data-wrangling",
    "href": "HandsOnExercise/HandsOn7/HandsOn7.html#aspatial-data-wrangling",
    "title": "Hands-On Exercise 7",
    "section": "6 Aspatial Data Wrangling",
    "text": "6 Aspatial Data Wrangling\n\n6.1 Importing the aspatial data\nThe condo_resale_2015 is in csv file format. The codes chunk below uses read_csv() function of readr package to import condo_resale_2015 into R as a tibble data frame called condo_resale.\n\n\nCode Chunk\ncondo_resale = read_csv(\"data/aspatial/Condo_resale_2015.csv\")\n\n\nAfter importing the data file into R, it is important for us to examine if the data file has been imported correctly.\nThe codes chunks below uses glimpse() to display the data structure of will do the job.\n\n\nCode Chunk\nglimpse(condo_resale)\n\n\nRows: 1,436\nColumns: 23\n$ LATITUDE             &lt;dbl&gt; 1.287145, 1.328698, 1.313727, 1.308563, 1.321437,…\n$ LONGITUDE            &lt;dbl&gt; 103.7802, 103.8123, 103.7971, 103.8247, 103.9505,…\n$ POSTCODE             &lt;dbl&gt; 118635, 288420, 267833, 258380, 467169, 466472, 3…\n$ SELLING_PRICE        &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1320…\n$ AREA_SQM             &lt;dbl&gt; 309, 290, 248, 127, 145, 139, 218, 141, 165, 168,…\n$ AGE                  &lt;dbl&gt; 30, 32, 33, 7, 28, 22, 24, 24, 27, 31, 17, 22, 6,…\n$ PROX_CBD             &lt;dbl&gt; 7.941259, 6.609797, 6.898000, 4.038861, 11.783402…\n$ PROX_CHILDCARE       &lt;dbl&gt; 0.16597932, 0.28027246, 0.42922669, 0.39473543, 0…\n$ PROX_ELDERLYCARE     &lt;dbl&gt; 2.5198118, 1.9333338, 0.5021395, 1.9910316, 1.121…\n$ PROX_URA_GROWTH_AREA &lt;dbl&gt; 6.618741, 7.505109, 6.463887, 4.906512, 6.410632,…\n$ PROX_HAWKER_MARKET   &lt;dbl&gt; 1.76542207, 0.54507614, 0.37789301, 1.68259969, 0…\n$ PROX_KINDERGARTEN    &lt;dbl&gt; 0.05835552, 0.61592412, 0.14120309, 0.38200076, 0…\n$ PROX_MRT             &lt;dbl&gt; 0.5607188, 0.6584461, 0.3053433, 0.6910183, 0.528…\n$ PROX_PARK            &lt;dbl&gt; 1.1710446, 0.1992269, 0.2779886, 0.9832843, 0.116…\n$ PROX_PRIMARY_SCH     &lt;dbl&gt; 1.6340256, 0.9747834, 1.4715016, 1.4546324, 0.709…\n$ PROX_TOP_PRIMARY_SCH &lt;dbl&gt; 3.3273195, 0.9747834, 1.4715016, 2.3006394, 0.709…\n$ PROX_SHOPPING_MALL   &lt;dbl&gt; 2.2102717, 2.9374279, 1.2256850, 0.3525671, 1.307…\n$ PROX_SUPERMARKET     &lt;dbl&gt; 0.9103958, 0.5900617, 0.4135583, 0.4162219, 0.581…\n$ PROX_BUS_STOP        &lt;dbl&gt; 0.10336166, 0.28673408, 0.28504777, 0.29872340, 0…\n$ NO_Of_UNITS          &lt;dbl&gt; 18, 20, 27, 30, 30, 31, 32, 32, 32, 32, 34, 34, 3…\n$ FAMILY_FRIENDLY      &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0…\n$ FREEHOLD             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1…\n$ LEASEHOLD_99YR       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\n\n\nCode Chunk\nhead(condo_resale$LONGITUDE) #see the data in XCOORD column\n\n\n[1] 103.7802 103.8123 103.7971 103.8247 103.9505 103.9386\n\n\nCode Chunk\nhead(condo_resale$LATITUDE) #see the data in YCOORD column\n\n\n[1] 1.287145 1.328698 1.313727 1.308563 1.321437 1.314198\n\n\nNext, summary() of base R is used to display the summary statistics of cond_resale tibble data frame.\n\n\nCode Chunk\nsummary(condo_resale)\n\n\n    LATITUDE       LONGITUDE        POSTCODE      SELLING_PRICE     \n Min.   :1.240   Min.   :103.7   Min.   : 18965   Min.   :  540000  \n 1st Qu.:1.309   1st Qu.:103.8   1st Qu.:259849   1st Qu.: 1100000  \n Median :1.328   Median :103.8   Median :469298   Median : 1383222  \n Mean   :1.334   Mean   :103.8   Mean   :440439   Mean   : 1751211  \n 3rd Qu.:1.357   3rd Qu.:103.9   3rd Qu.:589486   3rd Qu.: 1950000  \n Max.   :1.454   Max.   :104.0   Max.   :828833   Max.   :18000000  \n    AREA_SQM          AGE           PROX_CBD       PROX_CHILDCARE    \n Min.   : 34.0   Min.   : 0.00   Min.   : 0.3869   Min.   :0.004927  \n 1st Qu.:103.0   1st Qu.: 5.00   1st Qu.: 5.5574   1st Qu.:0.174481  \n Median :121.0   Median :11.00   Median : 9.3567   Median :0.258135  \n Mean   :136.5   Mean   :12.14   Mean   : 9.3254   Mean   :0.326313  \n 3rd Qu.:156.0   3rd Qu.:18.00   3rd Qu.:12.6661   3rd Qu.:0.368293  \n Max.   :619.0   Max.   :37.00   Max.   :19.1804   Max.   :3.465726  \n PROX_ELDERLYCARE  PROX_URA_GROWTH_AREA PROX_HAWKER_MARKET PROX_KINDERGARTEN \n Min.   :0.05451   Min.   :0.2145       Min.   :0.05182    Min.   :0.004927  \n 1st Qu.:0.61254   1st Qu.:3.1643       1st Qu.:0.55245    1st Qu.:0.276345  \n Median :0.94179   Median :4.6186       Median :0.90842    Median :0.413385  \n Mean   :1.05351   Mean   :4.5981       Mean   :1.27987    Mean   :0.458903  \n 3rd Qu.:1.35122   3rd Qu.:5.7550       3rd Qu.:1.68578    3rd Qu.:0.578474  \n Max.   :3.94916   Max.   :9.1554       Max.   :5.37435    Max.   :2.229045  \n    PROX_MRT         PROX_PARK       PROX_PRIMARY_SCH  PROX_TOP_PRIMARY_SCH\n Min.   :0.05278   Min.   :0.02906   Min.   :0.07711   Min.   :0.07711     \n 1st Qu.:0.34646   1st Qu.:0.26211   1st Qu.:0.44024   1st Qu.:1.34451     \n Median :0.57430   Median :0.39926   Median :0.63505   Median :1.88213     \n Mean   :0.67316   Mean   :0.49802   Mean   :0.75471   Mean   :2.27347     \n 3rd Qu.:0.84844   3rd Qu.:0.65592   3rd Qu.:0.95104   3rd Qu.:2.90954     \n Max.   :3.48037   Max.   :2.16105   Max.   :3.92899   Max.   :6.74819     \n PROX_SHOPPING_MALL PROX_SUPERMARKET PROX_BUS_STOP       NO_Of_UNITS    \n Min.   :0.0000     Min.   :0.0000   Min.   :0.001595   Min.   :  18.0  \n 1st Qu.:0.5258     1st Qu.:0.3695   1st Qu.:0.098356   1st Qu.: 188.8  \n Median :0.9357     Median :0.5687   Median :0.151710   Median : 360.0  \n Mean   :1.0455     Mean   :0.6141   Mean   :0.193974   Mean   : 409.2  \n 3rd Qu.:1.3994     3rd Qu.:0.7862   3rd Qu.:0.220466   3rd Qu.: 590.0  \n Max.   :3.4774     Max.   :2.2441   Max.   :2.476639   Max.   :1703.0  \n FAMILY_FRIENDLY     FREEHOLD      LEASEHOLD_99YR  \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.0000  \n Mean   :0.4868   Mean   :0.4227   Mean   :0.4882  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n\n\n\n\n6.2 Converting aspatial data frame into a sf object\nCurrently, the condo_resale tibble data frame is aspatial. We will convert it to a sf object. The code chunk below converts condo_resale data frame into a simple feature data frame by using st_as_sf() of sf packages.\n\n\nCode Chunk\ncondo_resale.sf &lt;- st_as_sf(condo_resale,\n                            coords = c(\"LONGITUDE\", \"LATITUDE\"),\n                            crs=4326) %&gt;%\n  st_transform(crs=3414)\n\n\nNotice that st_transform() of sf package is used to convert the coordinates from wgs84 (i.e. crs:4326) to svy21 (i.e. crs=3414).\nNext, head() is used to list the content of condo_resale.sf object.\n\n\nCode Chunk\nhead(condo_resale.sf)\n\n\nSimple feature collection with 6 features and 21 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 22085.12 ymin: 29951.54 xmax: 41042.56 ymax: 34546.2\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 6 × 22\n  POSTCODE SELLING_PRICE AREA_SQM   AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE\n     &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;\n1   118635       3000000      309    30     7.94          0.166            2.52 \n2   288420       3880000      290    32     6.61          0.280            1.93 \n3   267833       3325000      248    33     6.90          0.429            0.502\n4   258380       4250000      127     7     4.04          0.395            1.99 \n5   467169       1400000      145    28    11.8           0.119            1.12 \n6   466472       1320000      139    22    10.3           0.125            0.789\n# ℹ 15 more variables: PROX_URA_GROWTH_AREA &lt;dbl&gt;, PROX_HAWKER_MARKET &lt;dbl&gt;,\n#   PROX_KINDERGARTEN &lt;dbl&gt;, PROX_MRT &lt;dbl&gt;, PROX_PARK &lt;dbl&gt;,\n#   PROX_PRIMARY_SCH &lt;dbl&gt;, PROX_TOP_PRIMARY_SCH &lt;dbl&gt;,\n#   PROX_SHOPPING_MALL &lt;dbl&gt;, PROX_SUPERMARKET &lt;dbl&gt;, PROX_BUS_STOP &lt;dbl&gt;,\n#   NO_Of_UNITS &lt;dbl&gt;, FAMILY_FRIENDLY &lt;dbl&gt;, FREEHOLD &lt;dbl&gt;,\n#   LEASEHOLD_99YR &lt;dbl&gt;, geometry &lt;POINT [m]&gt;\n\n\nNotice that the output is in point feature data frame."
  },
  {
    "objectID": "HandsOnExercise/HandsOn7/HandsOn7.html#exploratory-data-analysis-eda",
    "href": "HandsOnExercise/HandsOn7/HandsOn7.html#exploratory-data-analysis-eda",
    "title": "Hands-On Exercise 7",
    "section": "7 Exploratory Data Analysis (EDA)",
    "text": "7 Exploratory Data Analysis (EDA)\nIn the section, you will learn how to use statistical graphics functions of ggplot2 package to perform EDA.\n\n7.1 EDA using statistical graphics\nWe can plot the distribution of SELLING_PRICE by using appropriate Exploratory Data Analysis (EDA) as shown in the code chunk below.\n\n\nCode Chunk\nggplot(data=condo_resale.sf, aes(x=`SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\n\n\n\n\n\n\n\n\nThe figure above reveals a right skewed distribution. This means that more condominium units were transacted at relative lower prices.\nStatistically, the skewed dsitribution can be normalised by using log transformation. The code chunk below is used to derive a new variable called LOG_SELLING_PRICE by using a log transformation on the variable SELLING_PRICE. It is performed using mutate() of dplyr package.\n\n\nCode Chunk\ncondo_resale.sf &lt;- condo_resale.sf %&gt;%\n  mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))\n\n\nNow, you can plot the LOG_SELLING_PRICE using the code chunk below.\n\n\nCode Chunk\nggplot(data=condo_resale.sf, aes(x=`LOG_SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\n\n\n\n\n\n\n\n\nNotice that the distribution is relatively less skewed after the transformation.\n\n\n7.2 Multiple Histogram Plots distribution of variables\nIn this section, you will learn how to draw a small multiple histograms (also known as trellis plot) by using ggarrange() of ggpubr package.\nThe code chunk below is used to create 12 histograms. Then, ggarrange() is used to organised these histogram into a 3 columns by 4 rows small multiple plot.\n\n\nCode Chunk\nAREA_SQM &lt;- ggplot(data=condo_resale.sf, aes(x= `AREA_SQM`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nAGE &lt;- ggplot(data=condo_resale.sf, aes(x= `AGE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_CBD &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_CBD`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_CHILDCARE &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_CHILDCARE`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_ELDERLYCARE &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_ELDERLYCARE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_URA_GROWTH_AREA &lt;- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_URA_GROWTH_AREA`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_HAWKER_MARKET &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_HAWKER_MARKET`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_KINDERGARTEN &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_KINDERGARTEN`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_MRT &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_MRT`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_PARK &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_PARK`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_PRIMARY_SCH &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_TOP_PRIMARY_SCH &lt;- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_TOP_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, \n          PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,\n          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  \n          ncol = 3, nrow = 4)\n\n\n\n\n\n\n\n\n\n\n\n7.3 Drawing Statistical Point Map\nLastly, we want to reveal the geospatial distribution condominium resale prices in Singapore. The map will be prepared by using tmap package.\nFirst, we will turn on the interactive mode of tmap by using the code chunk below.\n\n\nCode Chunk\ntmap_mode(\"view\")\n\n\nNext, the code chunks below is used to create an interactive point symbol map.\n\n\nCode Chunk\ntm_shape(mpsz_svy21)+\n  tm_polygons() +\ntm_shape(condo_resale.sf) +  \n  tm_dots(col = \"SELLING_PRICE\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n\n\nNotice that tm_dots() is used instead of tm_bubbles().\nset.zoom.limits argument of tm_view() sets the minimum and maximum zoom level to 11 and 14 respectively.\nBefore moving on to the next section, the code below will be used to turn R display into plot mode.\n\n\nCode Chunk\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "HandsOnExercise/HandsOn7/HandsOn7.html#hedonic-pricing-modelling-in-r",
    "href": "HandsOnExercise/HandsOn7/HandsOn7.html#hedonic-pricing-modelling-in-r",
    "title": "Hands-On Exercise 7",
    "section": "8 Hedonic Pricing Modelling in R",
    "text": "8 Hedonic Pricing Modelling in R\nIn this section, you will learn how to building hedonic pricing models for condominium resale units using lm() of R base.\n\n8.1 Simple Linear Regression Method\nFirst, we will build a simple linear regression model by using SELLING_PRICE as the dependent variable and AREA_SQM as the independent variable.\n\n\nCode Chunk\ncondo.slr &lt;- lm(formula=SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\n\n\nlm() returns an object of class “lm” or for multiple responses of class c(“mlm”, “lm”).\nThe functions summary() and anova() can be used to obtain and print a summary and analysis of variance table of the results. The generic accessor functions coefficients, effects, fitted.values and residuals extract various useful features of the value returned by lm.\n\n\nCode Chunk\nsummary(condo.slr)\n\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3695815  -391764   -87517   258900 13503875 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -258121.1    63517.2  -4.064 5.09e-05 ***\nAREA_SQM      14719.0      428.1  34.381  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 942700 on 1434 degrees of freedom\nMultiple R-squared:  0.4518,    Adjusted R-squared:  0.4515 \nF-statistic:  1182 on 1 and 1434 DF,  p-value: &lt; 2.2e-16\n\n\nThe output report reveals that the SELLING_PRICE can be explained by using the formula:\n      *y = -258121.1 + 14719x1*\nThe R-squared of 0.4518 reveals that the simple regression model built is able to explain about 45% of the resale prices.\nSince p-value is much smaller than 0.0001, we will reject the null hypothesis that mean is a good estimator of SELLING_PRICE. This will allow us to infer that simple linear regression model above is a good estimator of SELLING_PRICE.\nThe Coefficients: section of the report reveals that the p-values of both the estimates of the Intercept and ARA_SQM are smaller than 0.001. In view of this, the null hypothesis of the B0 and B1 are equal to 0 will be rejected. As a results, we will be able to infer that the B0 and B1 are good parameter estimates.\nTo visualise the best fit curve on a scatterplot, we can incorporate lm() as a method function in ggplot’s geometry as shown in the code chunk below.\n\n\nCode Chunk\nggplot(data=condo_resale.sf,  \n       aes(x=`AREA_SQM`, y=`SELLING_PRICE`)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\n\n\n\n\n\n\n\nFigure above reveals that there are a few statistical outliers with relatively high selling prices.\n\n\n8.2 Multiple Linear Regression Method\n\n8.2.1 Visualising the relationships of the independent variables\nBefore building a multiple regression model, it is important to ensure that the indepdent variables used are not highly correlated to each other. If these highly correlated independent variables are used in building a regression model by mistake, the quality of the model will be compromised. This phenomenon is known as multicollinearity in statistics.\nCorrelation matrix is commonly used to visualise the relationships between the independent variables. Beside the pairs() of R, there are many packages support the display of a correlation matrix. In this section, the corrplot package will be used.\nThe code chunk below is used to plot a scatterplot matrix of the relationship between the independent variables in condo_resale data.frame.\n\n\nCode Chunk\ncorrplot(cor(condo_resale[, 5:23]), diag = FALSE, order = \"AOE\",\n         tl.pos = \"td\", tl.cex = 0.5, method = \"number\", type = \"upper\")\n\n\n\n\n\n\n\n\n\nMatrix reorder is very important for mining the hiden structure and patter in the matrix. There are four methods in corrplot (parameter order), named “AOE”, “FPC”, “hclust”, “alphabet”. In the code chunk above, AOE order is used. It orders the variables by using the angular order of the eigenvectors method suggested by Michael Friendly.\nFrom the scatterplot matrix, it is clear that Freehold is highly correlated to LEASE_99YEAR. In view of this, it is wiser to only include either one of them in the subsequent model building. As a result, LEASE_99YEAR is excluded in the subsequent model building.\n\n\n\n8.3 Building a hedonic pricing model using multiple linear regression method\nThe code chunk below using lm() to calibrate the multiple linear regression model.\n\n\nCode Chunk\ncondo.mlr &lt;- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE    + \n                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                  PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + PROX_KINDERGARTEN + \n                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + \n                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n                  PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                data=condo_resale.sf)\nsummary(condo.mlr)\n\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + \n    PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + \n    PROX_KINDERGARTEN + PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + \n    PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3475964  -293923   -23069   241043 12260381 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           481728.40  121441.01   3.967 7.65e-05 ***\nAREA_SQM               12708.32     369.59  34.385  &lt; 2e-16 ***\nAGE                   -24440.82    2763.16  -8.845  &lt; 2e-16 ***\nPROX_CBD              -78669.78    6768.97 -11.622  &lt; 2e-16 ***\nPROX_CHILDCARE       -351617.91  109467.25  -3.212  0.00135 ** \nPROX_ELDERLYCARE      171029.42   42110.51   4.061 5.14e-05 ***\nPROX_URA_GROWTH_AREA   38474.53   12523.57   3.072  0.00217 ** \nPROX_HAWKER_MARKET     23746.10   29299.76   0.810  0.41782    \nPROX_KINDERGARTEN     147468.99   82668.87   1.784  0.07466 .  \nPROX_MRT             -314599.68   57947.44  -5.429 6.66e-08 ***\nPROX_PARK             563280.50   66551.68   8.464  &lt; 2e-16 ***\nPROX_PRIMARY_SCH      180186.08   65237.95   2.762  0.00582 ** \nPROX_TOP_PRIMARY_SCH    2280.04   20410.43   0.112  0.91107    \nPROX_SHOPPING_MALL   -206604.06   42840.60  -4.823 1.57e-06 ***\nPROX_SUPERMARKET      -44991.80   77082.64  -0.584  0.55953    \nPROX_BUS_STOP         683121.35  138353.28   4.938 8.85e-07 ***\nNO_Of_UNITS             -231.18      89.03  -2.597  0.00951 ** \nFAMILY_FRIENDLY       140340.77   47020.55   2.985  0.00289 ** \nFREEHOLD              359913.01   49220.22   7.312 4.38e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 755800 on 1417 degrees of freedom\nMultiple R-squared:  0.6518,    Adjusted R-squared:  0.6474 \nF-statistic: 147.4 on 18 and 1417 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n8.4 Preparing Publication Quality Table: olsrr method\nWith reference to the report above, it is clear that not all the independent variables are statistically significant. We will revised the model by removing those variables which are not statistically significant.\nNow, we are ready to calibrate the revised model by using the code chunk below.\n\n\nCode Chunk\ncondo.mlr1 &lt;- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                   PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                   PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK + \n                   PROX_PRIMARY_SCH + PROX_SHOPPING_MALL    + PROX_BUS_STOP + \n                   NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,\n                 data=condo_resale.sf)\nols_regress(condo.mlr1)\n\n\n                                Model Summary                                 \n-----------------------------------------------------------------------------\nR                            0.807       RMSE                     751998.679 \nR-Squared                    0.651       MSE                571471422208.592 \nAdj. R-Squared               0.647       Coef. Var                    43.168 \nPred R-Squared               0.638       AIC                       42966.758 \nMAE                     414819.628       SBC                       43051.072 \n-----------------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                     ANOVA                                       \n--------------------------------------------------------------------------------\n                    Sum of                                                      \n                   Squares          DF         Mean Square       F         Sig. \n--------------------------------------------------------------------------------\nRegression    1.512586e+15          14        1.080418e+14    189.059    0.0000 \nResidual      8.120609e+14        1421    571471422208.592                      \nTotal         2.324647e+15        1435                                          \n--------------------------------------------------------------------------------\n\n                                               Parameter Estimates                                                \n-----------------------------------------------------------------------------------------------------------------\n               model           Beta    Std. Error    Std. Beta       t        Sig           lower          upper \n-----------------------------------------------------------------------------------------------------------------\n         (Intercept)     527633.222    108183.223                   4.877    0.000     315417.244     739849.200 \n            AREA_SQM      12777.523       367.479        0.584     34.771    0.000      12056.663      13498.382 \n                 AGE     -24687.739      2754.845       -0.167     -8.962    0.000     -30091.739     -19283.740 \n            PROX_CBD     -77131.323      5763.125       -0.263    -13.384    0.000     -88436.469     -65826.176 \n      PROX_CHILDCARE    -318472.751    107959.512       -0.084     -2.950    0.003    -530249.889    -106695.613 \n    PROX_ELDERLYCARE     185575.623     39901.864        0.090      4.651    0.000     107302.737     263848.510 \nPROX_URA_GROWTH_AREA      39163.254     11754.829        0.060      3.332    0.001      16104.571      62221.936 \n            PROX_MRT    -294745.107     56916.367       -0.112     -5.179    0.000    -406394.234    -183095.980 \n           PROX_PARK     570504.807     65507.029        0.150      8.709    0.000     442003.938     699005.677 \n    PROX_PRIMARY_SCH     159856.136     60234.599        0.062      2.654    0.008      41697.849     278014.424 \n  PROX_SHOPPING_MALL    -220947.251     36561.832       -0.115     -6.043    0.000    -292668.213    -149226.288 \n       PROX_BUS_STOP     682482.221    134513.243        0.134      5.074    0.000     418616.359     946348.082 \n         NO_Of_UNITS       -245.480        87.947       -0.053     -2.791    0.005       -418.000        -72.961 \n     FAMILY_FRIENDLY     146307.576     46893.021        0.057      3.120    0.002      54320.593     238294.560 \n            FREEHOLD     350599.812     48506.485        0.136      7.228    0.000     255447.802     445751.821 \n-----------------------------------------------------------------------------------------------------------------\n\n\n\n\n8.5 Preparing Publication Quality Table: gtsummary method\nThe gtsummary package provides an elegant and flexible way to create publication-ready summary tables in R.\nIn the code chunk below, tbl_regression() is used to create a well formatted regression report.\n\n\nCode Chunk\ntbl_regression(condo.mlr1, intercept = TRUE)\n\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\n(Intercept)\n527,633\n315,417, 739,849\n&lt;0.001\n\n\nAREA_SQM\n12,778\n12,057, 13,498\n&lt;0.001\n\n\nAGE\n-24,688\n-30,092, -19,284\n&lt;0.001\n\n\nPROX_CBD\n-77,131\n-88,436, -65,826\n&lt;0.001\n\n\nPROX_CHILDCARE\n-318,473\n-530,250, -106,696\n0.003\n\n\nPROX_ELDERLYCARE\n185,576\n107,303, 263,849\n&lt;0.001\n\n\nPROX_URA_GROWTH_AREA\n39,163\n16,105, 62,222\n&lt;0.001\n\n\nPROX_MRT\n-294,745\n-406,394, -183,096\n&lt;0.001\n\n\nPROX_PARK\n570,505\n442,004, 699,006\n&lt;0.001\n\n\nPROX_PRIMARY_SCH\n159,856\n41,698, 278,014\n0.008\n\n\nPROX_SHOPPING_MALL\n-220,947\n-292,668, -149,226\n&lt;0.001\n\n\nPROX_BUS_STOP\n682,482\n418,616, 946,348\n&lt;0.001\n\n\nNO_Of_UNITS\n-245\n-418, -73\n0.005\n\n\nFAMILY_FRIENDLY\n146,308\n54,321, 238,295\n0.002\n\n\nFREEHOLD\n350,600\n255,448, 445,752\n&lt;0.001\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\n\nWith gtsummary package, model statistics can be included in the report by either appending them to the report table by using add_glance_table() or adding as a table source note by using add_glance_source_note() as shown in the code chunk below.\n\n\nCode Chunk\ntbl_regression(condo.mlr1, \n               intercept = TRUE) %&gt;% \n  add_glance_source_note(\n    label = list(sigma ~ \"\\U03C3\"),\n    include = c(r.squared, adj.r.squared, \n                AIC, statistic,\n                p.value, sigma))\n\n\n\n\n\n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n527,633\n315,417, 739,849\n&lt;0.001\n    AREA_SQM\n12,778\n12,057, 13,498\n&lt;0.001\n    AGE\n-24,688\n-30,092, -19,284\n&lt;0.001\n    PROX_CBD\n-77,131\n-88,436, -65,826\n&lt;0.001\n    PROX_CHILDCARE\n-318,473\n-530,250, -106,696\n0.003\n    PROX_ELDERLYCARE\n185,576\n107,303, 263,849\n&lt;0.001\n    PROX_URA_GROWTH_AREA\n39,163\n16,105, 62,222\n&lt;0.001\n    PROX_MRT\n-294,745\n-406,394, -183,096\n&lt;0.001\n    PROX_PARK\n570,505\n442,004, 699,006\n&lt;0.001\n    PROX_PRIMARY_SCH\n159,856\n41,698, 278,014\n0.008\n    PROX_SHOPPING_MALL\n-220,947\n-292,668, -149,226\n&lt;0.001\n    PROX_BUS_STOP\n682,482\n418,616, 946,348\n&lt;0.001\n    NO_Of_UNITS\n-245\n-418, -73\n0.005\n    FAMILY_FRIENDLY\n146,308\n54,321, 238,295\n0.002\n    FREEHOLD\n350,600\n255,448, 445,752\n&lt;0.001\n  \n  \n    \n      R² = 0.651; Adjusted R² = 0.647; AIC = 42,967; Statistic = 189; p-value = &lt;0.001; σ = 755,957\n    \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nFor more customisation options, refer to Tutorial: tbl_regression\n\n8.5.1 Checking for multicolinearity\nIn this section, we would like to introduce you a fantastic R package specially programmed for performing OLS regression. It is called olsrr. It provides a collection of very useful methods for building better multiple linear regression models:\n\ncomprehensive regression output\nresidual diagnostics\nmeasures of influence\nheteroskedasticity tests\ncollinearity diagnostics\nmodel fit assessment\nvariable contribution assessment\nvariable selection procedures\n\nIn the code chunk below, the ols_vif_tol() of olsrr package is used to test if there are sign of multicollinearity.\n\n\nCode Chunk\nols_vif_tol(condo.mlr1)\n\n\n              Variables Tolerance      VIF\n1              AREA_SQM 0.8728554 1.145665\n2                   AGE 0.7071275 1.414172\n3              PROX_CBD 0.6356147 1.573280\n4        PROX_CHILDCARE 0.3066019 3.261559\n5      PROX_ELDERLYCARE 0.6598479 1.515501\n6  PROX_URA_GROWTH_AREA 0.7510311 1.331503\n7              PROX_MRT 0.5236090 1.909822\n8             PROX_PARK 0.8279261 1.207837\n9      PROX_PRIMARY_SCH 0.4524628 2.210126\n10   PROX_SHOPPING_MALL 0.6738795 1.483945\n11        PROX_BUS_STOP 0.3514118 2.845664\n12          NO_Of_UNITS 0.6901036 1.449058\n13      FAMILY_FRIENDLY 0.7244157 1.380423\n14             FREEHOLD 0.6931163 1.442759\n\n\nSince the VIF of the independent variables are less than 10. We can safely conclude that there are no sign of multicollinearity among the independent variables.\n\n\n8.5.2 Test for Non-Linearity\nIn multiple linear regression, it is important for us to test the assumption that linearity and additivity of the relationship between dependent and independent variables.\nIn the code chunk below, the ols_plot_resid_fit() of olsrr package is used to perform linearity assumption test.\n\n\nCode Chunk\nols_plot_resid_fit(condo.mlr1)\n\n\n\n\n\n\n\n\n\nThe figure above reveals that most of the data poitns are scattered around the 0 line, hence we can safely conclude that the relationships between the dependent variable and independent variables are linear.\n\n\n8.5.3 Test for Normality Assumption\nLastly, the code chunk below uses ols_plot_resid_hist() of olsrr package to perform normality assumption test.\n\n\nCode Chunk\nols_plot_resid_hist(condo.mlr1)\n\n\n\n\n\n\n\n\n\nThe figure reveals that the residual of the multiple linear regression model (i.e. condo.mlr1) is resemble normal distribution.\nIf you prefer formal statistical test methods, the ols_test_normality() of olsrr package can be used as shown in the code chun below.\n\n\nCode Chunk\nols_test_normality(condo.mlr1)\n\n\n-----------------------------------------------\n       Test             Statistic       pvalue  \n-----------------------------------------------\nShapiro-Wilk              0.6856         0.0000 \nKolmogorov-Smirnov        0.1366         0.0000 \nCramer-von Mises         121.0768        0.0000 \nAnderson-Darling         67.9551         0.0000 \n-----------------------------------------------\n\n\nThe summary table above reveals that the p-values of the four tests are way smaller than the alpha value of 0.05. Hence we will reject the null hypothesis and infer that there is statistical evidence that the residual are not normally distributed.\n\n\n8.5.4 Testing for Spatial Autocorrelation\nThe hedonic model we try to build are using geographically referenced attributes, hence it is also important for us to visual the residual of the hedonic pricing model.\nIn order to perform spatial autocorrelation test, we need to convert condo_resale.sf from sf data frame into a SpatialPointsDataFrame.\nFirst, we will export the residual of the hedonic pricing model and save it as a data frame.\n\n\nCode Chunk\nmlr.output &lt;- as.data.frame(condo.mlr1$residuals)\n\n\nNext, we will join the newly created data frame with condo_resale.sf object.\n\n\nCode Chunk\ncondo_resale.res.sf &lt;- cbind(condo_resale.sf, \n                        condo.mlr1$residuals) %&gt;%\nrename(`MLR_RES` = `condo.mlr1.residuals`)\n\n\nNext, we will convert condo_resale.res.sf from simple feature object into a SpatialPointsDataFrame because spdep package can only process sp conformed spatial data objects.\nThe code chunk below will be used to perform the data conversion process.\n\n\nCode Chunk\ncondo_resale.sp &lt;- as_Spatial(condo_resale.res.sf)\ncondo_resale.sp\n\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1436 \nextent      : 14940.85, 43352.45, 24765.67, 48382.81  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 23\nnames       : POSTCODE, SELLING_PRICE, AREA_SQM, AGE,    PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN,    PROX_MRT,   PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH, PROX_SHOPPING_MALL, ... \nmin values  :    18965,        540000,       34,   0, 0.386916393,    0.004927023,      0.054508623,          0.214539508,        0.051817113,       0.004927023, 0.052779424, 0.029064164,      0.077106132,          0.077106132,                  0, ... \nmax values  :   828833,       1.8e+07,      619,  37, 19.18042832,     3.46572633,      3.949157205,           9.15540001,        5.374348075,       2.229045366,  3.48037319,  2.16104919,      3.928989144,          6.748192062,        3.477433767, ... \n\n\nNext, we will use tmap package to display the distribution of the residuals on an interactive map.\nThe code churn below will turn on the interactive mode of tmap.\n\n\nCode Chunk\ntmap_mode(\"view\")\n\n\nThe code chunks below is used to create an interactive point symbol map.\n\n\nCode Chunk\ntm_shape(mpsz_svy21)+\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons(alpha = 0.4) +\ntm_shape(condo_resale.res.sf) +  \n  tm_dots(col = \"MLR_RES\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\n\n\nRemember to switch back to “plot” mode before continue.\n\n\nCode Chunk\ntmap_mode(\"plot\")\n\n\nThe figure above reveal that there is sign of spatial autocorrelation.\nTo proof that our observation is indeed true, the Moran’s I test will be performed\nFirst, we will compute the distance-based weight matrix by using dnearneigh() function of spdep.\n\n\nCode Chunk\nnb &lt;- dnearneigh(coordinates(condo_resale.sp), 0, 1500, longlat = FALSE)\nsummary(nb)\n\n\nNeighbour list object:\nNumber of regions: 1436 \nNumber of nonzero links: 66266 \nPercentage nonzero weights: 3.213526 \nAverage number of links: 46.14624 \n10 disjoint connected subgraphs\nLink number distribution:\n\n  1   3   5   7   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24 \n  3   3   9   4   3  15  10  19  17  45  19   5  14  29  19   6  35  45  18  47 \n 25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44 \n 16  43  22  26  21  11   9  23  22  13  16  25  21  37  16  18   8  21   4  12 \n 45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64 \n  8  36  18  14  14  43  11  12   8  13  12  13   4   5   6  12  11  20  29  33 \n 65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84 \n 15  20  10  14  15  15  11  16  12  10   8  19  12  14   9   8   4  13  11   6 \n 85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 \n  4   9   4   4   4   6   2  16   9   4   5   9   3   9   4   2   1   2   1   1 \n105 106 107 108 109 110 112 116 125 \n  1   5   9   2   1   3   1   1   1 \n3 least connected regions:\n193 194 277 with 1 link\n1 most connected region:\n285 with 125 links\n\n\nNext, nb2listw() of spdep packge will be used to convert the output neighbours lists (i.e. nb) into a spatial weights.\n\n\nCode Chunk\nnb_lw &lt;- nb2listw(nb, style = 'W')\nsummary(nb_lw)\n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 1436 \nNumber of nonzero links: 66266 \nPercentage nonzero weights: 3.213526 \nAverage number of links: 46.14624 \n10 disjoint connected subgraphs\nLink number distribution:\n\n  1   3   5   7   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24 \n  3   3   9   4   3  15  10  19  17  45  19   5  14  29  19   6  35  45  18  47 \n 25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44 \n 16  43  22  26  21  11   9  23  22  13  16  25  21  37  16  18   8  21   4  12 \n 45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64 \n  8  36  18  14  14  43  11  12   8  13  12  13   4   5   6  12  11  20  29  33 \n 65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84 \n 15  20  10  14  15  15  11  16  12  10   8  19  12  14   9   8   4  13  11   6 \n 85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 \n  4   9   4   4   4   6   2  16   9   4   5   9   3   9   4   2   1   2   1   1 \n105 106 107 108 109 110 112 116 125 \n  1   5   9   2   1   3   1   1   1 \n3 least connected regions:\n193 194 277 with 1 link\n1 most connected region:\n285 with 125 links\n\nWeights style: W \nWeights constants summary:\n     n      nn   S0       S1       S2\nW 1436 2062096 1436 94.81916 5798.341\n\n\nNext, lm.morantest() of spdep package will be used to perform Moran’s I test for residual spatial autocorrelation\n\n\nCode Chunk\nlm.morantest(condo.mlr1, nb_lw)\n\n\n\n    Global Moran I for regression residuals\n\ndata:  \nmodel: lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD +\nPROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_MRT +\nPROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP +\nNO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data = condo_resale.sf)\nweights: nb_lw\n\nMoran I statistic standard deviate = 24.366, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nObserved Moran I      Expectation         Variance \n    1.438876e-01    -5.487594e-03     3.758259e-05 \n\n\nThe Global Moran’s I test for residual spatial autocorrelation shows that it’s p-value is less than 0.00000000000000022 which is less than the alpha value of 0.05. Hence, we will reject the null hypothesis that the residuals are randomly distributed.\nSince the Observed Global Moran I = 0.1424418 which is greater than 0, we can infer than the residuals resemble cluster distribution."
  },
  {
    "objectID": "HandsOnExercise/HandsOn7/HandsOn7.html#building-hedonic-pricing-models-using-gwmodel",
    "href": "HandsOnExercise/HandsOn7/HandsOn7.html#building-hedonic-pricing-models-using-gwmodel",
    "title": "Hands-On Exercise 7",
    "section": "9 Building Hedonic Pricing Models using GWmodel",
    "text": "9 Building Hedonic Pricing Models using GWmodel\nIn this section, you are going to learn how to modelling hedonic pricing using both the fixed and adaptive bandwidth schemes\n\n9.1 Building Fixed Bandwidth GWR Model\n\n9.1.1 Computing fixed bandwith\nIn the code chunk below bw.gwr() of GWModel package is used to determine the optimal fixed bandwidth to use in the model. Notice that the argument adaptive is set to FALSE indicates that we are interested to compute the fixed bandwidth.\nThere are two possible approaches can be uused to determine the stopping rule, they are: CV cross-validation approach and AIC corrected (AICc) approach. We define the stopping rule using approach argeement.\n\n\nCode Chunk\nbw.fixed &lt;- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n                     PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n                     PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                     PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                     FAMILY_FRIENDLY + FREEHOLD, \n                   data=condo_resale.sp, \n                   approach=\"CV\", \n                   kernel=\"gaussian\", \n                   adaptive=FALSE, \n                   longlat=FALSE)\n\n\nFixed bandwidth: 17660.96 CV score: 8.259118e+14 \nFixed bandwidth: 10917.26 CV score: 7.970454e+14 \nFixed bandwidth: 6749.419 CV score: 7.273273e+14 \nFixed bandwidth: 4173.553 CV score: 6.300006e+14 \nFixed bandwidth: 2581.58 CV score: 5.404958e+14 \nFixed bandwidth: 1597.687 CV score: 4.857515e+14 \nFixed bandwidth: 989.6077 CV score: 4.722431e+14 \nFixed bandwidth: 613.7939 CV score: 1.379526e+16 \nFixed bandwidth: 1221.873 CV score: 4.778717e+14 \nFixed bandwidth: 846.0596 CV score: 4.791629e+14 \nFixed bandwidth: 1078.325 CV score: 4.751406e+14 \nFixed bandwidth: 934.7772 CV score: 4.72518e+14 \nFixed bandwidth: 1023.495 CV score: 4.730305e+14 \nFixed bandwidth: 968.6643 CV score: 4.721317e+14 \nFixed bandwidth: 955.7206 CV score: 4.722072e+14 \nFixed bandwidth: 976.6639 CV score: 4.721387e+14 \nFixed bandwidth: 963.7202 CV score: 4.721484e+14 \nFixed bandwidth: 971.7199 CV score: 4.721293e+14 \nFixed bandwidth: 973.6083 CV score: 4.721309e+14 \nFixed bandwidth: 970.5527 CV score: 4.721295e+14 \nFixed bandwidth: 972.4412 CV score: 4.721296e+14 \nFixed bandwidth: 971.2741 CV score: 4.721292e+14 \nFixed bandwidth: 970.9985 CV score: 4.721293e+14 \nFixed bandwidth: 971.4443 CV score: 4.721292e+14 \nFixed bandwidth: 971.5496 CV score: 4.721293e+14 \nFixed bandwidth: 971.3793 CV score: 4.721292e+14 \nFixed bandwidth: 971.3391 CV score: 4.721292e+14 \nFixed bandwidth: 971.3143 CV score: 4.721292e+14 \nFixed bandwidth: 971.3545 CV score: 4.721292e+14 \nFixed bandwidth: 971.3296 CV score: 4.721292e+14 \nFixed bandwidth: 971.345 CV score: 4.721292e+14 \nFixed bandwidth: 971.3355 CV score: 4.721292e+14 \nFixed bandwidth: 971.3413 CV score: 4.721292e+14 \nFixed bandwidth: 971.3377 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \nFixed bandwidth: 971.3396 CV score: 4.721292e+14 \nFixed bandwidth: 971.3402 CV score: 4.721292e+14 \nFixed bandwidth: 971.3398 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \nFixed bandwidth: 971.3399 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \n\n\nThe result shows that the recommended bandwidth is 971.3405 metres. (Quiz: Do you know why it is in metre?)\n\n\n9.1.2 GWModel method - fixed bandwith\nNow we can use the code chunk below to calibrate the gwr model using fixed bandwidth and gaussian kernel.\n\n\nCode Chunk\ngwr.fixed &lt;- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n                         PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n                         PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                         PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                         FAMILY_FRIENDLY + FREEHOLD, \n                       data=condo_resale.sp, \n                       bw=bw.fixed, \n                       kernel = 'gaussian', \n                       longlat = FALSE)\n\n\nThe output is saved in a list of class “gwrm”. The code below can be used to display the model output.\n\n\nCode Chunk\ngwr.fixed\n\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-10-13 16:12:54.896587 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sp, bw = bw.fixed, kernel = \"gaussian\", \n    longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Fixed bandwidth: 971.34 \n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -3.5988e+07 -5.1998e+05  7.6780e+05  1.7412e+06\n   AREA_SQM              1.0003e+03  5.2758e+03  7.4740e+03  1.2301e+04\n   AGE                  -1.3475e+05 -2.0813e+04 -8.6260e+03 -3.7784e+03\n   PROX_CBD             -7.7047e+07 -2.3608e+05 -8.3599e+04  3.4646e+04\n   PROX_CHILDCARE       -6.0097e+06 -3.3667e+05 -9.7426e+04  2.9007e+05\n   PROX_ELDERLYCARE     -3.5001e+06 -1.5970e+05  3.1970e+04  1.9577e+05\n   PROX_URA_GROWTH_AREA -3.0170e+06 -8.2013e+04  7.0749e+04  2.2612e+05\n   PROX_MRT             -3.5282e+06 -6.5836e+05 -1.8833e+05  3.6922e+04\n   PROX_PARK            -1.2062e+06 -2.1732e+05  3.5383e+04  4.1335e+05\n   PROX_PRIMARY_SCH     -2.2695e+07 -1.7066e+05  4.8472e+04  5.1555e+05\n   PROX_SHOPPING_MALL   -7.2585e+06 -1.6684e+05 -1.0517e+04  1.5923e+05\n   PROX_BUS_STOP        -1.4676e+06 -4.5207e+04  3.7601e+05  1.1664e+06\n   NO_Of_UNITS          -1.3170e+03 -2.4822e+02 -3.0846e+01  2.5496e+02\n   FAMILY_FRIENDLY      -2.2749e+06 -1.1140e+05  7.6214e+03  1.6107e+05\n   FREEHOLD             -9.2067e+06  3.8074e+04  1.5169e+05  3.7528e+05\n                             Max.\n   Intercept            112794435\n   AREA_SQM                 21575\n   AGE                     434203\n   PROX_CBD               2704604\n   PROX_CHILDCARE         1654086\n   PROX_ELDERLYCARE      38867861\n   PROX_URA_GROWTH_AREA  78515805\n   PROX_MRT               3124325\n   PROX_PARK             18122439\n   PROX_PRIMARY_SCH       4637517\n   PROX_SHOPPING_MALL     1529953\n   PROX_BUS_STOP         11342209\n   NO_Of_UNITS              12907\n   FAMILY_FRIENDLY        1720745\n   FREEHOLD               6073642\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 438.3807 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 997.6193 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 42263.61 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41632.36 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 42515.71 \n   Residual sum of squares: 2.534069e+14 \n   R-square value:  0.8909912 \n   Adjusted R-square value:  0.8430418 \n\n   ***********************************************************************\n   Program stops at: 2024-10-13 16:12:56.216766 \n\n\nThe report shows that the AICc of the gwr is 42263.61 which is significantly smaller than the globel multiple linear regression model of 42967.1.\n\n\n\n9.2 Building Adaptive Bandwidth GWR Model\nIn this section, we will calibrate the gwr-based hedonic pricing model by using adaptive bandwidth approach.\n\n9.2.1 Computing the adaptive bandwidth\nSimilar to the earlier section, we will first use bw.gwr() to determine the recommended data point to use.\nThe code chunk used look very similar to the one used to compute the fixed bandwidth except the adaptive argument has changed to TRUE.\n\n\nCode Chunk\nbw.adaptive &lt;- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE  + \n                        PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE    + \n                        PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n                        PROX_PRIMARY_SCH + PROX_SHOPPING_MALL   + PROX_BUS_STOP + \n                        NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                      data=condo_resale.sp, \n                      approach=\"CV\", \n                      kernel=\"gaussian\", \n                      adaptive=TRUE, \n                      longlat=FALSE)\n\n\nAdaptive bandwidth: 895 CV score: 7.952401e+14 \nAdaptive bandwidth: 561 CV score: 7.667364e+14 \nAdaptive bandwidth: 354 CV score: 6.953454e+14 \nAdaptive bandwidth: 226 CV score: 6.15223e+14 \nAdaptive bandwidth: 147 CV score: 5.674373e+14 \nAdaptive bandwidth: 98 CV score: 5.426745e+14 \nAdaptive bandwidth: 68 CV score: 5.168117e+14 \nAdaptive bandwidth: 49 CV score: 4.859631e+14 \nAdaptive bandwidth: 37 CV score: 4.646518e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \nAdaptive bandwidth: 25 CV score: 4.430816e+14 \nAdaptive bandwidth: 32 CV score: 4.505602e+14 \nAdaptive bandwidth: 27 CV score: 4.462172e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \n\n\nThe result shows that the 30 is the recommended data points to be used.\n\n\n9.2.2 Constructing the adaptive bandwidth gwr model\nNow, we can go ahead to calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and gaussian kernel as shown in the code chunk below.\n\n\nCode Chunk\ngwr.adaptive &lt;- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                            PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + \n                            PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n                            PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + \n                            NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                          data=condo_resale.sp, bw=bw.adaptive, \n                          kernel = 'gaussian', \n                          adaptive=TRUE, \n                          longlat = FALSE)\n\n\nThe code below can be used to display the model output.\n\n\nCode Chunk\ngwr.adaptive\n\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-10-13 16:13:05.586701 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sp, bw = bw.adaptive, kernel = \"gaussian\", \n    adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 30 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -1.3487e+08 -2.4669e+05  7.7928e+05  1.6194e+06\n   AREA_SQM              3.3188e+03  5.6285e+03  7.7825e+03  1.2738e+04\n   AGE                  -9.6746e+04 -2.9288e+04 -1.4043e+04 -5.6119e+03\n   PROX_CBD             -2.5330e+06 -1.6256e+05 -7.7242e+04  2.6624e+03\n   PROX_CHILDCARE       -1.2790e+06 -2.0175e+05  8.7158e+03  3.7778e+05\n   PROX_ELDERLYCARE     -1.6212e+06 -9.2050e+04  6.1029e+04  2.8184e+05\n   PROX_URA_GROWTH_AREA -7.2686e+06 -3.0350e+04  4.5869e+04  2.4613e+05\n   PROX_MRT             -4.3781e+07 -6.7282e+05 -2.2115e+05 -7.4593e+04\n   PROX_PARK            -2.9020e+06 -1.6782e+05  1.1601e+05  4.6572e+05\n   PROX_PRIMARY_SCH     -8.6418e+05 -1.6627e+05 -7.7853e+03  4.3222e+05\n   PROX_SHOPPING_MALL   -1.8272e+06 -1.3175e+05 -1.4049e+04  1.3799e+05\n   PROX_BUS_STOP        -2.0579e+06 -7.1461e+04  4.1104e+05  1.2071e+06\n   NO_Of_UNITS          -2.1993e+03 -2.3685e+02 -3.4699e+01  1.1657e+02\n   FAMILY_FRIENDLY      -5.9879e+05 -5.0927e+04  2.6173e+04  2.2481e+05\n   FREEHOLD             -1.6340e+05  4.0765e+04  1.9023e+05  3.7960e+05\n                            Max.\n   Intercept            18758355\n   AREA_SQM                23064\n   AGE                     13303\n   PROX_CBD             11346650\n   PROX_CHILDCARE        2892127\n   PROX_ELDERLYCARE      2465671\n   PROX_URA_GROWTH_AREA  7384059\n   PROX_MRT              1186242\n   PROX_PARK             2588497\n   PROX_PRIMARY_SCH      3381462\n   PROX_SHOPPING_MALL   38038564\n   PROX_BUS_STOP        12081592\n   NO_Of_UNITS              1010\n   FAMILY_FRIENDLY       2072414\n   FREEHOLD              1813995\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 350.3088 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 1085.691 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 41982.22 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41546.74 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 41914.08 \n   Residual sum of squares: 2.528227e+14 \n   R-square value:  0.8912425 \n   Adjusted R-square value:  0.8561185 \n\n   ***********************************************************************\n   Program stops at: 2024-10-13 16:13:07.009928 \n\n\nThe report shows that the AICc the adaptive distance gwr is 41982.22 which is even smaller than the AICc of the fixed distance gwr of 42263.61.\n\n\n\n9.3 Visualising GWR Output\nIn addition to regression residuals, the output feature class table includes fields for observed and predicted y values, condition number (cond), Local R2, residuals, and explanatory variable coefficients and standard errors:\n\nCondition Number: this diagnostic evaluates local collinearity. In the presence of strong local collinearity, results become unstable. Results associated with condition numbers larger than 30, may be unreliable.\nLocal R2: these values range between 0.0 and 1.0 and indicate how well the local regression model fits observed y values. Very low values indicate the local model is performing poorly. Mapping the Local R2 values to see where GWR predicts well and where it predicts poorly may provide clues about important variables that may be missing from the regression model.\nPredicted: these are the estimated (or fitted) y values 3. computed by GWR.\nResiduals: to obtain the residual values, the fitted y values are subtracted from the observed y values. Standardized residuals have a mean of zero and a standard deviation of 1. A cold-to-hot rendered map of standardized residuals can be produce by using these values.\nCoefficient Standard Error: these values measure the reliability of each coefficient estimate. Confidence in those estimates are higher when standard errors are small in relation to the actual coefficient values. Large standard errors may indicate problems with local collinearity.\n\nThey are all stored in a SpatialPointsDataFrame or SpatialPolygonsDataFrame object integrated with fit.points, GWR coefficient estimates, y value, predicted values, coefficient standard errors and t-values in its “data” slot in an object called SDF of the output list.\n\n\n9.4 Converting SDF into sf data.frame\nTo visualise the fields in SDF, we need to first covert it into sf data.frame by using the code chunk below.\n\n\nCode Chunk\ncondo_resale.sf.adaptive &lt;- st_as_sf(gwr.adaptive$SDF) %&gt;%\n  st_transform(crs=3414)\n\n\n\n\nCode Chunk\ncondo_resale.sf.adaptive.svy21 &lt;- st_transform(condo_resale.sf.adaptive, 3414)\ncondo_resale.sf.adaptive.svy21  \n\n\nSimple feature collection with 1436 features and 51 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 14940.85 ymin: 24765.67 xmax: 43352.45 ymax: 48382.81\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n    Intercept  AREA_SQM        AGE  PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE\n1   2050011.7  9561.892  -9514.634 -120681.9      319266.92       -393417.79\n2   1633128.2 16576.853 -58185.479 -149434.2      441102.18        325188.74\n3   3433608.2 13091.861 -26707.386 -259397.8     -120116.82        535855.81\n4    234358.9 20730.601 -93308.988 2426853.7      480825.28        314783.72\n5   2285804.9  6722.836 -17608.018 -316835.5       90764.78       -137384.61\n6  -3568877.4  6039.581 -26535.592  327306.1     -152531.19       -700392.85\n7  -2874842.4 16843.575 -59166.727 -983577.2     -177810.50       -122384.02\n8   2038086.0  6905.135 -17681.897 -285076.6       70259.40        -96012.78\n9   1718478.4  9580.703 -14401.128  105803.4     -657698.02       -123276.00\n10  3457054.0 14072.011 -31579.884 -234895.4       79961.45        548581.04\n   PROX_URA_GROWTH_AREA    PROX_MRT  PROX_PARK PROX_PRIMARY_SCH\n1            -159980.20  -299742.96 -172104.47        242668.03\n2            -142290.39 -2510522.23  523379.72       1106830.66\n3            -253621.21  -936853.28  209099.85        571462.33\n4           -2679297.89 -2039479.50 -759153.26       3127477.21\n5             303714.81   -44567.05  -10284.62         30413.56\n6             -28051.25   733566.47 1511488.92        320878.23\n7            1397676.38 -2745430.34  710114.74       1786570.95\n8             269368.71   -14552.99   73533.34         53359.73\n9            -361974.72  -476785.32 -132067.59        -40128.92\n10           -150024.38 -1503835.53  574155.47        108996.67\n   PROX_SHOPPING_MALL PROX_BUS_STOP  NO_Of_UNITS FAMILY_FRIENDLY  FREEHOLD\n1          300881.390     1210615.4  104.8290640       -9075.370  303955.6\n2          -87693.378     1843587.2 -288.3441183      310074.664  396221.3\n3         -126732.712     1411924.9   -9.5532945        5949.746  168821.7\n4          -29593.342     7225577.5 -161.3551620     1556178.531 1212515.6\n5           -7490.586      677577.0   42.2659674       58986.951  328175.2\n6          258583.881     1086012.6 -214.3671271      201992.641  471873.1\n7         -384251.210     5094060.5   -0.9212521      359659.512  408871.9\n8          -39634.902      735767.1   30.1741069       55602.506  347075.0\n9          276718.757     2815772.4  675.1615559      -30453.297  503872.8\n10        -454726.822     2123557.0  -21.3044311     -100935.586  213324.6\n         y    yhat    residual CV_Score Stud_residual Intercept_SE AREA_SQM_SE\n1  3000000 2886532   113468.16        0    0.38207013     516105.5    823.2860\n2  3880000 3466801   413198.52        0    1.01433140     488083.5    825.2380\n3  3325000 3616527  -291527.20        0   -0.83780678     963711.4    988.2240\n4  4250000 5435482 -1185481.63        0   -2.84614670     444185.5    617.4007\n5  1400000 1388166    11834.26        0    0.03404453    2119620.6   1376.2778\n6  1320000 1516702  -196701.95        0   -0.72065801   28572883.7   2348.0091\n7  3410000 3266881   143118.77        0    0.41291992     679546.6    893.5893\n8  1420000 1431955   -11955.27        0   -0.03033109    2217773.1   1415.2604\n9  2025000 1832799   192200.83        0    0.52018109     814281.8    943.8434\n10 2550000 2223364   326635.53        0    1.10559735    2410252.0   1271.4073\n      AGE_SE PROX_CBD_SE PROX_CHILDCARE_SE PROX_ELDERLYCARE_SE\n1   5889.782    37411.22          319111.1           120633.34\n2   6226.916    23615.06          299705.3            84546.69\n3   6510.236    56103.77          349128.5           129687.07\n4   6010.511   469337.41          304965.2           127150.69\n5   8180.361   410644.47          698720.6           327371.55\n6  14601.909  5272846.47         1141599.8          1653002.19\n7   8970.629   346164.20          530101.1           148598.71\n8   8661.309   438035.69          742532.8           399221.05\n9  11791.208    89148.35          704630.7           329683.30\n10  9941.980   173532.77          500976.2           281876.74\n   PROX_URA_GROWTH_AREA_SE PROX_MRT_SE PROX_PARK_SE PROX_PRIMARY_SCH_SE\n1                 56207.39    185181.3     205499.6            152400.7\n2                 76956.50    281133.9     229358.7            165150.7\n3                 95774.60    275483.7     314124.3            196662.6\n4                470762.12    279877.1     227249.4            240878.9\n5                474339.56    363830.0     364580.9            249087.7\n6               5496627.21    730453.2    1741712.0            683265.5\n7                371692.97    375511.9     297400.9            344602.8\n8                517977.91    423155.4     440984.4            261251.2\n9                153436.22    285325.4     304998.4            278258.5\n10               239182.57    571355.7     599131.8            331284.8\n   PROX_SHOPPING_MALL_SE PROX_BUS_STOP_SE NO_Of_UNITS_SE FAMILY_FRIENDLY_SE\n1               109268.8         600668.6       218.1258           131474.7\n2                98906.8         410222.1       208.9410           114989.1\n3               119913.3         464156.7       210.9828           146607.2\n4               177104.1         562810.8       361.7767           108726.6\n5               301032.9         740922.4       299.5034           160663.7\n6              2931208.6        1418333.3       602.5571           331727.0\n7               249969.5         821236.4       532.1978           129241.2\n8               351634.0         775038.4       338.6777           171895.1\n9               289872.7         850095.5       439.9037           220223.4\n10              265529.7         631399.2       259.0169           189125.5\n   FREEHOLD_SE Intercept_TV AREA_SQM_TV     AGE_TV PROX_CBD_TV\n1     115954.0    3.9720784   11.614302  -1.615447 -3.22582173\n2     130110.0    3.3460017   20.087361  -9.344188 -6.32792021\n3     141031.5    3.5629010   13.247868  -4.102368 -4.62353528\n4     138239.1    0.5276150   33.577223 -15.524302  5.17080808\n5     210641.1    1.0784029    4.884795  -2.152474 -0.77155660\n6     374347.3   -0.1249043    2.572214  -1.817269  0.06207388\n7     182216.9   -4.2305303   18.849348  -6.595605 -2.84136028\n8     216649.4    0.9189786    4.879056  -2.041481 -0.65080678\n9     220473.7    2.1104224   10.150733  -1.221345  1.18682383\n10    206346.2    1.4343123   11.068059  -3.176418 -1.35360852\n   PROX_CHILDCARE_TV PROX_ELDERLYCARE_TV PROX_URA_GROWTH_AREA_TV PROX_MRT_TV\n1         1.00048819          -3.2612693            -2.846248368 -1.61864578\n2         1.47178634           3.8462625            -1.848971738 -8.92998600\n3        -0.34404755           4.1319138            -2.648105057 -3.40075727\n4         1.57665606           2.4756745            -5.691404992 -7.28705261\n5         0.12990138          -0.4196596             0.640289855 -0.12249416\n6        -0.13361179          -0.4237096            -0.005103357  1.00426206\n7        -0.33542751          -0.8235874             3.760298131 -7.31116712\n8         0.09462126          -0.2405003             0.520038994 -0.03439159\n9        -0.93339393          -0.3739225            -2.359121712 -1.67102293\n10        0.15961128           1.9461735            -0.627237944 -2.63204802\n   PROX_PARK_TV PROX_PRIMARY_SCH_TV PROX_SHOPPING_MALL_TV PROX_BUS_STOP_TV\n1   -0.83749312           1.5923022            2.75358842        2.0154464\n2    2.28192684           6.7019454           -0.88662640        4.4941192\n3    0.66565951           2.9058009           -1.05686949        3.0419145\n4   -3.34061770          12.9836105           -0.16709578       12.8383775\n5   -0.02820944           0.1220998           -0.02488294        0.9145046\n6    0.86781794           0.4696245            0.08821750        0.7656963\n7    2.38773567           5.1844351           -1.53719231        6.2029165\n8    0.16674816           0.2042469           -0.11271635        0.9493299\n9   -0.43301073          -0.1442145            0.95462153        3.3123012\n10   0.95831249           0.3290120           -1.71252687        3.3632555\n   NO_Of_UNITS_TV FAMILY_FRIENDLY_TV FREEHOLD_TV  Local_R2\n1     0.480589953        -0.06902748    2.621347 0.8846744\n2    -1.380026395         2.69655779    3.045280 0.8899773\n3    -0.045279967         0.04058290    1.197050 0.8947007\n4    -0.446007570        14.31276425    8.771149 0.9073605\n5     0.141120178         0.36714544    1.557983 0.9510057\n6    -0.355762335         0.60891234    1.260522 0.9247586\n7    -0.001731033         2.78285441    2.243875 0.8310458\n8     0.089093858         0.32346758    1.602012 0.9463936\n9     1.534793921        -0.13828365    2.285410 0.8380365\n10   -0.082251138        -0.53369623    1.033819 0.9080753\n                    geometry\n1  POINT (22085.12 29951.54)\n2   POINT (25656.84 34546.2)\n3   POINT (23963.99 32890.8)\n4  POINT (27044.28 32319.77)\n5  POINT (41042.56 33743.64)\n6   POINT (39717.04 32943.1)\n7   POINT (28419.1 33513.37)\n8  POINT (40763.57 33879.61)\n9  POINT (23595.63 28884.78)\n10 POINT (24586.56 33194.31)\n\n\n\n\nCode Chunk\ngwr.adaptive.output &lt;- as.data.frame(gwr.adaptive$SDF)\ncondo_resale.sf.adaptive &lt;- cbind(condo_resale.res.sf, as.matrix(gwr.adaptive.output))\n\n\nNext, glimpse() is used to display the content of condo_resale.sf.adaptive sf data frame.\n\n\nCode Chunk\nglimpse(condo_resale.sf.adaptive)\n\n\nRows: 1,436\nColumns: 77\n$ POSTCODE                &lt;dbl&gt; 118635, 288420, 267833, 258380, 467169, 466472…\n$ SELLING_PRICE           &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1…\n$ AREA_SQM                &lt;dbl&gt; 309, 290, 248, 127, 145, 139, 218, 141, 165, 1…\n$ AGE                     &lt;dbl&gt; 30, 32, 33, 7, 28, 22, 24, 24, 27, 31, 17, 22,…\n$ PROX_CBD                &lt;dbl&gt; 7.941259, 6.609797, 6.898000, 4.038861, 11.783…\n$ PROX_CHILDCARE          &lt;dbl&gt; 0.16597932, 0.28027246, 0.42922669, 0.39473543…\n$ PROX_ELDERLYCARE        &lt;dbl&gt; 2.5198118, 1.9333338, 0.5021395, 1.9910316, 1.…\n$ PROX_URA_GROWTH_AREA    &lt;dbl&gt; 6.618741, 7.505109, 6.463887, 4.906512, 6.4106…\n$ PROX_HAWKER_MARKET      &lt;dbl&gt; 1.76542207, 0.54507614, 0.37789301, 1.68259969…\n$ PROX_KINDERGARTEN       &lt;dbl&gt; 0.05835552, 0.61592412, 0.14120309, 0.38200076…\n$ PROX_MRT                &lt;dbl&gt; 0.5607188, 0.6584461, 0.3053433, 0.6910183, 0.…\n$ PROX_PARK               &lt;dbl&gt; 1.1710446, 0.1992269, 0.2779886, 0.9832843, 0.…\n$ PROX_PRIMARY_SCH        &lt;dbl&gt; 1.6340256, 0.9747834, 1.4715016, 1.4546324, 0.…\n$ PROX_TOP_PRIMARY_SCH    &lt;dbl&gt; 3.3273195, 0.9747834, 1.4715016, 2.3006394, 0.…\n$ PROX_SHOPPING_MALL      &lt;dbl&gt; 2.2102717, 2.9374279, 1.2256850, 0.3525671, 1.…\n$ PROX_SUPERMARKET        &lt;dbl&gt; 0.9103958, 0.5900617, 0.4135583, 0.4162219, 0.…\n$ PROX_BUS_STOP           &lt;dbl&gt; 0.10336166, 0.28673408, 0.28504777, 0.29872340…\n$ NO_Of_UNITS             &lt;dbl&gt; 18, 20, 27, 30, 30, 31, 32, 32, 32, 32, 34, 34…\n$ FAMILY_FRIENDLY         &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0…\n$ FREEHOLD                &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1…\n$ LEASEHOLD_99YR          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ LOG_SELLING_PRICE       &lt;dbl&gt; 14.91412, 15.17135, 15.01698, 15.26243, 14.151…\n$ MLR_RES                 &lt;dbl&gt; -1489099.55, 415494.57, 194129.69, 1088992.71,…\n$ Intercept               &lt;dbl&gt; 2050011.67, 1633128.24, 3433608.17, 234358.91,…\n$ AREA_SQM.1              &lt;dbl&gt; 9561.892, 16576.853, 13091.861, 20730.601, 672…\n$ AGE.1                   &lt;dbl&gt; -9514.634, -58185.479, -26707.386, -93308.988,…\n$ PROX_CBD.1              &lt;dbl&gt; -120681.94, -149434.22, -259397.77, 2426853.66…\n$ PROX_CHILDCARE.1        &lt;dbl&gt; 319266.925, 441102.177, -120116.816, 480825.28…\n$ PROX_ELDERLYCARE.1      &lt;dbl&gt; -393417.795, 325188.741, 535855.806, 314783.72…\n$ PROX_URA_GROWTH_AREA.1  &lt;dbl&gt; -159980.203, -142290.389, -253621.206, -267929…\n$ PROX_MRT.1              &lt;dbl&gt; -299742.96, -2510522.23, -936853.28, -2039479.…\n$ PROX_PARK.1             &lt;dbl&gt; -172104.47, 523379.72, 209099.85, -759153.26, …\n$ PROX_PRIMARY_SCH.1      &lt;dbl&gt; 242668.03, 1106830.66, 571462.33, 3127477.21, …\n$ PROX_SHOPPING_MALL.1    &lt;dbl&gt; 300881.390, -87693.378, -126732.712, -29593.34…\n$ PROX_BUS_STOP.1         &lt;dbl&gt; 1210615.44, 1843587.22, 1411924.90, 7225577.51…\n$ NO_Of_UNITS.1           &lt;dbl&gt; 104.8290640, -288.3441183, -9.5532945, -161.35…\n$ FAMILY_FRIENDLY.1       &lt;dbl&gt; -9075.370, 310074.664, 5949.746, 1556178.531, …\n$ FREEHOLD.1              &lt;dbl&gt; 303955.61, 396221.27, 168821.75, 1212515.58, 3…\n$ y                       &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1…\n$ yhat                    &lt;dbl&gt; 2886531.8, 3466801.5, 3616527.2, 5435481.6, 13…\n$ residual                &lt;dbl&gt; 113468.16, 413198.52, -291527.20, -1185481.63,…\n$ CV_Score                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Stud_residual           &lt;dbl&gt; 0.38207013, 1.01433140, -0.83780678, -2.846146…\n$ Intercept_SE            &lt;dbl&gt; 516105.5, 488083.5, 963711.4, 444185.5, 211962…\n$ AREA_SQM_SE             &lt;dbl&gt; 823.2860, 825.2380, 988.2240, 617.4007, 1376.2…\n$ AGE_SE                  &lt;dbl&gt; 5889.782, 6226.916, 6510.236, 6010.511, 8180.3…\n$ PROX_CBD_SE             &lt;dbl&gt; 37411.22, 23615.06, 56103.77, 469337.41, 41064…\n$ PROX_CHILDCARE_SE       &lt;dbl&gt; 319111.1, 299705.3, 349128.5, 304965.2, 698720…\n$ PROX_ELDERLYCARE_SE     &lt;dbl&gt; 120633.34, 84546.69, 129687.07, 127150.69, 327…\n$ PROX_URA_GROWTH_AREA_SE &lt;dbl&gt; 56207.39, 76956.50, 95774.60, 470762.12, 47433…\n$ PROX_MRT_SE             &lt;dbl&gt; 185181.3, 281133.9, 275483.7, 279877.1, 363830…\n$ PROX_PARK_SE            &lt;dbl&gt; 205499.6, 229358.7, 314124.3, 227249.4, 364580…\n$ PROX_PRIMARY_SCH_SE     &lt;dbl&gt; 152400.7, 165150.7, 196662.6, 240878.9, 249087…\n$ PROX_SHOPPING_MALL_SE   &lt;dbl&gt; 109268.8, 98906.8, 119913.3, 177104.1, 301032.…\n$ PROX_BUS_STOP_SE        &lt;dbl&gt; 600668.6, 410222.1, 464156.7, 562810.8, 740922…\n$ NO_Of_UNITS_SE          &lt;dbl&gt; 218.1258, 208.9410, 210.9828, 361.7767, 299.50…\n$ FAMILY_FRIENDLY_SE      &lt;dbl&gt; 131474.73, 114989.07, 146607.22, 108726.62, 16…\n$ FREEHOLD_SE             &lt;dbl&gt; 115954.0, 130110.0, 141031.5, 138239.1, 210641…\n$ Intercept_TV            &lt;dbl&gt; 3.9720784, 3.3460017, 3.5629010, 0.5276150, 1.…\n$ AREA_SQM_TV             &lt;dbl&gt; 11.614302, 20.087361, 13.247868, 33.577223, 4.…\n$ AGE_TV                  &lt;dbl&gt; -1.6154474, -9.3441881, -4.1023685, -15.524301…\n$ PROX_CBD_TV             &lt;dbl&gt; -3.22582173, -6.32792021, -4.62353528, 5.17080…\n$ PROX_CHILDCARE_TV       &lt;dbl&gt; 1.000488185, 1.471786337, -0.344047555, 1.5766…\n$ PROX_ELDERLYCARE_TV     &lt;dbl&gt; -3.26126929, 3.84626245, 4.13191383, 2.4756745…\n$ PROX_URA_GROWTH_AREA_TV &lt;dbl&gt; -2.846248368, -1.848971738, -2.648105057, -5.6…\n$ PROX_MRT_TV             &lt;dbl&gt; -1.61864578, -8.92998600, -3.40075727, -7.2870…\n$ PROX_PARK_TV            &lt;dbl&gt; -0.83749312, 2.28192684, 0.66565951, -3.340617…\n$ PROX_PRIMARY_SCH_TV     &lt;dbl&gt; 1.59230221, 6.70194543, 2.90580089, 12.9836104…\n$ PROX_SHOPPING_MALL_TV   &lt;dbl&gt; 2.753588422, -0.886626400, -1.056869486, -0.16…\n$ PROX_BUS_STOP_TV        &lt;dbl&gt; 2.0154464, 4.4941192, 3.0419145, 12.8383775, 0…\n$ NO_Of_UNITS_TV          &lt;dbl&gt; 0.480589953, -1.380026395, -0.045279967, -0.44…\n$ FAMILY_FRIENDLY_TV      &lt;dbl&gt; -0.06902748, 2.69655779, 0.04058290, 14.312764…\n$ FREEHOLD_TV             &lt;dbl&gt; 2.6213469, 3.0452799, 1.1970499, 8.7711485, 1.…\n$ Local_R2                &lt;dbl&gt; 0.8846744, 0.8899773, 0.8947007, 0.9073605, 0.…\n$ coords.x1               &lt;dbl&gt; 22085.12, 25656.84, 23963.99, 27044.28, 41042.…\n$ coords.x2               &lt;dbl&gt; 29951.54, 34546.20, 32890.80, 32319.77, 33743.…\n$ geometry                &lt;POINT [m]&gt; POINT (22085.12 29951.54), POINT (25656.…\n\n\n\n\nCode Chunk\nsummary(gwr.adaptive$SDF$yhat)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n  171347  1102001  1385528  1751842  1982307 13887901 \n\n\n\n\n9.5 Visualising local R2\nThe code chunks below is used to create an interactive point symbol map.\n\n\nCode Chunk\ntmap_mode(\"view\")\ntm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"Local_R2\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\n\n\n\n\nCode Chunk\ntmap_mode(\"plot\")\n\n\n\n\n9.6 Visualising coefficient estimates\nThe code chunks below is used to create an interactive point symbol map.\n\n\nCode Chunk\ntmap_mode(\"view\")\nAREA_SQM_SE &lt;- tm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"AREA_SQM_SE\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\nAREA_SQM_TV &lt;- tm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"AREA_SQM_TV\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\ntmap_arrange(AREA_SQM_SE, AREA_SQM_TV, \n             asp=1, ncol=2,\n             sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\ntmap_mode(\"plot\")\n\n\n\n9.6.1 By URA Plannign Region\n\n\nCode Chunk\ntm_shape(mpsz_svy21[mpsz_svy21$REGION_N==\"CENTRAL REGION\", ])+\n  tm_polygons()+\ntm_shape(condo_resale.sf.adaptive) + \n  tm_bubbles(col = \"Local_R2\",\n           size = 0.15,\n           border.col = \"gray60\",\n           border.lwd = 1)"
  },
  {
    "objectID": "HandsOnExercise/HandsOn7/HandsOn7.html#reference",
    "href": "HandsOnExercise/HandsOn7/HandsOn7.html#reference",
    "title": "Hands-On Exercise 7",
    "section": "10 Reference",
    "text": "10 Reference\nGollini I, Lu B, Charlton M, Brunsdon C, Harris P (2015) “GWmodel: an R Package for exploring Spatial Heterogeneity using Geographically Weighted Models”. Journal of Statistical Software, 63(17):1-50, http://www.jstatsoft.org/v63/i17/\nLu B, Harris P, Charlton M, Brunsdon C (2014) “The GWmodel R Package: further topics for exploring Spatial Heterogeneity using GeographicallyWeighted Models”. Geo-spatial Information Science 17(2): 85-101, http://www.tandfonline.com/doi/abs/10.1080/1009502.2014.917453"
  },
  {
    "objectID": "InClassExercise/InClass7/InClass7.html",
    "href": "InClassExercise/InClass7/InClass7.html",
    "title": "In-Class Exercise 7",
    "section": "",
    "text": "1 Lesson 7: Geographically Weighted Regression\n\n\nCode Chunk\npacman::p_load(olsrr,ggstatsplot, ggpubr,\n               sf, spdep, GWmodel, tmap,\n               tidyverse, gtsummary, performance,\n               see, sfdep)\n\n\n\n\nCode Chunk\ncondo_resale &lt;- \nread_csv(\"data/aspatial/Condo_resale_2015.csv\")\n\n\n\n\nCode Chunk\nmpsz &lt;- read_rds(\"data/rds/mpsz.rds\")\n\n\n\n\nCode Chunk\ncondo_resale.sf &lt;- st_as_sf(condo_resale,\n                            coords = c(\"LONGTITUDE\", \"LATITUDE\"),\n                            crs)\n\n\n\n\nCode Chunk\ncondo_resale_res.s\n\n\nFOR TAKE HOME 2:\n\n\nCode Chunk\ntm_shape(thaiboundary) +\n  tm_polygons()\nview(prov_sf)\n\n\n\n\nCode Chunk\nsf_polygon &lt;- thaiboundary %&gt;%\n  st_cast(\"POLYGON\") %&gt;%"
  },
  {
    "objectID": "HandsOnExercise/HandsOn8/HandsOn8.html",
    "href": "HandsOnExercise/HandsOn8/HandsOn8.html",
    "title": "Hands-On Exercise 8",
    "section": "",
    "text": "Predictive modelling uses statistical learning or machine learning techniques to predict outcomes. By and large, the event one wants to predict is in the future. However, a set of known outcome and predictors (also known as variables) will be used to calibrate the predictive models.\nGeospatial predictive modelling is conceptually rooted in the principle that the occurrences of events being modeled are limited in distribution. When geographically referenced data are used, occurrences of events are neither uniform nor random in distribution over space. There are geospatial factors (infrastructure, sociocultural, topographic, etc.) that constrain and influence where the locations of events occur. Geospatial predictive modeling attempts to describe those constraints and influences by spatially correlating occurrences of historical geospatial locations with environmental factors that represent those constraints and influences.\n\n\nIn this in-class exercise, you will learn how to build predictive model by using geographical random forest method. By the end of this hands-on exercise, you will acquire the skills of:\n\npreparing training and test data sets by using appropriate data sampling methods\ncalibrating predictive models by using both geospatial statistical learning and machine learning methods,\ncomparing and selecting the best model for predicting the future outcome,\npredicting the future outcomes by using the best model calibrated.\n\n\n\n\n\n\nAspatial dataset:\n\nHDB Resale data: a list of HDB resale transacted prices in Singapore from Jan 2017 onwards. It is in csv format which can be downloaded from Data.gov.sg.\n\nGeospatial dataset:\n\nMP14_SUBZONE_WEB_PL: a polygon feature data providing information of URA 2014 Master Plan Planning Subzone boundary data. It is in ESRI shapefile format. This data set was also downloaded from Data.gov.sg\n\nLocational factors with geographic coordinates:\n\nDownloaded from Data.gov.sg.\n\nEldercare data is a list of eldercare in Singapore. It is in shapefile format.\nHawker Centre data is a list of hawker centres in Singapore. It is in geojson format.\nParks data is a list of parks in Singapore. It is in geojson format.\nSupermarket data is a list of supermarkets in Singapore. It is in geojson format.\nCHAS clinics data is a list of CHAS clinics in Singapore. It is in geojson format.\nChildcare service data is a list of childcare services in Singapore. It is in geojson format\nKindergartens data is a list of kindergartens in Singapore. It is in geojson format.\n\nDownloaded from Datamall.lta.gov.sg.\n\nMRT data is a list of MRT/LRT stations in Singapore with the station names and codes. It is in shapefile format.\nBus stops data is a list of bus stops in Singapore. It is in shapefile format.\n\n\nLocational factors without geographic coordinates:\n\nDownloaded from Data.gov.sg.\n\nPrimary school data is extracted from the list on General information of schools from data.gov portal. It is in csv format.\n\nRetrieved/Scraped from other sources\n\nCBD coordinates obtained from Google.\nShopping malls data is a list of Shopping malls in Singapore obtained from Wikipedia.\nGood primary schools is a list of primary schools that are ordered in ranking in terms of popularity and this can be found at Local Salary Forum.\n\n\n\n\n\n\nThis code chunk performs 3 tasks:\n\nA list called packages will be created and will consists of all the R packages required to accomplish this exercises\nCheck if R packages on package have been installed in R and if not, they will be installed\nAfter all the R packages have been installed, they will be loaded.\n\n\n\nCode Chunk\npacman::p_load(sf, spdep, GWmodel, SpatialML, \n               tmap, rsample, Metrics, tidyverse)\n\n\n\n\n\n\n\nReading the input data sets. It is in simple feature data frame.\n\n\nCode Chunk\nmdata &lt;- read_rds(\"data/model/mdata.rds\")\n\n\n\n\n\nThe entire data are split into training and test data sets with 65% and 35% respectively by using initial_split() of rsample package. rsample is one of the package of tigymodels.\n\n\nCode Chunk\nset.seed(1234)\nresale_split &lt;- initial_split(mdata, \n                              prop = 6.5/10,)\ntrain_data &lt;- training(resale_split)\ntest_data &lt;- testing(resale_split)\n\n\n\n\nCode Chunk\nwrite_rds(train_data, \"data/model/train_data.rds\")\nwrite_rds(test_data, \"data/model/test_data.rds\")\n\n\n\n\n\n\nBefore loading the predictors into a predictive model, it is always a good practice to use correlation matrix to examine if there is sign of multicolinearity.\n\n\nCode Chunk\nmdata_nogeo &lt;- mdata %&gt;%\n  st_drop_geometry()\ncorrplot::corrplot(cor(mdata_nogeo[, 2:17]), \n                   diag = FALSE, \n                   order = \"AOE\",\n                   tl.pos = \"td\", \n                   tl.cex = 0.5, \n                   method = \"number\", \n                   type = \"upper\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\ntrain_data &lt;- read_rds(\"data/model/train_data.rds\")\ntest_data &lt;- read_rds(\"data/model/test_data.rds\")\n\n\n\n\n\n\n\nCode Chunk\nprice_mlr &lt;- lm(resale_price ~ floor_area_sqm +\n                  storey_order + remaining_lease_mths +\n                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n                  PROX_MRT + PROX_PARK + PROX_MALL + \n                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                data=train_data)\nsummary(price_mlr)\n\n\n\nCall:\nlm(formula = resale_price ~ floor_area_sqm + storey_order + remaining_lease_mths + \n    PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK + \n    PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN + \n    WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH, \n    data = train_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205193  -39120   -1930   36545  472355 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              107601.073  10601.261  10.150  &lt; 2e-16 ***\nfloor_area_sqm             2780.698     90.579  30.699  &lt; 2e-16 ***\nstorey_order              14299.298    339.115  42.167  &lt; 2e-16 ***\nremaining_lease_mths        344.490      4.592  75.027  &lt; 2e-16 ***\nPROX_CBD                 -16930.196    201.254 -84.124  &lt; 2e-16 ***\nPROX_ELDERLYCARE         -14441.025    994.867 -14.516  &lt; 2e-16 ***\nPROX_HAWKER              -19265.648   1273.597 -15.127  &lt; 2e-16 ***\nPROX_MRT                 -32564.272   1744.232 -18.670  &lt; 2e-16 ***\nPROX_PARK                 -5712.625   1483.885  -3.850 0.000119 ***\nPROX_MALL                -14717.388   2007.818  -7.330 2.47e-13 ***\nPROX_SUPERMARKET         -26881.938   4189.624  -6.416 1.46e-10 ***\nWITHIN_350M_KINDERGARTEN   8520.472    632.812  13.464  &lt; 2e-16 ***\nWITHIN_350M_CHILDCARE     -4510.650    354.015 -12.741  &lt; 2e-16 ***\nWITHIN_350M_BUS             813.493    222.574   3.655 0.000259 ***\nWITHIN_1KM_PRISCH         -8010.834    491.512 -16.298  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 61650 on 10320 degrees of freedom\nMultiple R-squared:  0.7373,    Adjusted R-squared:  0.737 \nF-statistic:  2069 on 14 and 10320 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCode Chunk\nwrite_rds(price_mlr, \"data/model/price_mlr.rds\" ) \n\n\n\n\n\nIn this section, you will learn how to calibrate a model to predict HDB resale price by using geographically weighted regression method of GWmodel package.\n\n\n\n\nCode Chunk\ntrain_data_sp &lt;- as_Spatial(train_data)\ntrain_data_sp\n\n\nclass       : SpatialPointsDataFrame \nfeatures    : 10335 \nextent      : 11597.31, 42623.63, 28217.39, 48741.06  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 17\nnames       : resale_price, floor_area_sqm, storey_order, remaining_lease_mths,          PROX_CBD,     PROX_ELDERLYCARE,        PROX_HAWKER,           PROX_MRT,          PROX_PARK,   PROX_GOOD_PRISCH,        PROX_MALL,            PROX_CHAS,     PROX_SUPERMARKET, WITHIN_350M_KINDERGARTEN, WITHIN_350M_CHILDCARE, ... \nmin values  :       218000,             74,            1,                  555, 0.999393538715878, 1.98943787433087e-08, 0.0333358643817954, 0.0220407324774434, 0.0441643212802781, 0.0652540365486641,                0, 6.20621206270077e-09, 1.21715176356525e-07,                        0,                     0, ... \nmax values  :      1186888,            133,           17,                 1164,  19.6500691667807,     3.30163731686804,   2.86763031236184,   2.13060636038504,   2.41313695915468,   10.6223726149914, 2.27100643784442,    0.808332738794272,     1.57131703651196,                        7,                    20, ... \n\n\n\n\n\nNext, bw.gwr() of GWmodel package will be used to determine the optimal bandwidth to be used. The code chunk below is used to determine adaptive bandwidth and CV method is used to determine the optimal bandwidth.\n\n\nCode Chunk\nbw_adaptive &lt;- bw.gwr(resale_price ~ floor_area_sqm +\n                  storey_order + remaining_lease_mths +\n                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n                  PROX_MRT + PROX_PARK + PROX_MALL + \n                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                  data=train_data_sp,\n                  approach=\"CV\",\n                  kernel=\"gaussian\",\n                  adaptive=TRUE,\n                  longlat=FALSE)\n\n\nTake a cup of tea and have a break, it will take a few minutes.\n          -----A kind suggestion from GWmodel development group\nAdaptive bandwidth: 6395 CV score: 3.60536e+13 \nAdaptive bandwidth: 3960 CV score: 3.320316e+13 \nAdaptive bandwidth: 2455 CV score: 2.928339e+13 \nAdaptive bandwidth: 1524 CV score: 2.550957e+13 \nAdaptive bandwidth: 950 CV score: 1.95632e+13 \nAdaptive bandwidth: 593 CV score: 1.58347e+13 \nAdaptive bandwidth: 375 CV score: 1.310042e+13 \nAdaptive bandwidth: 237 CV score: 1.113152e+13 \nAdaptive bandwidth: 155 CV score: 9.572037e+12 \nAdaptive bandwidth: 101 CV score: 8.457003e+12 \nAdaptive bandwidth: 71 CV score: 7.605058e+12 \nAdaptive bandwidth: 49 CV score: 6.966278e+12 \nAdaptive bandwidth: 38 CV score: 8.841916e+12 \nAdaptive bandwidth: 58 CV score: 7.275234e+12 \nAdaptive bandwidth: 45 CV score: 6.871966e+12 \nAdaptive bandwidth: 41 CV score: 6.793327e+12 \nAdaptive bandwidth: 40 CV score: 6.780974e+12 \nAdaptive bandwidth: 38 CV score: 8.841916e+12 \nAdaptive bandwidth: 40 CV score: 6.780974e+12 \n\n\nThe result shows that 40 neighbour points will be the optimal bandwidth to be used if adaptive bandwidth is used for this data set.\n\n\nCode Chunk\nwrite_rds(bw_adaptive, \"data/model/bw_adaptive.rds\")\n\n\n\n\n\nFirst, let us call the save bandwidth by using the code chunk below.\n\n\nCode Chunk\nbw_adaptive &lt;- read_rds(\"data/model/bw_adaptive.rds\")\n\n\nNow, we can go ahead to calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and Gaussian kernel as shown in the code chunk below.\n\n\nCode Chunk\ngwr_adaptive &lt;- gwr.basic(formula = resale_price ~\n                            floor_area_sqm + storey_order +\n                            remaining_lease_mths + PROX_CBD + \n                            PROX_ELDERLYCARE + PROX_HAWKER +\n                            PROX_MRT + PROX_PARK + PROX_MALL + \n                            PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                            WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                            WITHIN_1KM_PRISCH,\n                          data=train_data_sp,\n                          bw=bw_adaptive, \n                          kernel = 'gaussian', \n                          adaptive=TRUE,\n                          longlat = FALSE)\n\n\nThe code chunk below will be used to save the model in rds format for future use.\n\n\nCode Chunk\nwrite_rds(gwr_adaptive, \"data/model/gwr_adaptive.rds\")\n\n\nThe code below can be used to display the model output.\n\n\nCode Chunk\ngwr_adaptive\n\n\n\n\n\n\n\nCode Chunk\ntest_data_sp &lt;- test_data %&gt;%\n  as_Spatial()\ntest_data_sp\n\n\nclass       : SpatialPointsDataFrame \nfeatures    : 5566 \nextent      : 11597.31, 42623.63, 28287.8, 48669.59  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 17\nnames       : resale_price, floor_area_sqm, storey_order, remaining_lease_mths,         PROX_CBD,     PROX_ELDERLYCARE,        PROX_HAWKER,           PROX_MRT,          PROX_PARK,   PROX_GOOD_PRISCH,        PROX_MALL,            PROX_CHAS,     PROX_SUPERMARKET, WITHIN_350M_KINDERGARTEN, WITHIN_350M_CHILDCARE, ... \nmin values  :       230888,             74,            1,                  546, 1.00583660772922, 3.34897933104965e-07, 0.0474019664161957, 0.0414043955932523, 0.0502664084494264, 0.0907500295577619,                0, 4.55547870890763e-09, 1.21715176356525e-07,                        0,                     0, ... \nmax values  :      1050000,            138,           14,                 1151,  19.632402730488,     3.30163731686804,   2.83106651960209,   2.13060636038504,   2.41313695915468,   10.6169590126272, 2.26056404492346,     0.79249074802552,     1.53786629004208,                        7,                    16, ... \n\n\n\n\n\n\n\nCode Chunk\ngwr_bw_test_adaptive &lt;- bw.gwr(resale_price ~ floor_area_sqm +\n                  storey_order + remaining_lease_mths +\n                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n                  PROX_MRT + PROX_PARK + PROX_MALL + \n                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                  data=test_data_sp,\n                  approach=\"CV\",\n                  kernel=\"gaussian\",\n                  adaptive=TRUE,\n                  longlat=FALSE)\n\n\nTake a cup of tea and have a break, it will take a few minutes.\n          -----A kind suggestion from GWmodel development group\nAdaptive bandwidth: 3447 CV score: 1.902155e+13 \nAdaptive bandwidth: 2138 CV score: 1.752645e+13 \nAdaptive bandwidth: 1328 CV score: 1.556299e+13 \nAdaptive bandwidth: 828 CV score: 1.357498e+13 \nAdaptive bandwidth: 518 CV score: 1.030751e+13 \nAdaptive bandwidth: 327 CV score: 8.348364e+12 \nAdaptive bandwidth: 208 CV score: 6.860544e+12 \nAdaptive bandwidth: 135 CV score: 5.969504e+12 \nAdaptive bandwidth: 89 CV score: 5.242221e+12 \nAdaptive bandwidth: 62 CV score: 4.742767e+12 \nAdaptive bandwidth: 43 CV score: 4.357839e+12 \nAdaptive bandwidth: 34 CV score: 4.125848e+12 \nAdaptive bandwidth: 25 CV score: 4.056699e+12 \nAdaptive bandwidth: 23 CV score: 4.236349e+13 \nAdaptive bandwidth: 30 CV score: 4.074906e+12 \nAdaptive bandwidth: 25 CV score: 4.056699e+12 \n\n\n\n\n\n\n\nCode Chunk\ngwr_pred &lt;- gwr.predict(formula = resale_price ~\n                          floor_area_sqm + storey_order +\n                          remaining_lease_mths + PROX_CBD + \n                          PROX_ELDERLYCARE + PROX_HAWKER + \n                          PROX_MRT + PROX_PARK + PROX_MALL + \n                          PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                          WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + \n                          WITHIN_1KM_PRISCH, \n                        data=train_data_sp, \n                        predictdata = test_data_sp, \n                        bw=40, \n                        kernel = 'gaussian', \n                        adaptive=TRUE, \n                        longlat = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\ncoords &lt;- st_coordinates(mdata)\ncoords_train &lt;- st_coordinates(train_data)\ncoords_test &lt;- st_coordinates(test_data)\n\n\nBefore continue, we write all the output into rds for future used.\n\n\nCode Chunk\ncoords_train &lt;- write_rds(coords_train, \"data/model/coords_train.rds\" )\ncoords_test &lt;- write_rds(coords_test, \"data/model/coords_test.rds\" )\n\n\n\n\n\nFirst, we will drop geometry column of the sf data.frame by using st_drop_geometry() of sf package.\n\n\nCode Chunk\ntrain_data &lt;- train_data %&gt;% \n  st_drop_geometry()\n\n\n\n\n\n\nIn this section, you will learn how to calibrate a model to predict HDB resale price by using random forest function of ranger package.\n\n\nCode Chunk\nset.seed(1234)\nrf &lt;- ranger(resale_price ~ floor_area_sqm + storey_order + \n               remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE + \n               PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL + \n               PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n               WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + \n               WITHIN_1KM_PRISCH,\n             data=train_data)\nrf\n\n\nRanger result\n\nCall:\n ranger(resale_price ~ floor_area_sqm + storey_order + remaining_lease_mths +      PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK +      PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +      WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH,      data = train_data) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      10335 \nNumber of independent variables:  14 \nMtry:                             3 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       731404460 \nR squared (OOB):                  0.9493789 \n\n\n\n\nCode Chunk\nwrite_rds(rf, \"data/model/rf.rds\")\n\n\n\n\nCode Chunk\nrf &lt;- read_rds(\"data/model/rf.rds\")\nrf\n\n\nRanger result\n\nCall:\n ranger(resale_price ~ floor_area_sqm + storey_order + remaining_lease_mths +      PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK +      PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +      WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH,      data = train_data) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      10335 \nNumber of independent variables:  14 \nMtry:                             3 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       731404460 \nR squared (OOB):                  0.9493789 \n\n\n\n\n\nIn this section, you will learn how to calibrate a model to predict HDB resale price by using grf() of SpatialML package.\n14.11.1 Calibrating using training data\nThe code chunk below calibrate a geographic ranform forest model by using grf() of SpatialML package.\n\n\nCode Chunk\nset.seed(1234)\ngwRF_adaptive &lt;- grf(formula = resale_price ~ floor_area_sqm + storey_order +\n                       remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE +\n                       PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL +\n                       PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                       WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                       WITHIN_1KM_PRISCH,\n                     dframe=train_data, \n                     bw=55,\n                     kernel=\"adaptive\",\n                     coords=coords_train)\n\n\nRanger result\n\nCall:\n ranger(resale_price ~ floor_area_sqm + storey_order + remaining_lease_mths +      PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK +      PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +      WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH,      data = train_data, num.trees = 500, mtry = 4, importance = \"impurity\",      num.threads = NULL) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      10335 \nNumber of independent variables:  14 \nMtry:                             4 \nTarget node size:                 5 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       697593819 \nR squared (OOB):                  0.9517189 \n\n\n          floor_area_sqm             storey_order     remaining_lease_mths \n            7.413197e+12             1.538950e+13             2.890637e+13 \n                PROX_CBD         PROX_ELDERLYCARE              PROX_HAWKER \n            5.310066e+13             7.285092e+12             5.568548e+12 \n                PROX_MRT                PROX_PARK                PROX_MALL \n            7.369745e+12             4.894344e+12             4.223286e+12 \n        PROX_SUPERMARKET WITHIN_350M_KINDERGARTEN    WITHIN_350M_CHILDCARE \n            2.793853e+12             1.018586e+12             1.710374e+12 \n         WITHIN_350M_BUS        WITHIN_1KM_PRISCH \n            1.589501e+12             6.794634e+12 \n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-236112.0  -13033.7     444.4     593.8   14831.5  358041.7 \n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-79279.83  -3510.70     54.56     50.98   3909.85  83074.08 \n\n\n                               Min          Max        Mean         StD\nfloor_area_sqm                   0 401562922035 18210850992 41426270899\nstorey_order             302736445 243728744368 16368419468 23620589843\nremaining_lease_mths     696564138 546463600727 34119912443 70328183398\nPROX_CBD                  55173040 382484896335 12154563393 29293290548\nPROX_ELDERLYCARE          45182031 344081962746 10597657883 24546405941\nPROX_HAWKER               43516026 342597797419 10551807020 23408387903\nPROX_MRT                  54234551 299075025906  9873129985 21055852211\nPROX_PARK                 49919822 322633843469  9353956995 19517077658\nPROX_MALL                 43296133 433263607933 11247374493 27537334970\nPROX_SUPERMARKET          52665827 417310417234 10802122271 26572460731\nWITHIN_350M_KINDERGARTEN         0 186468064682  2848177740 12928886968\nWITHIN_350M_CHILDCARE            0 255236737234  5526292324 18109971102\nWITHIN_350M_BUS                  0 193828795378  4747552546 11886064288\nWITHIN_1KM_PRISCH                0 178360608427  1778262602  7163381668\n\n\nLet’s save the model output by using the code chunk below.\n\n\nCode Chunk\nwrite_rds(gwRF_adaptive, \"data/model/gwRF_adaptive.rds\")\n\n\nThe code chunk below can be used to retrieve the save model in future.\n\n\nCode Chunk\ngwRF_adaptive &lt;- read_rds(\"data/model/gwRF_adaptive.rds\")\n\n\n\n\nPreparing the test data\nThe code chunk below will be used to combine the test data with its corresponding coordinates data.\n\n\nCode Chunk\ntest_data &lt;- cbind(test_data, coords_test) %&gt;%\n  st_drop_geometry()\n\n\n\n\nNext, predict.grf() of spatialML package will be used to predict the resale value by using the test data and gwRF_adaptive model calibrated earlier.\n\n\nCode Chunk\ngwRF_pred &lt;- predict.grf(gwRF_adaptive, \n                           test_data, \n                           x.var.name=\"X\",\n                           y.var.name=\"Y\", \n                           local.w=1,\n                           global.w=0)\n\n\nBefore moving on, let us save the output into rds file for future use.\n\n\nCode Chunk\nGRF_pred &lt;- write_rds(gwRF_pred, \"data/model/GRF_pred.rds\")\n\n\n\n\n\n\n\n\n\n\nCode Chunk\nGRF_pred &lt;- read_rds(\"data/model/GRF_pred.rds\")\nGRF_pred_df &lt;- as.data.frame(GRF_pred)\n\n\nIn the code chunk below, cbind() is used to append the predicted values onto test_datathe\n\n\nCode Chunk\ntest_data_p &lt;- cbind(test_data, GRF_pred_df)\n\n\n\n\nCode Chunk\nwrite_rds(test_data_p, \"data/model/test_data_p.rds\")\n\n\n\n\n\n\nThe root mean square error (RMSE) allows us to measure how far predicted values are from observed values in a regression analysis. In the code chunk below, rmse() of Metrics package is used to compute the RMSE.\n\n\nCode Chunk\nrmse(test_data_p$resale_price, \n     test_data_p$GRF_pred)\n\n\n[1] 27302.9\n\n\n\n\n\nAlternatively, scatterplot can be used to visualise the actual resale price and the predicted resale price by using the code chunk below.\n\n\nCode Chunk\nggplot(data = test_data_p,\n       aes(x = GRF_pred,\n           y = resale_price)) +\n  geom_point()"
  },
  {
    "objectID": "HandsOnExercise/HandsOn8/HandsOn8.html#overview",
    "href": "HandsOnExercise/HandsOn8/HandsOn8.html#overview",
    "title": "Hands-On Exercise 8",
    "section": "",
    "text": "Predictive modelling uses statistical learning or machine learning techniques to predict outcomes. By and large, the event one wants to predict is in the future. However, a set of known outcome and predictors (also known as variables) will be used to calibrate the predictive models.\nGeospatial predictive modelling is conceptually rooted in the principle that the occurrences of events being modeled are limited in distribution. When geographically referenced data are used, occurrences of events are neither uniform nor random in distribution over space. There are geospatial factors (infrastructure, sociocultural, topographic, etc.) that constrain and influence where the locations of events occur. Geospatial predictive modeling attempts to describe those constraints and influences by spatially correlating occurrences of historical geospatial locations with environmental factors that represent those constraints and influences.\n\n\nIn this in-class exercise, you will learn how to build predictive model by using geographical random forest method. By the end of this hands-on exercise, you will acquire the skills of:\n\npreparing training and test data sets by using appropriate data sampling methods\ncalibrating predictive models by using both geospatial statistical learning and machine learning methods,\ncomparing and selecting the best model for predicting the future outcome,\npredicting the future outcomes by using the best model calibrated."
  },
  {
    "objectID": "HandsOnExercise/HandsOn8/HandsOn8.html#the-data",
    "href": "HandsOnExercise/HandsOn8/HandsOn8.html#the-data",
    "title": "Hands-On Exercise 8",
    "section": "",
    "text": "Aspatial dataset:\n\nHDB Resale data: a list of HDB resale transacted prices in Singapore from Jan 2017 onwards. It is in csv format which can be downloaded from Data.gov.sg.\n\nGeospatial dataset:\n\nMP14_SUBZONE_WEB_PL: a polygon feature data providing information of URA 2014 Master Plan Planning Subzone boundary data. It is in ESRI shapefile format. This data set was also downloaded from Data.gov.sg\n\nLocational factors with geographic coordinates:\n\nDownloaded from Data.gov.sg.\n\nEldercare data is a list of eldercare in Singapore. It is in shapefile format.\nHawker Centre data is a list of hawker centres in Singapore. It is in geojson format.\nParks data is a list of parks in Singapore. It is in geojson format.\nSupermarket data is a list of supermarkets in Singapore. It is in geojson format.\nCHAS clinics data is a list of CHAS clinics in Singapore. It is in geojson format.\nChildcare service data is a list of childcare services in Singapore. It is in geojson format\nKindergartens data is a list of kindergartens in Singapore. It is in geojson format.\n\nDownloaded from Datamall.lta.gov.sg.\n\nMRT data is a list of MRT/LRT stations in Singapore with the station names and codes. It is in shapefile format.\nBus stops data is a list of bus stops in Singapore. It is in shapefile format.\n\n\nLocational factors without geographic coordinates:\n\nDownloaded from Data.gov.sg.\n\nPrimary school data is extracted from the list on General information of schools from data.gov portal. It is in csv format.\n\nRetrieved/Scraped from other sources\n\nCBD coordinates obtained from Google.\nShopping malls data is a list of Shopping malls in Singapore obtained from Wikipedia.\nGood primary schools is a list of primary schools that are ordered in ranking in terms of popularity and this can be found at Local Salary Forum."
  },
  {
    "objectID": "HandsOnExercise/HandsOn8/HandsOn8.html#installing-and-loading-r-packages",
    "href": "HandsOnExercise/HandsOn8/HandsOn8.html#installing-and-loading-r-packages",
    "title": "Hands-On Exercise 8",
    "section": "",
    "text": "This code chunk performs 3 tasks:\n\nA list called packages will be created and will consists of all the R packages required to accomplish this exercises\nCheck if R packages on package have been installed in R and if not, they will be installed\nAfter all the R packages have been installed, they will be loaded.\n\n\n\nCode Chunk\npacman::p_load(sf, spdep, GWmodel, SpatialML, \n               tmap, rsample, Metrics, tidyverse)"
  },
  {
    "objectID": "HandsOnExercise/HandsOn8/HandsOn8.html#preparing-data",
    "href": "HandsOnExercise/HandsOn8/HandsOn8.html#preparing-data",
    "title": "Hands-On Exercise 8",
    "section": "",
    "text": "Reading the input data sets. It is in simple feature data frame.\n\n\nCode Chunk\nmdata &lt;- read_rds(\"data/model/mdata.rds\")\n\n\n\n\n\nThe entire data are split into training and test data sets with 65% and 35% respectively by using initial_split() of rsample package. rsample is one of the package of tigymodels.\n\n\nCode Chunk\nset.seed(1234)\nresale_split &lt;- initial_split(mdata, \n                              prop = 6.5/10,)\ntrain_data &lt;- training(resale_split)\ntest_data &lt;- testing(resale_split)\n\n\n\n\nCode Chunk\nwrite_rds(train_data, \"data/model/train_data.rds\")\nwrite_rds(test_data, \"data/model/test_data.rds\")"
  },
  {
    "objectID": "HandsOnExercise/HandsOn8/HandsOn8.html#computing-correlation-matrix",
    "href": "HandsOnExercise/HandsOn8/HandsOn8.html#computing-correlation-matrix",
    "title": "Hands-On Exercise 8",
    "section": "",
    "text": "Before loading the predictors into a predictive model, it is always a good practice to use correlation matrix to examine if there is sign of multicolinearity.\n\n\nCode Chunk\nmdata_nogeo &lt;- mdata %&gt;%\n  st_drop_geometry()\ncorrplot::corrplot(cor(mdata_nogeo[, 2:17]), \n                   diag = FALSE, \n                   order = \"AOE\",\n                   tl.pos = \"td\", \n                   tl.cex = 0.5, \n                   method = \"number\", \n                   type = \"upper\")"
  },
  {
    "objectID": "HandsOnExercise/HandsOn8/HandsOn8.html#retriving-the-stored-data",
    "href": "HandsOnExercise/HandsOn8/HandsOn8.html#retriving-the-stored-data",
    "title": "Hands-On Exercise 8",
    "section": "",
    "text": "Code Chunk\ntrain_data &lt;- read_rds(\"data/model/train_data.rds\")\ntest_data &lt;- read_rds(\"data/model/test_data.rds\")"
  },
  {
    "objectID": "HandsOnExercise/HandsOn8/HandsOn8.html#building-a-non-spatial-multiple-linear-regression",
    "href": "HandsOnExercise/HandsOn8/HandsOn8.html#building-a-non-spatial-multiple-linear-regression",
    "title": "Hands-On Exercise 8",
    "section": "",
    "text": "Code Chunk\nprice_mlr &lt;- lm(resale_price ~ floor_area_sqm +\n                  storey_order + remaining_lease_mths +\n                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n                  PROX_MRT + PROX_PARK + PROX_MALL + \n                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                data=train_data)\nsummary(price_mlr)\n\n\n\nCall:\nlm(formula = resale_price ~ floor_area_sqm + storey_order + remaining_lease_mths + \n    PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK + \n    PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN + \n    WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH, \n    data = train_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205193  -39120   -1930   36545  472355 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              107601.073  10601.261  10.150  &lt; 2e-16 ***\nfloor_area_sqm             2780.698     90.579  30.699  &lt; 2e-16 ***\nstorey_order              14299.298    339.115  42.167  &lt; 2e-16 ***\nremaining_lease_mths        344.490      4.592  75.027  &lt; 2e-16 ***\nPROX_CBD                 -16930.196    201.254 -84.124  &lt; 2e-16 ***\nPROX_ELDERLYCARE         -14441.025    994.867 -14.516  &lt; 2e-16 ***\nPROX_HAWKER              -19265.648   1273.597 -15.127  &lt; 2e-16 ***\nPROX_MRT                 -32564.272   1744.232 -18.670  &lt; 2e-16 ***\nPROX_PARK                 -5712.625   1483.885  -3.850 0.000119 ***\nPROX_MALL                -14717.388   2007.818  -7.330 2.47e-13 ***\nPROX_SUPERMARKET         -26881.938   4189.624  -6.416 1.46e-10 ***\nWITHIN_350M_KINDERGARTEN   8520.472    632.812  13.464  &lt; 2e-16 ***\nWITHIN_350M_CHILDCARE     -4510.650    354.015 -12.741  &lt; 2e-16 ***\nWITHIN_350M_BUS             813.493    222.574   3.655 0.000259 ***\nWITHIN_1KM_PRISCH         -8010.834    491.512 -16.298  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 61650 on 10320 degrees of freedom\nMultiple R-squared:  0.7373,    Adjusted R-squared:  0.737 \nF-statistic:  2069 on 14 and 10320 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCode Chunk\nwrite_rds(price_mlr, \"data/model/price_mlr.rds\" )"
  },
  {
    "objectID": "HandsOnExercise/HandsOn8/HandsOn8.html#gwr-predictive-method",
    "href": "HandsOnExercise/HandsOn8/HandsOn8.html#gwr-predictive-method",
    "title": "Hands-On Exercise 8",
    "section": "",
    "text": "In this section, you will learn how to calibrate a model to predict HDB resale price by using geographically weighted regression method of GWmodel package.\n\n\n\n\nCode Chunk\ntrain_data_sp &lt;- as_Spatial(train_data)\ntrain_data_sp\n\n\nclass       : SpatialPointsDataFrame \nfeatures    : 10335 \nextent      : 11597.31, 42623.63, 28217.39, 48741.06  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 17\nnames       : resale_price, floor_area_sqm, storey_order, remaining_lease_mths,          PROX_CBD,     PROX_ELDERLYCARE,        PROX_HAWKER,           PROX_MRT,          PROX_PARK,   PROX_GOOD_PRISCH,        PROX_MALL,            PROX_CHAS,     PROX_SUPERMARKET, WITHIN_350M_KINDERGARTEN, WITHIN_350M_CHILDCARE, ... \nmin values  :       218000,             74,            1,                  555, 0.999393538715878, 1.98943787433087e-08, 0.0333358643817954, 0.0220407324774434, 0.0441643212802781, 0.0652540365486641,                0, 6.20621206270077e-09, 1.21715176356525e-07,                        0,                     0, ... \nmax values  :      1186888,            133,           17,                 1164,  19.6500691667807,     3.30163731686804,   2.86763031236184,   2.13060636038504,   2.41313695915468,   10.6223726149914, 2.27100643784442,    0.808332738794272,     1.57131703651196,                        7,                    20, ... \n\n\n\n\n\nNext, bw.gwr() of GWmodel package will be used to determine the optimal bandwidth to be used. The code chunk below is used to determine adaptive bandwidth and CV method is used to determine the optimal bandwidth.\n\n\nCode Chunk\nbw_adaptive &lt;- bw.gwr(resale_price ~ floor_area_sqm +\n                  storey_order + remaining_lease_mths +\n                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n                  PROX_MRT + PROX_PARK + PROX_MALL + \n                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                  data=train_data_sp,\n                  approach=\"CV\",\n                  kernel=\"gaussian\",\n                  adaptive=TRUE,\n                  longlat=FALSE)\n\n\nTake a cup of tea and have a break, it will take a few minutes.\n          -----A kind suggestion from GWmodel development group\nAdaptive bandwidth: 6395 CV score: 3.60536e+13 \nAdaptive bandwidth: 3960 CV score: 3.320316e+13 \nAdaptive bandwidth: 2455 CV score: 2.928339e+13 \nAdaptive bandwidth: 1524 CV score: 2.550957e+13 \nAdaptive bandwidth: 950 CV score: 1.95632e+13 \nAdaptive bandwidth: 593 CV score: 1.58347e+13 \nAdaptive bandwidth: 375 CV score: 1.310042e+13 \nAdaptive bandwidth: 237 CV score: 1.113152e+13 \nAdaptive bandwidth: 155 CV score: 9.572037e+12 \nAdaptive bandwidth: 101 CV score: 8.457003e+12 \nAdaptive bandwidth: 71 CV score: 7.605058e+12 \nAdaptive bandwidth: 49 CV score: 6.966278e+12 \nAdaptive bandwidth: 38 CV score: 8.841916e+12 \nAdaptive bandwidth: 58 CV score: 7.275234e+12 \nAdaptive bandwidth: 45 CV score: 6.871966e+12 \nAdaptive bandwidth: 41 CV score: 6.793327e+12 \nAdaptive bandwidth: 40 CV score: 6.780974e+12 \nAdaptive bandwidth: 38 CV score: 8.841916e+12 \nAdaptive bandwidth: 40 CV score: 6.780974e+12 \n\n\nThe result shows that 40 neighbour points will be the optimal bandwidth to be used if adaptive bandwidth is used for this data set.\n\n\nCode Chunk\nwrite_rds(bw_adaptive, \"data/model/bw_adaptive.rds\")\n\n\n\n\n\nFirst, let us call the save bandwidth by using the code chunk below.\n\n\nCode Chunk\nbw_adaptive &lt;- read_rds(\"data/model/bw_adaptive.rds\")\n\n\nNow, we can go ahead to calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and Gaussian kernel as shown in the code chunk below.\n\n\nCode Chunk\ngwr_adaptive &lt;- gwr.basic(formula = resale_price ~\n                            floor_area_sqm + storey_order +\n                            remaining_lease_mths + PROX_CBD + \n                            PROX_ELDERLYCARE + PROX_HAWKER +\n                            PROX_MRT + PROX_PARK + PROX_MALL + \n                            PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                            WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                            WITHIN_1KM_PRISCH,\n                          data=train_data_sp,\n                          bw=bw_adaptive, \n                          kernel = 'gaussian', \n                          adaptive=TRUE,\n                          longlat = FALSE)\n\n\nThe code chunk below will be used to save the model in rds format for future use.\n\n\nCode Chunk\nwrite_rds(gwr_adaptive, \"data/model/gwr_adaptive.rds\")\n\n\nThe code below can be used to display the model output.\n\n\nCode Chunk\ngwr_adaptive\n\n\n\n\n\n\n\nCode Chunk\ntest_data_sp &lt;- test_data %&gt;%\n  as_Spatial()\ntest_data_sp\n\n\nclass       : SpatialPointsDataFrame \nfeatures    : 5566 \nextent      : 11597.31, 42623.63, 28287.8, 48669.59  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 17\nnames       : resale_price, floor_area_sqm, storey_order, remaining_lease_mths,         PROX_CBD,     PROX_ELDERLYCARE,        PROX_HAWKER,           PROX_MRT,          PROX_PARK,   PROX_GOOD_PRISCH,        PROX_MALL,            PROX_CHAS,     PROX_SUPERMARKET, WITHIN_350M_KINDERGARTEN, WITHIN_350M_CHILDCARE, ... \nmin values  :       230888,             74,            1,                  546, 1.00583660772922, 3.34897933104965e-07, 0.0474019664161957, 0.0414043955932523, 0.0502664084494264, 0.0907500295577619,                0, 4.55547870890763e-09, 1.21715176356525e-07,                        0,                     0, ... \nmax values  :      1050000,            138,           14,                 1151,  19.632402730488,     3.30163731686804,   2.83106651960209,   2.13060636038504,   2.41313695915468,   10.6169590126272, 2.26056404492346,     0.79249074802552,     1.53786629004208,                        7,                    16, ... \n\n\n\n\n\n\n\nCode Chunk\ngwr_bw_test_adaptive &lt;- bw.gwr(resale_price ~ floor_area_sqm +\n                  storey_order + remaining_lease_mths +\n                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n                  PROX_MRT + PROX_PARK + PROX_MALL + \n                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                  data=test_data_sp,\n                  approach=\"CV\",\n                  kernel=\"gaussian\",\n                  adaptive=TRUE,\n                  longlat=FALSE)\n\n\nTake a cup of tea and have a break, it will take a few minutes.\n          -----A kind suggestion from GWmodel development group\nAdaptive bandwidth: 3447 CV score: 1.902155e+13 \nAdaptive bandwidth: 2138 CV score: 1.752645e+13 \nAdaptive bandwidth: 1328 CV score: 1.556299e+13 \nAdaptive bandwidth: 828 CV score: 1.357498e+13 \nAdaptive bandwidth: 518 CV score: 1.030751e+13 \nAdaptive bandwidth: 327 CV score: 8.348364e+12 \nAdaptive bandwidth: 208 CV score: 6.860544e+12 \nAdaptive bandwidth: 135 CV score: 5.969504e+12 \nAdaptive bandwidth: 89 CV score: 5.242221e+12 \nAdaptive bandwidth: 62 CV score: 4.742767e+12 \nAdaptive bandwidth: 43 CV score: 4.357839e+12 \nAdaptive bandwidth: 34 CV score: 4.125848e+12 \nAdaptive bandwidth: 25 CV score: 4.056699e+12 \nAdaptive bandwidth: 23 CV score: 4.236349e+13 \nAdaptive bandwidth: 30 CV score: 4.074906e+12 \nAdaptive bandwidth: 25 CV score: 4.056699e+12 \n\n\n\n\n\n\n\nCode Chunk\ngwr_pred &lt;- gwr.predict(formula = resale_price ~\n                          floor_area_sqm + storey_order +\n                          remaining_lease_mths + PROX_CBD + \n                          PROX_ELDERLYCARE + PROX_HAWKER + \n                          PROX_MRT + PROX_PARK + PROX_MALL + \n                          PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                          WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + \n                          WITHIN_1KM_PRISCH, \n                        data=train_data_sp, \n                        predictdata = test_data_sp, \n                        bw=40, \n                        kernel = 'gaussian', \n                        adaptive=TRUE, \n                        longlat = FALSE)"
  },
  {
    "objectID": "HandsOnExercise/HandsOn8/HandsOn8.html#preparing-coordinates-data",
    "href": "HandsOnExercise/HandsOn8/HandsOn8.html#preparing-coordinates-data",
    "title": "Hands-On Exercise 8",
    "section": "",
    "text": "Code Chunk\ncoords &lt;- st_coordinates(mdata)\ncoords_train &lt;- st_coordinates(train_data)\ncoords_test &lt;- st_coordinates(test_data)\n\n\nBefore continue, we write all the output into rds for future used.\n\n\nCode Chunk\ncoords_train &lt;- write_rds(coords_train, \"data/model/coords_train.rds\" )\ncoords_test &lt;- write_rds(coords_test, \"data/model/coords_test.rds\" )\n\n\n\n\n\nFirst, we will drop geometry column of the sf data.frame by using st_drop_geometry() of sf package.\n\n\nCode Chunk\ntrain_data &lt;- train_data %&gt;% \n  st_drop_geometry()"
  },
  {
    "objectID": "HandsOnExercise/HandsOn8/HandsOn8.html#calibrating-random-forest-model",
    "href": "HandsOnExercise/HandsOn8/HandsOn8.html#calibrating-random-forest-model",
    "title": "Hands-On Exercise 8",
    "section": "",
    "text": "In this section, you will learn how to calibrate a model to predict HDB resale price by using random forest function of ranger package.\n\n\nCode Chunk\nset.seed(1234)\nrf &lt;- ranger(resale_price ~ floor_area_sqm + storey_order + \n               remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE + \n               PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL + \n               PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n               WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + \n               WITHIN_1KM_PRISCH,\n             data=train_data)\nrf\n\n\nRanger result\n\nCall:\n ranger(resale_price ~ floor_area_sqm + storey_order + remaining_lease_mths +      PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK +      PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +      WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH,      data = train_data) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      10335 \nNumber of independent variables:  14 \nMtry:                             3 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       731404460 \nR squared (OOB):                  0.9493789 \n\n\n\n\nCode Chunk\nwrite_rds(rf, \"data/model/rf.rds\")\n\n\n\n\nCode Chunk\nrf &lt;- read_rds(\"data/model/rf.rds\")\nrf\n\n\nRanger result\n\nCall:\n ranger(resale_price ~ floor_area_sqm + storey_order + remaining_lease_mths +      PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK +      PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +      WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH,      data = train_data) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      10335 \nNumber of independent variables:  14 \nMtry:                             3 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       731404460 \nR squared (OOB):                  0.9493789"
  },
  {
    "objectID": "HandsOnExercise/HandsOn8/HandsOn8.html#calibrating-geographical-random-forest-model",
    "href": "HandsOnExercise/HandsOn8/HandsOn8.html#calibrating-geographical-random-forest-model",
    "title": "Hands-On Exercise 8",
    "section": "",
    "text": "In this section, you will learn how to calibrate a model to predict HDB resale price by using grf() of SpatialML package.\n14.11.1 Calibrating using training data\nThe code chunk below calibrate a geographic ranform forest model by using grf() of SpatialML package.\n\n\nCode Chunk\nset.seed(1234)\ngwRF_adaptive &lt;- grf(formula = resale_price ~ floor_area_sqm + storey_order +\n                       remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE +\n                       PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL +\n                       PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                       WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                       WITHIN_1KM_PRISCH,\n                     dframe=train_data, \n                     bw=55,\n                     kernel=\"adaptive\",\n                     coords=coords_train)\n\n\nRanger result\n\nCall:\n ranger(resale_price ~ floor_area_sqm + storey_order + remaining_lease_mths +      PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK +      PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +      WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH,      data = train_data, num.trees = 500, mtry = 4, importance = \"impurity\",      num.threads = NULL) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      10335 \nNumber of independent variables:  14 \nMtry:                             4 \nTarget node size:                 5 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       697593819 \nR squared (OOB):                  0.9517189 \n\n\n          floor_area_sqm             storey_order     remaining_lease_mths \n            7.413197e+12             1.538950e+13             2.890637e+13 \n                PROX_CBD         PROX_ELDERLYCARE              PROX_HAWKER \n            5.310066e+13             7.285092e+12             5.568548e+12 \n                PROX_MRT                PROX_PARK                PROX_MALL \n            7.369745e+12             4.894344e+12             4.223286e+12 \n        PROX_SUPERMARKET WITHIN_350M_KINDERGARTEN    WITHIN_350M_CHILDCARE \n            2.793853e+12             1.018586e+12             1.710374e+12 \n         WITHIN_350M_BUS        WITHIN_1KM_PRISCH \n            1.589501e+12             6.794634e+12 \n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-236112.0  -13033.7     444.4     593.8   14831.5  358041.7 \n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-79279.83  -3510.70     54.56     50.98   3909.85  83074.08 \n\n\n                               Min          Max        Mean         StD\nfloor_area_sqm                   0 401562922035 18210850992 41426270899\nstorey_order             302736445 243728744368 16368419468 23620589843\nremaining_lease_mths     696564138 546463600727 34119912443 70328183398\nPROX_CBD                  55173040 382484896335 12154563393 29293290548\nPROX_ELDERLYCARE          45182031 344081962746 10597657883 24546405941\nPROX_HAWKER               43516026 342597797419 10551807020 23408387903\nPROX_MRT                  54234551 299075025906  9873129985 21055852211\nPROX_PARK                 49919822 322633843469  9353956995 19517077658\nPROX_MALL                 43296133 433263607933 11247374493 27537334970\nPROX_SUPERMARKET          52665827 417310417234 10802122271 26572460731\nWITHIN_350M_KINDERGARTEN         0 186468064682  2848177740 12928886968\nWITHIN_350M_CHILDCARE            0 255236737234  5526292324 18109971102\nWITHIN_350M_BUS                  0 193828795378  4747552546 11886064288\nWITHIN_1KM_PRISCH                0 178360608427  1778262602  7163381668\n\n\nLet’s save the model output by using the code chunk below.\n\n\nCode Chunk\nwrite_rds(gwRF_adaptive, \"data/model/gwRF_adaptive.rds\")\n\n\nThe code chunk below can be used to retrieve the save model in future.\n\n\nCode Chunk\ngwRF_adaptive &lt;- read_rds(\"data/model/gwRF_adaptive.rds\")\n\n\n\n\nPreparing the test data\nThe code chunk below will be used to combine the test data with its corresponding coordinates data.\n\n\nCode Chunk\ntest_data &lt;- cbind(test_data, coords_test) %&gt;%\n  st_drop_geometry()\n\n\n\n\nNext, predict.grf() of spatialML package will be used to predict the resale value by using the test data and gwRF_adaptive model calibrated earlier.\n\n\nCode Chunk\ngwRF_pred &lt;- predict.grf(gwRF_adaptive, \n                           test_data, \n                           x.var.name=\"X\",\n                           y.var.name=\"Y\", \n                           local.w=1,\n                           global.w=0)\n\n\nBefore moving on, let us save the output into rds file for future use.\n\n\nCode Chunk\nGRF_pred &lt;- write_rds(gwRF_pred, \"data/model/GRF_pred.rds\")\n\n\n\n\n\n\n\n\n\n\nCode Chunk\nGRF_pred &lt;- read_rds(\"data/model/GRF_pred.rds\")\nGRF_pred_df &lt;- as.data.frame(GRF_pred)\n\n\nIn the code chunk below, cbind() is used to append the predicted values onto test_datathe\n\n\nCode Chunk\ntest_data_p &lt;- cbind(test_data, GRF_pred_df)\n\n\n\n\nCode Chunk\nwrite_rds(test_data_p, \"data/model/test_data_p.rds\")\n\n\n\n\n\n\nThe root mean square error (RMSE) allows us to measure how far predicted values are from observed values in a regression analysis. In the code chunk below, rmse() of Metrics package is used to compute the RMSE.\n\n\nCode Chunk\nrmse(test_data_p$resale_price, \n     test_data_p$GRF_pred)\n\n\n[1] 27302.9\n\n\n\n\n\nAlternatively, scatterplot can be used to visualise the actual resale price and the predicted resale price by using the code chunk below.\n\n\nCode Chunk\nggplot(data = test_data_p,\n       aes(x = GRF_pred,\n           y = resale_price)) +\n  geom_point()"
  },
  {
    "objectID": "InClassExercise/InClass8/InClass8.html",
    "href": "InClassExercise/InClass8/InClass8.html",
    "title": "In-Class Exercise 8",
    "section": "",
    "text": "#In class Exercise 9"
  },
  {
    "objectID": "HandsOnExercise/HandsOn9/HandsOn9.html",
    "href": "HandsOnExercise/HandsOn9/HandsOn9.html",
    "title": "Hands-On 9",
    "section": "",
    "text": "In this hands-on exercise, you will gain hands-on experience on how to model geographical accessibility by using R’s geospatial analysis packages.\n\n\nBy the end of this hands-on exercise, you will be able:\n\nto import GIS polygon data into R and save them as simple feature data frame by using appropriate functions of sf package of R;\nto import aspatial data into R and save them as simple feature data frame by using appropriate functions of sf package of R;\nto computer accessibility measure by using Hansen’s potential model and Spatial Accessibility Measure (SAM); and\nto visualise the accessibility measures by using tmap and ggplot2 packages.\n\n\n\n\n\nFour data sets will be used in this hands-on exercise, they are:\n\nMP14_SUBZONE_NO_SEA_PL: URA Master Plan 2014 subzone boundary GIS data. This data set is downloaded from data.gov.sg.\nhexagons: A 250m radius hexagons GIS data. This data set was created by using st_make_grid() of sf package. It is in ESRI shapefile format.\nELDERCARE: GIS data showing location of eldercare service. This data is downloaded from data.gov.sg. There are two versions. One in ESRI shapefile format. The other one in Google kml file format. For the purpose of this hands-on exercise, ESRI shapefile format is provided.\nOD_Matrix: a distance matrix in csv format. There are six fields in the data file. They are:\n\norigin_id: the unique id values of the origin (i.e. fid of hexagon data set.),\ndestination_id: the unique id values of the destination (i.e. fid of ELDERCARE data set.),\nentry_cost: the perpendicular distance between the origins and the nearest road),\nnetwork_cost: the actual network distance from the origin and destination,\nexit_cost: the perpendicular distance between the destination and the nearest road), and\ntotal_cost: the summation of entry_cost, network_cost and exit_cost.\nAll the values of the cost related fields are in metres.\n\n\nAll the values of the cost related filed are in metres\nReminder: Except MP14_SUBZONE_NO_SEA_PL data set, the other three data set are specially prepared by Prof. Kam for teaching and research purpose. Students taking IS415 Geospatial Analytics and Applications are allowed to use them for hands-on exercise purpose. Please obtain formal approval from Prof. Kam if you want to use them for other courses or usage.\nJosh: Yes Prof 😭\n\n\n\nBefore we getting started, it is important for us to install the necessary R packages and launch them into RStudio environment.\nThe R packages need for this exercise are as follows:\n\nSpatial data handling\n\nsf\n\nModelling geographical accessibility\n\nspatialAcc\n\nAttribute data handling\n\ntidyverse, especially readr and dplyr\n\nthematic mapping\n\ntmap\n\nStatistical graphic\n\nggplot2\n\nStatistical analysis\n\nggstatsplot\n\n\nThe code chunk below installs and launches these R packages into RStudio environment.\n\n\nCode Chunk\npacman::p_load(tmap, SpatialAcc, sf,\n               ggstatsplot, reshape2,\n               tidyverse)\n\n\n\n\n\n\n\n\nPower of Tidyverse\n\n\n\nNotice that with tidyverse, we do not have to install readr, dplyr and ggplots packages separately. In fact, tidyverse also installs other R packages such as tidyr, stringr, forcats, tibble, purrr and magrittr.\n\n\n\n\n\n\n\nThree geospatial data will be imported from the data/geospatial sub-folder. They are MP14_SUBZONE_NO_SEA_PL, hexagons and ELDERCARE.\nThe code chunk below is used to import these three data sets shapefile by using st_read() of sf packages.\n\n\nCode Chunk\nmpsz &lt;- st_read(dsn = \"data/geospatial\",\n                layer=\"MP14_SUBZONE_NO_SEA_PL\")\n\n\nReading layer `MP14_SUBZONE_NO_SEA_PL' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/HandsOnExercise/HandsOn9/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\nCode Chunk\nhexagons &lt;- st_read(dsn = \"data/geospatial\",\n                    layer = \"hexagons\")\n\n\nReading layer `hexagons' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/HandsOnExercise/HandsOn9/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 3125 features and 6 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 21506.33 xmax: 50010.26 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\n\n\n\n\nCode Chunk\neldercare &lt;-st_read(dsn = \"data/geospatial\",\n                    layer = \"ELDERCARE\")\n\n\nReading layer `ELDERCARE' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/HandsOnExercise/HandsOn9/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 120 features and 19 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 14481.92 ymin: 28218.43 xmax: 41665.14 ymax: 46804.9\nProjected CRS: SVY21 / Singapore TM\n\n\nThe report above shows that the R object used to contain the imported MP14_SUBZONE_WEB_PL shapefile is called mpsz and it is a simple feature object. The geometry type is multipolygon. it is also important to note that mpsz simple feature object does not have EPSG information.\n\n\n\nThe code chunk below updates the newly imported mpsz with the correct ESPG code (i.e. 3414)\n\n\nCode Chunk\nmpsz &lt;- st_transform(mpsz, 3414)\nhexagons &lt;- st_transform(hexagons, 3414)\neldercare &lt;- st_transform(eldercare, 3414)\n\n\nAfter transforming the projection metadata, you can verify the projection of the newly transformed mpsz_svy21 by using st_crs() of sf package.\nThe code chunk below will be used to varify the newly transformed mpsz_svy21.\n\n\nCode Chunk\nst_crs(mpsz)\n\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\n\n\nThere are many redundant fields in the data tables of both eldercare and hexagons. The code chunks below will be used to exclude those redundant fields. At the same time, a new field called demand and a new field called capacity will be added into the data table of hexagons and eldercare sf data frame respectively. Both fields are derive using mutate() of dplyr package.\n\n\n\n\n\n\nCLARIFY CODE BELOW\n\n\n\n\n\n\n\n\nCode Chunk\neldercare &lt;- eldercare %&gt;% #cmd + shift + m\n  select(fid, ADDRESSPOS) %&gt;%\n  mutate(capacity = 100)\n\n\n\n\nCode Chunk\nhexagons &lt;- hexagons %&gt;% \n  select(fid) %&gt;% \n  mutate(demand = 100)\n\n\nNotice that for the purpose of this hands-on exercise, a constant value of 100 is used. In practice, actual demand of the hexagon and capacity of the eldercare centre should be used.\n\n\n\n\n\n\nThe code chunk below uses read_cvs() of readr package to import OD_Matrix.csv into RStudio. The imported object is a tibble data.frame called ODMatrix.\n\n\nCode Chunk\nODMatrix &lt;- read_csv(\"data/aspatial/OD_Matrix.csv\")\n\n\n\n\n\nThe imported ODMatrix organised the distance matrix columnwise.\n\nOn the other hands, most of the modelling packages in R is expecting a matrix look similar to the figure below.\n\nThe rows represent origins (i.e. also know as from field) and the columns represent destination (i.e. also known as to field.)\nThe code chunk below uses spread() of tidyr package is used to transform the O-D matrix from a thin format into a fat format.\n\n\nCode Chunk\ndistmat &lt;- ODMatrix %&gt;% \n  select(origin_id, destination_id, total_cost) %&gt;% \n  spread(destination_id, total_cost) %&gt;% \n  select(-c('origin_id'))\n\n\nNote: Since tidyr version 1.0 a new function called pivot_wider() is introduce. You should use pivot_wider() instead of spread()\nCurrently, the distance is measured in metre because SVY21 projected coordinate system is used. The code chunk below will be used to convert the unit f measurement from metre to kilometre.\n\n\nCode Chunk\ndistmat_km &lt;- as.matrix(distmat/1000)\n\n\n\n\n\n\n\n\nNow, we ready to compute Hansen’s accessibility by using ac() of SpatialAcc package. Before getting started, you are encourage to read the arguments of the function at least once in order to ensure that the required inputs are available.\nThe code chunk below calculates Hansen’s accessibility using ac() of SpatialAcc and data.frame() is used to save the output in a data frame called acc_Handsen.\n\n\nCode Chunk\nacc_Hansen &lt;- data.frame(ac(hexagons$demand,\n                            eldercare$capacity,\n                            distmat_km, \n                            #d0 = 50,\n                            power = 2, \n                            family = \"Hansen\"))\n\n\n\nThe default field name is very messy, we will rename it to accHansen by using the code chunk below.\n\n\nCode Chunk\ncolnames(acc_Hansen) &lt;- \"accHansen\"\n\n\nNotice that the field name is much more tidier now.\nLastly, bind_cols() of dplyr will be used to join the acc_Hansen tibble data frame with the hexagons simple feature dataframe. The output is called hexagon_Hansen.\n\n\nCode Chunk\nhexagon_Hansen &lt;- bind_cols(hexagons, acc_Hansen)\n\n\nNotice that hexagon_Hansen is a simple feature data frame and not a typical tibble data frame.\n\nActually, the steps above can be perform by using a single code chunk as shown below.\n\n\nCode Chunk\nacc_Hansen &lt;- data.frame(ac(hexagons$demand,\n                            eldercare$capacity,\n                            distmat_km, \n                            #d0 = 50,\n                            power = 0.5, \n                            family = \"Hansen\"))\n\ncolnames(acc_Hansen) &lt;- \"accHansen\"\nacc_Hansen &lt;- tbl_df(acc_Hansen)\nhexagon_Hansen &lt;- bind_cols(hexagons, acc_Hansen)\n\n\n\n\n\n\n\nFirstly, we will extract the extend of hexagons simple feature data frame by by using st_bbox() of sf package.\n\n\nCode Chunk\nmapex &lt;- st_bbox(hexagons)\n\n\nThe code chunk below uses a collection of mapping fucntions of tmap package to create a high cartographic quality accessibility to eldercare centre in Singapore.\n\n\nCode Chunk\ntmap_mode(\"plot\")\ntm_shape(hexagon_Hansen,\n         bbox = mapex) + \n  tm_fill(col = \"accHansen\",\n          n = 10,\n          style = \"quantile\",\n          border.col = \"black\",\n          border.lwd = 1) +\ntm_shape(eldercare) +\n  tm_symbols(size = 0.1) +\n  tm_layout(main.title = \"Accessibility to eldercare: Hansen method\",\n            main.title.position = \"center\",\n            main.title.size = 2,\n            legend.outside = FALSE,\n            legend.height = 0.45, \n            legend.width = 3.0,\n            legend.format = list(digits = 6),\n            legend.position = c(\"right\", \"top\"),\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this section, we are going to compare the distribution of Hansen’s accessibility values by URA Planning Region.\nFirstly, we need to add the planning region field into haxegon_Hansen simple feature data frame by using the code chunk below.\n\n\nCode Chunk\nhexagon_Hansen &lt;- st_join(hexagon_Hansen, mpsz,\n                          join = st_intersects)\n\n\nNext, ggplot() will be used to plot the distribution by using boxplot graphical method.\n\n\nCode Chunk\nggplot(data=hexagon_Hansen, \n       aes(y = log(accHansen), \n           x= REGION_N)) +\n  geom_boxplot() +\n  geom_point(stat=\"summary\", \n             fun.y=\"mean\", \n             colour =\"red\", \n             size=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this section, you are going to repeat most of the steps you had learned in previous section to perform the analysis. However, some of the codes will be combined into one code chunk.\n\nThe code chunk below calculates Hansen’s accessibility using ac() of SpatialAcc and data.frame() is used to save the output in a data frame called acc_KD2SFCA. Notice that KD2SFCA is used for family argument.\n\n\nCode Chunk\nacc_KD2SFCA &lt;- data.frame(ac(hexagons$demand,\n                            eldercare$capacity,\n                            distmat_km, \n                            d0 = 50,\n                            power = 2, \n                            family = \"KD2SFCA\"))\n\ncolnames(acc_KD2SFCA) &lt;- \"accKD2SFCA\"\nacc_KD2SFCA &lt;- tbl_df(acc_KD2SFCA)\nhexagon_KD2SFCA &lt;- bind_cols(hexagons, acc_KD2SFCA)\n\n\n\n\n\nThe code chunk below uses a collection of mapping fucntions of tmap package to create a high cartographic quality accessibility to eldercare centre in Singapore. Notice that mapex is reused for bbox argument.\n\n\nCode Chunk\ntmap_mode(\"plot\")\ntm_shape(hexagon_KD2SFCA,\n         bbox = mapex) + \n  tm_fill(col = \"accKD2SFCA\",\n          n = 10,\n          style = \"quantile\",\n          border.col = \"black\",\n          border.lwd = 1) +\ntm_shape(eldercare) +\n  tm_symbols(size = 0.1) +\n  tm_layout(main.title = \"Accessibility to eldercare: KD2SFCA method\",\n            main.title.position = \"center\",\n            main.title.size = 2,\n            legend.outside = FALSE,\n            legend.height = 0.45, \n            legend.width = 3.0,\n            legend.format = list(digits = 6),\n            legend.position = c(\"right\", \"top\"),\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\nNow, we are going to compare the distribution of KD2CFA accessibility values by URA Planning Region.\nFirstly, we need to add the planning region field into hexagon_KD2SFCA simple feature data frame by using the code chunk below.\n\n\nCode Chunk\nhexagon_KD2SFCA &lt;- st_join(hexagon_KD2SFCA, mpsz, \n                          join = st_intersects)\n\n\nNext, ggplot() will be used to plot the distribution by using boxplot graphical method.\n\n\nCode Chunk\nggplot(data=hexagon_KD2SFCA, \n       aes(y = accKD2SFCA, \n           x= REGION_N)) +\n  geom_boxplot() +\n  geom_point(stat=\"summary\", \n             fun.y=\"mean\", \n             colour =\"red\", \n             size=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this section, you are going to repeat most of the steps you had learned in previous section to perform the analysis. However, some of the codes will be combined into one code chunk.\n\nThe code chunk below calculates Hansen’s accessibility using ac() of SpatialAcc and data.frame() is used to save the output in a data frame called acc_SAM. Notice that SAM is used for family argument.\n\n\nCode Chunk\nacc_SAM &lt;- data.frame(ac(hexagons$demand,\n                         eldercare$capacity,\n                         distmat_km,\n                         d0 = 50,\n                         power = 2,\n                         family = \"SAM\"))\n\ncolnames(acc_SAM) &lt;- \"accSAM\"\nacc_SAM &lt;- tbl_df(acc_SAM)\nhexagon_SAM &lt;- bind_cols(hexagons, acc_SAM)\n\n\n\n\n\nThe code chunk below uses a collection of mapping fucntions of tmap package to create a high cartographic quality accessibility to eldercare centre in Singapore. Notice that mapex is reused for bbox argument.\n\n\nCode Chunk\ntmap_mode(\"plot\")\ntm_shape(hexagon_SAM,\n         bbox = mapex) +\n  tm_fill(col = \"accSAM\",\n          n=10,\n          style = \"quantile\",\n          border.col = \"black\",\n          border.lwd = 1) +\n  tm_shape(eldercare) +\n  tm_symbols(size = 0.1) +\n  tm_layout(main.title = \"Accessibility to eldercare: SAM method\",\n            main.title.position = \"center\",\n            main.title.size = 2,\n            legend.outside = FALSE,\n            legend.height = 0.45, \n            legend.width = 3.0,\n            legend.format = list(digits = 3),\n            legend.position = c(\"right\", \"top\"),\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\nNow, we are going to compare the distribution of SAM accessibility values by URA Planning Region.\nFirstly, we need to add the planning region field into hexagon_SAM simple feature data frame by using the code chunk below.\n\n\nCode Chunk\nhexagon_SAM &lt;- st_join(hexagon_SAM, mpsz,\n                       join = st_intersects)\n\n\nNext, ggplot() will be used to plot the distribution by using boxplot graphical method.\n\n\nCode Chunk\nggplot(data=hexagon_SAM, \n       aes(y = accSAM, \n           x= REGION_N)) +\n  geom_boxplot() +\n  geom_point(stat=\"summary\", \n             fun.y=\"mean\", \n             colour =\"red\", \n             size=2)"
  },
  {
    "objectID": "HandsOnExercise/HandsOn9/HandsOn9.html#introduction",
    "href": "HandsOnExercise/HandsOn9/HandsOn9.html#introduction",
    "title": "Hands-On 9",
    "section": "",
    "text": "In this hands-on exercise, you will gain hands-on experience on how to model geographical accessibility by using R’s geospatial analysis packages.\n\n\nBy the end of this hands-on exercise, you will be able:\n\nto import GIS polygon data into R and save them as simple feature data frame by using appropriate functions of sf package of R;\nto import aspatial data into R and save them as simple feature data frame by using appropriate functions of sf package of R;\nto computer accessibility measure by using Hansen’s potential model and Spatial Accessibility Measure (SAM); and\nto visualise the accessibility measures by using tmap and ggplot2 packages."
  },
  {
    "objectID": "HandsOnExercise/HandsOn9/HandsOn9.html#data",
    "href": "HandsOnExercise/HandsOn9/HandsOn9.html#data",
    "title": "Hands-On 9",
    "section": "",
    "text": "Four data sets will be used in this hands-on exercise, they are:\n\nMP14_SUBZONE_NO_SEA_PL: URA Master Plan 2014 subzone boundary GIS data. This data set is downloaded from data.gov.sg.\nhexagons: A 250m radius hexagons GIS data. This data set was created by using st_make_grid() of sf package. It is in ESRI shapefile format.\nELDERCARE: GIS data showing location of eldercare service. This data is downloaded from data.gov.sg. There are two versions. One in ESRI shapefile format. The other one in Google kml file format. For the purpose of this hands-on exercise, ESRI shapefile format is provided.\nOD_Matrix: a distance matrix in csv format. There are six fields in the data file. They are:\n\norigin_id: the unique id values of the origin (i.e. fid of hexagon data set.),\ndestination_id: the unique id values of the destination (i.e. fid of ELDERCARE data set.),\nentry_cost: the perpendicular distance between the origins and the nearest road),\nnetwork_cost: the actual network distance from the origin and destination,\nexit_cost: the perpendicular distance between the destination and the nearest road), and\ntotal_cost: the summation of entry_cost, network_cost and exit_cost.\nAll the values of the cost related fields are in metres.\n\n\nAll the values of the cost related filed are in metres\nReminder: Except MP14_SUBZONE_NO_SEA_PL data set, the other three data set are specially prepared by Prof. Kam for teaching and research purpose. Students taking IS415 Geospatial Analytics and Applications are allowed to use them for hands-on exercise purpose. Please obtain formal approval from Prof. Kam if you want to use them for other courses or usage.\nJosh: Yes Prof 😭"
  },
  {
    "objectID": "HandsOnExercise/HandsOn9/HandsOn9.html#getting-started",
    "href": "HandsOnExercise/HandsOn9/HandsOn9.html#getting-started",
    "title": "Hands-On 9",
    "section": "",
    "text": "Before we getting started, it is important for us to install the necessary R packages and launch them into RStudio environment.\nThe R packages need for this exercise are as follows:\n\nSpatial data handling\n\nsf\n\nModelling geographical accessibility\n\nspatialAcc\n\nAttribute data handling\n\ntidyverse, especially readr and dplyr\n\nthematic mapping\n\ntmap\n\nStatistical graphic\n\nggplot2\n\nStatistical analysis\n\nggstatsplot\n\n\nThe code chunk below installs and launches these R packages into RStudio environment.\n\n\nCode Chunk\npacman::p_load(tmap, SpatialAcc, sf,\n               ggstatsplot, reshape2,\n               tidyverse)\n\n\n\n\n\n\n\n\nPower of Tidyverse\n\n\n\nNotice that with tidyverse, we do not have to install readr, dplyr and ggplots packages separately. In fact, tidyverse also installs other R packages such as tidyr, stringr, forcats, tibble, purrr and magrittr."
  },
  {
    "objectID": "HandsOnExercise/HandsOn9/HandsOn9.html#geospatial-data-wrangling",
    "href": "HandsOnExercise/HandsOn9/HandsOn9.html#geospatial-data-wrangling",
    "title": "Hands-On 9",
    "section": "",
    "text": "Three geospatial data will be imported from the data/geospatial sub-folder. They are MP14_SUBZONE_NO_SEA_PL, hexagons and ELDERCARE.\nThe code chunk below is used to import these three data sets shapefile by using st_read() of sf packages.\n\n\nCode Chunk\nmpsz &lt;- st_read(dsn = \"data/geospatial\",\n                layer=\"MP14_SUBZONE_NO_SEA_PL\")\n\n\nReading layer `MP14_SUBZONE_NO_SEA_PL' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/HandsOnExercise/HandsOn9/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\nCode Chunk\nhexagons &lt;- st_read(dsn = \"data/geospatial\",\n                    layer = \"hexagons\")\n\n\nReading layer `hexagons' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/HandsOnExercise/HandsOn9/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 3125 features and 6 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 21506.33 xmax: 50010.26 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\n\n\n\n\nCode Chunk\neldercare &lt;-st_read(dsn = \"data/geospatial\",\n                    layer = \"ELDERCARE\")\n\n\nReading layer `ELDERCARE' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/HandsOnExercise/HandsOn9/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 120 features and 19 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 14481.92 ymin: 28218.43 xmax: 41665.14 ymax: 46804.9\nProjected CRS: SVY21 / Singapore TM\n\n\nThe report above shows that the R object used to contain the imported MP14_SUBZONE_WEB_PL shapefile is called mpsz and it is a simple feature object. The geometry type is multipolygon. it is also important to note that mpsz simple feature object does not have EPSG information.\n\n\n\nThe code chunk below updates the newly imported mpsz with the correct ESPG code (i.e. 3414)\n\n\nCode Chunk\nmpsz &lt;- st_transform(mpsz, 3414)\nhexagons &lt;- st_transform(hexagons, 3414)\neldercare &lt;- st_transform(eldercare, 3414)\n\n\nAfter transforming the projection metadata, you can verify the projection of the newly transformed mpsz_svy21 by using st_crs() of sf package.\nThe code chunk below will be used to varify the newly transformed mpsz_svy21.\n\n\nCode Chunk\nst_crs(mpsz)\n\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\n\n\nThere are many redundant fields in the data tables of both eldercare and hexagons. The code chunks below will be used to exclude those redundant fields. At the same time, a new field called demand and a new field called capacity will be added into the data table of hexagons and eldercare sf data frame respectively. Both fields are derive using mutate() of dplyr package.\n\n\n\n\n\n\nCLARIFY CODE BELOW\n\n\n\n\n\n\n\n\nCode Chunk\neldercare &lt;- eldercare %&gt;% #cmd + shift + m\n  select(fid, ADDRESSPOS) %&gt;%\n  mutate(capacity = 100)\n\n\n\n\nCode Chunk\nhexagons &lt;- hexagons %&gt;% \n  select(fid) %&gt;% \n  mutate(demand = 100)\n\n\nNotice that for the purpose of this hands-on exercise, a constant value of 100 is used. In practice, actual demand of the hexagon and capacity of the eldercare centre should be used."
  },
  {
    "objectID": "HandsOnExercise/HandsOn9/HandsOn9.html#apsaital-data-handling-and-wrangling",
    "href": "HandsOnExercise/HandsOn9/HandsOn9.html#apsaital-data-handling-and-wrangling",
    "title": "Hands-On 9",
    "section": "",
    "text": "The code chunk below uses read_cvs() of readr package to import OD_Matrix.csv into RStudio. The imported object is a tibble data.frame called ODMatrix.\n\n\nCode Chunk\nODMatrix &lt;- read_csv(\"data/aspatial/OD_Matrix.csv\")\n\n\n\n\n\nThe imported ODMatrix organised the distance matrix columnwise.\n\nOn the other hands, most of the modelling packages in R is expecting a matrix look similar to the figure below.\n\nThe rows represent origins (i.e. also know as from field) and the columns represent destination (i.e. also known as to field.)\nThe code chunk below uses spread() of tidyr package is used to transform the O-D matrix from a thin format into a fat format.\n\n\nCode Chunk\ndistmat &lt;- ODMatrix %&gt;% \n  select(origin_id, destination_id, total_cost) %&gt;% \n  spread(destination_id, total_cost) %&gt;% \n  select(-c('origin_id'))\n\n\nNote: Since tidyr version 1.0 a new function called pivot_wider() is introduce. You should use pivot_wider() instead of spread()\nCurrently, the distance is measured in metre because SVY21 projected coordinate system is used. The code chunk below will be used to convert the unit f measurement from metre to kilometre.\n\n\nCode Chunk\ndistmat_km &lt;- as.matrix(distmat/1000)"
  },
  {
    "objectID": "HandsOnExercise/HandsOn9/HandsOn9.html#modelling-and-visualising-accessibility-using-hansen-method",
    "href": "HandsOnExercise/HandsOn9/HandsOn9.html#modelling-and-visualising-accessibility-using-hansen-method",
    "title": "Hands-On 9",
    "section": "",
    "text": "Now, we ready to compute Hansen’s accessibility by using ac() of SpatialAcc package. Before getting started, you are encourage to read the arguments of the function at least once in order to ensure that the required inputs are available.\nThe code chunk below calculates Hansen’s accessibility using ac() of SpatialAcc and data.frame() is used to save the output in a data frame called acc_Handsen.\n\n\nCode Chunk\nacc_Hansen &lt;- data.frame(ac(hexagons$demand,\n                            eldercare$capacity,\n                            distmat_km, \n                            #d0 = 50,\n                            power = 2, \n                            family = \"Hansen\"))\n\n\n\nThe default field name is very messy, we will rename it to accHansen by using the code chunk below.\n\n\nCode Chunk\ncolnames(acc_Hansen) &lt;- \"accHansen\"\n\n\nNotice that the field name is much more tidier now.\nLastly, bind_cols() of dplyr will be used to join the acc_Hansen tibble data frame with the hexagons simple feature dataframe. The output is called hexagon_Hansen.\n\n\nCode Chunk\nhexagon_Hansen &lt;- bind_cols(hexagons, acc_Hansen)\n\n\nNotice that hexagon_Hansen is a simple feature data frame and not a typical tibble data frame.\n\nActually, the steps above can be perform by using a single code chunk as shown below.\n\n\nCode Chunk\nacc_Hansen &lt;- data.frame(ac(hexagons$demand,\n                            eldercare$capacity,\n                            distmat_km, \n                            #d0 = 50,\n                            power = 0.5, \n                            family = \"Hansen\"))\n\ncolnames(acc_Hansen) &lt;- \"accHansen\"\nacc_Hansen &lt;- tbl_df(acc_Hansen)\nhexagon_Hansen &lt;- bind_cols(hexagons, acc_Hansen)\n\n\n\n\n\n\n\nFirstly, we will extract the extend of hexagons simple feature data frame by by using st_bbox() of sf package.\n\n\nCode Chunk\nmapex &lt;- st_bbox(hexagons)\n\n\nThe code chunk below uses a collection of mapping fucntions of tmap package to create a high cartographic quality accessibility to eldercare centre in Singapore.\n\n\nCode Chunk\ntmap_mode(\"plot\")\ntm_shape(hexagon_Hansen,\n         bbox = mapex) + \n  tm_fill(col = \"accHansen\",\n          n = 10,\n          style = \"quantile\",\n          border.col = \"black\",\n          border.lwd = 1) +\ntm_shape(eldercare) +\n  tm_symbols(size = 0.1) +\n  tm_layout(main.title = \"Accessibility to eldercare: Hansen method\",\n            main.title.position = \"center\",\n            main.title.size = 2,\n            legend.outside = FALSE,\n            legend.height = 0.45, \n            legend.width = 3.0,\n            legend.format = list(digits = 6),\n            legend.position = c(\"right\", \"top\"),\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this section, we are going to compare the distribution of Hansen’s accessibility values by URA Planning Region.\nFirstly, we need to add the planning region field into haxegon_Hansen simple feature data frame by using the code chunk below.\n\n\nCode Chunk\nhexagon_Hansen &lt;- st_join(hexagon_Hansen, mpsz,\n                          join = st_intersects)\n\n\nNext, ggplot() will be used to plot the distribution by using boxplot graphical method.\n\n\nCode Chunk\nggplot(data=hexagon_Hansen, \n       aes(y = log(accHansen), \n           x= REGION_N)) +\n  geom_boxplot() +\n  geom_point(stat=\"summary\", \n             fun.y=\"mean\", \n             colour =\"red\", \n             size=2)"
  },
  {
    "objectID": "HandsOnExercise/HandsOn9/HandsOn9.html#modelling-and-visualising-accessibility-using-kd2sfca-method",
    "href": "HandsOnExercise/HandsOn9/HandsOn9.html#modelling-and-visualising-accessibility-using-kd2sfca-method",
    "title": "Hands-On 9",
    "section": "",
    "text": "In this section, you are going to repeat most of the steps you had learned in previous section to perform the analysis. However, some of the codes will be combined into one code chunk.\n\nThe code chunk below calculates Hansen’s accessibility using ac() of SpatialAcc and data.frame() is used to save the output in a data frame called acc_KD2SFCA. Notice that KD2SFCA is used for family argument.\n\n\nCode Chunk\nacc_KD2SFCA &lt;- data.frame(ac(hexagons$demand,\n                            eldercare$capacity,\n                            distmat_km, \n                            d0 = 50,\n                            power = 2, \n                            family = \"KD2SFCA\"))\n\ncolnames(acc_KD2SFCA) &lt;- \"accKD2SFCA\"\nacc_KD2SFCA &lt;- tbl_df(acc_KD2SFCA)\nhexagon_KD2SFCA &lt;- bind_cols(hexagons, acc_KD2SFCA)\n\n\n\n\n\nThe code chunk below uses a collection of mapping fucntions of tmap package to create a high cartographic quality accessibility to eldercare centre in Singapore. Notice that mapex is reused for bbox argument.\n\n\nCode Chunk\ntmap_mode(\"plot\")\ntm_shape(hexagon_KD2SFCA,\n         bbox = mapex) + \n  tm_fill(col = \"accKD2SFCA\",\n          n = 10,\n          style = \"quantile\",\n          border.col = \"black\",\n          border.lwd = 1) +\ntm_shape(eldercare) +\n  tm_symbols(size = 0.1) +\n  tm_layout(main.title = \"Accessibility to eldercare: KD2SFCA method\",\n            main.title.position = \"center\",\n            main.title.size = 2,\n            legend.outside = FALSE,\n            legend.height = 0.45, \n            legend.width = 3.0,\n            legend.format = list(digits = 6),\n            legend.position = c(\"right\", \"top\"),\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\nNow, we are going to compare the distribution of KD2CFA accessibility values by URA Planning Region.\nFirstly, we need to add the planning region field into hexagon_KD2SFCA simple feature data frame by using the code chunk below.\n\n\nCode Chunk\nhexagon_KD2SFCA &lt;- st_join(hexagon_KD2SFCA, mpsz, \n                          join = st_intersects)\n\n\nNext, ggplot() will be used to plot the distribution by using boxplot graphical method.\n\n\nCode Chunk\nggplot(data=hexagon_KD2SFCA, \n       aes(y = accKD2SFCA, \n           x= REGION_N)) +\n  geom_boxplot() +\n  geom_point(stat=\"summary\", \n             fun.y=\"mean\", \n             colour =\"red\", \n             size=2)"
  },
  {
    "objectID": "HandsOnExercise/HandsOn9/HandsOn9.html#modelling-and-visualising-accessibility-using-spatial-accessibility-measure-sam-method",
    "href": "HandsOnExercise/HandsOn9/HandsOn9.html#modelling-and-visualising-accessibility-using-spatial-accessibility-measure-sam-method",
    "title": "Hands-On 9",
    "section": "",
    "text": "In this section, you are going to repeat most of the steps you had learned in previous section to perform the analysis. However, some of the codes will be combined into one code chunk.\n\nThe code chunk below calculates Hansen’s accessibility using ac() of SpatialAcc and data.frame() is used to save the output in a data frame called acc_SAM. Notice that SAM is used for family argument.\n\n\nCode Chunk\nacc_SAM &lt;- data.frame(ac(hexagons$demand,\n                         eldercare$capacity,\n                         distmat_km,\n                         d0 = 50,\n                         power = 2,\n                         family = \"SAM\"))\n\ncolnames(acc_SAM) &lt;- \"accSAM\"\nacc_SAM &lt;- tbl_df(acc_SAM)\nhexagon_SAM &lt;- bind_cols(hexagons, acc_SAM)\n\n\n\n\n\nThe code chunk below uses a collection of mapping fucntions of tmap package to create a high cartographic quality accessibility to eldercare centre in Singapore. Notice that mapex is reused for bbox argument.\n\n\nCode Chunk\ntmap_mode(\"plot\")\ntm_shape(hexagon_SAM,\n         bbox = mapex) +\n  tm_fill(col = \"accSAM\",\n          n=10,\n          style = \"quantile\",\n          border.col = \"black\",\n          border.lwd = 1) +\n  tm_shape(eldercare) +\n  tm_symbols(size = 0.1) +\n  tm_layout(main.title = \"Accessibility to eldercare: SAM method\",\n            main.title.position = \"center\",\n            main.title.size = 2,\n            legend.outside = FALSE,\n            legend.height = 0.45, \n            legend.width = 3.0,\n            legend.format = list(digits = 3),\n            legend.position = c(\"right\", \"top\"),\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\nNow, we are going to compare the distribution of SAM accessibility values by URA Planning Region.\nFirstly, we need to add the planning region field into hexagon_SAM simple feature data frame by using the code chunk below.\n\n\nCode Chunk\nhexagon_SAM &lt;- st_join(hexagon_SAM, mpsz,\n                       join = st_intersects)\n\n\nNext, ggplot() will be used to plot the distribution by using boxplot graphical method.\n\n\nCode Chunk\nggplot(data=hexagon_SAM, \n       aes(y = accSAM, \n           x= REGION_N)) +\n  geom_boxplot() +\n  geom_point(stat=\"summary\", \n             fun.y=\"mean\", \n             colour =\"red\", \n             size=2)"
  },
  {
    "objectID": "InClassExercise/InClass8/Take-home_Ex3b-HDBDataPrep.html",
    "href": "InClassExercise/InClass8/Take-home_Ex3b-HDBDataPrep.html",
    "title": "Preparing HDB data",
    "section": "",
    "text": "pacman::p_load(tidyverse, sf, httr, jsonlite, rvest)\n\n\nresale &lt;- read_csv(\"data/HDB/rawdata/resale.csv\") %&gt;%\n  filter(month &gt;= \"2023-01\" & month &lt;= \"2024-09\")\n\n\nresale_tidy &lt;- resale %&gt;%\n  mutate(address = paste(block,street_name)) %&gt;%\n  mutate(remaining_lease_yr = as.integer(\n    str_sub(remaining_lease, 0, 2)))%&gt;%\n  mutate(remaining_lease_mth = as.integer(\n    str_sub(remaining_lease, 9, 11)))\n\n\nresale_selected &lt;- resale_tidy %&gt;%\n  filter(month == \"2024-09\")\n\n\nadd_list &lt;- sort(unique(resale_selected$address))\n\n\nget_coords &lt;- function(add_list){\n  \n  # Create a data frame to store all retrieved coordinates\n  postal_coords &lt;- data.frame()\n    \n  for (i in add_list){\n    #print(i)\n\n    r &lt;- GET('https://www.onemap.gov.sg/api/common/elastic/search?',\n           query=list(searchVal=i,\n                     returnGeom='Y',\n                     getAddrDetails='Y'))\n    data &lt;- fromJSON(rawToChar(r$content))\n    found &lt;- data$found\n    res &lt;- data$results\n    \n    # Create a new data frame for each address\n    new_row &lt;- data.frame()\n    \n    # If single result, append \n    if (found == 1){\n      postal &lt;- res$POSTAL \n      lat &lt;- res$LATITUDE\n      lng &lt;- res$LONGITUDE\n      new_row &lt;- data.frame(address= i, \n                            postal = postal, \n                            latitude = lat, \n                            longitude = lng)\n    }\n    \n    # If multiple results, drop NIL and append top 1\n    else if (found &gt; 1){\n      # Remove those with NIL as postal\n      res_sub &lt;- res[res$POSTAL != \"NIL\", ]\n      \n      # Set as NA first if no Postal\n      if (nrow(res_sub) == 0) {\n          new_row &lt;- data.frame(address= i, \n                                postal = NA, \n                                latitude = NA, \n                                longitude = NA)\n      }\n      \n      else{\n        top1 &lt;- head(res_sub, n = 1)\n        postal &lt;- top1$POSTAL \n        lat &lt;- top1$LATITUDE\n        lng &lt;- top1$LONGITUDE\n        new_row &lt;- data.frame(address= i, \n                              postal = postal, \n                              latitude = lat, \n                              longitude = lng)\n      }\n    }\n\n    else {\n      new_row &lt;- data.frame(address= i, \n                            postal = NA, \n                            latitude = NA, \n                            longitude = NA)\n    }\n    \n    # Add the row\n    postal_coords &lt;- rbind(postal_coords, new_row)\n  }\n  return(postal_coords)\n}\n\n\ncoords &lt;- get_coords(add_list)\n\n\nwrite_rds(coords, \"data/HDB/rds/coords.rds\")"
  },
  {
    "objectID": "TakeHomeExercise/TakeHome3/TakeHome3.html",
    "href": "TakeHomeExercise/TakeHome3/TakeHome3.html",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "In this take-home exercise, We will calibrate a predictive model to predict HDB resale prices between July-September 2024 by using HDB resale transaction records in 2023.\n\n\n\n\n\n\nCode Chunk\npacman::p_load(tidyverse, readr, sf, httr, jsonlite, rvest, dplyr, units, lubridate, tmap, ggplot2,ggpubr, beepr, corrplot, spdep, GWmodel, SpatialML, rsample, Metrics)\n\n\n\n\n\n\n\n\n\n\nPackage\nFunction\nExplanation\n\n\n\n\ntidyverse\ndplyr::select, dplyr::mutate, ggplot2::ggplot, tibble::tibble\nA collection of R packages for data manipulation (dplyr), data visualization (ggplot2), and reading (readr). tibble is for working with data frames.\n\n\nreadr\nreadr::read_csv, readr::write_csv\nFunctions for reading and writing CSV files efficiently.\n\n\nsf\nsf::st_as_sf, sf::st_transform, sf::st_drop_geometry, sf::st_is_empty, sf::st_coordinates\nFunctions for handling spatial data, including converting data to sf objects, transforming coordinates, and checking for empty geometries.\n\n\nhttr\nhttr::GET, httr::POST\nFunctions for making HTTP requests, such as GET and POST, to interact with web APIs.\n\n\njsonlite\njsonlite::fromJSON, jsonlite::toJSON\nFunctions to parse JSON data into R objects and to convert R objects into JSON format.\n\n\nrvest\nrvest::read_html, rvest::html_nodes, rvest::html_table\nUsed for web scraping, including reading HTML content and extracting elements from web pages.\n\n\ndplyr\ndplyr::select, dplyr::mutate, dplyr::filter, dplyr::summarize\nA core package for data manipulation, used for selecting columns, creating new columns, filtering data, and summarizing results.\n\n\nunits\nunits::set_units, units::as_units\nProvides functions for handling and converting physical units in R.\n\n\nlubridate\nlubridate::ymd, lubridate::mdy, lubridate::today\nFunctions to parse, manipulate, and work with date-time objects in a variety of formats.\n\n\ntmap\ntmap::tm_shape, tmap::tm_borders, tmap::tm_fill\nA package for thematic mapping. Functions allow for creating maps with geographic boundaries, fill colors, and more.\n\n\nggplot2\nggplot2::ggplot, ggplot2::geom_point, ggplot2::geom_line\nThe main visualization package in R, used for creating static graphics such as scatter plots, line charts, histograms, and more.\n\n\nggpubr\nggpubr::ggarrange, ggpubr::stat_cor\nEnhances ggplot2 by allowing for arranging multiple plots and adding statistical tests, like correlation coefficients, to plots.\n\n\nbeepr\nbeepr::beep\nA simple function to play sounds as notifications, commonly used to alert the user when an operation is complete.\n\n\ncorrplot\ncorrplot::corrplot\nUsed to visualize correlation matrices, offering various plotting styles and methods for better presentation of correlations.\n\n\nspdep\nspdep::nb2listw, spdep::spautolm\nProvides spatial econometrics tools, such as calculating spatial weights and performing spatial autoregressive models.\n\n\nGWmodel\nGWmodel::gwr.basic, GWmodel::grf, GWmodel::bw.gwr\nFunctions for geographically weighted regression (GWR), including model fitting (gwr.basic), bandwidth selection, and fitting spatial regression trees.\n\n\nSpatialML\nSpatialML::SpatialCV, SpatialML::SpatialML\nUsed for machine learning models in spatial contexts, including spatial cross-validation and spatially explicit machine learning techniques.\n\n\nrsample\nrsample::initial_split, rsample::training, rsample::testing\nProvides tools for resampling and splitting data into training and testing sets for model validation.\n\n\nMetrics\nMetrics::rmse, Metrics::mae, Metrics::mse\nFunctions to calculate various performance metrics for model evaluation, including Root Mean Square Error (RMSE), Mean Absolute Error (MAE), and Mean Squared Error (MSE).\n\n\n\n\n\n\n\nAspatial Dataset:\n\n\nHDB Resale Flat Prices from Data.gov.sg\n\n\nGeospatial Dataset:\n\n\nMPSZ: Boundaries of Singapore from ura.gov.sg\nBus Stops: A list of bus stops locally from datamall.lta.gov.sg\nEldercare (SHP): A list of elder care centres locally from data.gov.sg\nHawker Centres (KML): a list of hawker centres from data.gov.sg\nKindergarten (KML) from data.gov.sg\nMalls (CSV) from kaggle.com\nTrain Station (SHP) from datamall.lta.gov.sg\nSDCP Park (kml) from data.gov.sg\nSchools (CSV) from data.gov.sg\nSupermarket (KML) from data.gov.sg\n\n\n\n\n\nAspatial Data:\n\n\nHDB Resale\n\n\n\nCode Chunk\nresale2023 &lt;- read_csv(\"data/rawdata/aspatial/hdb/resale.csv\") %&gt;%\n  filter(month &gt;= \"2023-01\" & month &lt;= \"2024-09\")\n\n\n\nGeospatial Data:\n\n\nMPSZ in shapefile format\n\n\n\nCode Chunk\nmpsz = st_read(dsn = \"data/rawdata/geospatial/mpsz\",\n               layer = \"MP14_SUBZONE_NO_SEA_PL\")\n\n\nReading layer `MP14_SUBZONE_NO_SEA_PL' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/TakeHomeExercise/TakeHome3/data/rawdata/geospatial/mpsz' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\nCode Chunk\nplot(mpsz)\n\n\n\n\n\n\n\n\n\n\nBus Stops in shapefile format\n\n\n\nCode Chunk\nbusstop &lt;- st_read(dsn = \"data/rawdata/geospatial/busstop\",\n                      layer = \"BusStop\")\n\n\nReading layer `BusStop' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/TakeHomeExercise/TakeHome3/data/rawdata/geospatial/busstop' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5166 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48285.52 ymax: 52983.82\nProjected CRS: SVY21\n\n\n\nEldercare in shapefile format\n\n\n\nCode Chunk\neldercare &lt;- st_read(dsn = \"data/rawdata/geospatial/eldercare\",\n                      layer = \"ELDERCARE\")\n\n\nReading layer `ELDERCARE' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/TakeHomeExercise/TakeHome3/data/rawdata/geospatial/eldercare' \n  using driver `ESRI Shapefile'\nSimple feature collection with 133 features and 18 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 14481.92 ymin: 28218.43 xmax: 41665.14 ymax: 46804.9\nProjected CRS: SVY21\n\n\n\nHawker Centres in kml format\n\n\n\nCode Chunk\nhawker &lt;- st_read(\"data/rawdata/geospatial/hawker/HawkerCentres.kml\")\n\n\nReading layer `HAWKERCENTRE' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/TakeHomeExercise/TakeHome3/data/rawdata/geospatial/hawker/HawkerCentres.kml' \n  using driver `KML'\nSimple feature collection with 125 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6974 ymin: 1.272716 xmax: 103.9882 ymax: 1.449017\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\nKindergarten in fml format\n\n\n\nCode Chunk\nkindergarten &lt;- st_read(\"data/rawdata/geospatial/kindergarten/Kindergartens.kml\")\n\n\nReading layer `KINDERGARTENS' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/TakeHomeExercise/TakeHome3/data/rawdata/geospatial/kindergarten/Kindergartens.kml' \n  using driver `KML'\nSimple feature collection with 448 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6887 ymin: 1.247759 xmax: 103.9717 ymax: 1.455452\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\nCode Chunk\nkindergarten &lt;- st_transform(kindergarten, 3414)\n\n\n\nShopping Malls in csv format\n\n\n\nCode Chunk\nmall &lt;- read_csv(\"data/rawdata/geospatial/mall/shopping_mall_coordinates.csv\")\n\n\n\nTrain Station in shapefile format\n\n\n\nCode Chunk\nmrt &lt;- st_read(dsn = \"data/rawdata/geospatial/mrt\",\n                layer = \"RapidTransitSystemStation\")\n\n\nReading layer `RapidTransitSystemStation' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/TakeHomeExercise/TakeHome3/data/rawdata/geospatial/mrt' \n  using driver `ESRI Shapefile'\n\n\nSimple feature collection with 230 features and 5 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 6068.209 ymin: 27478.44 xmax: 45377.5 ymax: 47913.58\nProjected CRS: SVY21\n\n\n\nSDCP Park in kml format\n\n\n\nCode Chunk\npark &lt;- st_read(\"data/rawdata/geospatial/park/NParksParksandNatureReservesKML.kml\")\n\n\nReading layer `NPARKS_PARKS' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/TakeHomeExercise/TakeHome3/data/rawdata/geospatial/park/NParksParksandNatureReservesKML.kml' \n  using driver `KML'\nSimple feature collection with 430 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY, XYZ\nBounding box:  xmin: 103.6925 ymin: 1.2115 xmax: 104.0544 ymax: 1.46419\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\nSchools in csv format\n\n\n\nCode Chunk\nschool &lt;- read_csv(\"data/rawdata/geospatial/school/Generalinformationofschools.csv\") %&gt;% \n  mutate(postal_code = as.character(postal_code))\n\n\n\nSupermarket in kml format\n\n\n\nCode Chunk\nsupermarket &lt;- st_read(\"data/rawdata/geospatial/supermarket/SupermarketsKML.kml\")\n\n\nReading layer `SUPERMARKETS' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/TakeHomeExercise/TakeHome3/data/rawdata/geospatial/supermarket/SupermarketsKML.kml' \n  using driver `KML'\nSimple feature collection with 526 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6258 ymin: 1.24715 xmax: 104.0036 ymax: 1.461526\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\n\nStandardising CRS\n\n\n\nIronically, all of these data sets are from Singapore, yet, it presents with varied CRS.\nTherefore, in this exercise, SVY21 (EPSG 3414) will be the standardised CRS as all data relates to local context.\n\n\n\n\n\n\n\nIn tidying up the HDB resale data, the column block & street_name are combined and remaining_lease are split in two columns for ease of manipulation. Additionally, we are changing the column remaining_lease_yr as an integer and splitting it to month and year.\n\n\nCode Chunk\nresale_tidy &lt;- resale2023 %&gt;%\n  mutate(address = paste(block,street_name)) %&gt;% #combined block & street name\n  mutate(remaining_lease_yr = as.integer(\n    str_sub(remaining_lease, 0, 2)))%&gt;% #extract remaining lease by yr\n  mutate(remaining_lease_mth = as.integer(\n    str_sub(remaining_lease, 9, 11))) #extract remaining lease by month\n\n\n\n\nCode Chunk\nresale_selected &lt;- resale_tidy %&gt;%\n  filter(month &gt;= \"2023-01\" & month &lt;= \"2024-09\")\n\n\n\n\nCode Chunk\nadd_list &lt;- sort(unique(resale_selected$address)) #parse a list as API cannot read df\n#unique reduces records to pass to portal\n#sort is used to easier to find geo codes\n\n\nFetching Data from onemap API\nAs the HDB resale data lack of coordinates, API was used to extract the coordinates.\n\n\nCode Chunk\nget_coords &lt;- function(add_list){\n\n  # Create a data frame to store all retrieved coordinates\n  postal_coords &lt;- data.frame()\n    \n  for (i in add_list){\n    r &lt;- GET('https://www.onemap.gov.sg/api/common/elastic/search?',\n           query=list(searchVal=i,\n                     returnGeom='Y',\n                     getAddrDetails='Y'))\n    data &lt;- fromJSON(rawToChar(r$content))\n    found &lt;- data$found\n    res &lt;- data$results\n    \n    # Create a new data frame for each address\n    new_row &lt;- data.frame()\n    \n    # If single result, append \n    if (found == 1){\n      postal &lt;- res$POSTAL \n      lat &lt;- res$LATITUDE\n      lng &lt;- res$LONGITUDE\n      new_row &lt;- data.frame(address = i, \n                           postal = postal, \n                           latitude_wgs84 = lat,  # renamed to clarify coordinate system\n                           longitude_wgs84 = lng) # renamed to clarify coordinate system\n    }\n    \n    # If multiple results, drop NIL and append top 1\n    else if (found &gt; 1){\n      # Remove those with NIL as postal\n      res_sub &lt;- res[res$POSTAL != \"NIL\", ]\n      \n      # Set as NA first if no Postal\n      if (nrow(res_sub) == 0) {\n          new_row &lt;- data.frame(address = i, \n                               postal = NA, \n                               latitude_wgs84 = NA, \n                               longitude_wgs84 = NA)\n      }\n      else{\n        top1 &lt;- head(res_sub, n = 1)\n        postal &lt;- top1$POSTAL \n        lat &lt;- top1$LATITUDE\n        lng &lt;- top1$LONGITUDE\n        new_row &lt;- data.frame(address = i, \n                             postal = postal, \n                             latitude_wgs84 = lat, \n                             longitude_wgs84 = lng)\n      }\n    }\n    else {\n      new_row &lt;- data.frame(address = i, \n                           postal = NA, \n                           latitude_wgs84 = NA, \n                           longitude_wgs84 = NA)\n    }\n    \n    # Add the row\n    postal_coords &lt;- rbind(postal_coords, new_row)\n  }\n  \n  # Convert to sf object with WGS84 coordinates (EPSG:4326)\n  # Filter out rows with NA coordinates first\n  valid_coords &lt;- postal_coords[!is.na(postal_coords$latitude_wgs84) & \n                              !is.na(postal_coords$longitude_wgs84), ]\n  \n  if(nrow(valid_coords) &gt; 0) {\n    coords_sf &lt;- st_as_sf(valid_coords, \n                         coords = c(\"longitude_wgs84\", \"latitude_wgs84\"),\n                         crs = 4326)\n    \n    # Transform to SVY21 (EPSG:3414)\n    coords_svy21 &lt;- st_transform(coords_sf, 3414)\n    \n    # Extract coordinates\n    coords_matrix &lt;- st_coordinates(coords_svy21)\n    \n    # Add SVY21 coordinates back to the original dataframe with desired column names\n    valid_coords$longitude &lt;- coords_matrix[, 1]  # SVY21 X coordinate as longitude\n    valid_coords$latitude &lt;- coords_matrix[, 2]   # SVY21 Y coordinate as latitude\n    \n    # Merge back with rows that had NA coordinates\n    result &lt;- merge(postal_coords, valid_coords[c(\"address\", \"longitude\", \"latitude\")], \n                   by = \"address\", all.x = TRUE)\n  } else {\n    # If no valid coordinates, add empty SVY21 columns\n    result &lt;- postal_coords\n    result$longitude &lt;- NA  # SVY21 coordinates\n    result$latitude &lt;- NA   # SVY21 coordinates\n  }\n  \n  return(result)\n}\n\n\nBelow is the code chunk that populates the coordinates in longitude, latitude and postal code against the address in the add_list.\n\n\nCode Chunk\ncoords &lt;- get_coords(add_list)\n\n\nThe longtitude and latitude is then combined into geometry and the crs has been set to EPSG = 3414.\n\n\nCode Chunk\ncoords_sf &lt;- coords %&gt;%\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 3414, remove = FALSE) %&gt;%\n  select(address, postal, longitude, latitude, geometry)\n\n\nFollowing which, the coords_sf is then combined to the resale_selected df by address, forming a new df resale_geom\n\n\nCode Chunk\nresale_geom &lt;- resale_selected %&gt;% \n  left_join(coords_sf, by = \"address\")\n\n\nAssigning CRS EPSG 3414 to resale_geom\n\n\nCode Chunk\nresale_geom &lt;- resale_geom %&gt;% \n  st_as_sf(coords =c(\"longitude\", \"latitude\"), crs = 3414, remove = FALSE)\n\n\nUsing st_crs() to check if it is the correct CRS.\n\n\nCode Chunk\nst_crs(resale_geom)\n\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nIn ensuring these are the variables we need, we use select() to include the variables. Then, we compute the unit age with the formula of 99 year (convention HDB lease years) - remaining lease year. Lastly, we change the column month to POSICxt so we can manipulate in ease later on.\n\n\nCode Chunk\nresale_geom &lt;- resale_geom %&gt;%\n  select(address, town, resale_price, month, flat_type, floor_area_sqm, remaining_lease_yr,longitude, latitude, geometry) %&gt;% \n  mutate(unit_age = 99 - remaining_lease_yr)\n\n\n\n\n\n\n\n\nWGS 84\nSVY21\nNo PCS / CSV\n\n\n\n\nHawker\nBus Stop\nHDB Resale\n\n\nKindergarten\nEldercare\nMall\n\n\nPark\nMRT\nSchool\n\n\nSupermarket\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCoordinate System\nViewing the sf, we noticed that the coordinates are in WGS84 - EPSG 4326. This may not be suitable for our data manipulation later on as we are standardising it to our local coordinates system which is SVY 21 EPSG 3414.\n\n\n\n\n\n\n\n\nTip\n\n\n\nOverview of Coordinate System\nWGS84 (EPSG: 4326) - Global Use\n\nDefinition: WGS84 (World Geodetic System 1984) is a global coordinate system that defines latitude and longitude on a three-dimensional ellipsoid. It is the standard used by the Global Positioning System (GPS).\nCoordinate System: It uses a geographic coordinate system based on a spheroid, where coordinates are given in degrees (latitude and longitude).\nApplication: WGS84 is used globally for mapping, navigation, and geolocation applications. It provides a standard reference for geographic information systems (GIS) and is essential for interoperability between different systems.\nUnits: The coordinates are expressed in degrees.\n\nSVY21 (EPSG: 3413) - Singapore Use\n\nDefinition: SVY21 (Singapore Vertical 21) is a national coordinate system specifically designed for Singapore. It is based on the Transverse Mercator projection and is tailored to provide accurate measurements within Singapore’s geographic boundaries.\nCoordinate System: SVY21 uses a projected coordinate system, where coordinates are given in meters. The system is more suitable for local applications because it minimizes distortions in a specific area.\nApplication: SVY21 is primarily used for urban planning, construction, and various governmental applications within Singapore. It provides higher accuracy for local measurements compared to global systems like WGS84.\nUnits: The coordinates are expressed in meters.\n\n\n\n\n\n\nFirst, we would like to check if there is any shared boudaries in this sf as this will churned as invalid polygons in the data. We will then noticed that there are several “Ring Self-Intersection” specifically, there are 9 polygons with self-intersection issues which is not good for the data.\n\n\nCode Chunk\nst_is_valid(mpsz, reason = TRUE)\n\n\n  [1] \"Valid Geometry\"                                           \n  [2] \"Valid Geometry\"                                           \n  [3] \"Valid Geometry\"                                           \n  [4] \"Valid Geometry\"                                           \n  [5] \"Valid Geometry\"                                           \n  [6] \"Valid Geometry\"                                           \n  [7] \"Valid Geometry\"                                           \n  [8] \"Valid Geometry\"                                           \n  [9] \"Valid Geometry\"                                           \n [10] \"Valid Geometry\"                                           \n [11] \"Valid Geometry\"                                           \n [12] \"Valid Geometry\"                                           \n [13] \"Valid Geometry\"                                           \n [14] \"Valid Geometry\"                                           \n [15] \"Valid Geometry\"                                           \n [16] \"Valid Geometry\"                                           \n [17] \"Valid Geometry\"                                           \n [18] \"Valid Geometry\"                                           \n [19] \"Valid Geometry\"                                           \n [20] \"Valid Geometry\"                                           \n [21] \"Valid Geometry\"                                           \n [22] \"Valid Geometry\"                                           \n [23] \"Valid Geometry\"                                           \n [24] \"Valid Geometry\"                                           \n [25] \"Valid Geometry\"                                           \n [26] \"Valid Geometry\"                                           \n [27] \"Valid Geometry\"                                           \n [28] \"Valid Geometry\"                                           \n [29] \"Valid Geometry\"                                           \n [30] \"Valid Geometry\"                                           \n [31] \"Valid Geometry\"                                           \n [32] \"Valid Geometry\"                                           \n [33] \"Valid Geometry\"                                           \n [34] \"Valid Geometry\"                                           \n [35] \"Valid Geometry\"                                           \n [36] \"Valid Geometry\"                                           \n [37] \"Valid Geometry\"                                           \n [38] \"Ring Self-intersection[27932.3925999999 21982.7971999999]\"\n [39] \"Ring Self-intersection[26885.4439000003 26668.3121000007]\"\n [40] \"Valid Geometry\"                                           \n [41] \"Valid Geometry\"                                           \n [42] \"Valid Geometry\"                                           \n [43] \"Ring Self-intersection[26920.1689999998 26978.5440999996]\"\n [44] \"Valid Geometry\"                                           \n [45] \"Valid Geometry\"                                           \n [46] \"Valid Geometry\"                                           \n [47] \"Valid Geometry\"                                           \n [48] \"Valid Geometry\"                                           \n [49] \"Valid Geometry\"                                           \n [50] \"Valid Geometry\"                                           \n [51] \"Valid Geometry\"                                           \n [52] \"Valid Geometry\"                                           \n [53] \"Valid Geometry\"                                           \n [54] \"Valid Geometry\"                                           \n [55] \"Valid Geometry\"                                           \n [56] \"Valid Geometry\"                                           \n [57] \"Valid Geometry\"                                           \n [58] \"Valid Geometry\"                                           \n [59] \"Valid Geometry\"                                           \n [60] \"Valid Geometry\"                                           \n [61] \"Valid Geometry\"                                           \n [62] \"Valid Geometry\"                                           \n [63] \"Valid Geometry\"                                           \n [64] \"Valid Geometry\"                                           \n [65] \"Valid Geometry\"                                           \n [66] \"Valid Geometry\"                                           \n [67] \"Valid Geometry\"                                           \n [68] \"Valid Geometry\"                                           \n [69] \"Valid Geometry\"                                           \n [70] \"Valid Geometry\"                                           \n [71] \"Valid Geometry\"                                           \n [72] \"Valid Geometry\"                                           \n [73] \"Valid Geometry\"                                           \n [74] \"Valid Geometry\"                                           \n [75] \"Valid Geometry\"                                           \n [76] \"Valid Geometry\"                                           \n [77] \"Valid Geometry\"                                           \n [78] \"Valid Geometry\"                                           \n [79] \"Valid Geometry\"                                           \n [80] \"Valid Geometry\"                                           \n [81] \"Valid Geometry\"                                           \n [82] \"Valid Geometry\"                                           \n [83] \"Valid Geometry\"                                           \n [84] \"Valid Geometry\"                                           \n [85] \"Valid Geometry\"                                           \n [86] \"Valid Geometry\"                                           \n [87] \"Valid Geometry\"                                           \n [88] \"Valid Geometry\"                                           \n [89] \"Valid Geometry\"                                           \n [90] \"Valid Geometry\"                                           \n [91] \"Valid Geometry\"                                           \n [92] \"Valid Geometry\"                                           \n [93] \"Valid Geometry\"                                           \n [94] \"Valid Geometry\"                                           \n [95] \"Valid Geometry\"                                           \n [96] \"Valid Geometry\"                                           \n [97] \"Ring Self-intersection[14484.6859999998 31330.1319999993]\"\n [98] \"Ring Self-intersection[12861.3828999996 32207.4923]\"      \n [99] \"Valid Geometry\"                                           \n[100] \"Valid Geometry\"                                           \n[101] \"Valid Geometry\"                                           \n[102] \"Valid Geometry\"                                           \n[103] \"Ring Self-intersection[19681.2353999997 31294.4521999992]\"\n[104] \"Valid Geometry\"                                           \n[105] \"Valid Geometry\"                                           \n[106] \"Valid Geometry\"                                           \n[107] \"Valid Geometry\"                                           \n[108] \"Valid Geometry\"                                           \n[109] \"Valid Geometry\"                                           \n[110] \"Valid Geometry\"                                           \n[111] \"Valid Geometry\"                                           \n[112] \"Valid Geometry\"                                           \n[113] \"Valid Geometry\"                                           \n[114] \"Valid Geometry\"                                           \n[115] \"Valid Geometry\"                                           \n[116] \"Valid Geometry\"                                           \n[117] \"Valid Geometry\"                                           \n[118] \"Valid Geometry\"                                           \n[119] \"Valid Geometry\"                                           \n[120] \"Valid Geometry\"                                           \n[121] \"Valid Geometry\"                                           \n[122] \"Valid Geometry\"                                           \n[123] \"Valid Geometry\"                                           \n[124] \"Valid Geometry\"                                           \n[125] \"Valid Geometry\"                                           \n[126] \"Valid Geometry\"                                           \n[127] \"Valid Geometry\"                                           \n[128] \"Valid Geometry\"                                           \n[129] \"Valid Geometry\"                                           \n[130] \"Valid Geometry\"                                           \n[131] \"Valid Geometry\"                                           \n[132] \"Valid Geometry\"                                           \n[133] \"Valid Geometry\"                                           \n[134] \"Valid Geometry\"                                           \n[135] \"Valid Geometry\"                                           \n[136] \"Valid Geometry\"                                           \n[137] \"Valid Geometry\"                                           \n[138] \"Valid Geometry\"                                           \n[139] \"Valid Geometry\"                                           \n[140] \"Valid Geometry\"                                           \n[141] \"Valid Geometry\"                                           \n[142] \"Valid Geometry\"                                           \n[143] \"Valid Geometry\"                                           \n[144] \"Valid Geometry\"                                           \n[145] \"Valid Geometry\"                                           \n[146] \"Valid Geometry\"                                           \n[147] \"Valid Geometry\"                                           \n[148] \"Valid Geometry\"                                           \n[149] \"Valid Geometry\"                                           \n[150] \"Valid Geometry\"                                           \n[151] \"Valid Geometry\"                                           \n[152] \"Valid Geometry\"                                           \n[153] \"Valid Geometry\"                                           \n[154] \"Valid Geometry\"                                           \n[155] \"Valid Geometry\"                                           \n[156] \"Valid Geometry\"                                           \n[157] \"Valid Geometry\"                                           \n[158] \"Valid Geometry\"                                           \n[159] \"Valid Geometry\"                                           \n[160] \"Valid Geometry\"                                           \n[161] \"Valid Geometry\"                                           \n[162] \"Valid Geometry\"                                           \n[163] \"Valid Geometry\"                                           \n[164] \"Valid Geometry\"                                           \n[165] \"Valid Geometry\"                                           \n[166] \"Valid Geometry\"                                           \n[167] \"Valid Geometry\"                                           \n[168] \"Valid Geometry\"                                           \n[169] \"Valid Geometry\"                                           \n[170] \"Valid Geometry\"                                           \n[171] \"Valid Geometry\"                                           \n[172] \"Valid Geometry\"                                           \n[173] \"Valid Geometry\"                                           \n[174] \"Valid Geometry\"                                           \n[175] \"Valid Geometry\"                                           \n[176] \"Valid Geometry\"                                           \n[177] \"Valid Geometry\"                                           \n[178] \"Valid Geometry\"                                           \n[179] \"Valid Geometry\"                                           \n[180] \"Valid Geometry\"                                           \n[181] \"Valid Geometry\"                                           \n[182] \"Valid Geometry\"                                           \n[183] \"Valid Geometry\"                                           \n[184] \"Valid Geometry\"                                           \n[185] \"Valid Geometry\"                                           \n[186] \"Valid Geometry\"                                           \n[187] \"Valid Geometry\"                                           \n[188] \"Valid Geometry\"                                           \n[189] \"Valid Geometry\"                                           \n[190] \"Valid Geometry\"                                           \n[191] \"Valid Geometry\"                                           \n[192] \"Valid Geometry\"                                           \n[193] \"Valid Geometry\"                                           \n[194] \"Valid Geometry\"                                           \n[195] \"Valid Geometry\"                                           \n[196] \"Valid Geometry\"                                           \n[197] \"Valid Geometry\"                                           \n[198] \"Valid Geometry\"                                           \n[199] \"Valid Geometry\"                                           \n[200] \"Valid Geometry\"                                           \n[201] \"Valid Geometry\"                                           \n[202] \"Valid Geometry\"                                           \n[203] \"Valid Geometry\"                                           \n[204] \"Valid Geometry\"                                           \n[205] \"Valid Geometry\"                                           \n[206] \"Valid Geometry\"                                           \n[207] \"Valid Geometry\"                                           \n[208] \"Valid Geometry\"                                           \n[209] \"Valid Geometry\"                                           \n[210] \"Valid Geometry\"                                           \n[211] \"Valid Geometry\"                                           \n[212] \"Valid Geometry\"                                           \n[213] \"Valid Geometry\"                                           \n[214] \"Valid Geometry\"                                           \n[215] \"Valid Geometry\"                                           \n[216] \"Valid Geometry\"                                           \n[217] \"Valid Geometry\"                                           \n[218] \"Valid Geometry\"                                           \n[219] \"Valid Geometry\"                                           \n[220] \"Valid Geometry\"                                           \n[221] \"Valid Geometry\"                                           \n[222] \"Valid Geometry\"                                           \n[223] \"Valid Geometry\"                                           \n[224] \"Valid Geometry\"                                           \n[225] \"Valid Geometry\"                                           \n[226] \"Valid Geometry\"                                           \n[227] \"Valid Geometry\"                                           \n[228] \"Valid Geometry\"                                           \n[229] \"Valid Geometry\"                                           \n[230] \"Valid Geometry\"                                           \n[231] \"Valid Geometry\"                                           \n[232] \"Valid Geometry\"                                           \n[233] \"Valid Geometry\"                                           \n[234] \"Valid Geometry\"                                           \n[235] \"Valid Geometry\"                                           \n[236] \"Valid Geometry\"                                           \n[237] \"Valid Geometry\"                                           \n[238] \"Valid Geometry\"                                           \n[239] \"Valid Geometry\"                                           \n[240] \"Valid Geometry\"                                           \n[241] \"Valid Geometry\"                                           \n[242] \"Valid Geometry\"                                           \n[243] \"Valid Geometry\"                                           \n[244] \"Valid Geometry\"                                           \n[245] \"Valid Geometry\"                                           \n[246] \"Valid Geometry\"                                           \n[247] \"Valid Geometry\"                                           \n[248] \"Valid Geometry\"                                           \n[249] \"Valid Geometry\"                                           \n[250] \"Valid Geometry\"                                           \n[251] \"Valid Geometry\"                                           \n[252] \"Valid Geometry\"                                           \n[253] \"Valid Geometry\"                                           \n[254] \"Valid Geometry\"                                           \n[255] \"Valid Geometry\"                                           \n[256] \"Valid Geometry\"                                           \n[257] \"Valid Geometry\"                                           \n[258] \"Valid Geometry\"                                           \n[259] \"Valid Geometry\"                                           \n[260] \"Valid Geometry\"                                           \n[261] \"Valid Geometry\"                                           \n[262] \"Valid Geometry\"                                           \n[263] \"Valid Geometry\"                                           \n[264] \"Valid Geometry\"                                           \n[265] \"Valid Geometry\"                                           \n[266] \"Valid Geometry\"                                           \n[267] \"Valid Geometry\"                                           \n[268] \"Valid Geometry\"                                           \n[269] \"Valid Geometry\"                                           \n[270] \"Valid Geometry\"                                           \n[271] \"Valid Geometry\"                                           \n[272] \"Valid Geometry\"                                           \n[273] \"Valid Geometry\"                                           \n[274] \"Valid Geometry\"                                           \n[275] \"Valid Geometry\"                                           \n[276] \"Ring Self-intersection[38542.2260999996 44605.4089000002]\"\n[277] \"Valid Geometry\"                                           \n[278] \"Valid Geometry\"                                           \n[279] \"Valid Geometry\"                                           \n[280] \"Valid Geometry\"                                           \n[281] \"Valid Geometry\"                                           \n[282] \"Valid Geometry\"                                           \n[283] \"Valid Geometry\"                                           \n[284] \"Valid Geometry\"                                           \n[285] \"Valid Geometry\"                                           \n[286] \"Valid Geometry\"                                           \n[287] \"Valid Geometry\"                                           \n[288] \"Valid Geometry\"                                           \n[289] \"Valid Geometry\"                                           \n[290] \"Ring Self-intersection[41375.108 40432.8588999994]\"       \n[291] \"Valid Geometry\"                                           \n[292] \"Valid Geometry\"                                           \n[293] \"Valid Geometry\"                                           \n[294] \"Valid Geometry\"                                           \n[295] \"Valid Geometry\"                                           \n[296] \"Valid Geometry\"                                           \n[297] \"Valid Geometry\"                                           \n[298] \"Valid Geometry\"                                           \n[299] \"Valid Geometry\"                                           \n[300] \"Valid Geometry\"                                           \n[301] \"Valid Geometry\"                                           \n[302] \"Valid Geometry\"                                           \n[303] \"Valid Geometry\"                                           \n[304] \"Valid Geometry\"                                           \n[305] \"Valid Geometry\"                                           \n[306] \"Valid Geometry\"                                           \n[307] \"Valid Geometry\"                                           \n[308] \"Valid Geometry\"                                           \n[309] \"Valid Geometry\"                                           \n[310] \"Valid Geometry\"                                           \n[311] \"Valid Geometry\"                                           \n[312] \"Valid Geometry\"                                           \n[313] \"Valid Geometry\"                                           \n[314] \"Valid Geometry\"                                           \n[315] \"Valid Geometry\"                                           \n[316] \"Valid Geometry\"                                           \n[317] \"Valid Geometry\"                                           \n[318] \"Valid Geometry\"                                           \n[319] \"Valid Geometry\"                                           \n[320] \"Valid Geometry\"                                           \n[321] \"Valid Geometry\"                                           \n[322] \"Ring Self-intersection[21702.5623000003 48125.1154999994]\"\n[323] \"Valid Geometry\"                                           \n\n\nThe below code chunk visualises the invalid geometries.\n\n\nCode Chunk\ninvalid_polygons &lt;- mpsz[!st_is_valid(mpsz),]\nplot(invalid_polygons)\n\n\n\n\n\n\n\n\n\nIn addressing the above point, we will use st_buffer() of sf package to compute a 5-metres buffers around the data.\n\n\nCode Chunk\nmpsz &lt;- st_buffer(mpsz, dist = 2)\n\n\n\n\n\nIn inspecting the sf, we noticed that when we sort the column “BUS_STOP_N”, there are two rows that retains the geometry however there are nil indication of the bus stop number and location. Hence both of the rows will be deleted.\nAdditionally, in checking for duplicates through its geometry, we noticed two rows share the geometry. One is named as “YUSEN LOGISTICS” while the other is “yusen logistics”\n\n\nCode Chunk\n# Check for duplicate geometries in the eldercare sf object\nduplicate_busstop &lt;- busstop[duplicated(busstop$geometry), ]\n\n# Display the duplicate geometries if any\nif (nrow(duplicate_busstop) &gt; 0) {\n  print(duplicate_busstop)\n} else {\n  print(\"No duplicate geometries found.\")\n}\n\n\nSimple feature collection with 1 feature and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 42187.23 ymin: 34995.78 xmax: 42187.23 ymax: 34995.78\nProjected CRS: SVY21\n     BUS_STOP_N BUS_ROOF_N        LOC_DESC                  geometry\n3195      96319        NIL YUSEN LOGISTICS POINT (42187.23 34995.78)\n\n\nHence, with the above adjustments and extracting coordinates, the data is cleaned.\n\n\nCode Chunk\nbusstop_cleaned &lt;- busstop %&gt;%\n  filter(LOC_DESC != \"YUSEN LOGISTICS\")  # Delete the specified row\n\n\n\n\nCode Chunk\n# Extract coordinates after filtering\ncoordinates &lt;- st_coordinates(busstop_cleaned)\n\n# Add longitude and latitude columns\nbusstop_cleaned &lt;- busstop_cleaned %&gt;%\n  mutate(longitude = coordinates[, 1],    # First column is longitude\n         latitude = coordinates[, 2]) %&gt;%  \n  select(LOC_DESC, longitude, latitude, geometry) \n\n\n\n\nCode Chunk\nst_crs(busstop_cleaned) &lt;- 3414\n\n\n\n\n\nIn ensuring there are no duplicates the below code chunk was churned. However,we noticed that there is two rows that shares the same geometry. Through closer inspection, we realised that both addresses are formatted differently “117 Bukit Merah View” & “Blk 117 Bukit Merah View #01-205”. Thus, one of the row will be deleted.\n\n\nCode Chunk\n# Check for duplicate geometries in the eldercare sf object\nduplicate_geometries &lt;- eldercare[duplicated(eldercare$geometry), ]\n\n# Display the duplicate geometries if any\nif (nrow(duplicate_geometries) &gt; 0) {\n  print(duplicate_geometries)\n} else {\n  print(\"No duplicate geometries found.\")\n}\n\n\nSimple feature collection with 13 features and 18 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 23147.94 ymin: 29642.15 xmax: 41665.14 ymax: 45761.17\nProjected CRS: SVY21\nFirst 10 features:\n    OBJECTID ADDRESSBLO ADDRESSBUI ADDRESSPOS\n51        51       &lt;NA&gt;       &lt;NA&gt;     190005\n59        59       &lt;NA&gt;       &lt;NA&gt;     190008\n62        62       &lt;NA&gt;       &lt;NA&gt;     731569\n65        65       &lt;NA&gt;       &lt;NA&gt;     540182\n66        66       &lt;NA&gt;       &lt;NA&gt;     523499\n70        70       &lt;NA&gt;       &lt;NA&gt;     151117\n97        97       &lt;NA&gt;       &lt;NA&gt;     560123\n102      102       &lt;NA&gt;       &lt;NA&gt;     312062\n106      106       &lt;NA&gt;       &lt;NA&gt;     560469\n114      114       &lt;NA&gt;       &lt;NA&gt;     151117\n                              ADDRESSSTR ADDRESSTYP DESCRIPTIO HYPERLINK\n51                   5 Beach Rd #02-4915       &lt;NA&gt;       &lt;NA&gt;      &lt;NA&gt;\n59                 Blk 8 North Bridge Rd       &lt;NA&gt;       &lt;NA&gt;      &lt;NA&gt;\n62                Blk 569A Champions Way       &lt;NA&gt;       &lt;NA&gt;      &lt;NA&gt;\n65            Blk 182 Rivervale Crescent       &lt;NA&gt;       &lt;NA&gt;      &lt;NA&gt;\n66               Blk 499C Tampines Ave 9       &lt;NA&gt;       &lt;NA&gt;      &lt;NA&gt;\n70                  117 Bukit Merah View       &lt;NA&gt;       &lt;NA&gt;      &lt;NA&gt;\n97  Blk 123 Ang Mo Kio Avenue 6 #01-4011       &lt;NA&gt;       &lt;NA&gt;      &lt;NA&gt;\n102       62B Lorong 4 Toa Payoh #02-121       &lt;NA&gt;       &lt;NA&gt;      &lt;NA&gt;\n106 Blk 469 Ang Mo Kio Avenue 10 #01-940       &lt;NA&gt;       &lt;NA&gt;      &lt;NA&gt;\n114     Blk 117 Bukit Merah View #01-205       &lt;NA&gt;       &lt;NA&gt;      &lt;NA&gt;\n    LANDXADDRE LANDYADDRE                                                NAME\n51           0          0                      Peace-Connect Cluster Operator\n59           0          0                   PEACE-Connect Senior Group Home@8\n62           0          0                            Care Corner SGH @ WL569A\n65           0          0       COMNET Senior Group Home @ Rivervale Crescent\n66           0          0 Lions Befrienders Senior Group Home @ Tampines 499C\n70           0          0           NTUC Health Senior Group Home @ Henderson\n97           0          0                         AWWA Senior Activity Centre\n102          0          0          Care Corner Senior Activity Centre (TP62B)\n106          0          0   Teck Ghee Senior Activity Centre Branch @ Blk 469\n114          0          0                    Henderson Senior Activity Centre\n    PHOTOURL ADDRESSFLO          INC_CRC FMEL_UPD_D ADDRESSUNI   X_ADDR\n51      &lt;NA&gt;       &lt;NA&gt; A2C058FC1751FFE7 2016-07-28       &lt;NA&gt; 31505.35\n59      &lt;NA&gt;       &lt;NA&gt; D1A1515DCC76C221 2016-07-28       &lt;NA&gt; 31415.01\n62      &lt;NA&gt;       &lt;NA&gt; 4DC6800EF15E4B70 2016-07-28       &lt;NA&gt; 23147.94\n65      &lt;NA&gt;       &lt;NA&gt; 6BB0D76986F8C512 2016-07-28       &lt;NA&gt; 36446.37\n66      &lt;NA&gt;       &lt;NA&gt; 5DB6B9F0F16BCA80 2016-07-28       &lt;NA&gt; 41665.14\n70      &lt;NA&gt;       &lt;NA&gt; 7FF38742987329FE 2016-07-28       &lt;NA&gt; 26715.04\n97      &lt;NA&gt;       &lt;NA&gt; B275FB9D13DD7091 2016-07-28       &lt;NA&gt; 29261.31\n102     &lt;NA&gt;       &lt;NA&gt; 13908BA85D11F6DC 2016-07-28       &lt;NA&gt; 29998.73\n106     &lt;NA&gt;       &lt;NA&gt; 8F9B74A5A73579D9 2016-07-28       &lt;NA&gt; 30594.50\n114     &lt;NA&gt;       &lt;NA&gt; 7FF387421A0883F4 2016-07-28       &lt;NA&gt; 26715.04\n      Y_ADDR                  geometry\n51  31853.52 POINT (31505.35 31853.52)\n59  31880.06 POINT (31415.01 31880.06)\n62  45761.17 POINT (23147.94 45761.17)\n65  41376.90  POINT (36446.37 41376.9)\n66  37956.92 POINT (41665.14 37956.92)\n70  29642.15 POINT (26715.04 29642.15)\n97  39169.61 POINT (29261.31 39169.61)\n102 35335.92 POINT (29998.73 35335.92)\n106 38507.25  POINT (30594.5 38507.25)\n114 29642.15 POINT (26715.04 29642.15)\n\n\nThe code below selects the necessary variables of ease of manipulation\n\n\nCode Chunk\neldercare_cleaned &lt;- eldercare %&gt;% \n  filter(INC_CRC != \"7FF38742987329FE\") %&gt;%  #deletes the duplicated row\n  rename(longitude = X_ADDR) %&gt;% \n  rename(latitude = Y_ADDR) %&gt;% \n  select(NAME, longitude, latitude, geometry)\n\n\n\n\nCode Chunk\nst_crs(eldercare_cleaned) &lt;- 3414\n\n\n\n\n\nThe code chunk below checks if there are any duplicates geometries and it returns nil.\n\n\nCode Chunk\nduplicate_geometries &lt;- hawker[duplicated(st_geometry(hawker)), ]\n\n# Display duplicate geometries if any\nif (nrow(duplicate_geometries) &gt; 0) {\n  print(duplicate_geometries)\n} else {\n  print(\"No duplicate geometries found.\")\n}\n\n\n[1] \"No duplicate geometries found.\"\n\n\nAs the CRS of hawker sf is in WGS84, st_transform() is then used to transform it to SVY21.\n\n\nCode Chunk\nhawker &lt;- st_transform(hawker, 3414)\n\n\nThe data is then split into longitude and latitude and selected columns were chosen.\n\n\nCode Chunk\ncoordinates &lt;- st_coordinates(hawker)\nhawker_cleaned &lt;- hawker %&gt;%\n  mutate(longitude = coordinates[, 1],    # First column is longitude\n         latitude = coordinates[, 2]) %&gt;%  \n  select(longitude, latitude, geometry)\n\n\n\n\n\nThe code chunk below checks if there are any duplicates geometries and it returns nil.\n\n\nCode Chunk\nduplicate_kindergarten &lt;- kindergarten[duplicated(st_geometry(kindergarten)), ]\n\n# Display duplicate geometries if any\nif (nrow(duplicate_kindergarten) &gt; 0) {\n  print(duplicate_kindergarten)\n} else {\n  print(\"No duplicate geometries found.\")\n}\n\n\nSimple feature collection with 66 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 12882.62 ymin: 25596.33 xmax: 41916.49 ymax: 48384.34\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n       Name\n271 kml_271\n284 kml_284\n288 kml_288\n291 kml_291\n349 kml_349\n354 kml_354\n355 kml_355\n356 kml_356\n358 kml_358\n360 kml_360\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Description\n271                                     &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;423731&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;252 Tembeling Road  #02-07 S(423731)&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Kindergartens&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;Seeds D' Learning House&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;30BE9FB7740C8654&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20211201143423&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n284                                 &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;287994&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;200 Turf Club Road   S(287994)&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Kindergartens&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;Swallows And Amazons Kindergarten&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;AC4EDF1099057112&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20211201143423&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n288 &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;287994&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;200, TURF CLUB ROAD, #06-08,THE GRANDSTAND, (S)287994&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Kindergartens&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;The Little House (Montessori) Kindergarten&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;AC4EDF101AAB7C05&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20211201143423&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n291                              &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;760102&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;102 Yishun Avenue 5  #03-115 S(760102)&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Kindergartens&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;Tots Town Preschool @ Yishun&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;0CEB47AFBABB448C&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20211201143423&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n349                                          &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;579792&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;4 Bishan Street 13   S(579792)&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Kindergartens&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;Zion Bishan Kindergarten&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;2809ADC0B5232EF6&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20211201143423&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n354                                      &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;519420&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;4 Pasir Ris Drive 6&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Kindergartens&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;Pentecost Methodist Church Kindergarten&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;8C80D608E5D43802&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20211201143423&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n355                                                           &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;299574&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;1 Dunearn Close&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Kindergartens&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;Pibos Garden Preschool&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;7F8CBC68533FFB92&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20211201143423&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n356                                                  &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;423731&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;252 Tembeling Road #01-07&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Kindergartens&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;Pink Tower Montessori&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;30BE9FB7FCB667DE&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20211201143423&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n358                                                 &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;522497&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;497C Tampines Street 45 #01-54&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Kindergartens&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;Prodigy Preschool&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;724ECBCAF5159E3E&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20211201143423&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n360                                               &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;730408&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;408 Woodlands Street 41&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Kindergartens&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;Putra - Putri Kindergarten&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;BA7D15B0567CD551&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20211201143423&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n                         geometry\n271  POINT Z (36012.9 32087.99 0)\n284 POINT Z (23527.86 35684.78 0)\n288 POINT Z (23527.86 35684.78 0)\n291 POINT Z (27493.03 45816.02 0)\n349 POINT Z (30399.37 36579.97 0)\n354 POINT Z (41780.92 39252.46 0)\n355 POINT Z (25915.86 33987.34 0)\n356  POINT Z (36012.9 32087.99 0)\n358 POINT Z (41916.49 37814.51 0)\n360 POINT Z (21217.24 45620.86 0)\n\n\n\n\nCode Chunk\nkindergarten &lt;- st_transform(kindergarten, 3414)\n\n\nThe data is then split into longitude and latitude and selected columns were chosen.\n\n\nCode Chunk\ncoordinates &lt;- st_coordinates(kindergarten)\nkindergarten_cleaned &lt;- kindergarten %&gt;%\n  mutate(longitude = coordinates[, 1],    # First column is longitude\n         latitude = coordinates[, 2]) %&gt;%  \n  select(longitude, latitude, geometry)\n\n\n\n\n\nDespite importing the data and ensuring that postal code is in character form, there are still incomplete postal code as seen in the below code.\n\n\nCode Chunk\nlist(school$postal_code)\n\n\n[[1]]\n  [1] \"738907\" \"737916\" \"768643\" \"768928\" \"579646\" \"159016\" \"544969\" \"569785\"\n  [9] \"569206\" \"569843\" \"569920\" \"569362\" \"487012\" \"139745\" \"309919\" \"139650\"\n [17] \"227988\" \"309918\" \"529366\" \"678117\" \"679697\" \"538403\" \"679944\" \"318990\"\n [25] \"469317\" \"469722\" \"468585\" \"469293\" \"339948\" \"327919\" \"109100\" \"649930\"\n [33] \"648354\" \"547529\" \"399935\" \"658962\" \"159050\" \"689809\" \"679676\" \"598112\"\n [41] \"659634\" \"659633\" \"757714\" \"757699\" \"387621\" \"88256\"  \"518935\" \"579767\"\n [49] \"297822\" \"349692\" \"349700\" \"529894\" \"529896\" \"424821\" \"99757\"  \"449150\"\n [57] \"659401\" \"558979\" \"534793\" \"679287\" \"319765\" \"319764\" \"548595\" \"569405\"\n [65] \"99138\"  \"768959\" \"529392\" \"737924\" \"689905\" \"688845\" \"439012\" \"768547\"\n [73] \"129903\" \"129904\" \"608784\" \"545083\" \"689814\" \"648347\" \"247961\" \"609561\"\n [81] \"479226\" \"479229\" \"659441\" \"689285\" \"569277\" \"659204\" \"436895\" \"529093\"\n [89] \"529258\" \"529231\" \"828869\" \"828814\" \"518866\" \"757521\" \"573838\" \"738908\"\n [97] \"738489\" \"139648\" \"139649\" \"217567\" \"469680\" \"797538\" \"797701\" \"319252\"\n[105] \"648200\" \"739063\" \"739062\" \"609647\" \"649410\" \"158901\" \"159561\" \"389706\"\n[113] \"389705\" \"529176\" \"828848\" \"828866\" \"677744\" \"679938\" \"737942\" \"579807\"\n[121] \"519421\" \"427072\" \"278790\" \"659250\" \"534786\" \"536451\" \"327829\" \"828819\"\n[129] \"534238\" \"534256\" \"649371\" \"768857\" \"269734\" \"737888\" \"768515\" \"569228\"\n[137] \"528906\" \"528933\" \"688258\" \"609476\" \"618652\" \"648368\" \"648348\" \"609790\"\n[145] \"649038\" \"659762\" \"127368\" \"319580\" \"399772\" \"689189\" \"689333\" \"579793\"\n[153] \"579795\" \"618310\" \"659243\" \"519073\" \"408931\" \"408940\" \"368051\" \"738927\"\n[161] \"739110\" \"297754\" \"569948\" \"569384\" \"828867\" \"518798\" \"518901\" \"599986\"\n[169] \"599986\" \"658965\" \"538786\" \"538785\" \"545079\" \"545080\" \"129956\" \"128806\"\n[177] \"288683\" \"556111\" \"268097\" \"288913\" \"769028\" \"768689\" \"148812\" \"139657\"\n[185] \"449149\" \"529283\" \"545088\" \"768960\" \"544974\" \"545081\" \"768692\" \"769026\"\n[193] \"768578\" \"327830\" \"757622\" \"828671\" \"129957\" \"828716\" \"458436\" \"768454\"\n[201] \"168622\" \"544822\" \"519524\" \"518934\" \"518968\" \"529400\" \"536741\" \"536742\"\n[209] \"319320\" \"597610\" \"797702\" \"129857\" \"556094\" \"575566\" \"768687\" \"649076\"\n[217] \"529067\" \"569845\" \"659163\" \"828674\" \"828772\" \"538787\" \"828870\" \"828845\"\n[225] \"128104\" \"738525\" \"149303\" \"148800\" \"149295\" \"99840\"  \"289072\" \"318871\"\n[233] \"575954\" \"469719\" \"689621\" \"649961\" \"237993\" \"737803\" \"738524\" \"545092\"\n[241] \"555855\" \"649295\" \"138572\" \"227968\" \"757715\" \"757704\" \"545166\" \"545090\"\n[249] \"797636\" \"555889\" \"534237\" \"649332\" \"739067\" \"309437\" \"309437\" \"737913\"\n[257] \"689762\" \"737758\" \"544799\" \"529593\" \"357691\" \"359337\" \"359342\" \"469701\"\n[265] \"469700\" \"556742\" \"556140\" \"529706\" \"528986\" \"308274\" \"309331\" \"387724\"\n[273] \"259240\" \"429058\" \"455789\" \"659322\" \"518799\" \"529565\" \"529426\" \"529427\"\n[281] \"439272\" \"437259\" \"438796\" \"449761\" \"569299\" \"688261\" \"479239\" \"469278\"\n[289] \"469300\" \"465561\" \"569730\" \"688268\" \"689143\" \"828728\" \"449035\" \"448880\"\n[297] \"828802\" \"757702\" \"649223\" \"679946\" \"677737\" \"677742\" \"649188\" \"648350\"\n[305] \"519075\" \"579747\" \"738079\" \"738990\" \"738853\" \"738240\" \"738239\" \"739111\"\n[313] \"538882\" \"649036\" \"538784\" \"538789\" \"768611\" \"556108\" \"689100\" \"538720\"\n[321] \"569868\" \"768675\" \"768679\" \"768516\" \"768610\" \"469623\" \"618654\" \"609558\"\n[329] \"649406\" \"529393\" \"658712\" \"538884\" \"169485\" \"679002\" \"677741\" \"556095\"\n[337] \"556123\"\n\n\nThe below code chunk churned out 3 incomplete postal codes.\n\n\nCode Chunk\n# Display postal codes that are not 6 digits\ninvalid_postal_codes &lt;- school %&gt;%\n  filter(nchar(as.character(postal_code)) != 6) %&gt;%\n  select(address, postal_code)\n\n# Show the result\nprint(invalid_postal_codes)\n\n\n# A tibble: 4 × 2\n  address                  postal_code\n  &lt;chr&gt;                    &lt;chr&gt;      \n1 1    Cantonment Close    88256      \n2 1    Bukit Teresa Road   99757      \n3 160  LOWER DELTA ROAD    99138      \n4 1    BUKIT PURMEI AVENUE 99840      \n\n\nThe correct postal code has been changed with the assistance of Google Maps and it piped back to the main df school.\n\n\nCode Chunk\nschool &lt;- school %&gt;% \n  mutate(postal_code = ifelse(postal_code == \"88256\", \"088256\", postal_code)) %&gt;% \n  mutate(postal_code = ifelse(postal_code == \"99757\", \"099757\", postal_code)) %&gt;% \n  mutate(postal_code = ifelse(postal_code == \"99840\", \"099840\", postal_code))\n\n\n\n\nCode Chunk\nschool &lt;- school %&gt;% \n  select(school_name, address, postal_code, mainlevel_code) %&gt;% \n  filter(mainlevel_code ==\"PRIMARY\")\n\n\n\n\nCode Chunk\npostal_list &lt;- sort(unique(school$postal_code)) #parse a list as API cannot read df\n#unique reduces records to pass to portal\n#sort is used to easier to find geo codes\n\n\nFetching Data from onemap API\nAs the primary schools lack of geometry coordinates, API was used to extract the coordinates.\nThis will return WGS84 xy coordinates alongside SVY21 xy coordinates.\n\n\nCode Chunk\nget_coords &lt;- function(postal_list){\n  # Create a data frame to store all retrieved coordinates\n  postal_coords &lt;- data.frame()\n    \n  for (postal in postal_list){\n    r &lt;- GET('https://www.onemap.gov.sg/api/common/elastic/search?',\n           query=list(searchVal=postal,\n                     returnGeom='Y',\n                     getAddrDetails='Y'))\n    data &lt;- fromJSON(rawToChar(r$content))\n    found &lt;- data$found\n    res &lt;- data$results\n    \n    # Create a new data frame for each postal code\n    new_row &lt;- data.frame()\n    \n    # If single result, append \n    if (found == 1){\n      postal_code &lt;- res$POSTAL \n      lat &lt;- res$LATITUDE\n      lng &lt;- res$LONGITUDE\n      new_row &lt;- data.frame(postal_code = postal, \n                           postal_found = postal_code, \n                           latitude_wgs84 = lat,\n                           longitude_wgs84 = lng)\n    }\n    \n    # If multiple results, use the exact postal code match\n    else if (found &gt; 1){\n      # Find exact match for postal code\n      res_match &lt;- res[res$POSTAL == postal, ]\n      \n      # If exact match found, use it\n      if (nrow(res_match) &gt; 0) {\n        postal_code &lt;- res_match$POSTAL[1]\n        lat &lt;- res_match$LATITUDE[1]\n        lng &lt;- res_match$LONGITUDE[1]\n        new_row &lt;- data.frame(postal_code = postal,\n                             postal_found = postal_code,\n                             latitude_wgs84 = lat,\n                             longitude_wgs84 = lng)\n      }\n      # If no exact match, set as NA\n      else {\n        new_row &lt;- data.frame(postal_code = postal,\n                             postal_found = NA,\n                             latitude_wgs84 = NA,\n                             longitude_wgs84 = NA)\n      }\n    }\n    # If no results found\n    else {\n      new_row &lt;- data.frame(postal_code = postal,\n                           postal_found = NA,\n                           latitude_wgs84 = NA,\n                           longitude_wgs84 = NA)\n    }\n    \n    # Add the row\n    postal_coords &lt;- rbind(postal_coords, new_row)\n  }\n  \n  # Convert to sf object with WGS84 coordinates (EPSG:4326)\n  # Filter out rows with NA coordinates first\n  valid_coords &lt;- postal_coords[!is.na(postal_coords$latitude_wgs84) & \n                              !is.na(postal_coords$longitude_wgs84), ]\n  \n  if(nrow(valid_coords) &gt; 0) {\n    coords_sf &lt;- st_as_sf(valid_coords, \n                         coords = c(\"longitude_wgs84\", \"latitude_wgs84\"),\n                         crs = 4326)\n    \n    # Transform to SVY21 (EPSG:3414)\n    coords_svy21 &lt;- st_transform(coords_sf, 3414)\n    \n    # Extract coordinates\n    coords_matrix &lt;- st_coordinates(coords_svy21)\n    \n    # Add SVY21 coordinates back to the original dataframe\n    valid_coords$longitude &lt;- coords_matrix[, 1]  # SVY21 X coordinate\n    valid_coords$latitude &lt;- coords_matrix[, 2]   # SVY21 Y coordinate\n    \n    # Add geometry column\n    valid_coords$geometry &lt;- st_geometry(coords_svy21)\n    \n    # Merge back with rows that had NA coordinates\n    result &lt;- merge(postal_coords, \n                   valid_coords[c(\"postal_code\", \"longitude\", \"latitude\", \"geometry\")], \n                   by = \"postal_code\", all.x = TRUE)\n  } else {\n    # If no valid coordinates, add empty SVY21 columns\n    result &lt;- postal_coords\n    result$longitude &lt;- NA\n    result$latitude &lt;- NA\n    result$geometry &lt;- NA\n  }\n  \n  return(result)\n}\n\n# Usage example:\nadd_list_school &lt;- sort(unique(school$postal_code))\ncoords_school &lt;- get_coords(add_list_school)\nprint(head(coords_school))\n\n\n  postal_code postal_found   latitude_wgs84  longitude_wgs84 longitude latitude\n1      088256       088256 1.27547252623201 103.839962631748  28739.43 28660.79\n2      099757       099757   1.275022722964 103.828157564896  27425.62 28611.06\n3      099840       099840 1.27489701529771 103.824115781658  26975.80 28597.16\n4      109100       109100 1.27612047924037 103.808628535239  25252.19 28732.45\n5      128104       128104  1.3132658326807 103.756629101811  19465.19 32839.91\n6      128806       128806 1.31920159956387 103.761095065761  19962.23 33496.24\n                   geometry\n1 POINT (28739.43 28660.79)\n2 POINT (27425.62 28611.06)\n3  POINT (26975.8 28597.16)\n4 POINT (25252.19 28732.45)\n5 POINT (19465.19 32839.91)\n6 POINT (19962.23 33496.24)\n\n\nCode Chunk\n# Optional: Convert result to sf object for spatial operations\ncoords_school_sf &lt;- st_as_sf(coords_school[!is.na(coords_school$geometry), ])\n\n\nBelow is the code chunk that populates the coordinates in longitude, latitude and postal code against the address in the add_list.\n\n\nCode Chunk\ncoords_school &lt;- get_coords(postal_list)\nprint(coords_school)\n\n\n    postal_code postal_found   latitude_wgs84  longitude_wgs84 longitude\n1        088256       088256 1.27547252623201 103.839962631748  28739.43\n2        099757       099757   1.275022722964 103.828157564896  27425.62\n3        099840       099840 1.27489701529771 103.824115781658  26975.80\n4        109100       109100 1.27612047924037 103.808628535239  25252.19\n5        128104       128104  1.3132658326807 103.756629101811  19465.19\n6        128806       128806 1.31920159956387 103.761095065761  19962.23\n7        129857       129857 1.31666529308968 103.767438999533  20668.24\n8        129903       129903 1.31506321827875 103.763144493687  20190.30\n9        139648       139648 1.30100441916733 103.785455606827  22673.28\n10       148812       148812 1.29981138884057 103.799964819895  24288.03\n11       149303       149303 1.29552905587752 103.807648475656  25143.14\n12       158901       158901 1.28559516494962 103.815547410064  26022.22\n13       159016       159016 1.29133439161334 103.824424680531  27010.19\n14       169485       169485 1.28421153855474 103.825951884637  27180.15\n15       217567       217567 1.31236867681371 103.850766402965  29941.78\n16       227988       227988 1.30935041274966 103.840950265464  28849.34\n17       237993       237993 1.29418347842837 103.836018941571  28300.53\n18       268097       268097 1.32107094241153 103.807681852799  25146.89\n19       278790       278790 1.31667646178347 103.784296227747  22544.29\n20       289072       289072 1.33004178068277 103.806397828938  25004.00\n21       297754       297754 1.34043840661368 103.839811736775  28722.62\n22       309331       309331 1.31781476118551 103.845633334522  29370.51\n23       309437       309437  1.3206340835184 103.828164966953  27426.45\n24       309918       309918 1.31837054523521 103.835609732354  28254.98\n25       319252       319252 1.34032299499938 103.855529906182  30471.88\n26       319320       319320 1.33677864390672 103.855341310212  30450.90\n27       319580       319580  1.3373381935639 103.847148501824  29539.12\n28       319765       319765 1.33275264711587 103.841847263786  28949.15\n29       327829       327829 1.32175999964553 103.857628468881  30705.45\n30       339948       339948 1.32181250780475 103.865404167629  31570.81\n31       349700       349700 1.33566084356265  103.87562006259  32707.71\n32       359337       359337 1.33140255308868 103.865117134385  31538.85\n33       387621       387621 1.32664424544839 103.882227964852  33443.12\n34       387724       387724 1.32439150290682 103.881624837373  33376.00\n35       389706       389706 1.31814416827374 103.883628501601  33599.00\n36       399772       399772 1.31099674009193 103.888352029067  34124.70\n37       408931       408931 1.32858023873554 103.901306904157  35566.41\n38       424821       424821 1.30648535277087 103.911105620496  36656.98\n39       427072       427072 1.31194344271383 103.902902557195  35744.04\n40       437259       437259 1.30498489044822 103.899999965407  35421.03\n41       449149       449149 1.30553224180437 103.917570005713  37376.41\n42       449761       449761 1.30528528687321 103.911553152326  36706.79\n43       455789       455789 1.31878997295135 103.917258129598  37341.65\n44       458436       458436 1.31996876377748 103.923752668632  38064.43\n45       469300       469300 1.31720341663064  103.94575943784  40513.58\n46       469317       469317 1.32344593287992 103.937878976352  39636.53\n47       469623       469623 1.33398037894072 103.932015036296  38983.89\n48       469680       469680 1.32980598760785 103.931710293058  38949.99\n49       469701       469701  1.3347253730189 103.941234868202  40009.96\n50       469719       469719 1.33405659580859 103.934317528484  39240.13\n51       479226       479226 1.33524602829747 103.921286435321  37789.90\n52       479239       479239 1.33109611001931 103.911005345855  36645.74\n53       518798       518798 1.37550677054835 103.934953276155  39310.69\n54       518866       518866 1.37505696315817 103.945289416873  40460.98\n55       518935       518935 1.37246063927999 103.957020286115  41766.50\n56       518968       518968 1.37245132722087 103.962922699031  42423.37\n57       519075       519075 1.36563610941695 103.960861814678  42194.05\n58       519524       519524 1.37801687647812   103.9392021807  39783.53\n59       528906       528906 1.34787074766746 103.939221709073  39785.85\n60       529067       529067 1.35765121877679 103.935246486174  39343.40\n61       529176       529176 1.35711606790292 103.949144797277  40890.13\n62       529258       529258 1.35268400746357 103.961676849165  42284.83\n63       529366       529366 1.34828400809545 103.951482746538  41150.37\n64       529392       529392 1.35061130568554 103.951317297552  41131.94\n65       529393       529393 1.35131569890862 103.950551091373  41046.67\n66       529426       529426  1.3504863946694 103.943573098509  40270.10\n67       529565       529565 1.36048786025607 103.948768900518  40848.28\n68       529706       529706 1.34967978969837 103.937016363397  39540.41\n69       529896       529896 1.34023163951633 103.952080114035  41216.89\n70       534238       534238 1.37738245231461 103.880805274557  33284.68\n71       534793       534793 1.37330297140128 103.897575728303  35151.03\n72       536451       536451 1.36693830877349 103.894114899795  34765.90\n73       536741       536741  1.3502545600894 103.884846876112  33734.52\n74       538720       538720 1.37793264345276 103.885643682171  33823.13\n75       538784       538784 1.37176798317468  103.88280991785  33507.78\n76       538786       538786 1.37362445234082 103.889741457901  34279.17\n77       538787       538787 1.37820062547641 103.894667159512  34827.33\n78       538882       538882 1.35799653782463 103.890216355459  34332.07\n79       544799       544799 1.39528544463377 103.889416413501  34242.94\n80       544822       544822 1.38376923022123 103.891354675017  34458.68\n81       544969       544969 1.39036998654612 103.887165375933  33992.45\n82       544974       544974 1.38255066171151  103.89626457548  35005.09\n83       545080       545080 1.39221149906592 103.891180928186  34439.32\n84       545088       545088 1.38706146494859 103.903202683028  35777.20\n85       545092       545092 1.39333506298213 103.904545253288  35926.59\n86       545166       545166 1.38925044778224 103.899527879229  35368.24\n87       555855       555855  1.3728583654058  103.87477170399  32613.23\n88       556095       556095 1.36026072476019 103.869712517383  32050.22\n89       556108       556108  1.3489768083841 103.868467129954  31911.64\n90       556742       556742 1.34930766795555 103.862309899986  31226.41\n91       558979       558979 1.35742952269615 103.864009397373  31415.53\n92       569228       569228 1.37196445720109 103.851763531681  30052.70\n93       569299       569299 1.36565018546903 103.851009800453  29968.82\n94       569730       569730 1.35994649477568 103.853768994775  30275.89\n95       569785       569785 1.38419941907925 103.841411716006  28900.66\n96       569920       569920 1.36932176584608 103.839630858752  28702.48\n97       569948       569948 1.37614621670476 103.835805246743  28276.74\n98       579646       579646  1.3605834338904 103.833020333986  27966.81\n99       579793       579793 1.34939813669536 103.855018233311  30414.93\n100      597610       597610 1.33807302121151 103.776250903095  21648.98\n101      598112       598112  1.3377498622588 103.766855344779  20603.35\n102      599986       599986 1.33266213599636 103.783382114018  22442.59\n103      609476       609476 1.34843859698946 103.733156566858  16853.08\n104      609558       609558 1.34291981622016 103.740861148879  17710.49\n105      609647       609647 1.33659726465816 103.736089481883  17179.43\n106      618310       618310 1.33820397704287 103.718089546664  15176.23\n107      648200       648200 1.33641298743234  103.69970173931  13129.85\n108      648347       648347 1.35148548772662 103.707578404944  14006.52\n109      648368       648368 1.33917519796104 103.698803732244  13029.92\n110      649036       649036 1.34270639669225 103.687588478681  11781.81\n111      649076       649076 1.34883617453427 103.695003812477  12607.09\n112      649188       649188  1.3477109654722 103.700467520791  13215.14\n113      649223       649223 1.34471211768915 103.698964212988  13047.82\n114      649295       649295 1.34673868889096 103.718457955487  15217.27\n115      649332       649332 1.34686967259067 103.721559387203  15562.43\n116      649930       649930 1.34284013791617 103.712858381937  14594.08\n117      659163       659163 1.34919098857585 103.740719208011  17694.72\n118      659243       659243 1.35424191127295 103.754165828711  19191.20\n119      659401       659401 1.36411948475833 103.749251919049  18644.37\n120      659441       659441  1.3591797447277  103.74847210784  18557.57\n121      659634       659634 1.34586054788695  103.75366641455  19135.59\n122      659762       659762 1.34552849960874 103.756449399351  19445.30\n123      677742       677742 1.38378546546228 103.760273432258  19871.00\n124      677744       677744  1.3858892193692 103.767794469258  20708.01\n125      679002       679002 1.37942873617052  103.76970317201  20920.40\n126      679287       679287   1.366603668561 103.767412204602  20665.41\n127      679676       679676 1.37350089986584 103.769417337654  20888.58\n128      679944       679944 1.38394936211823 103.773632022975  21357.65\n129      679946       679946 1.38917971808454  103.76639772288  20552.58\n130      688261       688261 1.38388660469396 103.753924558586  19164.46\n131      688268       688268 1.40278303219036 103.746801233973  18371.80\n132      689100       689100 1.39672603497195 103.751817992962  18930.07\n133      689189       689189 1.39356050497104 103.747362720939  18434.25\n134      689285       689285  1.3947078296399 103.743201812831  17971.20\n135      689762       689762 1.38143123987698    103.747153671  18410.93\n136      689814       689814 1.38026516111464 103.736554295636  17231.35\n137      689905       689905 1.37775890093367 103.741832917421  17818.78\n138      737803       737803 1.44700426493785 103.801889031334  24502.39\n139      737888       737888 1.42896700684852 103.790607539185  23246.90\n140      737942       737942 1.43989958331782 103.804725587384  24818.04\n141      738079       738079 1.43273620725049  103.79023031392  23204.93\n142      738240       738240 1.43485179380236  103.79741434351  24004.40\n143      738525       738525 1.44203596557592 103.788339762066  22994.56\n144      738853       738853 1.43658270720199 103.791791282236  23378.65\n145      738907       738907  1.4426347903311 103.800040119743  24296.63\n146      738908       738908 1.44414835451585 103.794545295922  23685.14\n147      738927       738927 1.43383904037059 103.773643013846  21359.01\n148      739063       739063 1.43047990810126 103.778192944298  21865.34\n149      739067       739067 1.43240935732525 103.786035065192  22738.06\n150      757521       757521 1.45388170417263 103.817174701639  26203.45\n151      757622       757622 1.45732867302094 103.814075611175  25858.57\n152      757702       757702 1.45168517614776 103.822516208087  26797.87\n153      757714       757714 1.45125024371543 103.815858858929  26057.01\n154      757715       757715  1.4457453096024 103.821152676215  26646.13\n155      768515       768515 1.42768847665227 103.830425490093  27678.04\n156      768611       768611 1.43347186925064 103.837755319448  28493.74\n157      768643       768643 1.43315271543517 103.832942401086  27958.14\n158      768679       768679 1.43397617334964 103.834050796658  28081.48\n159      768687       768687 1.41741627551251 103.830143209875  27646.63\n160      768857       768857 1.42697284512936 103.844240226564  29215.42\n161      768959       768959 1.43839562897006 103.839309173817  28666.66\n162      768960       768960 1.42745159462566 103.848378442893  29675.94\n163      769026       769026 1.42130404159807 103.840792923345  28831.79\n164      769028       769028 1.41590195697649 103.839100590207  28643.46\n165      797538       797538 1.39726966050479 103.880330305684  33231.78\n166      797636       797636  1.3925400930811 103.874984869551  32636.91\n167      797701       797701 1.39030227963797 103.874445458032  32576.88\n168      828671       828671 1.41842322079348 103.905147201163  35993.49\n169      828674       828674 1.41158455562454  103.89890448429  35298.79\n170      828716       828716 1.40507184758584 103.911200193835  36667.16\n171      828728       828728 1.40744168163777 103.898762173662  35282.97\n172      828772       828772 1.40172590450528 103.898794262307  35286.55\n173      828802       828802 1.39889261226315 103.918585924582  37489.11\n174      828819       828819   1.399550034492 103.913404855665  36912.52\n175      828845       828845 1.40505250226058 103.905299026568  36010.44\n176      828848       828848 1.39648195114256  103.91233462071  36793.43\n177      828867       828867 1.39485356081865  103.90857257736  36374.77\n178      828869       828869 1.40009139510996 103.907847599114  36294.07\n    latitude                  geometry\n1   28660.79 POINT (28739.43 28660.79)\n2   28611.06 POINT (27425.62 28611.06)\n3   28597.16  POINT (26975.8 28597.16)\n4   28732.45 POINT (25252.19 28732.45)\n5   32839.91 POINT (19465.19 32839.91)\n6   33496.24 POINT (19962.23 33496.24)\n7   33215.77 POINT (20668.24 33215.77)\n8   33038.64  POINT (20190.3 33038.64)\n9   31484.03 POINT (22673.28 31484.03)\n10  31352.08 POINT (24288.03 31352.08)\n11  30878.55 POINT (25143.14 30878.55)\n12  29780.11 POINT (26022.22 29780.11)\n13  30414.72 POINT (27010.19 30414.72)\n14  29627.11 POINT (27180.15 29627.11)\n15  32740.58 POINT (29941.78 32740.58)\n16  32406.84 POINT (28849.34 32406.84)\n17  30729.75 POINT (28300.53 30729.75)\n18  33702.84 POINT (25146.89 33702.84)\n19  33216.96 POINT (22544.29 33216.96)\n20  34694.80     POINT (25004 34694.8)\n21  35844.39 POINT (28722.62 35844.39)\n22  33342.78 POINT (29370.51 33342.78)\n23  33654.52 POINT (27426.45 33654.52)\n24  33404.23 POINT (28254.98 33404.23)\n25  35831.63 POINT (30471.88 35831.63)\n26  35439.72  POINT (30450.9 35439.72)\n27  35501.58 POINT (29539.12 35501.58)\n28  34994.53 POINT (28949.15 34994.53)\n29  33779.04 POINT (30705.45 33779.04)\n30  33784.85 POINT (31570.81 33784.85)\n31  35316.15 POINT (32707.71 35316.15)\n32  34845.27 POINT (31538.85 34845.27)\n33  34319.15 POINT (33443.12 34319.15)\n34  34070.05    POINT (33376 34070.05)\n35  33379.26    POINT (33599 33379.26)\n36  32588.94  POINT (34124.7 32588.94)\n37  34533.27 POINT (35566.41 34533.27)\n38  32090.16 POINT (36656.98 32090.16)\n39  32693.67 POINT (35744.04 32693.67)\n40  31924.22 POINT (35421.03 31924.22)\n41  31984.80  POINT (37376.41 31984.8)\n42  31957.47 POINT (36706.79 31957.47)\n43  33450.77 POINT (37341.65 33450.77)\n44  33581.14 POINT (38064.43 33581.14)\n45  33275.46 POINT (40513.58 33275.46)\n46  33965.69 POINT (39636.53 33965.69)\n47  35130.51 POINT (38983.89 35130.51)\n48  34668.92 POINT (38949.99 34668.92)\n49  35212.93 POINT (40009.96 35212.93)\n50  35138.95 POINT (39240.13 35138.95)\n51  35270.41  POINT (37789.9 35270.41)\n52  34811.50  POINT (36645.74 34811.5)\n53  39722.31 POINT (39310.69 39722.31)\n54  39672.62 POINT (40460.98 39672.62)\n55  39385.60   POINT (41766.5 39385.6)\n56  39384.60  POINT (42423.37 39384.6)\n57  38630.99 POINT (42194.05 38630.99)\n58  39999.88 POINT (39783.53 39999.88)\n59  36666.47 POINT (39785.85 36666.47)\n60  37747.93  POINT (39343.4 37747.93)\n61  37688.82 POINT (40890.13 37688.82)\n62  37198.82 POINT (42284.83 37198.82)\n63  36712.23 POINT (41150.37 36712.23)\n64  36969.57 POINT (41131.94 36969.57)\n65  37047.45 POINT (41046.67 37047.45)\n66  36955.72  POINT (40270.1 36955.72)\n67  38061.66 POINT (40848.28 38061.66)\n68  36866.50  POINT (39540.41 36866.5)\n69  35821.84 POINT (41216.89 35821.84)\n70  39929.52 POINT (33284.68 39929.52)\n71  39478.48 POINT (35151.03 39478.48)\n72  38774.69  POINT (34765.9 38774.69)\n73  36929.87 POINT (33734.52 36929.87)\n74  39990.37 POINT (33823.13 39990.37)\n75  39308.71 POINT (33507.78 39308.71)\n76  39514.00    POINT (34279.17 39514)\n77  40020.03 POINT (34827.33 40020.03)\n78  37785.95 POINT (34332.07 37785.95)\n79  41909.17 POINT (34242.94 41909.17)\n80  40635.77 POINT (34458.68 40635.77)\n81  41365.63 POINT (33992.45 41365.63)\n82  40501.04 POINT (35005.09 40501.04)\n83  41569.27 POINT (34439.32 41569.27)\n84  40999.84  POINT (35777.2 40999.84)\n85  41693.55 POINT (35926.59 41693.55)\n86  41241.87 POINT (35368.24 41241.87)\n87  39429.26 POINT (32613.23 39429.26)\n88  38036.27 POINT (32050.22 38036.27)\n89  36788.55 POINT (31911.64 36788.55)\n90  36825.12 POINT (31226.41 36825.12)\n91  37723.20  POINT (31415.53 37723.2)\n92  39330.38  POINT (30052.7 39330.38)\n93  38632.18 POINT (29968.82 38632.18)\n94  38001.50  POINT (30275.89 38001.5)\n95  40683.26 POINT (28900.66 40683.26)\n96  39038.16 POINT (28702.48 39038.16)\n97  39792.77 POINT (28276.74 39792.77)\n98  38071.92 POINT (27966.81 38071.92)\n99  36835.12 POINT (30414.93 36835.12)\n100 35582.91 POINT (21648.98 35582.91)\n101 35547.20  POINT (20603.35 35547.2)\n102 34984.58 POINT (22442.59 34984.58)\n103 36729.23 POINT (16853.08 36729.23)\n104 36118.96 POINT (17710.49 36118.96)\n105 35419.87 POINT (17179.43 35419.87)\n106 35597.61 POINT (15176.23 35597.61)\n107 35399.68 POINT (13129.85 35399.68)\n108 37066.28 POINT (14006.52 37066.28)\n109 35705.12 POINT (13029.92 35705.12)\n110 36095.65 POINT (11781.81 36095.65)\n111 36773.40  POINT (12607.09 36773.4)\n112 36648.95 POINT (13215.14 36648.95)\n113 36317.36 POINT (13047.82 36317.36)\n114 36541.34 POINT (15217.27 36541.34)\n115 36555.81 POINT (15562.43 36555.81)\n116 36110.29 POINT (14594.08 36110.29)\n117 36812.40  POINT (17694.72 36812.4)\n118 37370.85  POINT (19191.2 37370.85)\n119 38463.08 POINT (18644.37 38463.08)\n120 37916.87 POINT (18557.57 37916.87)\n121 36444.08 POINT (19135.59 36444.08)\n122 36407.36  POINT (19445.3 36407.36)\n123 40637.61    POINT (19871 40637.61)\n124 40870.21 POINT (20708.01 40870.21)\n125 40155.83  POINT (20920.4 40155.83)\n126 38737.71 POINT (20665.41 38737.71)\n127 39500.36 POINT (20888.58 39500.36)\n128 40655.69 POINT (21357.65 40655.69)\n129 41234.06 POINT (20552.58 41234.06)\n130 40648.81 POINT (19164.46 40648.81)\n131 42738.31  POINT (18371.8 42738.31)\n132 42068.54 POINT (18930.07 42068.54)\n133 41718.53 POINT (18434.25 41718.53)\n134 41845.41  POINT (17971.2 41845.41)\n135 40377.34 POINT (18410.93 40377.34)\n136 40248.44 POINT (17231.35 40248.44)\n137 39971.29 POINT (17818.78 39971.29)\n138 47627.92 POINT (24502.39 47627.92)\n139 45633.47  POINT (23246.9 45633.47)\n140 46842.32 POINT (24818.04 46842.32)\n141 46050.25 POINT (23204.93 46050.25)\n142 46284.17  POINT (24004.4 46284.17)\n143 47078.58 POINT (22994.56 47078.58)\n144 46475.58 POINT (23378.65 46475.58)\n145 47144.77 POINT (24296.63 47144.77)\n146 47312.14 POINT (23685.14 47312.14)\n147 46172.24 POINT (21359.01 46172.24)\n148 45800.79 POINT (21865.34 45800.79)\n149 46014.12 POINT (22738.06 46014.12)\n150 48388.38 POINT (26203.45 48388.38)\n151 48769.53 POINT (25858.57 48769.53)\n152 48145.49 POINT (26797.87 48145.49)\n153 48097.40  POINT (26057.01 48097.4)\n154 47488.69 POINT (26646.13 47488.69)\n155 45492.05 POINT (27678.04 45492.05)\n156 46131.55 POINT (28493.74 46131.55)\n157 46096.26 POINT (27958.14 46096.26)\n158 46187.32 POINT (28081.48 46187.32)\n159 44356.21 POINT (27646.63 44356.21)\n160 45412.93 POINT (29215.42 45412.93)\n161 46676.00    POINT (28666.66 46676)\n162 45465.87 POINT (29675.94 45465.87)\n163 44786.10  POINT (28831.79 44786.1)\n164 44188.76 POINT (28643.46 44188.76)\n165 42128.55 POINT (33231.78 42128.55)\n166 41605.56 POINT (32636.91 41605.56)\n167 41358.12 POINT (32576.88 41358.12)\n168 44467.67 POINT (35993.49 44467.67)\n169 43711.47 POINT (35298.79 43711.47)\n170 42991.37 POINT (36667.16 42991.37)\n171 43253.37 POINT (35282.97 43253.37)\n172 42621.35 POINT (35286.55 42621.35)\n173 42308.13 POINT (37489.11 42308.13)\n174 42380.80  POINT (36912.52 42380.8)\n175 42989.21 POINT (36010.44 42989.21)\n176 42041.54 POINT (36793.43 42041.54)\n177 41861.47 POINT (36374.77 41861.47)\n178 42440.64 POINT (36294.07 42440.64)\n\n\n\n\nCode Chunk\n# Combine latitude and longitude in `coords` int a single \"geometry\" column and convert to an sf object\ncoords_school &lt;- coords_school %&gt;%\n  select(postal_code, geometry)\n\n\nFollowing which, the coords_sf is then combined to the resale_selected df by address, forming a new df resale_geom\n\n\nCode Chunk\nschool_cleaned &lt;- school %&gt;% \n  left_join(coords_school, by = \"postal_code\")\n\n\nIn the code chunk, we will extract the longitude and latitude from the geometry to facilitate transforming into sf.\n\n\nCode Chunk\nschool_cleaned &lt;- school_cleaned %&gt;%\n  mutate(\n    longitude = st_coordinates(geometry)[, 1],  # Extract longitude\n    latitude = st_coordinates(geometry)[, 2]    # Extract latitude\n  )\n\n\nThen, we are able to transform this into sf with the new longitude and latitude columns.\n\n\nCode Chunk\nschool_cleaned&lt;- st_as_sf(school_cleaned, coords = c(\"longitude\", \"latitude\"), crs = 3414)\n\n\n\n\n\nAs the af mall was in WGS84, it will transformed into SVY21 EPSG 3414.\n\n\nCode Chunk\n# Create an sf object with WGS 84 CRS\nmall &lt;- st_as_sf(mall, coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4326)\n# Transform to EPSG:3414\nmall_cleaned &lt;- st_transform(mall, crs = 3414)\n\n\nFrom the code below, we noticed that there are two malls that are duplicated.\n\n\nCode Chunk\ngeom_text &lt;- st_as_text(st_geometry(mall_cleaned))\n\n# Find duplicated geometries\nduplicates &lt;- mall_cleaned[duplicated(geom_text) | duplicated(geom_text, fromLast = TRUE), ]\n\n# Print summary\nprint(paste(\"Number of duplicate geometries found:\", nrow(duplicates)))\n\n\n[1] \"Number of duplicate geometries found: 4\"\n\n\nCode Chunk\nif(nrow(duplicates) &gt; 0) {\n  print(\"\\nDuplicate records:\")\n  print(duplicates)\n}\n\n\n[1] \"\\nDuplicate records:\"\nSimple feature collection with 4 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 29695.5 ymin: 34261.3 xmax: 38710.09 ymax: 36920.73\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 4 × 2\n  `Mall Name`                  geometry\n  &lt;chr&gt;                     &lt;POINT [m]&gt;\n1 Junction 8         (29695.5 36920.73)\n2 Djitsun Mall Bedok (38710.09 34261.3)\n3 Djitsun Mall       (38710.09 34261.3)\n4 Junction 8         (29695.5 36920.73)\n\n\nHence, with the code below, we will remove the duplicate.\n\n\nCode Chunk\n# Convert geometry to text for comparison\ngeom_text &lt;- st_as_text(st_geometry(mall_cleaned))\n\n# Add row numbers to track which entries we keep\nmall_cleaned$row_num &lt;- 1:nrow(mall_cleaned)\n\n# Remove duplicates, keeping first occurrence only\nmall_cleaned&lt;- mall_cleaned[!duplicated(geom_text), ]\n\nmall_cleaned &lt;- mall_cleaned %&gt;% \n  select(`Mall Name`, geometry)\n\n\n\n\n\nThe code chunk below checks if there are any duplicates geometries and it returns nil.\n\n\nCode Chunk\n# Get the geometries as text for easier comparison\ngeom_park &lt;- st_as_text(st_geometry(park))\n\n# Find duplicated geometries\nduplicates_park &lt;- park[duplicated(geom_park) | duplicated(geom_park, fromLast = TRUE), ]\n\n\nAs the sf park was in WGS84, it will transformed into SVY21 EPSG 3414.\n\n\nCode Chunk\n# Create an sf object with WGS 84 CRS\npark &lt;- st_as_sf(park, coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4326)\n\n# Transform to EPSG:3414\npark_cleaned &lt;- st_transform(park, crs = 3414)\n\n\n\n\n\nLet’s take a quick glimpse at the data. We noticed that there duplicated station names, depots, command centres (i.e. BOCC - to which I derived it as a Bus Operations Control Centre, stations that are under construction.\n\n\nCode Chunk\nglimpse(mrt)\n\n\nRows: 230\nColumns: 6\n$ TYP_CD     &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ STN_NAM    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ATTACHEMEN &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ TYP_CD_DES &lt;chr&gt; \"MRT\", \"MRT\", \"MRT\", \"MRT\", \"MRT\", \"MRT\", \"MRT\", \"MRT\", \"LR…\n$ STN_NAM_DE &lt;chr&gt; \"GALI BATU DEPOT\", \"HILLVIEW MRT STATION\", \"BEAUTY WORLD MR…\n$ geometry   &lt;POLYGON [m]&gt; POLYGON ((19210.61 41858.04..., POLYGON ((20650.33 …\n\n\nThus, with the code chunk below, we use the duplicate function to check which are the duplciated stations. Noticeably, these stations are actually interchange where it consists of more than 1 metro line (i.e. Outram Park where it houses East-West, Thomson-EastCoast, and North-East Line). However, Bayshore MRT isn’t an interchange but there are 2 duplicates of it that is likely due to the construction of Bedok South where both links. Regardless, the decision to keep these variables retains as it serves as an important proximate to the residential areas in geospatial setting.\n\n\nCode Chunk\n# Check for duplicates in the STN_NAM_DE column and display the whole rows\nduplicates &lt;- mrt %&gt;%\n  group_by(STN_NAM_DE) %&gt;%\n  filter(n() &gt; 1) %&gt;%\n  ungroup()\n\n# Display duplicates if any\nif (nrow(duplicates) &gt; 0) {\n  print(duplicates)\n} else {\n  print(\"No duplicates found in the STN_NAM_DE column.\")\n}\n\n\nSimple feature collection with 50 features and 5 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 22616.72 ymin: 27478.44 xmax: 42456.47 ymax: 46579.58\nProjected CRS: SVY21\n# A tibble: 50 × 6\n   TYP_CD STN_NAM ATTACHEMEN     TYP_CD_DES STN_NAM_DE                  geometry\n    &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;chr&gt;      &lt;chr&gt;                  &lt;POLYGON [m]&gt;\n 1      0 &lt;NA&gt;    &lt;NA&gt;           MRT        DHOBY GHA… ((29293.51 31312.53, 293…\n 2      0 &lt;NA&gt;    CC15_BSH STN.… MRT        BISHAN MR… ((29683.2 37087.82, 2970…\n 3      0 &lt;NA&gt;    CC13_SER STN.… MRT        SERANGOON… ((32244.31 36987.67, 322…\n 4      0 &lt;NA&gt;    &lt;NA&gt;           MRT        TAMPINES … ((40488.79 37181.92, 404…\n 5      0 &lt;NA&gt;    &lt;NA&gt;           MRT        STEVENS M… ((27140.4 33629.43, 2714…\n 6      0 &lt;NA&gt;    CC10_MPS STN.… MRT        MACPHERSO… ((34310.59 34386.88, 343…\n 7      0 &lt;NA&gt;    C504_S_CGA_MA… MRT        EXPO MRT … ((42293.7 35208.13, 4229…\n 8      0 &lt;NA&gt;    NE1_HBF STN.z… MRT        HARBOURFR… ((26584.75 27512.49, 265…\n 9      0 &lt;NA&gt;    CE2_MRB STN.z… MRT        MARINA BA… ((30397.79 28652.09, 303…\n10      0 &lt;NA&gt;    CC9_PYL STN.z… MRT        PAYA LEBA… ((34495.6 33384.44, 3452…\n# ℹ 40 more rows\n\n\nCode Chunk\nglimpse(duplicates)\n\n\nRows: 50\nColumns: 6\n$ TYP_CD     &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ STN_NAM    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ATTACHEMEN &lt;chr&gt; NA, \"CC15_BSH STN.zip\", \"CC13_SER STN.zip\", NA, NA, \"CC10_M…\n$ TYP_CD_DES &lt;chr&gt; \"MRT\", \"MRT\", \"MRT\", \"MRT\", \"MRT\", \"MRT\", \"MRT\", \"MRT\", \"MR…\n$ STN_NAM_DE &lt;chr&gt; \"DHOBY GHAUT MRT STATION\", \"BISHAN MRT STATION\", \"SERANGOON…\n$ geometry   &lt;POLYGON [m]&gt; POLYGON ((29293.51 31312.53..., POLYGON ((29683.2 3…\n\n\n\n\n\n\n\n\n\nMRT System Map\n\n\nIn the below code chunk, we will cross-check with the MRT map above and remove the names are that are depots, operation centres, sub stations and columns that are not needed. The code chunks includes stations with “MRT STATION” or “LRT STATION”.\n\n\nCode Chunk\nmrt_cleaned &lt;- mrt %&gt;%\n  select(-TYP_CD, -STN_NAM, -ATTACHEMEN) %&gt;%  # Remove specified columns\n  filter(grepl(\"MRT STATION|LRT STATION\", STN_NAM_DE, ignore.case = TRUE))  # Filter rows\n\n# Display the cleaned sf object\nprint(mrt_cleaned)\n\n\nSimple feature collection with 215 features and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 6068.209 ymin: 27478.44 xmax: 45377.5 ymax: 47913.58\nProjected CRS: SVY21\nFirst 10 features:\n   TYP_CD_DES                STN_NAM_DE                       geometry\n1         MRT      HILLVIEW MRT STATION POLYGON ((20650.33 38282.33...\n2         MRT  BEAUTY WORLD MRT STATION POLYGON ((21594.72 35882.94...\n3         MRT          HUME MRT STATION POLYGON ((20808 37457.72, 2...\n4         MRT BUKIT PANJANG MRT STATION POLYGON ((19996.27 40187.21...\n5         MRT        CASHEW MRT STATION POLYGON ((20340.38 39136.76...\n6         MRT   DHOBY GHAUT MRT STATION POLYGON ((29293.51 31312.53...\n7         MRT      LAVENDER MRT STATION POLYGON ((31236.5 32085.76,...\n8         LRT       RENJONG LRT STATION POLYGON ((34382.66 40949.64...\n9         MRT         DOVER MRT STATION POLYGON ((21987.25 32576.91...\n10        LRT       PHOENIX LRT STATION POLYGON ((19602.92 40048.64...\n\n\nIn the list, we noticed that some of the MRT stations are actually under construction. Hence we cross-checked with the MRT map above and exclude those that are under construction.\nFirst, we create a list of excluded MRT list. Then we use Regex to exclude it using if function.\n\n\nCode Chunk\n# Specify the words to exclude\nwords_to_exclude &lt;- c(\"HUME\", \"SUNGEI BEDOK\", \"BEDOK SOUTH\", \"XILIN\", \n                       \"PUNGGOL COAST\", \"BUKIT BROWN\", \"MOUNT PLEASANT\", \n                       \"FOUNDERS' MEMORIAL\")\n\n# Create a regular expression pattern to match any of the words\npattern &lt;- paste(words_to_exclude, collapse = \"|\")\n\n# Ensure the train object is an sf object\nif (!inherits(mrt_cleaned, \"sf\")) {\n  stop(\"The 'train' object is not an sf object.\")\n}\n\n# Filter the train sf object to exclude rows with specified words in STN_NAM_DE\nmrt_cleaned  &lt;- mrt_cleaned %&gt;%\n  filter(!str_detect(STN_NAM_DE, regex(pattern, ignore_case = TRUE)))\n\n# Verify the results\nprint(head(mrt_cleaned))  # Print the first few rows of the filtered object\n\n\nSimple feature collection with 6 features and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 19932.72 ymin: 31220.26 xmax: 31375.01 ymax: 40255.73\nProjected CRS: SVY21\n  TYP_CD_DES                STN_NAM_DE                       geometry\n1        MRT      HILLVIEW MRT STATION POLYGON ((20650.33 38282.33...\n2        MRT  BEAUTY WORLD MRT STATION POLYGON ((21594.72 35882.94...\n3        MRT BUKIT PANJANG MRT STATION POLYGON ((19996.27 40187.21...\n4        MRT        CASHEW MRT STATION POLYGON ((20340.38 39136.76...\n5        MRT   DHOBY GHAUT MRT STATION POLYGON ((29293.51 31312.53...\n6        MRT      LAVENDER MRT STATION POLYGON ((31236.5 32085.76,...\n\n\nCode Chunk\nsummary(mrt_cleaned )      # Summary of the filtered object\n\n\n  TYP_CD_DES         STN_NAM_DE                 geometry  \n Length:211         Length:211         POLYGON      :211  \n Class :character   Class :character   epsg:NA      :  0  \n Mode  :character   Mode  :character   +proj=tmer...:  0  \n\n\nWe will then view() the mrt_cleaned sf and search for the excluded MRT stations in ensuring that it is not there.\n\n\nCode Chunk\nview(mrt_cleaned)\n\n\n\n\nCode Chunk\nst_crs(mrt_cleaned) &lt;- 3414\n\n\n\n\n\nThe code chunk below checks if there are any duplicates geometries and it returns several duplicates.\nIn the table below, we noticed that there are 33 rows of geometries returned with 2-4 duplicates in each geometry. In explaining this issue, we can zoom into the description column and it will surface different supermarket names sharing the same geometry. This is likely due to the changing of the tenants.\n\n\nCode Chunk\nsupermarket = read_rds(\"data/rds/geospatial/supermarket.rds\")\n\n\n\n\nCode Chunk\n# Find duplicated geometries\nduplicate_ids &lt;- supermarket %&gt;%\n  filter(duplicated(st_geometry(.)) | duplicated(st_geometry(.), fromLast = TRUE)) %&gt;%\n  select(geometry) %&gt;%\n  unique() %&gt;%\n  pull(geometry)\n\n# Create a table with duplicates together in a row and count\nduplicates_table &lt;- supermarket %&gt;%\n  filter(st_geometry(geometry) %in% duplicate_ids) %&gt;%\n  group_by(geometry) %&gt;%\n  summarise(duplicate_entries = list(cur_data()), \n            count = n(),         # Count the number of duplicates\n            .groups = 'drop')\n\n# View the duplicated entries together in a row with counts\nif (nrow(duplicates_table) &gt; 0) {\n  print(\"Duplicated geometries found:\")\n  print(duplicates_table)\n  total_duplicates &lt;- sum(duplicates_table$count)\n  cat(\"Total number of duplicates:\", total_duplicates, \"\\n\")\n} else {\n  print(\"No duplicated geometries found.\")\n}\n\n\n[1] \"Duplicated geometries found:\"\nSimple feature collection with 33 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6342 ymin: 1.27031 xmax: 103.9624 ymax: 1.451325\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n# A tibble: 33 × 3\n                  geometry duplicate_entries count\n               &lt;POINT [°]&gt; &lt;list&gt;            &lt;int&gt;\n 1 Z (103.8142 1.322573 0) &lt;tibble [2 × 2]&gt;      2\n 2  Z (103.7627 1.37835 0) &lt;tibble [2 × 2]&gt;      2\n 3 Z (103.8007 1.439916 0) &lt;tibble [2 × 2]&gt;      2\n 4  Z (103.857 1.307785 0) &lt;tibble [2 × 2]&gt;      2\n 5 Z (103.9132 1.402878 0) &lt;tibble [2 × 2]&gt;      2\n 6 Z (103.8435 1.274588 0) &lt;tibble [2 × 2]&gt;      2\n 7 Z (103.9051 1.301191 0) &lt;tibble [4 × 2]&gt;      4\n 8 Z (103.8945 1.314692 0) &lt;tibble [2 × 2]&gt;      2\n 9 Z (103.7653 1.303583 0) &lt;tibble [2 × 2]&gt;      2\n10 Z (103.7882 1.306849 0) &lt;tibble [2 × 2]&gt;      2\n# ℹ 23 more rows\nTotal number of duplicates: 74 \n\n\nIn addressing the duplicate, only one geometry will be retained while the rest will be eradicated.\n\n\nCode Chunk\n# Remove duplicates while retaining unique geometries\nsupermarket &lt;- supermarket %&gt;%\n  distinct(geometry, .keep_all = TRUE)  # Keep all columns for the unique geometries\n\n\nFollowing which, we can check again to see if there’s any duplicates remaining and it returns nil.\n\n\nCode Chunk\n# Find duplicated geometries\nduplicate_ids &lt;- supermarket %&gt;%\n  filter(duplicated(st_geometry(.)) | duplicated(st_geometry(.), fromLast = TRUE)) %&gt;%\n  select(geometry) %&gt;%\n  unique() %&gt;%\n  pull(geometry)\n\n# Create a table with duplicates together in a row and count\nduplicates_table &lt;- supermarket %&gt;%\n  filter(st_geometry(geometry) %in% duplicate_ids) %&gt;%\n  group_by(geometry) %&gt;%\n  summarise(duplicate_entries = list(cur_data()), \n            count = n(),         # Count the number of duplicates\n            .groups = 'drop')\n\n# View the duplicated entries together in a row with counts\nif (nrow(duplicates_table) &gt; 0) {\n  print(\"Duplicated geometries found:\")\n  print(duplicates_table)\n  total_duplicates &lt;- sum(duplicates_table$count)\n  cat(\"Total number of duplicates:\", total_duplicates, \"\\n\")\n} else {\n  print(\"No duplicated geometries found.\")\n}\n\n\n[1] \"No duplicated geometries found.\"\n\n\nThe data is then transformed from WGS84 to SVY21.\n\n\nCode Chunk\n# Create an sf object with WGS 84 CRS\nsupermarket &lt;- st_as_sf(supermarket, coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4326)\n\n# Transform to EPSG:3414\nsupermarket_cleaned &lt;- st_transform(supermarket, crs = 3414)\n\n\n\n\nCode Chunk\ncoordinates &lt;- st_coordinates(supermarket_cleaned)\nsupermarket_cleaned &lt;- supermarket_cleaned %&gt;%\n  mutate(longitude = coordinates[, 1],    # First column is longitude\n         latitude = coordinates[, 2]) %&gt;%  \n  select(longitude, latitude, geometry)\n\n\n\n\n\n\n\n\n\n\nCode Chunk\n# Create busstop_resale with just the two columns we want\nbusstop_resale &lt;- data.frame(\n  busstop_prox = numeric(nrow(resale_geom)),\n  within_350m_busstop = numeric(nrow(resale_geom)))\n\n# Loop through each resale point\nfor (i in 1:nrow(resale_geom)) {\n  # Get current resale point\n  current_point &lt;- resale_geom[i, ]\n  \n  # Calculate distances to all bus stops\n  distances &lt;- st_distance(current_point, busstop_cleaned)\n  \n  # Store minimum distance in km in busstop_prox\n  busstop_resale$busstop_prox[i] &lt;- as.numeric(min(distances)) / 1000\n  \n  # Count bus stops within 350 meters\n  busstop_resale$within_350m_busstop[i] &lt;- sum(as.numeric(distances) &lt;= 350)\n}\n\n# Verify results\nprint(head(busstop_resale))             # Print the first few rows\n\n\n  busstop_prox within_350m_busstop\n1   0.09807302                  10\n2   0.11396279                   6\n3   0.12282400                   3\n4   0.12282400                   3\n5   0.10566508                   3\n6   0.16643716                   6\n\n\nCode Chunk\nsummary(busstop_resale)                 # Summary of both columns\n\n\n  busstop_prox     within_350m_busstop\n Min.   :0.01543   Min.   : 0.000     \n 1st Qu.:0.07418   1st Qu.: 6.000     \n Median :0.10727   Median : 8.000     \n Mean   :0.11449   Mean   : 7.907     \n 3rd Qu.:0.14582   3rd Qu.:10.000     \n Max.   :0.39147   Max.   :19.000     \n\n\n\n\n\n\n\nCode Chunk\n# Create eldercare_resale with just the two columns we want\neldercare_resale &lt;- data.frame(\n  eldercare_prox = numeric(nrow(resale_geom)),\n  within_350m_eldercare = numeric(nrow(resale_geom)))\n\n# Loop through each point in busstop_resale\nfor (i in 1:nrow(resale_geom)) {\n  # Get current point\n  current_point &lt;- resale_geom[i, ]\n  \n  # Calculate distances to all eldercare centers\n  distances &lt;- st_distance(current_point, eldercare_cleaned)\n  \n  # Store minimum distance in km in eldercare_prox\n  eldercare_resale$eldercare_prox[i] &lt;- as.numeric(min(distances)) / 1000\n  \n  # Count eldercare facilities within 350 meters\n  eldercare_resale$within_350m_eldercare[i] &lt;- sum(as.numeric(distances) &lt;= 350)\n}\n\n# Verify results\nprint(head(eldercare_resale))             # Print the first few rows\n\n\n  eldercare_prox within_350m_eldercare\n1      0.3419924                     1\n2      0.2597442                     1\n3      0.2811279                     1\n4      0.2811279                     1\n5      0.4296838                     0\n6      0.4043587                     0\n\n\nCode Chunk\nsummary(eldercare_resale)   \n\n\n eldercare_prox   within_350m_eldercare\n Min.   :0.0000   Min.   :0.0000       \n 1st Qu.:0.3303   1st Qu.:0.0000       \n Median :0.6308   Median :0.0000       \n Mean   :0.7983   Mean   :0.3898       \n 3rd Qu.:1.0931   3rd Qu.:1.0000       \n Max.   :4.7675   Max.   :7.0000       \n\n\nCode Chunk\nbeep()\n\n\n\n\n\n\n\nCode Chunk\n# Create eldercare_resale with just the two columns we want\nhawker_resale &lt;- data.frame(\n  hawker_prox = numeric(nrow(resale_geom)))\n\n# Loop through each point in busstop_resale\nfor (i in 1:nrow(resale_geom)) {\n  # Get current point\n  current_point &lt;- resale_geom[i, ]\n  \n  # Calculate distances to all eldercare centers\n  distances &lt;- st_distance(current_point, hawker_cleaned)\n  \n  # Store minimum distance in km in eldercare_prox\n  hawker_resale$hawker_prox[i] &lt;- as.numeric(min(distances)) / 1000\n  \n}\n\n# Verify results\nprint(head(hawker_resale))             # Print the first few rows\n\n\n  hawker_prox\n1   0.1861061\n2   0.4248262\n3   0.3108075\n4   0.3108075\n5   0.3143604\n6   0.1378719\n\n\nCode Chunk\nbeep(3)\n\n\n\n\n\n\n\nCode Chunk\n# Create eldercare_resale with just the two columns we want\nkindergarten_resale &lt;- data.frame(\n  kindergarten_prox = numeric(nrow(resale_geom)),\n  within_350m_kindergarten = numeric(nrow(resale_geom)))\n\n# Loop through each point in busstop_resale\nfor (i in 1:nrow(resale_geom)) {\n  # Get current point\n  current_point &lt;- resale_geom[i, ]\n  \n  # Calculate distances to all eldercare centers\n  distances &lt;- st_distance(current_point, kindergarten_cleaned)\n  \n  # Store minimum distance in km in eldercare_prox\n  kindergarten_resale$kindergarten_prox[i] &lt;- as.numeric(min(distances)) / 1000\n  \n  # Count eldercare facilities within 350 meters\n  kindergarten_resale$within_350m_kindergarten[i] &lt;- sum(as.numeric(distances) &lt;= 350)\n}\n\n# Verify results\nprint(head(kindergarten_resale))             # Print the first few rows\n\n\n  kindergarten_prox within_350m_kindergarten\n1         0.5507577                        0\n2         0.2085693                        1\n3         0.2082784                        1\n4         0.2082784                        1\n5         0.1232351                        1\n6         0.1951542                        2\n\n\nCode Chunk\nsummary(kindergarten_resale)   # Summary of both columns\n\n\n kindergarten_prox within_350m_kindergarten\n Min.   :0.0000    Min.   :0.0000          \n 1st Qu.:0.1773    1st Qu.:0.0000          \n Median :0.2810    Median :1.0000          \n Mean   :0.3052    Mean   :0.9791          \n 3rd Qu.:0.4021    3rd Qu.:1.0000          \n Max.   :3.1675    Max.   :8.0000          \n\n\nCode Chunk\nbeep(3)              \n\n\n\n\n\n\n\nCode Chunk\n#change CRS of mall_cleaned to align to CRS EPSG 3414 \nst_crs(mall_cleaned) &lt;- 3414\n\n# Create eldercare_resale with just the two columns we want\nmall_resale &lt;- data.frame(\n  mall_prox = numeric(nrow(resale_geom)))\n\n# Loop through each point in busstop_resale\nfor (i in 1:nrow(resale_geom)) {\n  # Get current point\n  current_point &lt;- resale_geom[i, ]\n  \n  # Calculate distances to all eldercare centers\n  distances &lt;- st_distance(current_point, mall_cleaned)\n  \n  # Store minimum distance in km in eldercare_prox\n  mall_resale$mall_prox[i] &lt;- as.numeric(min(distances)) / 1000\n  \n}\n\n# Verify results\nprint(head(mall_resale))             # Print the first few rows\n\n\n  mall_prox\n1  48.70776\n2  48.80401\n3  48.81649\n4  48.81649\n5  48.60214\n6  48.12243\n\n\nCode Chunk\nbeep(3)\n\n\n\n\n\n\n\nCode Chunk\n# Create eldercare_resale with just the two columns we want\nmrt_resale &lt;- data.frame(\n  mrt_prox = numeric(nrow(resale_geom)))\n\n# Loop through each point in busstop_resale\nfor (i in 1:nrow(resale_geom)) {\n  # Get current point\n  current_point &lt;- resale_geom[i, ]\n  \n  # Calculate distances to all eldercare centers\n  distances &lt;- st_distance(current_point, mrt_cleaned)\n  \n  # Store minimum distance in km in eldercare_prox\n  mrt_resale$mrt_prox[i] &lt;- as.numeric(min(distances)) / 1000\n  \n}\n\n# Verify results\nprint(head(mrt_resale))             # Print the first few rows\n\n\n    mrt_prox\n1 0.90926089\n2 0.27613980\n3 0.33461983\n4 0.33461983\n5 0.06951106\n6 0.39898970\n\n\nCode Chunk\nbeep(3)\n\n\n\n\n\n\n\nCode Chunk\n# Create eldercare_resale with just the two columns we want\npark_resale &lt;- data.frame(\n  park_prox = numeric(nrow(resale_geom)))\n\n# Loop through each point in busstop_resale\nfor (i in 1:nrow(resale_geom)) {\n  # Get current point\n  current_point &lt;- resale_geom[i, ]\n  \n  # Calculate distances to all eldercare centers\n  distances &lt;- st_distance(current_point, park_cleaned)\n  \n  # Store minimum distance in km in eldercare_prox\n  park_resale$park_prox[i] &lt;- as.numeric(min(distances)) / 1000\n  \n}\n\n# Verify results\nprint(head(park_resale))             # Print the first few rows\n\n\n  park_prox\n1 0.2815657\n2 0.3108840\n3 0.3411321\n4 0.3411321\n5 0.2436839\n6 0.1985831\n\n\nCode Chunk\nbeep(3)\n\n\n\n\n\n\n\nCode Chunk\n# Create eldercare_resale with just the two columns we want\nschool_resale &lt;- data.frame(\n  school_prox = numeric(nrow(resale_geom)),\n  within_1km_school = numeric(nrow(resale_geom)))\n\n# Loop through each point in busstop_resale\nfor (i in 1:nrow(resale_geom)) {\n  # Get current point\n  current_point &lt;- resale_geom[i, ]\n  \n  # Calculate distances to all eldercare centers\n  distances &lt;- st_distance(current_point, school_cleaned)\n  \n  # Store minimum distance in km in eldercare_prox\n  school_resale$school_prox[i] &lt;- as.numeric(min(distances)) / 1000\n  \n  # Count eldercare facilities within 1000 meters\n  school_resale$within_1km_school[i] &lt;- sum(as.numeric(distances) &lt;= 1000)\n}\n\n# Verify results\nprint(head(school_resale))             # Print the first few rows\n\n\n  school_prox within_1km_school\n1   0.2279026                 2\n2   0.4437104                 3\n3   0.1210031                 3\n4   0.1210031                 3\n5   0.2472647                 2\n6   0.2692533                 2\n\n\nCode Chunk\nsummary(school_resale)   # Summary of both columns\n\n\n  school_prox      within_1km_school\n Min.   :0.04354   Min.   :0.000    \n 1st Qu.:0.24063   1st Qu.:2.000    \n Median :0.37618   Median :3.000    \n Mean   :0.42942   Mean   :2.979    \n 3rd Qu.:0.54703   3rd Qu.:4.000    \n Max.   :3.29166   Max.   :9.000    \n\n\nCode Chunk\nbeep(3)    \n\n\n\n\n\n\n\nCode Chunk\n# Create eldercare_resale with just the two columns we want\nsupermarket_resale &lt;- data.frame(\n  supermarket_prox = numeric(nrow(resale_geom)))\n\n# Loop through each point in busstop_resale\nfor (i in 1:nrow(resale_geom)) {\n  # Get current point\n  current_point &lt;- resale_geom[i, ]\n  \n  # Calculate distances to all eldercare centers\n  distances &lt;- st_distance(current_point, supermarket_cleaned)\n  \n  # Store minimum distance in km in eldercare_prox\n  supermarket_resale$supermarket_prox[i] &lt;- as.numeric(min(distances)) / 1000\n  \n}\n\n# Verify results\nprint(head(supermarket_resale))             # Print the first few rows\n\n\n  supermarket_prox\n1        0.3872724\n2        0.1666790\n3        0.3212926\n4        0.3212926\n5        0.3413032\n6        0.1577571\n\n\nCode Chunk\nbeep(3)\n\n\n\n\n\n\nIn the code chunk below, we are able to view the mininum, maximum, median and quadrants of the data. Interesting, the minimum and maximum resale price were $150,000 and $1.58m. And the smallest age of unit was 3 whereas the oldest was 58.\n\n\nCode Chunk\nsummary(resale_geom)\n\n\n   address              town            resale_price        month          \n Length:47423       Length:47423       Min.   : 150000   Length:47423      \n Class :character   Class :character   1st Qu.: 460000   Class :character  \n Mode  :character   Mode  :character   Median : 565000   Mode  :character  \n                                       Mean   : 587538                     \n                                       3rd Qu.: 685000                     \n                                       Max.   :1588000                     \n  flat_type         floor_area_sqm   remaining_lease_yr   longitude    \n Length:47423       Min.   : 31.00   Min.   :41.0       Min.   :11519  \n Class :character   1st Qu.: 74.00   1st Qu.:60.0       1st Qu.:21661  \n Mode  :character   Median : 93.00   Median :73.0       Median :29281  \n                    Mean   : 95.22   Mean   :73.3       Mean   :28655  \n                    3rd Qu.:111.00   3rd Qu.:90.0       3rd Qu.:35078  \n                    Max.   :366.70   Max.   :96.0       Max.   :45192  \n    latitude              geometry        unit_age   \n Min.   :28098   POINT        :47423   Min.   : 3.0  \n 1st Qu.:35670   epsg:3414    :    0   1st Qu.: 9.0  \n Median :38929   +proj=tmer...:    0   Median :26.0  \n Mean   :39069                         Mean   :25.7  \n 3rd Qu.:42401                         3rd Qu.:39.0  \n Max.   :48741                         Max.   :58.0  \n\n\nWe will use tmap to create an iteractive map of the resale prices. Noticebly, the “hotter” resale flats are concentrated in the south and middle area with scatters around the east and west.\n\n\nCode Chunk\ntmap_mode('plot')\ntm_shape(mpsz)+\n  tm_polygons() +\ntm_shape(resale_geom) +  \n  tm_dots(col = \"resale_price\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\ntmap_mode('plot')\ntm_shape(mpsz)+\n  tm_polygons() +\ntm_shape(resale_geom) +  \n  tm_dots(col = \"unit_age\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\n\n\n\n\n\nIn barchart below, we have populated the percentage of transactions stratified by town. Visibly, Punggol attains the highest 7.7% followed by Woodlands at 7.6%. Hence We would want to predict the HDB resale prices for these two towns.\n\n\nCode Chunk\ntown_percentages &lt;- resale_geom %&gt;%\n  group_by(town) %&gt;%\n  summarise(count = n()) %&gt;%\n  mutate(percentage = (count/sum(count))*100) %&gt;%\n  arrange(desc(percentage))  # Sort in descending order\n\n# Create the bar chart\nggplot(town_percentages, aes(x = reorder(town, -percentage), y = percentage)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  geom_text(aes(label = sprintf(\"%.1f%%\", percentage)), \n            vjust = -0.5, \n            size = 3) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(title = \"Percentage Distribution of Resale Transactions by Town\",\n       x = \"Town\",\n       y = \"Percentage of Total Transactions (%)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\ndata &lt;- resale_geom %&gt;%\n  cbind(busstop_resale, eldercare_resale, hawker_resale,\n            kindergarten_resale, mall_resale, mrt_resale,\n            park_resale, school_resale, supermarket_resale)\n\n\nFirstly, we will change the column “month” to POSIXct for easier manipulation.\nAs we are focusing on 4-room HDB flats, we will filter according to that.\nLastly, as we’re working with an sf (spatial features) object, we need to use a different approach to remove columns instead of using dplyr. We will use select(!matches) to remove the column address.\n\n\nCode Chunk\ndata &lt;- data %&gt;%\n  mutate(month = as.POSIXct(paste0(month, \"-01\"), format = \"%Y-%m-%d\")) %&gt;% \n  filter(flat_type == \"4 ROOM\") %&gt;% # Change from int to num\n  select(-c(address, flat_type)) %&gt;% \n  mutate(remaining_lease_yr = as.numeric(remaining_lease_yr)) # Change from int to num\n\n\n\n\n\n\n\nThe entire data are split into training (Jan 2023 - June 2024) and testing (July 2024 - Sept 2024) data sets into by using initial_split() of rsample package. rsample is one of the package of tigymodels.\nIn this section, we will split the data into training data - Jan 2023 to June 2024 & test Data - July 2024 to Sept 2024 that is stratified by Punggol & Woodlands. We can see there are 1549 training records and 300 testing records for Punggol whereas in Woodlands there are 1391 training records and 238 testing records.\n\nPunggolWoodlands\n\n\n\n\nCode Chunk\nset.seed(1234)\ntrain_data_punggol &lt;- data %&gt;%\n  filter(month &gt;= as.POSIXct(\"2023-01-01\") & \n         month &lt;= as.POSIXct(\"2024-06-30\") &\n         town == \"PUNGGOL\") %&gt;% \n  select(-c(month))\n\ntest_data_punggol &lt;- data %&gt;%\n  filter(month &gt;= as.POSIXct(\"2024-07-01\") & \n         month &lt;= as.POSIXct(\"2024-09-30\") &\n         town == \"PUNGGOL\") %&gt;% \n  select(-c(month))\n\n# Optional: to check the filtered data\nprint(paste(\"Number of training records:\", nrow(train_data_punggol)))\n\n\n[1] \"Number of training records: 1549\"\n\n\nCode Chunk\nprint(paste(\"Number of testing records:\", nrow(test_data_punggol)))\n\n\n[1] \"Number of testing records: 300\"\n\n\n\n\nCode Chunk\nwrite_rds(train_data_punggol, \"data/rds/ml/punggol/train_data_punggol.rds\")\nwrite_rds(test_data_punggol, \"data/rds/ml/punggol/test_data_punggol.rds\")\n\n\n\n\n\n\nCode Chunk\nset.seed(1234)\ntrain_data_woodlands &lt;- data %&gt;%\n  filter(month &gt;= as.POSIXct(\"2023-01-01\") & \n         month &lt;= as.POSIXct(\"2024-06-30\") &\n         town == \"WOODLANDS\") %&gt;% \n  select(-c(month))\n\ntest_data_woodlands &lt;- data %&gt;%\n  filter(month &gt;= as.POSIXct(\"2024-07-01\") & \n         month &lt;= as.POSIXct(\"2024-09-30\") &\n         town == \"WOODLANDS\") %&gt;% \n  select(-c(month))\n\n# Optional: to check the filtered data\nprint(paste(\"Number of training records:\", nrow(train_data_woodlands)))\n\n\n[1] \"Number of training records: 1391\"\n\n\nCode Chunk\nprint(paste(\"Number of testing records:\", nrow(test_data_woodlands)))\n\n\n[1] \"Number of testing records: 238\"\n\n\n\n\nCode Chunk\nwrite_rds(train_data_woodlands, \"data/rds/ml/woodlands/train_data_woodlands.rds\")\nwrite_rds(test_data_woodlands, \"data/rds/ml/woodlands/test_data_woodlands.rds\")\n\n\n\n\n\n\n\n\nPrior to loading predictors into predictive model, a correlation matrix will be use to check for sign of multicollinearity. From the correlation matrix below, all correlation values are below 0.8, indicating no sign of multicollinearity.\n\n\nCode Chunk\ndata_nogeo &lt;- data %&gt;%\n   select(-c(month)) %&gt;%\n  st_drop_geometry()\n\n\n\n\nCode Chunk\ncorrplot::corrplot(cor(data_nogeo[, 2:19]), \n                   diag = FALSE, \n                   order = \"AOE\",\n                   tl.pos = \"td\", \n                   tl.cex = 0.5, \n                   method = \"number\", \n                   type = \"upper\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nPunggolWoodlands\n\n\n\n\nCode Chunk\ntrain_data_punggol &lt;- read_rds(\"data/rds/ml/punggol/train_data_punggol.rds\")\ntest_data_punggol &lt;- read_rds(\"data/rds/ml/punggol/test_data_punggol.rds\")\n\n\n\n\n\n\nCode Chunk\ntrain_data_woodlands &lt;- read_rds(\"data/rds/ml/woodlands/train_data_woodlands.rds\")\ntest_data_woodlands &lt;- read_rds(\"data/rds/ml/woodlands/test_data_woodlands.rds\")\n\n\n\n\n\n\n\n\n\n\nPunggolWoodlands\n\n\n\n\nCode Chunk\n# Fit the linear model for resale_price using the specified variables\nprice_mlr_p &lt;- lm(resale_price ~ floor_area_sqm +\n                 unit_age + within_350m_busstop +\n                 within_350m_eldercare + hawker_prox + \n                 within_350m_kindergarten + mall_prox +\n                 mrt_prox + park_prox +\n                 within_1km_school +\n                 supermarket_prox,\n               data = train_data_punggol)\n\n# Summary of the linear model\nsummary(price_mlr_p)\n\n\n\nCall:\nlm(formula = resale_price ~ floor_area_sqm + unit_age + within_350m_busstop + \n    within_350m_eldercare + hawker_prox + within_350m_kindergarten + \n    mall_prox + mrt_prox + park_prox + within_1km_school + supermarket_prox, \n    data = train_data_punggol)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-136613  -26146     -96   26415  140294 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              -2425118.0   181420.5 -13.367  &lt; 2e-16 ***\nfloor_area_sqm              11207.8      669.0  16.754  &lt; 2e-16 ***\nunit_age                    -3655.1      390.9  -9.351  &lt; 2e-16 ***\nwithin_350m_busstop          -601.6      515.7  -1.167 0.243557    \nwithin_350m_eldercare       18135.0     2457.4   7.380 2.59e-13 ***\nhawker_prox                -73597.2     5073.0 -14.508  &lt; 2e-16 ***\nwithin_350m_kindergarten     -788.8     2040.8  -0.387 0.699178    \nmall_prox                   37612.1     3120.5  12.053  &lt; 2e-16 ***\nmrt_prox                   -81583.3    11403.9  -7.154 1.30e-12 ***\npark_prox                   64924.6     7539.8   8.611  &lt; 2e-16 ***\nwithin_1km_school           -5807.9     1023.9  -5.673 1.68e-08 ***\nsupermarket_prox            41305.5    11410.9   3.620 0.000304 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 39990 on 1537 degrees of freedom\nMultiple R-squared:  0.5345,    Adjusted R-squared:  0.5311 \nF-statistic: 160.4 on 11 and 1537 DF,  p-value: &lt; 2.2e-16\n\n\nCode Chunk\nbeep(3)\n\n\n\n\nCode Chunk\nwrite_rds(price_mlr_p, \"data/rds/ml/punggol/price_mlr_p.rds\" ) \n\n\n\n\n\n\nCode Chunk\n# Fit the linear model for resale_price using the specified variables\nprice_mlr_w &lt;- lm(resale_price ~ floor_area_sqm +\n                 unit_age + within_350m_busstop +\n                 within_350m_eldercare + hawker_prox + \n                 within_350m_kindergarten + mall_prox +\n                 mrt_prox + park_prox +\n                 within_1km_school +\n                 supermarket_prox,\n               data = train_data_woodlands)\n\n# Summary of the linear model\nsummary(price_mlr_w)\n\n\n\nCall:\nlm(formula = resale_price ~ floor_area_sqm + unit_age + within_350m_busstop + \n    within_350m_eldercare + hawker_prox + within_350m_kindergarten + \n    mall_prox + mrt_prox + park_prox + within_1km_school + supermarket_prox, \n    data = train_data_woodlands)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-108605  -18479    -861   17651  118328 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              475932.61   72526.66   6.562 7.48e-11 ***\nfloor_area_sqm             2320.91     142.03  16.341  &lt; 2e-16 ***\nunit_age                  -4309.72      84.93 -50.742  &lt; 2e-16 ***\nwithin_350m_busstop         386.65     394.41   0.980   0.3271    \nwithin_350m_eldercare      1333.49    1282.39   1.040   0.2986    \nhawker_prox              -16533.03    3163.02  -5.227 1.99e-07 ***\nwithin_350m_kindergarten   2531.64    1171.15   2.162   0.0308 *  \nmall_prox                 -1004.92    1442.73  -0.697   0.4862    \nmrt_prox                 -44163.93    4902.52  -9.008  &lt; 2e-16 ***\npark_prox                -15524.87    2149.37  -7.223 8.38e-13 ***\nwithin_1km_school           693.69     670.29   1.035   0.3009    \nsupermarket_prox             23.45    5293.65   0.004   0.9965    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 28570 on 1379 degrees of freedom\nMultiple R-squared:  0.748, Adjusted R-squared:  0.746 \nF-statistic: 372.1 on 11 and 1379 DF,  p-value: &lt; 2.2e-16\n\n\nCode Chunk\nbeep(3)\n\n\n\n\nCode Chunk\nwrite_rds(price_mlr_w, \"data/rds/ml/woodlands/price_mlr_w.rds\" ) \n\n\n\n\n\n\n\n\nNow, we will calibrate a model to predict HDB resale price by using geographically weighted regression method of GWmodel package.\n\n\n\nPunggolWoodlands\n\n\n\n\nCode Chunk\ntrain_data_sp_p &lt;- as_Spatial(train_data_punggol)\ntrain_data_sp_p\n\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1549 \nextent      : 34320.85, 37615.35, 41556.12, 43933.95  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 20\nnames       :    town, resale_price, floor_area_sqm, remaining_lease_yr,        longitude,         latitude, unit_age,       busstop_prox, within_350m_busstop,       eldercare_prox, within_350m_eldercare,      hawker_prox,    kindergarten_prox, within_350m_kindergarten,        mall_prox, ... \nmin values  : PUNGGOL,       456000,             85,                 78, 34320.8500594051, 41556.1217510505,        4, 0.0232098182955299,                   2, 9.08133417710502e-07,                     0, 0.23549332350391, 3.08593427233971e-07,                        0, 54.6825044697734, ... \nmax values  : PUNGGOL,        8e+05,             99,                 95, 37615.3527353946, 43933.9470685477,       21,  0.320676170721283,                  14,     1.69953951976499,                     2, 1.98905282800319,    0.804615824046136,                        3, 56.7005204705412, ... \n\n\n\n\n\n\nCode Chunk\ntrain_data_sp_w &lt;- as_Spatial(train_data_woodlands)\ntrain_data_sp_w\n\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1391 \nextent      : 21177.26, 25268.25, 45373.58, 47860.89  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 20\nnames       :      town, resale_price, floor_area_sqm, remaining_lease_yr,        longitude,         latitude, unit_age,       busstop_prox, within_350m_busstop,     eldercare_prox, within_350m_eldercare,        hawker_prox,    kindergarten_prox, within_350m_kindergarten,        mall_prox, ... \nmin values  : WOODLANDS,       350000,             83,                 49, 21177.2576852251, 45373.5757321722,        4, 0.0199595426953728,                   4, 0.0547017433825683,                     0, 0.0666360841718605, 6.84731226937912e-07,                        0, 50.0713376927117, ... \nmax values  : WOODLANDS,       690000,            119,                 95, 25268.2547251521, 47860.8875505569,       50,  0.253082170943171,                  16,   1.73109192014879,                     3,   1.56805500852822,    0.760166885640459,                        4, 53.6923986369607, ... \n\n\n\n\n\n\n\n\nNext, bw.gwr() of GWmodel package will be used to determine the optimal bandwidth to be used.\n\nPunggolWoodlands\n\n\n\n\nCode Chunk\nbw_adaptive_p &lt;- bw.gwr(resale_price ~ floor_area_sqm +\n                 unit_age + within_350m_busstop +\n                 within_350m_eldercare + hawker_prox + \n                 within_350m_kindergarten + mall_prox +\n                 mrt_prox + park_prox +\n                 within_1km_school +\n                 supermarket_prox,\n                  data=train_data_sp_p,\n                  approach=\"CV\",\n                  kernel=\"gaussian\",\n                  adaptive=TRUE,\n                  longlat=FALSE)\n\n\nTake a cup of tea and have a break, it will take a few minutes.\n          -----A kind suggestion from GWmodel development group\nAdaptive bandwidth: 964 CV score: 2.37468e+12 \nAdaptive bandwidth: 604 CV score: 2.273545e+12 \nAdaptive bandwidth: 379 CV score: 2.179535e+12 \nAdaptive bandwidth: 243 CV score: 2.073315e+12 \nAdaptive bandwidth: 155 CV score: 2.011629e+12 \nAdaptive bandwidth: 105 CV score: 1.983025e+12 \nAdaptive bandwidth: 69 CV score: 1.946889e+12 \nAdaptive bandwidth: 52 CV score: 1.916621e+12 \nAdaptive bandwidth: 36 CV score: 1.894903e+12 \nAdaptive bandwidth: 31 CV score: 1.888189e+12 \nAdaptive bandwidth: 23 CV score: 1.901021e+12 \nAdaptive bandwidth: 31 CV score: 1.888189e+12 \n\n\nCode Chunk\nbeep(3)\n\n\nThe result shows that 31 neighbour points will be the optimal bandwidth to be used if adaptive bandwidth is used for this data set.\n\n\n\n\n\n\nNote\n\n\n\nInsights\nThe result shows that 86 neighbour points will be the optimal bandwidth to be used if adaptive bandwidth is used for this data set.\n\n\n\n\nCode Chunk\nwrite_rds(bw_adaptive_p, \"data/rds/ml/punggol/bw_adaptive_p.rds\")\n\n\n\n\n\n\nCode Chunk\nbw_adaptive_w &lt;- bw.gwr(resale_price ~ floor_area_sqm +\n                 unit_age + within_350m_busstop +\n                 within_350m_eldercare + hawker_prox + \n                 within_350m_kindergarten + mall_prox +\n                 mrt_prox + park_prox +\n                 within_1km_school +\n                 supermarket_prox,\n                  data=train_data_sp_w,\n                  approach=\"CV\",\n                  kernel=\"gaussian\",\n                  adaptive=TRUE,\n                  longlat=FALSE)\n\n\nAdaptive bandwidth: 867 CV score: 1.120907e+12 \nAdaptive bandwidth: 544 CV score: 1.112173e+12 \nAdaptive bandwidth: 343 CV score: 1.101484e+12 \nAdaptive bandwidth: 220 CV score: 1.079251e+12 \nAdaptive bandwidth: 142 CV score: 1.057357e+12 \nAdaptive bandwidth: 96 CV score: 1.047183e+12 \nAdaptive bandwidth: 65 CV score: 1.027565e+12 \nAdaptive bandwidth: 48 CV score: 1.015162e+12 \nAdaptive bandwidth: 35 CV score: 1.307044e+12 \nAdaptive bandwidth: 53 CV score: 1.017467e+12 \nAdaptive bandwidth: 41 CV score: 1.303681e+12 \nAdaptive bandwidth: 48 CV score: 1.015162e+12 \n\n\nCode Chunk\nbeep(3)\n\n\n\n\n\n\n\n\nNote\n\n\n\nInsights\nThe results shows that 48 neighbour points will be the optimal bandwidth to be used if adaptive bandwidth is used for this data set.\n\n\n\n\nCode Chunk\nwrite_rds(bw_adaptive_w, \"data/rds/ml/woodlands/bw_adaptive_w.rds\")\n\n\n\n\n\n\n\n\n\n\nCode Chunk\nbw_adaptive_p &lt;- read_rds(\"data/rds/ml/punggol/bw_adaptive_p.rds\")\n\n\n\n\nCode Chunk\nbw_adaptive_w &lt;- read_rds(\"data/rds/ml/woodlands/bw_adaptive_w.rds\")\n\n\nNow, we can go ahead to calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and Gaussian kernel as shown in the code chunk below.\n\nPunggolWoodlands\n\n\n\n\nCode Chunk\ngwr_adaptive_p &lt;- gwr.basic(formula = resale_price ~ floor_area_sqm +\n                 unit_age + within_350m_busstop +\n                 within_350m_eldercare + hawker_prox + \n                 within_350m_kindergarten + mall_prox +\n                 mrt_prox + park_prox +\n                 within_1km_school +\n                 supermarket_prox,\n                  data=train_data_sp_p,\n                          bw=bw_adaptive_p, \n                          kernel = 'gaussian', \n                          adaptive=TRUE,\n                          longlat = FALSE)\nbeep(3)\n\n\n\n\nCode Chunk\nwrite_rds(gwr_adaptive_p, \"data/rds/ml/punggol/gwr_adaptive_p.rds\")\n\n\n\n\n\n\nCode Chunk\ngwr_adaptive_w &lt;- gwr.basic(formula = resale_price ~ floor_area_sqm +\n                 unit_age + within_350m_busstop +\n                 within_350m_eldercare + hawker_prox + \n                 within_350m_kindergarten + mall_prox +\n                 mrt_prox + park_prox +\n                 within_1km_school +\n                 supermarket_prox,\n                  data=train_data_sp_w,\n                          bw=bw_adaptive_w, \n                          kernel = 'gaussian', \n                          adaptive=TRUE,\n                          longlat = FALSE)\nbeep(3)\n\n\n\n\nCode Chunk\nwrite_rds(gwr_adaptive_w, \"data/rds/ml/woodlands/gwr_adaptive_w.rds\")\n\n\n\n\n\n\n\n\nThe code below can be used to display the model output.\n\nPunggolWoodlands\n\n\n\n\nCode Chunk\ngwr_adaptive_p &lt;- read_rds(\"data/rds/ml/punggol/gwr_adaptive_p.rds\")\n\n\n\n\nCode Chunk\ngwr_adaptive_p\n\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-11-11 01:51:55.810254 \n   Call:\n   gwr.basic(formula = resale_price ~ floor_area_sqm + unit_age + \n    within_350m_busstop + within_350m_eldercare + hawker_prox + \n    within_350m_kindergarten + mall_prox + mrt_prox + park_prox + \n    within_1km_school + supermarket_prox, data = train_data_sp_p, \n    bw = bw_adaptive_p, kernel = \"gaussian\", adaptive = TRUE, \n    longlat = FALSE)\n\n   Dependent (y) variable:  resale_price\n   Independent variables:  floor_area_sqm unit_age within_350m_busstop within_350m_eldercare hawker_prox within_350m_kindergarten mall_prox mrt_prox park_prox within_1km_school supermarket_prox\n   Number of data points: 1549\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n    Min      1Q  Median      3Q     Max \n-136613  -26146     -96   26415  140294 \n\n   Coefficients:\n                              Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)              -2425118.0   181420.5 -13.367  &lt; 2e-16 ***\n   floor_area_sqm              11207.8      669.0  16.754  &lt; 2e-16 ***\n   unit_age                    -3655.1      390.9  -9.351  &lt; 2e-16 ***\n   within_350m_busstop          -601.6      515.7  -1.167 0.243557    \n   within_350m_eldercare       18135.0     2457.4   7.380 2.59e-13 ***\n   hawker_prox                -73597.2     5073.0 -14.508  &lt; 2e-16 ***\n   within_350m_kindergarten     -788.8     2040.8  -0.387 0.699178    \n   mall_prox                   37612.1     3120.5  12.053  &lt; 2e-16 ***\n   mrt_prox                   -81583.3    11403.9  -7.154 1.30e-12 ***\n   park_prox                   64924.6     7539.8   8.611  &lt; 2e-16 ***\n   within_1km_school           -5807.9     1023.9  -5.673 1.68e-08 ***\n   supermarket_prox            41305.5    11410.9   3.620 0.000304 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 39990 on 1537 degrees of freedom\n   Multiple R-squared: 0.5345\n   Adjusted R-squared: 0.5311 \n   F-statistic: 160.4 on 11 and 1537 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 2.457709e+12\n   Sigma(hat): 39858.42\n   AIC:  37237.26\n   AICc:  37237.5\n   BIC:  35853.24\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 31 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                                   Min.     1st Qu.      Median     3rd Qu.\n   Intercept                -3.6026e+08 -1.8903e+07 -5.2468e+06  3.2394e+06\n   floor_area_sqm           -9.3131e+05  4.9672e+03  9.1804e+03  1.6247e+04\n   unit_age                 -7.2954e+03 -2.3589e+03  9.9553e+03  1.8568e+04\n   within_350m_busstop      -2.1840e+04 -3.5978e+03  7.9328e+02  3.5622e+03\n   within_350m_eldercare    -5.0557e+05 -2.9912e+03  1.7167e+04  5.6494e+04\n   hawker_prox              -1.0870e+07 -3.7718e+05 -8.1130e+04  1.2870e+05\n   within_350m_kindergarten -8.6012e+05 -2.8299e+04 -7.3968e+03  1.1648e+04\n   mall_prox                -6.1324e+06 -5.6280e+04  8.8829e+04  3.3087e+05\n   mrt_prox                 -1.5787e+06 -1.4296e+05 -9.5298e+03  2.0465e+05\n   park_prox                -8.1186e+06 -1.7270e+05  1.5589e+04  3.1357e+05\n   within_1km_school        -1.1002e+05 -7.9972e+03  4.7641e+02  8.1719e+03\n   supermarket_prox         -3.4463e+06 -1.1782e+05 -4.4088e+04  5.4404e+04\n                                 Max.\n   Intercept                344376485\n   floor_area_sqm              163021\n   unit_age                     45035\n   within_350m_busstop          20457\n   within_350m_eldercare      3933539\n   hawker_prox                1825411\n   within_350m_kindergarten    323139\n   mall_prox                  6261469\n   mrt_prox                   8775702\n   park_prox                  6609422\n   within_1km_school           119709\n   supermarket_prox          11102587\n   ************************Diagnostic information*************************\n   Number of data points: 1549 \n   Effective number of parameters (2trace(S) - trace(S'S)): 216.9356 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 1332.064 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 36831.82 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 36605.4 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 36183.21 \n   Residual sum of squares: 1.482095e+12 \n   R-square value:  0.7192638 \n   Adjusted R-square value:  0.6735097 \n\n   ***********************************************************************\n   Program stops at: 2024-11-11 01:51:57.180324 \n\n\n\n\n\n\n\n\nNote\n\n\n\nInsights of 2 Regression Analysis in predicting resale price - Punggol\n\nGlobal Regression\n\nResiduals: The residuals indicate the difference between the observed and predicted resale prices. The residuals range from -144,029 to 135,047, with a median of -1,271, suggesting that the model may not perfectly fit all data points, particularly those with extreme values.\nCoefficients: The estimated coefficients for each predictor variable indicate their respective impact on resale price.\n\nSignificant Variables: Variables like floor_area_sqm, unit_age, and hawker_prox are statistically significant with p-values less than 0.001, suggesting strong relationships with the resale price.\nNon-significant Variables: within_350m_busstop did not show significant effects on resale price, with p-values greater than 0.05.\n\nModel Fit: The global regression model explains about 51.57% of the variance in resale price (R-squared = 0.5157). The Adjusted R-squared is 0.5123, showing a modest fit after adjusting for the number of predictors. The F-statistic of 148.8, with a p-value less than 2.2e-16, indicates that the overall model is highly significant.\nAIC/BIC: The Akaike Information Criterion (AIC = 37,298.31) and the Bayesian Information Criterion (BIC = 35,914.29) suggest that the global model provides a reasonable fit, although these values are used for model comparison rather than absolute goodness-of-fit assessment.\n\nGeographically Weighted Regression\n\nThis model was calibrated using the adaptive Gaussian kernel with an adaptive bandwidth of 31, meaning the model uses data from the 31 nearest neighbors to each observation.\n\nModel Calibration: The GWR model was fitted using the same set of predictor variables as the global regression model. The use of an adaptive kernel and Euclidean distance metric enables the model to account for spatial heterogeneity in the relationships between predictors and resale price.\nCoefficient Estimates:\n\nThe GWR model shows significant variation in the coefficient estimates across geographic locations. For instance:\n\nThe coefficient for floor_area_sqm ranges from negative to positive values, indicating that the impact of floor area on resale price varies spatially.\nThe coefficient for unit_age shows a similar spatial variation, with the impact differing from location to location.\nhawker_prox and supermarket_prox also exhibit varying effects on resale price across locations.\n\n\nModel Fit:\nR-squared: The GWR model explains approximately 71.98% of the variance in resale price (R-squared = 0.7198), which is a substantial improvement over the global regression model. The Adjusted R-squared is 0.6743, indicating that the model explains a significant portion of the variance while accounting for the number of predictors.\nAIC/BIC: The GWR model shows lower AIC (36,602.13) and BIC (36,179.57) values than the global regression model, suggesting that the GWR model provides a better fit to the data when accounting for spatial variability.\n\nOverall\nSpatially varying coefficients highlight that the impact of certain factors, such as floor_area_sqm, unit_age, and proximity to amenities like hawker centers and supermarkets, can differ significantly across different locations, suggesting that location-specific interventions or policies might be more effective in real estate pricing.\n\n\n\n\n\n\nCode Chunk\ngwr_adaptive_w &lt;- read_rds(\"data/rds/ml/woodlands/gwr_adaptive_w.rds\")\n\n\n\n\nCode Chunk\ngwr_adaptive_w\n\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-11-11 01:51:57.292957 \n   Call:\n   gwr.basic(formula = resale_price ~ floor_area_sqm + unit_age + \n    within_350m_busstop + within_350m_eldercare + hawker_prox + \n    within_350m_kindergarten + mall_prox + mrt_prox + park_prox + \n    within_1km_school + supermarket_prox, data = train_data_sp_w, \n    bw = bw_adaptive_w, kernel = \"gaussian\", adaptive = TRUE, \n    longlat = FALSE)\n\n   Dependent (y) variable:  resale_price\n   Independent variables:  floor_area_sqm unit_age within_350m_busstop within_350m_eldercare hawker_prox within_350m_kindergarten mall_prox mrt_prox park_prox within_1km_school supermarket_prox\n   Number of data points: 1391\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n    Min      1Q  Median      3Q     Max \n-108605  -18479    -861   17651  118328 \n\n   Coefficients:\n                             Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)              475932.61   72526.66   6.562 7.48e-11 ***\n   floor_area_sqm             2320.91     142.03  16.341  &lt; 2e-16 ***\n   unit_age                  -4309.72      84.93 -50.742  &lt; 2e-16 ***\n   within_350m_busstop         386.65     394.41   0.980   0.3271    \n   within_350m_eldercare      1333.49    1282.39   1.040   0.2986    \n   hawker_prox              -16533.03    3163.02  -5.227 1.99e-07 ***\n   within_350m_kindergarten   2531.64    1171.15   2.162   0.0308 *  \n   mall_prox                 -1004.92    1442.73  -0.697   0.4862    \n   mrt_prox                 -44163.93    4902.52  -9.008  &lt; 2e-16 ***\n   park_prox                -15524.87    2149.37  -7.223 8.38e-13 ***\n   within_1km_school           693.69     670.29   1.035   0.3009    \n   supermarket_prox             23.45    5293.65   0.004   0.9965    \n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 28570 on 1379 degrees of freedom\n   Multiple R-squared: 0.748\n   Adjusted R-squared: 0.746 \n   F-statistic: 372.1 on 11 and 1379 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 1.125708e+12\n   Sigma(hat): 28468.32\n   AIC:  32505.2\n   AICc:  32505.46\n   BIC:  31276.38\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 48 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                                   Min.     1st Qu.      Median     3rd Qu.\n   Intercept                -1.1312e+09 -1.5632e+06  3.9476e+05  1.9149e+06\n   floor_area_sqm           -1.5228e+04  9.8373e+02  2.1764e+03  2.7568e+03\n   unit_age                 -5.2038e+03 -4.5082e+03 -3.9677e+03 -2.2334e+03\n   within_350m_busstop      -5.5262e+03 -1.7354e+03 -5.3815e+02  1.2150e+03\n   within_350m_eldercare    -4.4071e+05 -1.9180e+04 -5.4921e+03  3.5630e+02\n   hawker_prox              -3.5823e+05 -3.8041e+04 -6.8955e+03  4.1019e+04\n   within_350m_kindergarten -3.9285e+04 -4.2950e+03  6.3734e+02  6.3378e+03\n   mall_prox                -2.4362e+05 -2.6020e+04  1.5904e+03  4.0938e+04\n   mrt_prox                 -6.7648e+05 -6.8419e+04 -1.5806e+04  2.0814e+04\n   park_prox                -3.5428e+05 -6.1010e+04 -2.2539e+04  2.6743e+03\n   within_1km_school        -5.6562e+05 -9.8708e+03 -5.0483e+03 -2.4167e+03\n   supermarket_prox         -5.4534e+05 -6.9462e+04 -3.9335e+04 -2.4152e+03\n                                  Max.\n   Intercept                13797986.0\n   floor_area_sqm              18382.0\n   unit_age                    13828.7\n   within_350m_busstop          4947.6\n   within_350m_eldercare     1085786.9\n   hawker_prox              22332689.8\n   within_350m_kindergarten   129965.6\n   mall_prox                21420842.0\n   mrt_prox                  3950844.9\n   park_prox                  787950.9\n   within_1km_school            7588.5\n   supermarket_prox            94873.0\n   ************************Diagnostic information*************************\n   Number of data points: 1391 \n   Effective number of parameters (2trace(S) - trace(S'S)): 131.5628 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 1259.437 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 32347.02 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 32221.73 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 31488.57 \n   Residual sum of squares: 867184876654 \n   R-square value:  0.8058492 \n   Adjusted R-square value:  0.7855518 \n\n   ***********************************************************************\n   Program stops at: 2024-11-11 01:51:58.372047 \n\n\n\n\n\n\n\n\nNote\n\n\n\nInsights of 2 Regression Analysis in predicting resale price - Woodlands\n\nGlobal Regression\n\n\nResiduals: The residuals range from -110,060 to 118,690, with a median of -926. This shows that the model can produce both underestimations and overestimations of resale price, with some extreme residuals indicating potential outliers.\nCoefficients:\n\nIntercept: The base resale price when all predictors are zero is estimated at 435,172.15.\nSignificant Variables:\n\nfloor_area_sqm: Each additional square meter increases the resale price by 2,247.33.\nunit_age: Each additional year of unit age decreases the resale price by 4,322.06.\nhawker_prox: Proximity to hawker centers decreases resale price by -14,811.35.\nmrt_prox: Proximity to MRT stations has a significant negative effect of -45,257.65 on resale price.\npark_prox: Proximity to parks also significantly decreases resale price by -15,817.05.\nwithin_350m_kindergarten: The presence of a kindergarten within 350 meters increases resale price by 3,083.36.\n\n\nModel Fit:\n\nR-squared: The model explains about 74.83% of the variance in resale price, indicating a strong overall fit.\nAdjusted R-squared: 74.63%, which adjusts for the number of predictors.\nF-statistic: The F-statistic is 372.7, with a p-value of less than 2.2e-16, indicating that the model is highly significant.\n\nAIC/BIC:\n\nAIC: 32,503.3, and BIC: 31,274.49, which provide a basis for model comparison and indicate that the global model fits the data reasonably well.\n\n\n\nGeographical Weighted Regression\n\n\nModel Calibration:\n\nAdaptive Bandwidth: 48 neighbors were used in the adaptive bandwidth, ensuring that the local influence of each observation is based on the proximity of other observations.\nKernel Function: A Gaussian kernel was used, which gives more weight to observations closer to each location.\n\nCoefficient Estimates: The coefficients for the predictors vary significantly across geographic locations:\n\nfloor_area_sqm: The effect of floor area on resale price ranges from negative to positive values. In some areas, additional floor space leads to a large increase in resale price, while in others, the impact is smaller.\nunit_age: The effect of unit age varies spatially, with some areas showing a significant negative impact on resale price, while in others, the impact is less pronounced.\nhawker_prox: The proximity to hawker centers shows a negative relationship with resale price in some regions, with a large range in coefficient estimates, suggesting that the effect of hawker centers on property prices varies significantly by location.\nmrt_prox: Proximity to MRT stations shows a negative effect in certain locations, with varying strength across different regions in Woodlands.\npark_prox: Proximity to parks also varies across locations, with some areas showing a stronger negative impact on resale price than others.\n\nModel Fit:\n\nR-squared: The GWR model explains 80.58% of the variance in resale price, which represents a significant improvement over the global model (74.83%). This indicates that incorporating spatial variability greatly enhances model accuracy.\nAdjusted R-squared: 78.52%, further indicating that the GWR model accounts for a significant amount of variance while adjusting for the number of predictors.\n\nAIC/BIC:\n\nAIC: 32,223.42 and BIC: 31,497.62, which are lower than the values for the global model, indicating that the GWR model provides a better fit for the data when accounting for spatial differences.\n\nResidual Sum of Squares (RSS): The GWR model has a residual sum of squares of 8.67507e+11, suggesting that the model has reduced unexplained variance compared to the global model.\n\nOverall\nThe GWR model provides a much better fit for the resale price data in Woodlands compared to the global regression model. The improvement in R-squared (from 74.83% to 80.58%) suggests that there are significant spatial differences in the factors affecting resale prices.\n\nFloor Area: The positive relationship between floor area and resale price varies significantly across Woodlands. In some areas, floor area is a strong predictor of resale price, while in others, it has a smaller effect.\nUnit Age: The negative effect of unit age on resale price is more pronounced in some parts of Woodlands, indicating that older units in certain locations have a larger impact on resale price.\nProximity to Amenities: Variables such as proximity to MRT stations, parks, and hawker centers show varying effects across locations. For example, properties near MRT stations tend to have lower resale prices in some regions, while in others, the proximity to parks and kindergartens can increase prices.\n\n\n\n\n\n\n\n\n\n\nPunggolWoodlands\n\n\n\n\nCode Chunk\ntest_data_sp_p &lt;- as_Spatial(test_data_punggol)\ntest_data_sp_p\n\n\nclass       : SpatialPointsDataFrame \nfeatures    : 300 \nextent      : 34320.85, 37549.49, 41631.21, 43933.95  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 20\nnames       :    town, resale_price, floor_area_sqm, remaining_lease_yr,        longitude,         latitude, unit_age,       busstop_prox, within_350m_busstop,       eldercare_prox, within_350m_eldercare,      hawker_prox,   kindergarten_prox, within_350m_kindergarten,        mall_prox, ... \nmin values  : PUNGGOL,       495000,             85,                 77, 34320.8500594051, 41631.2121389891,        5, 0.0287942812318591,                   2, 9.08133417710502e-07,                     0, 0.28631929470226, 5.4646933332265e-07,                        0, 54.6825044697734, ... \nmax values  : PUNGGOL,       788000,             99,                 94, 37549.4930230187, 43933.9470685477,       22,  0.286960879696002,                  14,     1.69953951976499,                     2, 1.94955185447985,   0.764704244775304,                        3, 56.6972972869625, ... \n\n\n\n\n\n\nCode Chunk\ntest_data_sp_w &lt;- as_Spatial(test_data_woodlands)\ntest_data_sp_w\n\n\nclass       : SpatialPointsDataFrame \nfeatures    : 238 \nextent      : 21177.26, 25186.96, 45373.58, 47860.89  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 20\nnames       :      town, resale_price, floor_area_sqm, remaining_lease_yr,        longitude,         latitude, unit_age,       busstop_prox, within_350m_busstop,     eldercare_prox, within_350m_eldercare,        hawker_prox,  kindergarten_prox, within_350m_kindergarten,        mall_prox, ... \nmin values  : WOODLANDS,       355000,             83,                 48, 21177.2576852251, 45373.5757321722,        5, 0.0205869708371814,                   4, 0.0656405943898966,                     0, 0.0666360841718605, 8.502567959163e-07,                        0, 50.0713376927117, ... \nmax values  : WOODLANDS,       701000,            111,                 94, 25186.9617010174, 47860.8875505569,       51,  0.253082170943171,                  16,   1.65460210369017,                     3,    1.5143514513644,  0.798763870107565,                        4, 53.6923986369607, ... \n\n\n\n\n\n\n\n\n\nPunggolWoodlands\n\n\n\n\nCode Chunk\ngwr_bw_test_adaptive_p &lt;- bw.gwr(resale_price ~ floor_area_sqm +\n                 unit_age + within_350m_busstop +\n                 within_350m_eldercare + hawker_prox + \n                 within_350m_kindergarten + mall_prox +\n                 mrt_prox + park_prox +\n                 within_1km_school +\n                 supermarket_prox,\n                  data=test_data_sp_p,\n                  approach=\"CV\",\n                  kernel=\"gaussian\",\n                  adaptive=TRUE,\n                  longlat=FALSE)\n\n\nAdaptive bandwidth: 193 CV score: 461863083474 \nAdaptive bandwidth: 127 CV score: 444482582747 \nAdaptive bandwidth: 86 CV score: 429860319803 \nAdaptive bandwidth: 60 CV score: 4.15545e+11 \nAdaptive bandwidth: 45 CV score: 405403507470 \nAdaptive bandwidth: 34 CV score: 406000083766 \nAdaptive bandwidth: 50 CV score: 408265912967 \nAdaptive bandwidth: 40 CV score: 406022699918 \nAdaptive bandwidth: 46 CV score: 405735445126 \nAdaptive bandwidth: 42 CV score: 406270763040 \nAdaptive bandwidth: 44 CV score: 405653093262 \nAdaptive bandwidth: 42 CV score: 406270763040 \nAdaptive bandwidth: 43 CV score: 4.05838e+11 \nAdaptive bandwidth: 42 CV score: 406270763040 \nAdaptive bandwidth: 42 CV score: 406270763040 \nAdaptive bandwidth: 41 CV score: 405877056522 \nAdaptive bandwidth: 41 CV score: 405877056522 \nAdaptive bandwidth: 40 CV score: 406022699918 \nAdaptive bandwidth: 40 CV score: 406022699918 \nAdaptive bandwidth: 39 CV score: 406410298495 \nAdaptive bandwidth: 39 CV score: 406410298495 \nAdaptive bandwidth: 38 CV score: 406480426659 \nAdaptive bandwidth: 38 CV score: 406480426659 \nAdaptive bandwidth: 37 CV score: 405643842893 \nAdaptive bandwidth: 37 CV score: 405643842893 \nAdaptive bandwidth: 36 CV score: 405801331672 \nAdaptive bandwidth: 36 CV score: 405801331672 \nAdaptive bandwidth: 35 CV score: 405721054702 \nAdaptive bandwidth: 35 CV score: 405721054702 \nAdaptive bandwidth: 34 CV score: 406000083766 \nAdaptive bandwidth: 34 CV score: 406000083766 \nAdaptive bandwidth: 33 CV score: 405311774904 \n\n\nCode Chunk\nbeep(3)\n\n\n\n\nCode Chunk\nwrite_rds(gwr_bw_test_adaptive_p, \"data/rds/ml/punggol/gwr_bw_test_adaptive_p.rds\")\n\n\n\n\n\n\nCode Chunk\ngwr_bw_test_adaptive_w&lt;- bw.gwr(resale_price ~ floor_area_sqm +\n                 unit_age + within_350m_busstop +\n                 within_350m_eldercare + hawker_prox + \n                 within_350m_kindergarten + mall_prox +\n                 mrt_prox + park_prox +\n                 within_1km_school +\n                 supermarket_prox,\n                  data=test_data_sp_w,\n                  approach=\"CV\",\n                  kernel=\"gaussian\",\n                  adaptive=TRUE,\n                  longlat=FALSE)\n\n\nAdaptive bandwidth: 154 CV score: 141474677352 \nAdaptive bandwidth: 103 CV score: 140090697462 \nAdaptive bandwidth: 70 CV score: 138907367486 \nAdaptive bandwidth: 51 CV score: 137432823055 \nAdaptive bandwidth: 38 CV score: 135519123552 \nAdaptive bandwidth: 31 CV score: 134512516189 \nAdaptive bandwidth: 25 CV score: 134598796510 \nAdaptive bandwidth: 33 CV score: 1.34745e+11 \nAdaptive bandwidth: 28 CV score: 135016725804 \nAdaptive bandwidth: 31 CV score: 134512516189 \n\n\nCode Chunk\nbeep(3)\n\n\n\n\nCode Chunk\nwrite_rds(gwr_bw_test_adaptive_w, \"data/rds/ml/woodlands/gwr_bw_test_adaptive_w.rds\")\n\n\n\n\n\n\n\n\n\nPunggolWoodlands\n\n\n\n\nCode Chunk\ngwr_pred_p &lt;- gwr.predict(formula = resale_price ~ floor_area_sqm +\n                 unit_age + within_350m_busstop +\n                 within_350m_eldercare + hawker_prox + \n                 within_350m_kindergarten + mall_prox +\n                 mrt_prox + park_prox +\n                 within_1km_school +\n                 supermarket_prox, \n                        data=train_data_sp_p, \n                        predictdata = test_data_sp_p, \n                        bw=100, \n                        kernel = 'gaussian', \n                        adaptive=TRUE, \n                        longlat = FALSE)\n\nbeep(3)\n\n\n\n\nCode Chunk\ngwr_pred_p\n\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-11-11 01:51:59.702211 \n   Call:\n   gwr.predict(formula = resale_price ~ floor_area_sqm + unit_age + \n    within_350m_busstop + within_350m_eldercare + hawker_prox + \n    within_350m_kindergarten + mall_prox + mrt_prox + park_prox + \n    within_1km_school + supermarket_prox, data = train_data_sp_p, \n    predictdata = test_data_sp_p, bw = 100, kernel = \"gaussian\", \n    adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable for prediction:  resale_price\n   Independent variables:  floor_area_sqm unit_age within_350m_busstop within_350m_eldercare hawker_prox within_350m_kindergarten mall_prox mrt_prox park_prox within_1km_school supermarket_prox\n   Number of data points: 1549\n   ***********************************************************************\n   *     Results of Geographically Weighted Regression for prediction    *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 100 (number of nearest neighbours)\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                                        Min.     1st Qu.      Median\n   Intercept_coef                -1.1137e+07 -4.0950e+06 -1.8008e+06\n   floor_area_sqm_coef            6.5380e+02  7.4184e+03  9.3916e+03\n   unit_age_coef                 -7.4305e+03 -3.9280e+03 -1.7500e+03\n   within_350m_busstop_coef      -5.3279e+03 -2.1329e+03 -5.4695e+02\n   within_350m_eldercare_coef    -3.0346e+04  2.0333e+03  1.8049e+04\n   hawker_prox_coef              -4.4176e+05 -2.1443e+05 -7.7957e+04\n   within_350m_kindergarten_coef -2.4241e+04 -1.6656e+04 -8.7511e+03\n   mall_prox_coef                -1.9105e+05  9.2298e+03  2.5243e+04\n   mrt_prox_coef                 -1.5919e+05 -8.3799e+04 -5.2551e+04\n   park_prox_coef                -9.6160e+04  1.4933e+04  5.4488e+04\n   within_1km_school_coef        -1.1990e+04 -6.1249e+03 -3.9312e+03\n   supermarket_prox_coef         -1.2277e+05 -5.0793e+04 -2.8030e+04\n                                     3rd Qu.       Max.\n   Intercept_coef                -5.5879e+05 10743979.3\n   floor_area_sqm_coef            1.2740e+04    19011.5\n   unit_age_coef                  4.2753e+03    18101.9\n   within_350m_busstop_coef       1.1316e+03     3852.9\n   within_350m_eldercare_coef     2.8801e+04    65407.6\n   hawker_prox_coef              -1.8368e+04   113086.4\n   within_350m_kindergarten_coef -3.7455e+02    15082.2\n   mall_prox_coef                 6.2916e+04   200516.3\n   mrt_prox_coef                 -1.7414e+04   165209.6\n   park_prox_coef                 1.3493e+05   365020.5\n   within_1km_school_coef        -1.1988e+03     7789.1\n   supermarket_prox_coef          3.2259e+04   117388.1\n\n   ****************       Results of GW prediction       ******************\n                        Min.    1st Qu.     Median    3rd Qu.       Max.\n   prediction         459054     588680     609707     646438     708324\n   prediction_var 1249829990 1264888497 1274282131 1284856043 1412261610\n\n   ***********************************************************************\n   Program stops at: 2024-11-11 01:52:47.242338 \n\n\n\n\nCode Chunk\nwrite_rds(gwr_pred_p, \"data/rds/ml/punggol/gwr_pred_p.rds\")\n\n\n\n\n\n\n\n\nNote\n\n\n\nGWR Coefficient Estimates\nInterpretation:\n\nThe Intercept varies greatly across locations, ranging from a negative coefficient (-1,441,251.41) to a positive coefficient (786,268.8), indicating that baseline resale prices are influenced differently across locations.\nfloor_area_sqm: The effect of floor area on resale price ranges from negative to positive. In some locations, a larger floor area has a smaller impact, while in others it has a much larger effect.\nunit_age: The negative relationship between unit age and resale price is more pronounced in some areas, suggesting that older units may have a larger negative impact on property values in certain locations.\nProximity to amenities: Variables such as hawker_prox, mrt_prox, park_prox, and supermarket_prox show a wide range of coefficients. For example, proximity to hawker centers has a strong negative impact in some regions, while proximity to parks and supermarkets has a positive impact in others.\n\nPrediction Results\nInterpretation:\n\nThe predicted resale prices range from $463,945 to $707,938, with a median prediction of $609,348. These values are within a reasonable range given the data and suggest a good predictive performance.\nThe prediction variance shows a substantial range, from about 1.25 billion to 1.4 billion, indicating that the predicted resale prices vary considerably. This is expected, as some locations may have more variability in resale price due to factors such as proximity to amenities and the overall market conditions.\n\nOverall\nKey findings include:\n\nThe effect of floor area, unit age, and proximity to amenities varies significantly across different locations in Woodlands.\nThe prediction results show a reasonable range of predicted resale prices, with substantial variance indicating the sensitivity of prices to local factors.\n\n\n\n\n\n\n\nCode Chunk\ngwr_pred_w &lt;- gwr.predict(formula = resale_price ~ floor_area_sqm +\n                 unit_age + within_350m_busstop +\n                 within_350m_eldercare + hawker_prox + \n                 within_350m_kindergarten + mall_prox +\n                 mrt_prox + park_prox +\n                 within_1km_school +\n                 supermarket_prox, \n                        data=train_data_sp_w, \n                        predictdata = test_data_sp_w, \n                        bw=100, \n                        kernel = 'gaussian', \n                        adaptive=TRUE, \n                        longlat = FALSE)\n\nbeep(3)\n\n\n\n\nCode Chunk\ngwr_pred_w\n\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-11-11 01:52:47.294196 \n   Call:\n   gwr.predict(formula = resale_price ~ floor_area_sqm + unit_age + \n    within_350m_busstop + within_350m_eldercare + hawker_prox + \n    within_350m_kindergarten + mall_prox + mrt_prox + park_prox + \n    within_1km_school + supermarket_prox, data = train_data_sp_w, \n    predictdata = test_data_sp_w, bw = 100, kernel = \"gaussian\", \n    adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable for prediction:  resale_price\n   Independent variables:  floor_area_sqm unit_age within_350m_busstop within_350m_eldercare hawker_prox within_350m_kindergarten mall_prox mrt_prox park_prox within_1km_school supermarket_prox\n   Number of data points: 1391\n   ***********************************************************************\n   *     Results of Geographically Weighted Regression for prediction    *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 100 (number of nearest neighbours)\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                                        Min.     1st Qu.      Median\n   Intercept_coef                -2.1902e+06 -9.3788e+04  6.7361e+05\n   floor_area_sqm_coef           -3.3112e+03  1.8210e+03  2.2761e+03\n   unit_age_coef                 -4.9718e+03 -4.5761e+03 -4.3469e+03\n   within_350m_busstop_coef      -2.2179e+03 -6.0128e+02 -9.4503e+01\n   within_350m_eldercare_coef    -4.5128e+04 -9.5403e+03 -5.8673e+03\n   hawker_prox_coef              -2.2862e+05 -2.6585e+04 -1.4338e+04\n   within_350m_kindergarten_coef -9.6328e+03 -2.6297e+03 -4.1932e+01\n   mall_prox_coef                -4.8135e+04 -1.0964e+04 -3.7845e+03\n   mrt_prox_coef                 -1.7215e+05 -4.2204e+04 -2.1621e+04\n   park_prox_coef                -1.3435e+05 -4.0264e+04 -2.7108e+04\n   within_1km_school_coef        -9.4865e+03 -5.4721e+03 -3.3587e+03\n   supermarket_prox_coef         -9.7199e+04 -4.7539e+04 -3.3118e+04\n                                     3rd Qu.      Max.\n   Intercept_coef                 1.1117e+06 2976576.6\n   floor_area_sqm_coef            2.5522e+03    3019.2\n   unit_age_coef                 -3.7747e+03   -1769.4\n   within_350m_busstop_coef       7.3075e+02    3301.4\n   within_350m_eldercare_coef    -1.4672e+03    4353.7\n   hawker_prox_coef               3.0435e+03  100863.4\n   within_350m_kindergarten_coef  3.8679e+03   24965.7\n   mall_prox_coef                 1.0691e+04   55502.5\n   mrt_prox_coef                 -7.2434e+03  148627.2\n   park_prox_coef                -1.3584e+04   94005.1\n   within_1km_school_coef        -2.7247e+02    2829.4\n   supermarket_prox_coef         -1.0807e+04   28141.1\n\n   ****************       Results of GW prediction       ******************\n                       Min.   1st Qu.    Median   3rd Qu.      Max.\n   prediction        354574    475267    494160    528380    617913\n   prediction_var 737877010 745713670 752120804 759546466 799428816\n\n   ***********************************************************************\n   Program stops at: 2024-11-11 01:53:19.356233 \n\n\n\n\nCode Chunk\nwrite_rds(gwr_pred_w, \"data/rds/ml/woodlands/gwr_pred_w.rds\")\n\n\n\n\n\n\n\n\nNote\n\n\n\nSummary of GWR Coefficients\nInterpretation:\n\nIntercept: The baseline resale price in Woodlands varies significantly across locations, with a range from 373,726.15 to 929,191.50. This suggests that the average resale price can be strongly influenced by local spatial factors.\nFloor Area: The effect of floor area on resale price varies from a negative to a positive coefficient. In some locations, additional floor area has a relatively small positive impact, while in other locations, the effect is stronger.\nUnit Age: The negative impact of unit age on resale price is consistent across locations. Older units tend to have a more significant negative effect on resale prices, particularly in areas where newer properties dominate.\nProximity to Amenities:\n\nBus Stops: Proximity to bus stops generally has a small positive effect in certain areas (range from -1,933.97 to 3,494.30).\nElder Care Centers: The proximity to eldercare centers has a more varied effect, with the coefficient ranging from negative to positive. In some regions, this proximity significantly increases resale prices.\nHawker Centers: The proximity to hawker centers shows a highly negative impact in many areas, especially in places where hawker centers are abundant and could be seen as less desirable.\nKindergartens: Proximity to kindergartens has a positive effect on property prices in certain locations, especially in family-oriented neighborhoods.\nMalls, MRT, Parks: Proximity to malls and MRT stations tends to have a negative impact on resale prices in specific areas, while proximity to parks shows a more varied effect, with some areas benefiting from this proximity more than others.\nSupermarkets: Proximity to supermarkets has a small negative effect on resale prices in some areas but is generally less influential compared to other amenities.\n\n\nPrediction Results\nInterpretation:\n\nThe predicted resale prices range from $359,170 to $616,562, with a median prediction of $493,786. This shows that the resale price in Woodlands varies significantly across locations.\nThe prediction variance is substantial, ranging from 735 million to 792 million. This suggests considerable variability in the predicted prices, likely due to differing local factors influencing property values across different areas of Woodlands.\n\nOverall\nThe Geographically Weighted Regression (GWR) model provides a more accurate prediction of resale prices in Woodlands compared to traditional global models. By accounting for spatial variability, the GWR model captures how the relationship between property features and resale price changes across different locations.\nKey findings:\n\nFloor area and unit age have spatially varying impacts, with some regions showing stronger relationships between these variables and resale prices.\nProximity to amenities such as hawker centers, MRT stations, and supermarkets shows spatial heterogeneity, with some areas benefiting more from proximity to these amenities than others.\nThe prediction results indicate that resale prices vary widely across Woodlands, with certain regions experiencing higher values based on local factors like proximity to key amenities.\n\n\n\n\n\n\n\n\n\n\n\n\n\nPunggolWoodlands\n\n\n\n\nCode Chunk\ncoords_train_p &lt;- st_coordinates(train_data_punggol)\ncoords_test_p &lt;- st_coordinates(test_data_punggol)\n\n\nBefore continue, we write all the output into rds for future used.\n\n\nCode Chunk\ncoords_train_p &lt;- write_rds(coords_train_p, \"data/rds/ml/punggol/coords_train_p.rds\" )\ncoords_test_p &lt;- write_rds(coords_test_p, \"data/rds/ml/punggol/coords_test_p.rds\" )\n\n\n\n\n\n\nCode Chunk\ncoords_train_w &lt;- st_coordinates(train_data_woodlands)\ncoords_test_w &lt;- st_coordinates(test_data_woodlands)\n\n\nBefore continue, we write all the output into rds for future used.\n\n\nCode Chunk\ncoords_train_w &lt;- write_rds(coords_train_w, \"data/rds/ml/coords_train_w.rds\" )\ncoords_test_w &lt;- write_rds(coords_test_w, \"data/rds/ml/coords_test_w.rds\" )\n\n\n\n\n\n\n\n\nFirst, we will drop geometry column of the sf data.frame by using st_drop_geometry() of sf package.\n\nPunggolWoodlands\n\n\n\n\nCode Chunk\ntrain_data_punggol &lt;- train_data_punggol %&gt;% \n  st_drop_geometry() %&gt;% \n  select(-town)\n\n\n\n\n\n\nCode Chunk\ntrain_data_woodlands &lt;- train_data_woodlands %&gt;% \n  st_drop_geometry() %&gt;% \n  select(-town)\n\n\n\n\n\n\n\n\n\nIn this section, you will learn how to calibrate a model to predict HDB resale price by using random forest function of ranger package.\n\nPunggolWoodlands\n\n\n\n\nCode Chunk\nset.seed(1234)\nrf_p &lt;- ranger(resale_price ~ floor_area_sqm +\n                 unit_age + within_350m_busstop +\n                 within_350m_eldercare + hawker_prox + \n                 within_350m_kindergarten + mall_prox +\n                 mrt_prox + park_prox +\n                 within_1km_school +\n                 supermarket_prox,\n             data=train_data_punggol)\nrf_p\n\n\nRanger result\n\nCall:\n ranger(resale_price ~ floor_area_sqm + unit_age + within_350m_busstop +      within_350m_eldercare + hawker_prox + within_350m_kindergarten +      mall_prox + mrt_prox + park_prox + within_1km_school + supermarket_prox,      data = train_data_punggol) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      1549 \nNumber of independent variables:  11 \nMtry:                             3 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       1285768219 \nR squared (OOB):                  0.6229873 \n\n\n\n\nCode Chunk\nwrite_rds(rf_p, \"data/rds/ml/punggol/rf_p.rds\")\n\n\n\n\nCode Chunk\nrf_p &lt;- read_rds(\"data/rds/ml/punggol/rf_p.rds\")\nrf_p\n\n\nRanger result\n\nCall:\n ranger(resale_price ~ floor_area_sqm + unit_age + within_350m_busstop +      within_350m_eldercare + hawker_prox + within_350m_kindergarten +      mall_prox + mrt_prox + park_prox + within_1km_school + supermarket_prox,      data = train_data_punggol) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      1549 \nNumber of independent variables:  11 \nMtry:                             3 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       1285768219 \nR squared (OOB):                  0.6229873 \n\n\n\n\n\n\nCode Chunk\nset.seed(1234)\nrf_w &lt;- ranger(resale_price ~ floor_area_sqm +\n                 unit_age + within_350m_busstop +\n                 within_350m_eldercare + hawker_prox + \n                 within_350m_kindergarten + mall_prox +\n                 mrt_prox + park_prox +\n                 within_1km_school +\n                 supermarket_prox,\n             data=train_data_woodlands)\nrf_w\n\n\nRanger result\n\nCall:\n ranger(resale_price ~ floor_area_sqm + unit_age + within_350m_busstop +      within_350m_eldercare + hawker_prox + within_350m_kindergarten +      mall_prox + mrt_prox + park_prox + within_1km_school + supermarket_prox,      data = train_data_woodlands) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      1391 \nNumber of independent variables:  11 \nMtry:                             3 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       681781375 \nR squared (OOB):                  0.7878284 \n\n\n\n\nCode Chunk\nwrite_rds(rf_w, \"data/rds/ml/woodlands/rf_w.rds\")\n\n\n\n\nCode Chunk\nrf_w &lt;- read_rds(\"data/rds/ml/woodlands/rf_w.rds\")\nrf_w\n\n\nRanger result\n\nCall:\n ranger(resale_price ~ floor_area_sqm + unit_age + within_350m_busstop +      within_350m_eldercare + hawker_prox + within_350m_kindergarten +      mall_prox + mrt_prox + park_prox + within_1km_school + supermarket_prox,      data = train_data_woodlands) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      1391 \nNumber of independent variables:  11 \nMtry:                             3 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       681781375 \nR squared (OOB):                  0.7878284 \n\n\n\n\n\n\n\n\nIn this section, you will learn how to calibrate a model to predict HDB resale price by using grf() of SpatialML package.\n\nPunggolWoodlands\n\n\n\n\nCode Chunk\nset.seed(1234)\ngwRF_adaptive_p &lt;- grf(formula = resale_price ~ floor_area_sqm +\n                 unit_age + within_350m_busstop +\n                 within_350m_eldercare + hawker_prox + \n                 within_350m_kindergarten + mall_prox +\n                 mrt_prox + park_prox +\n                 within_1km_school +\n                 supermarket_prox,\n                      dframe=train_data_punggol, \n                     bw=55,\n                     kernel=\"adaptive\",\n                     coords=coords_train_p,\n                     ntree = 50)\n\n\nRanger result\n\nCall:\n ranger(resale_price ~ floor_area_sqm + unit_age + within_350m_busstop +      within_350m_eldercare + hawker_prox + within_350m_kindergarten +      mall_prox + mrt_prox + park_prox + within_1km_school + supermarket_prox,      data = train_data_punggol, num.trees = 50, mtry = 3, importance = \"impurity\",      num.threads = NULL) \n\nType:                             Regression \nNumber of trees:                  50 \nSample size:                      1549 \nNumber of independent variables:  11 \nMtry:                             3 \nTarget node size:                 5 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       1310728297 \nR squared (OOB):                  0.6156685 \n\n\n          floor_area_sqm                 unit_age      within_350m_busstop \n            6.326567e+11             1.156479e+12             1.244582e+11 \n   within_350m_eldercare              hawker_prox within_350m_kindergarten \n            1.596830e+11             5.174056e+11             1.464665e+11 \n               mall_prox                 mrt_prox                park_prox \n            5.932477e+11             3.333798e+11             2.561672e+11 \n       within_1km_school         supermarket_prox \n            1.749702e+11             3.213997e+11 \n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-126714  -16526    2460    2945   23939  129080 \n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-96078.65 -13775.10    855.28    -58.52  14384.32 119674.72 \n\n\n                               Min         Max       Mean        StD\nfloor_area_sqm                   0 91014202939 5906598515 9572494892\nunit_age                 772992909 70292040371 9653682844 7816982408\nwithin_350m_busstop              0 23284152693 2079171556 2163833036\nwithin_350m_eldercare            0 22318151206  229763601  939144823\nhawker_prox              125992847 77365795037 4763798655 4696341821\nwithin_350m_kindergarten         0 22978183199  737940280 1744984043\nmall_prox                 89425394 51937220048 4875866267 4755292035\nmrt_prox                 131598882 29974992166 4057921881 3311668548\npark_prox                108935979 38285549900 4649691712 4018345994\nwithin_1km_school                0 22017741359  794052428 1434695546\nsupermarket_prox          38701460 63205009429 4233523256 4566974721\n\n\nCode Chunk\nbeep(3)\n\n\n\n\nCode Chunk\nwrite_rds(gwRF_adaptive_p, \"data/rds/ml/punggol/gwRF_adaptive_p.rds\")\n\n\n\n\nCode Chunk\ngwRF_adaptive_p &lt;- read_rds(\"data/rds/ml/punggol/gwRF_adaptive_p.rds\")\n\n\n\n\n\n\nCode Chunk\nset.seed(1234)\ngwRF_adaptive_w &lt;- grf(formula = resale_price ~ floor_area_sqm +\n                 unit_age + within_350m_busstop +\n                 within_350m_eldercare + hawker_prox + \n                 within_350m_kindergarten + mall_prox +\n                 mrt_prox + park_prox +\n                 within_1km_school +\n                 supermarket_prox,\n                      dframe=train_data_woodlands, \n                     bw=55,\n                     kernel=\"adaptive\",\n                     coords=coords_train_w,\n                     ntree = 50)\n\n\nRanger result\n\nCall:\n ranger(resale_price ~ floor_area_sqm + unit_age + within_350m_busstop +      within_350m_eldercare + hawker_prox + within_350m_kindergarten +      mall_prox + mrt_prox + park_prox + within_1km_school + supermarket_prox,      data = train_data_woodlands, num.trees = 50, mtry = 3, importance = \"impurity\",      num.threads = NULL) \n\nType:                             Regression \nNumber of trees:                  50 \nSample size:                      1391 \nNumber of independent variables:  11 \nMtry:                             3 \nTarget node size:                 5 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       699465884 \nR squared (OOB):                  0.7823249 \n\n\n          floor_area_sqm                 unit_age      within_350m_busstop \n            6.498645e+11             1.523757e+12             8.640691e+10 \n   within_350m_eldercare              hawker_prox within_350m_kindergarten \n            2.443447e+10             1.374084e+11             6.735891e+10 \n               mall_prox                 mrt_prox                park_prox \n            2.366135e+11             4.865546e+11             2.638215e+11 \n       within_1km_school         supermarket_prox \n            2.269810e+11             3.863464e+11 \n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-162753.7  -14222.8       0.0     207.3   15208.6  104482.1 \n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-73326.46  -8051.71    102.83    -40.93   8585.95  73157.76 \n\n\n                               Min          Max       Mean         StD\nfloor_area_sqm                   0 119507423558 7235009419 12283139013\nunit_age                 290434405 162351531337 9787543745 16190314956\nwithin_350m_busstop       19024539 101166507333 3038063745  9092850294\nwithin_350m_eldercare            0  43950239157  391375201  1601131616\nhawker_prox              205434461 162703114194 4496604507  7941092644\nwithin_350m_kindergarten         0  28289156658  710152654  1940630865\nmall_prox                165366043  81654791451 4917757094  7011474221\nmrt_prox                 310086561  77191474562 4237235607  5629134036\npark_prox                166726290 137269067979 5310916373 12030607333\nwithin_1km_school                0  70335686672 1024510779  4531548436\nsupermarket_prox         154822760 168893795321 5782731401 13086332964\n\n\nCode Chunk\nbeep(3)\n\n\n\n\nCode Chunk\nwrite_rds(gwRF_adaptive_w, \"data/rds/ml/woodlands/gwRF_adaptive_w.rds\")\n\n\n\n\nCode Chunk\ngwRF_adaptive_w &lt;- read_rds(\"data/rds/ml/woodlands/gwRF_adaptive_w.rds\")\n\n\n\n\n\n\n\n\n\nThe code chunk below will be used to combine the test data with its corresponding coordinates data.\n\nPunggolWoodlands\n\n\n\n\nCode Chunk\ntest_data_punggol &lt;- cbind(test_data_punggol, coords_test_p) %&gt;%\n  st_drop_geometry()\n\n\n\n\n\n\nCode Chunk\ntest_data_woodlands &lt;- cbind(test_data_woodlands, coords_test_w) %&gt;%\n  st_drop_geometry()\n\n\n\n\n\n\n\n\nNext, predict.grf() of spatialML package will be used to predict the resale value by using the test data and gwRF_adaptive model calibrated earlier.\n\nPunggolWoodlands\n\n\n\n\nCode Chunk\ngwRF_pred_p &lt;- predict.grf(gwRF_adaptive_p, \n                           test_data_punggol, \n                           x.var.name=\"X\",\n                           y.var.name=\"Y\", \n                           local.w=1,\n                           global.w=0)\n\n\n\n\nCode Chunk\nGRF_pred_p &lt;- write_rds(gwRF_pred_p, \"data/rds/ml/punggol/gwRF_adaptive_p.rds\")\n\n\n\n\n\n\nCode Chunk\ngwRF_pred_w &lt;- predict.grf(gwRF_adaptive_w, \n                           test_data_woodlands, \n                           x.var.name=\"X\",\n                           y.var.name=\"Y\", \n                           local.w=1,\n                           global.w=0)\n\n\n\n\nCode Chunk\nGRF_pred_w &lt;- write_rds(gwRF_pred_w, \"data/rds/ml/woodlands/gwRF_adaptive_w.rds\")\n\n\n\n\n\n\n\n\nThe output of the predict.grf() is a vector of predicted values. It is wiser to convert it into a data frame for further visualisation and analysis.\n\nPunggolWoodlands\n\n\n\n\nCode Chunk\nGRF_pred_p &lt;- read_rds(\"data/rds/ml/punggol/gwRF_adaptive_p.rds\")\n\n\n\n\nCode Chunk\nGRF_pred_df_p &lt;- as.data.frame(GRF_pred_p)\n\n\nIn the code chunk below, cbind() is used to append the predicted values onto test_data\n\n\nCode Chunk\ntest_data_p &lt;- cbind(test_data_punggol, GRF_pred_df_p)\n\n\n\n\nCode Chunk\nwrite_rds(test_data_p, \"data/rds/ml/punggol/test_data_p.rds\")\n\n\n\n\n\n\nCode Chunk\nGRF_pred_w &lt;- read_rds(\"data/rds/ml/woodlands/gwRF_adaptive_w.rds\")\n\n\n\n\nCode Chunk\nGRF_pred_df_w &lt;- as.data.frame(GRF_pred_w)\n\n\nIn the code chunk below, cbind() is used to append the predicted values onto test_data\n\n\nCode Chunk\ntest_data_w &lt;- cbind(test_data_woodlands, GRF_pred_df_w)\n\n\n\n\nCode Chunk\nwrite_rds(test_data_w, \"data/rds/ml/woodlands/test_data_w.rds\")\n\n\n\n\n\n\n\n\n\nThe root mean square error (RMSE) allows us to measure how far predicted values are from observed values in a regression analysis. In the code chunk below, rmse() of Metrics package is used to compute the RMSE.\n\nPunggolWoodlands\n\n\n\n\nCode Chunk\nrmse(test_data_punggol$resale_price, \n     test_data_p$GRF_pred_p)\n\n\n[1] 49338.38\n\n\n\n\nCode Chunk\nsummary(train_data_punggol$resale_price)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 456000  566000  605000  606094  645000  800000 \n\n\n\n\nCode Chunk\nsummary(test_data_p$resale_price)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 495000  610000  648000  650865  689166  788000 \n\n\n\n\n\n\n\n\nNote\n\n\n\nInterpretation\n\nMagnitude of the Error:\n\nThe RMSE of $49,423.08 indicates that, on average, the model’s predicted resale prices deviate from the actual resale prices by approximately $49,423.08.\nGiven that the mean resale price for the test dataset is $650,865, this RMSE represents about 7.5% of the mean resale price. This indicates a relatively reasonable model performance, with predictions being on average about 7.5% off from actual resale prices.\n\nComparison to Median:\n\nThe median resale price in the test dataset is $648,000.\nThe RMSE of $49,423.08 represents about 7.6% of the median resale price, which suggests that the model’s predictions are, on average, off by 7.6% from the actual resale price at the median.\nSince the RMSE is around 7.5-8% of both the mean and median resale prices, it suggests that the model performs fairly well, but there is still some degree of prediction error.\n\nModel Performance:\n\nAn RMSE of $49,423.08 is typical for real estate predictive models. It shows that while the model has a strong performance, especially considering it accounts for the variance in resale prices, there’s still room for improvement, particularly for properties at the lower and higher price ranges.\n\nRange of Resale Prices:\n\nThe resale prices in the test dataset range from $495,000 to $788,000. Given this price range, the RMSE of $49,423.08 suggests that the model is accurate within 7-8% for most properties, but the error could be higher for properties at the extremes (either low or high price points).\nThe training dataset has similar price distributions, but with a slightly lower mean of $606,094 compared to the test dataset’s mean of $650,865, which suggests that the model may be slightly more optimistic for higher resale prices in the test set.\n\nComparison to the Training Data:\n\nThe training dataset has a slightly higher mean resale price ($606,094) compared to the test dataset ($650,865), suggesting that the model might have been slightly undertrained or overfitted. This could be further analyzed by evaluating how well the model generalizes to the test data.\nThe relatively large spread between the lower and upper bounds of the resale prices (from $456,000 to $800,000 in the training set) might also contribute to some variance in the predictions, though the RMSE indicates that the model is generally effective.\n\n\nConclusion:\nThe RMSE of $$49,423.08 for the Punggol test dataset, where the median resale price is 648,000, indicates that the model’s predictions are on average about 7.6% off from the actual resale prices. This suggests that the model performs reasonably well, with error margins that are typical for real estate prediction models. While this level of error is acceptable, there is still room for improvement, particularly for properties at the lower and higher ends of the price spectrum.\n\n\n\n\n\n\nCode Chunk\nrmse(test_data_woodlands$resale_price, \n     test_data_w$GRF_pred_w)\n\n\n[1] 41784.94\n\n\n\n\nCode Chunk\nsummary(train_data_woodlands$resale_price)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 350000  470000  500000  507168  545000  690000 \n\n\n\n\nCode Chunk\nsummary(test_data_w$resale_price)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 355000  508500  530000  540097  570000  701000 \n\n\n\n\n\n\n\n\nNote\n\n\n\nInterpretation\n1. Magnitude of the Error\n\nThe RMSE of $42,038.73 indicates that, on average, the model’s predicted resale prices deviate from the actual resale prices by $42,038.73\nThis is a 7.8% error when compared to the mean resale price in the test dataset ($540,097). This suggests that, on average, the model’s predictions are off by about 7.8% of the actual resale price, which is a reasonable error margin for many predictive models.\n\n2. Comparison to the Median\n\nThe median resale price in the test dataset is $530,000.\nThe RMSE of $42,038.73 represents about 7.9% of the median resale price. This level of error is typical for real estate predictive models, and the fact that the RMSE is about 7.9% of the median indicates that the model performs well for most of the data points, but there may still be some variability in predictions for properties priced far below or above the median.\n\n3. Model Performance\n\nThe RMSE of $42,038.73 shows that the model has a moderate level of predictive accuracy. An RMSE that is around 7-8% of the median and mean resale prices is common for real estate prediction models, which often deal with diverse factors influencing property prices (e.g., location, condition of the property, etc.).\nThe fact that the RMSE is relatively small compared to the range of resale prices suggests that the model captures the general trends well. However, there could be room for improvement, especially for properties with prices that are on the lower or upper extremes of the price spectrum.\n\n4. Range of Resale Prices\n\nThe range of resale prices in the test dataset spans from $355,000 to $701,000, with the median at $530,000 and the mean at $540,097.\nThe RMSE of $42,038.73 indicates that, on average, the predicted resale prices are within 7-8% of the actual resale prices. This is reasonable for most properties, but properties at the lower end ($355,000) or the upper end ($701,000) of the price range might have higher prediction errors. This suggests that the model works well for mid-range properties, but its accuracy decreases as we move toward the extremes of the price range.\n\n5. Comparison to Training Data\n\nThe median resale price in the training dataset is $$500,000, which is 30,000 lower than the median resale price in the test dataset (5$30,000).\nThe mean resale price in the training dataset is $507,168, which is also slightly lower than the mean of $540,097 in the test dataset.\nThis suggests that the test data might contain higher-value properties than the training data. Despite this difference, the model’s performance (as indicated by the RMSE) is fairly consistent, with the error remaining around 7.8% of the test dataset’s mean resale price.\nThe RMSE of $42,038.73 relative to the training dataset’s median resale price of $500,000 is approximately 8.4%, which is slightly higher than the RMSE relative to the test dataset’s median. This indicates that the model performs slightly better on the test dataset, potentially due to the difference in price distributions.\n\nConclusion:\nThe RMSE of $42,038.73 for the Woodlands test dataset, where the median resale price is $530,000, suggests that the model’s predictions are, on average, about 7.9% off from the actual resale prices. This is a reasonable level of predictive accuracy, indicating that the model is performing well overall, with small but acceptable prediction errors for properties within the typical price range.\nThe model appears to perform slightly better on the test dataset, which has higher property prices than the training dataset. Further model refinement or feature adjustment may help reduce errors for properties at the extremes of the price range. However, for general purposes, the model seems reliable for predicting resale prices within the observed price ranges.\n\n\n\n\n\n\n\n\n\nAlternatively, scatterplot can be used to visualise the actual resale price and the predicted resale price by using the code chunk below.\n\nPunggolWoodlands\n\n\n\n\nCode Chunk\nggplot(data = test_data_p,\n       aes(x = GRF_pred_p,\n           y = resale_price)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nObservation\n\nPositive Correlation:\n\nThere is a clear positive relationship between the predicted resale prices (GRF_pred_p) and the actual resale prices (resale_price). As the predicted resale prices increase, the actual resale prices also tend to increase.\nThis suggests that the model’s predictions are generally in line with the observed data, as higher predicted values correspond to higher actual resale prices.\n\nTrend and Distribution:\n\nThe points follow an upward trend, with a generally tight clustering around the line of best fit (not shown, but implied by the alignment of points). This indicates that the model is effectively capturing the main trend in the data.\nThe scatter is somewhat spread out, indicating some variance between the predicted and actual resale prices. This is typical in any real-world prediction model, as it shows that the model is not perfect and that there are some discrepancies between predicted and actual values.\n\nOutliers:\n\nAlthough the points generally follow a clear upward trend, there might be a few outliers or data points that deviate significantly from the general trend. These outliers could represent cases where the model performs poorly or where there are unusual observations in the data.\n\nModel Fit:\n\nThe strong linear trend suggests that the model has captured the general relationship between the variables, but some level of variability remains. To assess how well the model performs, you would typically calculate performance metrics like R-squared, RMSE, or MAE to quantify how well the model’s predictions match the actual values.\n\n\nConclusion:\nThe scatterplot shows that there is a positive relationship between predicted resale prices (GRF_pred_p) and actual resale prices (resale_price). This suggests that the model is successful in capturing the general trend of the data, but some discrepancies remain, which is expected in any predictive model. Further analysis, such as residual analysis or model evaluation metrics, could be done to refine the model and assess its performance more thoroughly.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nObservation\n\nPositive Correlation:\n\nThere is a strong positive correlation between GRF_pred_w (predicted resale prices) and resale_price (actual resale prices). As the predicted resale prices increase, the actual resale prices also tend to increase.\nThis indicates that the model’s predictions are generally aligned with the observed data, and the higher predicted values tend to correspond to higher actual resale prices.\n\nTrend and Distribution:\n\nThe points show a clear upward trend, indicating a good linear relationship between the predicted and actual values.\nThe scatter around the trend is relatively tight, meaning that the model’s predictions are fairly close to the actual resale prices. This suggests that the model is capturing the trend in the data well.\n\nOutliers:\n\nSome outliers may exist at the higher end of the resale price range. These points appear to deviate slightly from the general trend, but they do not seem to be too far from the line of best fit. These outliers could represent cases where the model does not predict well for certain properties, possibly due to extreme values or unique characteristics of those observations.\n\nModel Fit:\n\nThe strong linear relationship in the scatterplot indicates that the model fits the data well. However, there may still be room for improvement in reducing the variance in predictions, as indicated by the slight spread of points, especially for lower resale prices.\n\nComparison to the Previous Scatterplot:\n\nCompared to the first scatterplot with GRF_pred_p, this scatterplot (with GRF_pred_w) shows a similar pattern of positive correlation between predicted and actual resale prices. Both scatterplots indicate good predictive performance, with minor deviations in predicted values at the lower and higher ends.\n\n\nConclusion:\nThe scatterplot suggests that the model with GRF_pred_w effectively captures the relationship between the predictors and actual resale prices, with strong predictive accuracy. The positive correlation and tight clustering of points indicate that the model is reliable for predicting resale prices. However, further evaluation using residual analysis or model evaluation metrics could help identify areas for improvement, particularly in the presence of outliers.\n\n\n\n\n\n\n\n\n\nPunggolWoodlands\n\n\nTo compare the two methods—Random Forest (RF) and Geographically Weighted Regression (GWR)—for predicting resale prices in Punggol, let’s focus on the prediction accuracy and model performance.\n1. Prediction Accuracy:\nRandom Forest:\n\nRMSE: The Random Forest model has an RMSE of 49,423.08, which measures the average error between the predicted and actual resale prices. A lower RMSE indicates better accuracy.\nPrice Distribution:\n\nTraining Data: Min 456,000, Median 605,000, Max 800,000.\nTest Data: Min 495,000, Median 648,000, Max 788,000.\n\n\nThe predicted resale prices in the test data are within the expected range and the RMSE indicates that the Random Forest model performs fairly well.\nGWR:\n\nPredicted Values: The predicted resale prices from the GWR model range from 463,945 to 707,938, with a median of 609,348.\nPrediction Variance: The variance of predictions ranges from 1.25 billion to 1.4 billion, indicating some spread in the predictions, which is typical of GWR since it accounts for spatial variability.\n\nThe predicted resale prices from GWR are also within the expected range, with a similar spread as the actual test data.\n\nModel Calibration:\n\nRandom Forest:\n\nStrength: Random Forest is well-suited for handling complex, non-linear relationships between predictors without explicit assumptions about their functional form.\nWeakness: It is harder to interpret and doesn’t directly provide insights into how individual variables (like distance to bus stops, malls, etc.) influence the resale prices.\n\nGWR:\n\nStrength: GWR allows for spatial variation in the coefficients, meaning it can show how the effect of each predictor (like floor_area_sqm, unit_age, etc.) changes across different locations. This makes GWR especially valuable for understanding local variations in resale prices.\nWeakness: GWR requires more complex interpretation, as it provides different coefficients for each observation based on the local neighborhood.\n\n\nWhich Model is Better?\n\n\nPrediction Accuracy: Both models provide similar ranges of predicted resale prices. However, Random Forest has a slightly better RMSE (49,423.08), which suggests it may perform slightly better in terms of overall prediction accuracy.\nInterpretability: GWR excels in interpretability, as it provides insights into how different factors affect resale prices in different parts of Punggol, thanks to its spatially varying coefficients. This is valuable if you need to understand the localized impact of each predictor.\n\n\n\n1. Prediction Accuracy:\nRandom Forest:\n\nRMSE: The RMSE for the Random Forest model is 42,038.73. Lower RMSE indicates better prediction accuracy, and this value is quite reasonable, suggesting good performance for predicting resale prices.\nPrice Summary:\n\nTraining Data: Min 350,000, Median 500,000, Max 690,000.\nTest Data: Min 355,000, Median 530,000, Max 701,000.\n\nThe test data’s price distribution suggests that the model is predicting within the expected range.\n\nGWR:\n\nPrediction: The GWR model gives predicted resale prices ranging from 359,170 to 616,562, with a median of 493,786. This is fairly close to the actual resale prices in the test data (from 355,000 to 701,000), indicating that GWR is also performing reasonably well for predictions.\nPrediction Variance: The prediction variance ranges from 735,748,898 to 792,698,476, suggesting that the GWR model’s predictions have some spread, but this is expected since GWR accounts for spatial variation.\n\n\nModel Calibration:\n\nRandom Forest:\n\nRandom Forest tends to perform well when capturing complex, non-linear relationships between predictors. It doesn’t offer easy interpretability, but it can model interactions between variables without explicitly specifying them. Given that the RMSE is relatively low (42,038.73), the model seems to be doing well in predicting resale prices.\n\nGWR:\n\nModel Calibration Information: The coefficients for the GWR model vary spatially, and the values seem reasonable (e.g., for floor_area_sqm, the coefficient ranges from -3419.735 to 3037.7). These coefficients represent the local effect of each variable on resale prices.\nInterpretability: One key strength of GWR is that it allows you to understand how each variable affects resale prices differently in various spatial locations (e.g., the effect of proximity to bus stops, hawker centers, or supermarkets varies across the region).\n\n\nWhich Model is Better?\n\n\nPrediction Accuracy: Both models give similar ranges for predicted resale prices, and both seem to predict the prices reasonably well. However, Random Forest generally provides a more reliable prediction with lower RMSE (42,038.73), indicating better overall performance in terms of predictive accuracy.\nInterpretability: If you need to understand how each factor influences resale prices in different regions of Woodlands, GWR is a better choice because of its spatially varying coefficients and ability to show how variables interact with location.\n\n\n\n\n\n\n\nThis study only looks into two the top two towns that has the most transactions and the transactions are slightly below 2 years. Hence, the sample size may be size which inevitably affecting the model. Thus, future proposal may look into top 5 - 10 towns over a span of 5 years. Additonally, only two predictive models were used. Future studies may consider other models such as Xboost in comparing across the models predictive abilities.\n\n\n\nTo summarise, Random Forest is better in predicting HDB resale prices. However, geospatial details are heavily embedded in HDB resale data, we shouldn’t discount the spatial aspects that will affect the resale prices too. If the study’s purpose is to understand spatial patterns, GWR is more suited. Conversely, if accuracy is a priority, Random Forest would be a better choice."
  },
  {
    "objectID": "TakeHomeExercise/TakeHome3/TakeHome3.html#overview",
    "href": "TakeHomeExercise/TakeHome3/TakeHome3.html#overview",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "In this take-home exercise, We will calibrate a predictive model to predict HDB resale prices between July-September 2024 by using HDB resale transaction records in 2023."
  },
  {
    "objectID": "TakeHomeExercise/TakeHome3/TakeHome3.html#getting-started",
    "href": "TakeHomeExercise/TakeHome3/TakeHome3.html#getting-started",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "Code Chunk\npacman::p_load(tidyverse, readr, sf, httr, jsonlite, rvest, dplyr, units, lubridate, tmap, ggplot2,ggpubr, beepr, corrplot, spdep, GWmodel, SpatialML, rsample, Metrics)\n\n\n\n\n\n\n\n\n\n\nPackage\nFunction\nExplanation\n\n\n\n\ntidyverse\ndplyr::select, dplyr::mutate, ggplot2::ggplot, tibble::tibble\nA collection of R packages for data manipulation (dplyr), data visualization (ggplot2), and reading (readr). tibble is for working with data frames.\n\n\nreadr\nreadr::read_csv, readr::write_csv\nFunctions for reading and writing CSV files efficiently.\n\n\nsf\nsf::st_as_sf, sf::st_transform, sf::st_drop_geometry, sf::st_is_empty, sf::st_coordinates\nFunctions for handling spatial data, including converting data to sf objects, transforming coordinates, and checking for empty geometries.\n\n\nhttr\nhttr::GET, httr::POST\nFunctions for making HTTP requests, such as GET and POST, to interact with web APIs.\n\n\njsonlite\njsonlite::fromJSON, jsonlite::toJSON\nFunctions to parse JSON data into R objects and to convert R objects into JSON format.\n\n\nrvest\nrvest::read_html, rvest::html_nodes, rvest::html_table\nUsed for web scraping, including reading HTML content and extracting elements from web pages.\n\n\ndplyr\ndplyr::select, dplyr::mutate, dplyr::filter, dplyr::summarize\nA core package for data manipulation, used for selecting columns, creating new columns, filtering data, and summarizing results.\n\n\nunits\nunits::set_units, units::as_units\nProvides functions for handling and converting physical units in R.\n\n\nlubridate\nlubridate::ymd, lubridate::mdy, lubridate::today\nFunctions to parse, manipulate, and work with date-time objects in a variety of formats.\n\n\ntmap\ntmap::tm_shape, tmap::tm_borders, tmap::tm_fill\nA package for thematic mapping. Functions allow for creating maps with geographic boundaries, fill colors, and more.\n\n\nggplot2\nggplot2::ggplot, ggplot2::geom_point, ggplot2::geom_line\nThe main visualization package in R, used for creating static graphics such as scatter plots, line charts, histograms, and more.\n\n\nggpubr\nggpubr::ggarrange, ggpubr::stat_cor\nEnhances ggplot2 by allowing for arranging multiple plots and adding statistical tests, like correlation coefficients, to plots.\n\n\nbeepr\nbeepr::beep\nA simple function to play sounds as notifications, commonly used to alert the user when an operation is complete.\n\n\ncorrplot\ncorrplot::corrplot\nUsed to visualize correlation matrices, offering various plotting styles and methods for better presentation of correlations.\n\n\nspdep\nspdep::nb2listw, spdep::spautolm\nProvides spatial econometrics tools, such as calculating spatial weights and performing spatial autoregressive models.\n\n\nGWmodel\nGWmodel::gwr.basic, GWmodel::grf, GWmodel::bw.gwr\nFunctions for geographically weighted regression (GWR), including model fitting (gwr.basic), bandwidth selection, and fitting spatial regression trees.\n\n\nSpatialML\nSpatialML::SpatialCV, SpatialML::SpatialML\nUsed for machine learning models in spatial contexts, including spatial cross-validation and spatially explicit machine learning techniques.\n\n\nrsample\nrsample::initial_split, rsample::training, rsample::testing\nProvides tools for resampling and splitting data into training and testing sets for model validation.\n\n\nMetrics\nMetrics::rmse, Metrics::mae, Metrics::mse\nFunctions to calculate various performance metrics for model evaluation, including Root Mean Square Error (RMSE), Mean Absolute Error (MAE), and Mean Squared Error (MSE)."
  },
  {
    "objectID": "TakeHomeExercise/TakeHome3/TakeHome3.html#the-data",
    "href": "TakeHomeExercise/TakeHome3/TakeHome3.html#the-data",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "Aspatial Dataset:\n\n\nHDB Resale Flat Prices from Data.gov.sg\n\n\nGeospatial Dataset:\n\n\nMPSZ: Boundaries of Singapore from ura.gov.sg\nBus Stops: A list of bus stops locally from datamall.lta.gov.sg\nEldercare (SHP): A list of elder care centres locally from data.gov.sg\nHawker Centres (KML): a list of hawker centres from data.gov.sg\nKindergarten (KML) from data.gov.sg\nMalls (CSV) from kaggle.com\nTrain Station (SHP) from datamall.lta.gov.sg\nSDCP Park (kml) from data.gov.sg\nSchools (CSV) from data.gov.sg\nSupermarket (KML) from data.gov.sg"
  },
  {
    "objectID": "TakeHomeExercise/TakeHome3/TakeHome3.html#importing-data",
    "href": "TakeHomeExercise/TakeHome3/TakeHome3.html#importing-data",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "Aspatial Data:\n\n\nHDB Resale\n\n\n\nCode Chunk\nresale2023 &lt;- read_csv(\"data/rawdata/aspatial/hdb/resale.csv\") %&gt;%\n  filter(month &gt;= \"2023-01\" & month &lt;= \"2024-09\")\n\n\n\nGeospatial Data:\n\n\nMPSZ in shapefile format\n\n\n\nCode Chunk\nmpsz = st_read(dsn = \"data/rawdata/geospatial/mpsz\",\n               layer = \"MP14_SUBZONE_NO_SEA_PL\")\n\n\nReading layer `MP14_SUBZONE_NO_SEA_PL' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/TakeHomeExercise/TakeHome3/data/rawdata/geospatial/mpsz' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\nCode Chunk\nplot(mpsz)\n\n\n\n\n\n\n\n\n\n\nBus Stops in shapefile format\n\n\n\nCode Chunk\nbusstop &lt;- st_read(dsn = \"data/rawdata/geospatial/busstop\",\n                      layer = \"BusStop\")\n\n\nReading layer `BusStop' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/TakeHomeExercise/TakeHome3/data/rawdata/geospatial/busstop' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5166 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48285.52 ymax: 52983.82\nProjected CRS: SVY21\n\n\n\nEldercare in shapefile format\n\n\n\nCode Chunk\neldercare &lt;- st_read(dsn = \"data/rawdata/geospatial/eldercare\",\n                      layer = \"ELDERCARE\")\n\n\nReading layer `ELDERCARE' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/TakeHomeExercise/TakeHome3/data/rawdata/geospatial/eldercare' \n  using driver `ESRI Shapefile'\nSimple feature collection with 133 features and 18 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 14481.92 ymin: 28218.43 xmax: 41665.14 ymax: 46804.9\nProjected CRS: SVY21\n\n\n\nHawker Centres in kml format\n\n\n\nCode Chunk\nhawker &lt;- st_read(\"data/rawdata/geospatial/hawker/HawkerCentres.kml\")\n\n\nReading layer `HAWKERCENTRE' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/TakeHomeExercise/TakeHome3/data/rawdata/geospatial/hawker/HawkerCentres.kml' \n  using driver `KML'\nSimple feature collection with 125 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6974 ymin: 1.272716 xmax: 103.9882 ymax: 1.449017\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\nKindergarten in fml format\n\n\n\nCode Chunk\nkindergarten &lt;- st_read(\"data/rawdata/geospatial/kindergarten/Kindergartens.kml\")\n\n\nReading layer `KINDERGARTENS' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/TakeHomeExercise/TakeHome3/data/rawdata/geospatial/kindergarten/Kindergartens.kml' \n  using driver `KML'\nSimple feature collection with 448 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6887 ymin: 1.247759 xmax: 103.9717 ymax: 1.455452\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\nCode Chunk\nkindergarten &lt;- st_transform(kindergarten, 3414)\n\n\n\nShopping Malls in csv format\n\n\n\nCode Chunk\nmall &lt;- read_csv(\"data/rawdata/geospatial/mall/shopping_mall_coordinates.csv\")\n\n\n\nTrain Station in shapefile format\n\n\n\nCode Chunk\nmrt &lt;- st_read(dsn = \"data/rawdata/geospatial/mrt\",\n                layer = \"RapidTransitSystemStation\")\n\n\nReading layer `RapidTransitSystemStation' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/TakeHomeExercise/TakeHome3/data/rawdata/geospatial/mrt' \n  using driver `ESRI Shapefile'\n\n\nSimple feature collection with 230 features and 5 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 6068.209 ymin: 27478.44 xmax: 45377.5 ymax: 47913.58\nProjected CRS: SVY21\n\n\n\nSDCP Park in kml format\n\n\n\nCode Chunk\npark &lt;- st_read(\"data/rawdata/geospatial/park/NParksParksandNatureReservesKML.kml\")\n\n\nReading layer `NPARKS_PARKS' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/TakeHomeExercise/TakeHome3/data/rawdata/geospatial/park/NParksParksandNatureReservesKML.kml' \n  using driver `KML'\nSimple feature collection with 430 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY, XYZ\nBounding box:  xmin: 103.6925 ymin: 1.2115 xmax: 104.0544 ymax: 1.46419\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\nSchools in csv format\n\n\n\nCode Chunk\nschool &lt;- read_csv(\"data/rawdata/geospatial/school/Generalinformationofschools.csv\") %&gt;% \n  mutate(postal_code = as.character(postal_code))\n\n\n\nSupermarket in kml format\n\n\n\nCode Chunk\nsupermarket &lt;- st_read(\"data/rawdata/geospatial/supermarket/SupermarketsKML.kml\")\n\n\nReading layer `SUPERMARKETS' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/TakeHomeExercise/TakeHome3/data/rawdata/geospatial/supermarket/SupermarketsKML.kml' \n  using driver `KML'\nSimple feature collection with 526 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6258 ymin: 1.24715 xmax: 104.0036 ymax: 1.461526\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\n\nStandardising CRS\n\n\n\nIronically, all of these data sets are from Singapore, yet, it presents with varied CRS.\nTherefore, in this exercise, SVY21 (EPSG 3414) will be the standardised CRS as all data relates to local context."
  },
  {
    "objectID": "TakeHomeExercise/TakeHome3/TakeHome3.html#data-wrangling",
    "href": "TakeHomeExercise/TakeHome3/TakeHome3.html#data-wrangling",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "In tidying up the HDB resale data, the column block & street_name are combined and remaining_lease are split in two columns for ease of manipulation. Additionally, we are changing the column remaining_lease_yr as an integer and splitting it to month and year.\n\n\nCode Chunk\nresale_tidy &lt;- resale2023 %&gt;%\n  mutate(address = paste(block,street_name)) %&gt;% #combined block & street name\n  mutate(remaining_lease_yr = as.integer(\n    str_sub(remaining_lease, 0, 2)))%&gt;% #extract remaining lease by yr\n  mutate(remaining_lease_mth = as.integer(\n    str_sub(remaining_lease, 9, 11))) #extract remaining lease by month\n\n\n\n\nCode Chunk\nresale_selected &lt;- resale_tidy %&gt;%\n  filter(month &gt;= \"2023-01\" & month &lt;= \"2024-09\")\n\n\n\n\nCode Chunk\nadd_list &lt;- sort(unique(resale_selected$address)) #parse a list as API cannot read df\n#unique reduces records to pass to portal\n#sort is used to easier to find geo codes\n\n\nFetching Data from onemap API\nAs the HDB resale data lack of coordinates, API was used to extract the coordinates.\n\n\nCode Chunk\nget_coords &lt;- function(add_list){\n\n  # Create a data frame to store all retrieved coordinates\n  postal_coords &lt;- data.frame()\n    \n  for (i in add_list){\n    r &lt;- GET('https://www.onemap.gov.sg/api/common/elastic/search?',\n           query=list(searchVal=i,\n                     returnGeom='Y',\n                     getAddrDetails='Y'))\n    data &lt;- fromJSON(rawToChar(r$content))\n    found &lt;- data$found\n    res &lt;- data$results\n    \n    # Create a new data frame for each address\n    new_row &lt;- data.frame()\n    \n    # If single result, append \n    if (found == 1){\n      postal &lt;- res$POSTAL \n      lat &lt;- res$LATITUDE\n      lng &lt;- res$LONGITUDE\n      new_row &lt;- data.frame(address = i, \n                           postal = postal, \n                           latitude_wgs84 = lat,  # renamed to clarify coordinate system\n                           longitude_wgs84 = lng) # renamed to clarify coordinate system\n    }\n    \n    # If multiple results, drop NIL and append top 1\n    else if (found &gt; 1){\n      # Remove those with NIL as postal\n      res_sub &lt;- res[res$POSTAL != \"NIL\", ]\n      \n      # Set as NA first if no Postal\n      if (nrow(res_sub) == 0) {\n          new_row &lt;- data.frame(address = i, \n                               postal = NA, \n                               latitude_wgs84 = NA, \n                               longitude_wgs84 = NA)\n      }\n      else{\n        top1 &lt;- head(res_sub, n = 1)\n        postal &lt;- top1$POSTAL \n        lat &lt;- top1$LATITUDE\n        lng &lt;- top1$LONGITUDE\n        new_row &lt;- data.frame(address = i, \n                             postal = postal, \n                             latitude_wgs84 = lat, \n                             longitude_wgs84 = lng)\n      }\n    }\n    else {\n      new_row &lt;- data.frame(address = i, \n                           postal = NA, \n                           latitude_wgs84 = NA, \n                           longitude_wgs84 = NA)\n    }\n    \n    # Add the row\n    postal_coords &lt;- rbind(postal_coords, new_row)\n  }\n  \n  # Convert to sf object with WGS84 coordinates (EPSG:4326)\n  # Filter out rows with NA coordinates first\n  valid_coords &lt;- postal_coords[!is.na(postal_coords$latitude_wgs84) & \n                              !is.na(postal_coords$longitude_wgs84), ]\n  \n  if(nrow(valid_coords) &gt; 0) {\n    coords_sf &lt;- st_as_sf(valid_coords, \n                         coords = c(\"longitude_wgs84\", \"latitude_wgs84\"),\n                         crs = 4326)\n    \n    # Transform to SVY21 (EPSG:3414)\n    coords_svy21 &lt;- st_transform(coords_sf, 3414)\n    \n    # Extract coordinates\n    coords_matrix &lt;- st_coordinates(coords_svy21)\n    \n    # Add SVY21 coordinates back to the original dataframe with desired column names\n    valid_coords$longitude &lt;- coords_matrix[, 1]  # SVY21 X coordinate as longitude\n    valid_coords$latitude &lt;- coords_matrix[, 2]   # SVY21 Y coordinate as latitude\n    \n    # Merge back with rows that had NA coordinates\n    result &lt;- merge(postal_coords, valid_coords[c(\"address\", \"longitude\", \"latitude\")], \n                   by = \"address\", all.x = TRUE)\n  } else {\n    # If no valid coordinates, add empty SVY21 columns\n    result &lt;- postal_coords\n    result$longitude &lt;- NA  # SVY21 coordinates\n    result$latitude &lt;- NA   # SVY21 coordinates\n  }\n  \n  return(result)\n}\n\n\nBelow is the code chunk that populates the coordinates in longitude, latitude and postal code against the address in the add_list.\n\n\nCode Chunk\ncoords &lt;- get_coords(add_list)\n\n\nThe longtitude and latitude is then combined into geometry and the crs has been set to EPSG = 3414.\n\n\nCode Chunk\ncoords_sf &lt;- coords %&gt;%\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 3414, remove = FALSE) %&gt;%\n  select(address, postal, longitude, latitude, geometry)\n\n\nFollowing which, the coords_sf is then combined to the resale_selected df by address, forming a new df resale_geom\n\n\nCode Chunk\nresale_geom &lt;- resale_selected %&gt;% \n  left_join(coords_sf, by = \"address\")\n\n\nAssigning CRS EPSG 3414 to resale_geom\n\n\nCode Chunk\nresale_geom &lt;- resale_geom %&gt;% \n  st_as_sf(coords =c(\"longitude\", \"latitude\"), crs = 3414, remove = FALSE)\n\n\nUsing st_crs() to check if it is the correct CRS.\n\n\nCode Chunk\nst_crs(resale_geom)\n\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nIn ensuring these are the variables we need, we use select() to include the variables. Then, we compute the unit age with the formula of 99 year (convention HDB lease years) - remaining lease year. Lastly, we change the column month to POSICxt so we can manipulate in ease later on.\n\n\nCode Chunk\nresale_geom &lt;- resale_geom %&gt;%\n  select(address, town, resale_price, month, flat_type, floor_area_sqm, remaining_lease_yr,longitude, latitude, geometry) %&gt;% \n  mutate(unit_age = 99 - remaining_lease_yr)\n\n\n\n\n\n\n\n\nWGS 84\nSVY21\nNo PCS / CSV\n\n\n\n\nHawker\nBus Stop\nHDB Resale\n\n\nKindergarten\nEldercare\nMall\n\n\nPark\nMRT\nSchool\n\n\nSupermarket\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCoordinate System\nViewing the sf, we noticed that the coordinates are in WGS84 - EPSG 4326. This may not be suitable for our data manipulation later on as we are standardising it to our local coordinates system which is SVY 21 EPSG 3414.\n\n\n\n\n\n\n\n\nTip\n\n\n\nOverview of Coordinate System\nWGS84 (EPSG: 4326) - Global Use\n\nDefinition: WGS84 (World Geodetic System 1984) is a global coordinate system that defines latitude and longitude on a three-dimensional ellipsoid. It is the standard used by the Global Positioning System (GPS).\nCoordinate System: It uses a geographic coordinate system based on a spheroid, where coordinates are given in degrees (latitude and longitude).\nApplication: WGS84 is used globally for mapping, navigation, and geolocation applications. It provides a standard reference for geographic information systems (GIS) and is essential for interoperability between different systems.\nUnits: The coordinates are expressed in degrees.\n\nSVY21 (EPSG: 3413) - Singapore Use\n\nDefinition: SVY21 (Singapore Vertical 21) is a national coordinate system specifically designed for Singapore. It is based on the Transverse Mercator projection and is tailored to provide accurate measurements within Singapore’s geographic boundaries.\nCoordinate System: SVY21 uses a projected coordinate system, where coordinates are given in meters. The system is more suitable for local applications because it minimizes distortions in a specific area.\nApplication: SVY21 is primarily used for urban planning, construction, and various governmental applications within Singapore. It provides higher accuracy for local measurements compared to global systems like WGS84.\nUnits: The coordinates are expressed in meters.\n\n\n\n\n\n\nFirst, we would like to check if there is any shared boudaries in this sf as this will churned as invalid polygons in the data. We will then noticed that there are several “Ring Self-Intersection” specifically, there are 9 polygons with self-intersection issues which is not good for the data.\n\n\nCode Chunk\nst_is_valid(mpsz, reason = TRUE)\n\n\n  [1] \"Valid Geometry\"                                           \n  [2] \"Valid Geometry\"                                           \n  [3] \"Valid Geometry\"                                           \n  [4] \"Valid Geometry\"                                           \n  [5] \"Valid Geometry\"                                           \n  [6] \"Valid Geometry\"                                           \n  [7] \"Valid Geometry\"                                           \n  [8] \"Valid Geometry\"                                           \n  [9] \"Valid Geometry\"                                           \n [10] \"Valid Geometry\"                                           \n [11] \"Valid Geometry\"                                           \n [12] \"Valid Geometry\"                                           \n [13] \"Valid Geometry\"                                           \n [14] \"Valid Geometry\"                                           \n [15] \"Valid Geometry\"                                           \n [16] \"Valid Geometry\"                                           \n [17] \"Valid Geometry\"                                           \n [18] \"Valid Geometry\"                                           \n [19] \"Valid Geometry\"                                           \n [20] \"Valid Geometry\"                                           \n [21] \"Valid Geometry\"                                           \n [22] \"Valid Geometry\"                                           \n [23] \"Valid Geometry\"                                           \n [24] \"Valid Geometry\"                                           \n [25] \"Valid Geometry\"                                           \n [26] \"Valid Geometry\"                                           \n [27] \"Valid Geometry\"                                           \n [28] \"Valid Geometry\"                                           \n [29] \"Valid Geometry\"                                           \n [30] \"Valid Geometry\"                                           \n [31] \"Valid Geometry\"                                           \n [32] \"Valid Geometry\"                                           \n [33] \"Valid Geometry\"                                           \n [34] \"Valid Geometry\"                                           \n [35] \"Valid Geometry\"                                           \n [36] \"Valid Geometry\"                                           \n [37] \"Valid Geometry\"                                           \n [38] \"Ring Self-intersection[27932.3925999999 21982.7971999999]\"\n [39] \"Ring Self-intersection[26885.4439000003 26668.3121000007]\"\n [40] \"Valid Geometry\"                                           \n [41] \"Valid Geometry\"                                           \n [42] \"Valid Geometry\"                                           \n [43] \"Ring Self-intersection[26920.1689999998 26978.5440999996]\"\n [44] \"Valid Geometry\"                                           \n [45] \"Valid Geometry\"                                           \n [46] \"Valid Geometry\"                                           \n [47] \"Valid Geometry\"                                           \n [48] \"Valid Geometry\"                                           \n [49] \"Valid Geometry\"                                           \n [50] \"Valid Geometry\"                                           \n [51] \"Valid Geometry\"                                           \n [52] \"Valid Geometry\"                                           \n [53] \"Valid Geometry\"                                           \n [54] \"Valid Geometry\"                                           \n [55] \"Valid Geometry\"                                           \n [56] \"Valid Geometry\"                                           \n [57] \"Valid Geometry\"                                           \n [58] \"Valid Geometry\"                                           \n [59] \"Valid Geometry\"                                           \n [60] \"Valid Geometry\"                                           \n [61] \"Valid Geometry\"                                           \n [62] \"Valid Geometry\"                                           \n [63] \"Valid Geometry\"                                           \n [64] \"Valid Geometry\"                                           \n [65] \"Valid Geometry\"                                           \n [66] \"Valid Geometry\"                                           \n [67] \"Valid Geometry\"                                           \n [68] \"Valid Geometry\"                                           \n [69] \"Valid Geometry\"                                           \n [70] \"Valid Geometry\"                                           \n [71] \"Valid Geometry\"                                           \n [72] \"Valid Geometry\"                                           \n [73] \"Valid Geometry\"                                           \n [74] \"Valid Geometry\"                                           \n [75] \"Valid Geometry\"                                           \n [76] \"Valid Geometry\"                                           \n [77] \"Valid Geometry\"                                           \n [78] \"Valid Geometry\"                                           \n [79] \"Valid Geometry\"                                           \n [80] \"Valid Geometry\"                                           \n [81] \"Valid Geometry\"                                           \n [82] \"Valid Geometry\"                                           \n [83] \"Valid Geometry\"                                           \n [84] \"Valid Geometry\"                                           \n [85] \"Valid Geometry\"                                           \n [86] \"Valid Geometry\"                                           \n [87] \"Valid Geometry\"                                           \n [88] \"Valid Geometry\"                                           \n [89] \"Valid Geometry\"                                           \n [90] \"Valid Geometry\"                                           \n [91] \"Valid Geometry\"                                           \n [92] \"Valid Geometry\"                                           \n [93] \"Valid Geometry\"                                           \n [94] \"Valid Geometry\"                                           \n [95] \"Valid Geometry\"                                           \n [96] \"Valid Geometry\"                                           \n [97] \"Ring Self-intersection[14484.6859999998 31330.1319999993]\"\n [98] \"Ring Self-intersection[12861.3828999996 32207.4923]\"      \n [99] \"Valid Geometry\"                                           \n[100] \"Valid Geometry\"                                           \n[101] \"Valid Geometry\"                                           \n[102] \"Valid Geometry\"                                           \n[103] \"Ring Self-intersection[19681.2353999997 31294.4521999992]\"\n[104] \"Valid Geometry\"                                           \n[105] \"Valid Geometry\"                                           \n[106] \"Valid Geometry\"                                           \n[107] \"Valid Geometry\"                                           \n[108] \"Valid Geometry\"                                           \n[109] \"Valid Geometry\"                                           \n[110] \"Valid Geometry\"                                           \n[111] \"Valid Geometry\"                                           \n[112] \"Valid Geometry\"                                           \n[113] \"Valid Geometry\"                                           \n[114] \"Valid Geometry\"                                           \n[115] \"Valid Geometry\"                                           \n[116] \"Valid Geometry\"                                           \n[117] \"Valid Geometry\"                                           \n[118] \"Valid Geometry\"                                           \n[119] \"Valid Geometry\"                                           \n[120] \"Valid Geometry\"                                           \n[121] \"Valid Geometry\"                                           \n[122] \"Valid Geometry\"                                           \n[123] \"Valid Geometry\"                                           \n[124] \"Valid Geometry\"                                           \n[125] \"Valid Geometry\"                                           \n[126] \"Valid Geometry\"                                           \n[127] \"Valid Geometry\"                                           \n[128] \"Valid Geometry\"                                           \n[129] \"Valid Geometry\"                                           \n[130] \"Valid Geometry\"                                           \n[131] \"Valid Geometry\"                                           \n[132] \"Valid Geometry\"                                           \n[133] \"Valid Geometry\"                                           \n[134] \"Valid Geometry\"                                           \n[135] \"Valid Geometry\"                                           \n[136] \"Valid Geometry\"                                           \n[137] \"Valid Geometry\"                                           \n[138] \"Valid Geometry\"                                           \n[139] \"Valid Geometry\"                                           \n[140] \"Valid Geometry\"                                           \n[141] \"Valid Geometry\"                                           \n[142] \"Valid Geometry\"                                           \n[143] \"Valid Geometry\"                                           \n[144] \"Valid Geometry\"                                           \n[145] \"Valid Geometry\"                                           \n[146] \"Valid Geometry\"                                           \n[147] \"Valid Geometry\"                                           \n[148] \"Valid Geometry\"                                           \n[149] \"Valid Geometry\"                                           \n[150] \"Valid Geometry\"                                           \n[151] \"Valid Geometry\"                                           \n[152] \"Valid Geometry\"                                           \n[153] \"Valid Geometry\"                                           \n[154] \"Valid Geometry\"                                           \n[155] \"Valid Geometry\"                                           \n[156] \"Valid Geometry\"                                           \n[157] \"Valid Geometry\"                                           \n[158] \"Valid Geometry\"                                           \n[159] \"Valid Geometry\"                                           \n[160] \"Valid Geometry\"                                           \n[161] \"Valid Geometry\"                                           \n[162] \"Valid Geometry\"                                           \n[163] \"Valid Geometry\"                                           \n[164] \"Valid Geometry\"                                           \n[165] \"Valid Geometry\"                                           \n[166] \"Valid Geometry\"                                           \n[167] \"Valid Geometry\"                                           \n[168] \"Valid Geometry\"                                           \n[169] \"Valid Geometry\"                                           \n[170] \"Valid Geometry\"                                           \n[171] \"Valid Geometry\"                                           \n[172] \"Valid Geometry\"                                           \n[173] \"Valid Geometry\"                                           \n[174] \"Valid Geometry\"                                           \n[175] \"Valid Geometry\"                                           \n[176] \"Valid Geometry\"                                           \n[177] \"Valid Geometry\"                                           \n[178] \"Valid Geometry\"                                           \n[179] \"Valid Geometry\"                                           \n[180] \"Valid Geometry\"                                           \n[181] \"Valid Geometry\"                                           \n[182] \"Valid Geometry\"                                           \n[183] \"Valid Geometry\"                                           \n[184] \"Valid Geometry\"                                           \n[185] \"Valid Geometry\"                                           \n[186] \"Valid Geometry\"                                           \n[187] \"Valid Geometry\"                                           \n[188] \"Valid Geometry\"                                           \n[189] \"Valid Geometry\"                                           \n[190] \"Valid Geometry\"                                           \n[191] \"Valid Geometry\"                                           \n[192] \"Valid Geometry\"                                           \n[193] \"Valid Geometry\"                                           \n[194] \"Valid Geometry\"                                           \n[195] \"Valid Geometry\"                                           \n[196] \"Valid Geometry\"                                           \n[197] \"Valid Geometry\"                                           \n[198] \"Valid Geometry\"                                           \n[199] \"Valid Geometry\"                                           \n[200] \"Valid Geometry\"                                           \n[201] \"Valid Geometry\"                                           \n[202] \"Valid Geometry\"                                           \n[203] \"Valid Geometry\"                                           \n[204] \"Valid Geometry\"                                           \n[205] \"Valid Geometry\"                                           \n[206] \"Valid Geometry\"                                           \n[207] \"Valid Geometry\"                                           \n[208] \"Valid Geometry\"                                           \n[209] \"Valid Geometry\"                                           \n[210] \"Valid Geometry\"                                           \n[211] \"Valid Geometry\"                                           \n[212] \"Valid Geometry\"                                           \n[213] \"Valid Geometry\"                                           \n[214] \"Valid Geometry\"                                           \n[215] \"Valid Geometry\"                                           \n[216] \"Valid Geometry\"                                           \n[217] \"Valid Geometry\"                                           \n[218] \"Valid Geometry\"                                           \n[219] \"Valid Geometry\"                                           \n[220] \"Valid Geometry\"                                           \n[221] \"Valid Geometry\"                                           \n[222] \"Valid Geometry\"                                           \n[223] \"Valid Geometry\"                                           \n[224] \"Valid Geometry\"                                           \n[225] \"Valid Geometry\"                                           \n[226] \"Valid Geometry\"                                           \n[227] \"Valid Geometry\"                                           \n[228] \"Valid Geometry\"                                           \n[229] \"Valid Geometry\"                                           \n[230] \"Valid Geometry\"                                           \n[231] \"Valid Geometry\"                                           \n[232] \"Valid Geometry\"                                           \n[233] \"Valid Geometry\"                                           \n[234] \"Valid Geometry\"                                           \n[235] \"Valid Geometry\"                                           \n[236] \"Valid Geometry\"                                           \n[237] \"Valid Geometry\"                                           \n[238] \"Valid Geometry\"                                           \n[239] \"Valid Geometry\"                                           \n[240] \"Valid Geometry\"                                           \n[241] \"Valid Geometry\"                                           \n[242] \"Valid Geometry\"                                           \n[243] \"Valid Geometry\"                                           \n[244] \"Valid Geometry\"                                           \n[245] \"Valid Geometry\"                                           \n[246] \"Valid Geometry\"                                           \n[247] \"Valid Geometry\"                                           \n[248] \"Valid Geometry\"                                           \n[249] \"Valid Geometry\"                                           \n[250] \"Valid Geometry\"                                           \n[251] \"Valid Geometry\"                                           \n[252] \"Valid Geometry\"                                           \n[253] \"Valid Geometry\"                                           \n[254] \"Valid Geometry\"                                           \n[255] \"Valid Geometry\"                                           \n[256] \"Valid Geometry\"                                           \n[257] \"Valid Geometry\"                                           \n[258] \"Valid Geometry\"                                           \n[259] \"Valid Geometry\"                                           \n[260] \"Valid Geometry\"                                           \n[261] \"Valid Geometry\"                                           \n[262] \"Valid Geometry\"                                           \n[263] \"Valid Geometry\"                                           \n[264] \"Valid Geometry\"                                           \n[265] \"Valid Geometry\"                                           \n[266] \"Valid Geometry\"                                           \n[267] \"Valid Geometry\"                                           \n[268] \"Valid Geometry\"                                           \n[269] \"Valid Geometry\"                                           \n[270] \"Valid Geometry\"                                           \n[271] \"Valid Geometry\"                                           \n[272] \"Valid Geometry\"                                           \n[273] \"Valid Geometry\"                                           \n[274] \"Valid Geometry\"                                           \n[275] \"Valid Geometry\"                                           \n[276] \"Ring Self-intersection[38542.2260999996 44605.4089000002]\"\n[277] \"Valid Geometry\"                                           \n[278] \"Valid Geometry\"                                           \n[279] \"Valid Geometry\"                                           \n[280] \"Valid Geometry\"                                           \n[281] \"Valid Geometry\"                                           \n[282] \"Valid Geometry\"                                           \n[283] \"Valid Geometry\"                                           \n[284] \"Valid Geometry\"                                           \n[285] \"Valid Geometry\"                                           \n[286] \"Valid Geometry\"                                           \n[287] \"Valid Geometry\"                                           \n[288] \"Valid Geometry\"                                           \n[289] \"Valid Geometry\"                                           \n[290] \"Ring Self-intersection[41375.108 40432.8588999994]\"       \n[291] \"Valid Geometry\"                                           \n[292] \"Valid Geometry\"                                           \n[293] \"Valid Geometry\"                                           \n[294] \"Valid Geometry\"                                           \n[295] \"Valid Geometry\"                                           \n[296] \"Valid Geometry\"                                           \n[297] \"Valid Geometry\"                                           \n[298] \"Valid Geometry\"                                           \n[299] \"Valid Geometry\"                                           \n[300] \"Valid Geometry\"                                           \n[301] \"Valid Geometry\"                                           \n[302] \"Valid Geometry\"                                           \n[303] \"Valid Geometry\"                                           \n[304] \"Valid Geometry\"                                           \n[305] \"Valid Geometry\"                                           \n[306] \"Valid Geometry\"                                           \n[307] \"Valid Geometry\"                                           \n[308] \"Valid Geometry\"                                           \n[309] \"Valid Geometry\"                                           \n[310] \"Valid Geometry\"                                           \n[311] \"Valid Geometry\"                                           \n[312] \"Valid Geometry\"                                           \n[313] \"Valid Geometry\"                                           \n[314] \"Valid Geometry\"                                           \n[315] \"Valid Geometry\"                                           \n[316] \"Valid Geometry\"                                           \n[317] \"Valid Geometry\"                                           \n[318] \"Valid Geometry\"                                           \n[319] \"Valid Geometry\"                                           \n[320] \"Valid Geometry\"                                           \n[321] \"Valid Geometry\"                                           \n[322] \"Ring Self-intersection[21702.5623000003 48125.1154999994]\"\n[323] \"Valid Geometry\"                                           \n\n\nThe below code chunk visualises the invalid geometries.\n\n\nCode Chunk\ninvalid_polygons &lt;- mpsz[!st_is_valid(mpsz),]\nplot(invalid_polygons)\n\n\n\n\n\n\n\n\n\nIn addressing the above point, we will use st_buffer() of sf package to compute a 5-metres buffers around the data.\n\n\nCode Chunk\nmpsz &lt;- st_buffer(mpsz, dist = 2)\n\n\n\n\n\nIn inspecting the sf, we noticed that when we sort the column “BUS_STOP_N”, there are two rows that retains the geometry however there are nil indication of the bus stop number and location. Hence both of the rows will be deleted.\nAdditionally, in checking for duplicates through its geometry, we noticed two rows share the geometry. One is named as “YUSEN LOGISTICS” while the other is “yusen logistics”\n\n\nCode Chunk\n# Check for duplicate geometries in the eldercare sf object\nduplicate_busstop &lt;- busstop[duplicated(busstop$geometry), ]\n\n# Display the duplicate geometries if any\nif (nrow(duplicate_busstop) &gt; 0) {\n  print(duplicate_busstop)\n} else {\n  print(\"No duplicate geometries found.\")\n}\n\n\nSimple feature collection with 1 feature and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 42187.23 ymin: 34995.78 xmax: 42187.23 ymax: 34995.78\nProjected CRS: SVY21\n     BUS_STOP_N BUS_ROOF_N        LOC_DESC                  geometry\n3195      96319        NIL YUSEN LOGISTICS POINT (42187.23 34995.78)\n\n\nHence, with the above adjustments and extracting coordinates, the data is cleaned.\n\n\nCode Chunk\nbusstop_cleaned &lt;- busstop %&gt;%\n  filter(LOC_DESC != \"YUSEN LOGISTICS\")  # Delete the specified row\n\n\n\n\nCode Chunk\n# Extract coordinates after filtering\ncoordinates &lt;- st_coordinates(busstop_cleaned)\n\n# Add longitude and latitude columns\nbusstop_cleaned &lt;- busstop_cleaned %&gt;%\n  mutate(longitude = coordinates[, 1],    # First column is longitude\n         latitude = coordinates[, 2]) %&gt;%  \n  select(LOC_DESC, longitude, latitude, geometry) \n\n\n\n\nCode Chunk\nst_crs(busstop_cleaned) &lt;- 3414\n\n\n\n\n\nIn ensuring there are no duplicates the below code chunk was churned. However,we noticed that there is two rows that shares the same geometry. Through closer inspection, we realised that both addresses are formatted differently “117 Bukit Merah View” & “Blk 117 Bukit Merah View #01-205”. Thus, one of the row will be deleted.\n\n\nCode Chunk\n# Check for duplicate geometries in the eldercare sf object\nduplicate_geometries &lt;- eldercare[duplicated(eldercare$geometry), ]\n\n# Display the duplicate geometries if any\nif (nrow(duplicate_geometries) &gt; 0) {\n  print(duplicate_geometries)\n} else {\n  print(\"No duplicate geometries found.\")\n}\n\n\nSimple feature collection with 13 features and 18 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 23147.94 ymin: 29642.15 xmax: 41665.14 ymax: 45761.17\nProjected CRS: SVY21\nFirst 10 features:\n    OBJECTID ADDRESSBLO ADDRESSBUI ADDRESSPOS\n51        51       &lt;NA&gt;       &lt;NA&gt;     190005\n59        59       &lt;NA&gt;       &lt;NA&gt;     190008\n62        62       &lt;NA&gt;       &lt;NA&gt;     731569\n65        65       &lt;NA&gt;       &lt;NA&gt;     540182\n66        66       &lt;NA&gt;       &lt;NA&gt;     523499\n70        70       &lt;NA&gt;       &lt;NA&gt;     151117\n97        97       &lt;NA&gt;       &lt;NA&gt;     560123\n102      102       &lt;NA&gt;       &lt;NA&gt;     312062\n106      106       &lt;NA&gt;       &lt;NA&gt;     560469\n114      114       &lt;NA&gt;       &lt;NA&gt;     151117\n                              ADDRESSSTR ADDRESSTYP DESCRIPTIO HYPERLINK\n51                   5 Beach Rd #02-4915       &lt;NA&gt;       &lt;NA&gt;      &lt;NA&gt;\n59                 Blk 8 North Bridge Rd       &lt;NA&gt;       &lt;NA&gt;      &lt;NA&gt;\n62                Blk 569A Champions Way       &lt;NA&gt;       &lt;NA&gt;      &lt;NA&gt;\n65            Blk 182 Rivervale Crescent       &lt;NA&gt;       &lt;NA&gt;      &lt;NA&gt;\n66               Blk 499C Tampines Ave 9       &lt;NA&gt;       &lt;NA&gt;      &lt;NA&gt;\n70                  117 Bukit Merah View       &lt;NA&gt;       &lt;NA&gt;      &lt;NA&gt;\n97  Blk 123 Ang Mo Kio Avenue 6 #01-4011       &lt;NA&gt;       &lt;NA&gt;      &lt;NA&gt;\n102       62B Lorong 4 Toa Payoh #02-121       &lt;NA&gt;       &lt;NA&gt;      &lt;NA&gt;\n106 Blk 469 Ang Mo Kio Avenue 10 #01-940       &lt;NA&gt;       &lt;NA&gt;      &lt;NA&gt;\n114     Blk 117 Bukit Merah View #01-205       &lt;NA&gt;       &lt;NA&gt;      &lt;NA&gt;\n    LANDXADDRE LANDYADDRE                                                NAME\n51           0          0                      Peace-Connect Cluster Operator\n59           0          0                   PEACE-Connect Senior Group Home@8\n62           0          0                            Care Corner SGH @ WL569A\n65           0          0       COMNET Senior Group Home @ Rivervale Crescent\n66           0          0 Lions Befrienders Senior Group Home @ Tampines 499C\n70           0          0           NTUC Health Senior Group Home @ Henderson\n97           0          0                         AWWA Senior Activity Centre\n102          0          0          Care Corner Senior Activity Centre (TP62B)\n106          0          0   Teck Ghee Senior Activity Centre Branch @ Blk 469\n114          0          0                    Henderson Senior Activity Centre\n    PHOTOURL ADDRESSFLO          INC_CRC FMEL_UPD_D ADDRESSUNI   X_ADDR\n51      &lt;NA&gt;       &lt;NA&gt; A2C058FC1751FFE7 2016-07-28       &lt;NA&gt; 31505.35\n59      &lt;NA&gt;       &lt;NA&gt; D1A1515DCC76C221 2016-07-28       &lt;NA&gt; 31415.01\n62      &lt;NA&gt;       &lt;NA&gt; 4DC6800EF15E4B70 2016-07-28       &lt;NA&gt; 23147.94\n65      &lt;NA&gt;       &lt;NA&gt; 6BB0D76986F8C512 2016-07-28       &lt;NA&gt; 36446.37\n66      &lt;NA&gt;       &lt;NA&gt; 5DB6B9F0F16BCA80 2016-07-28       &lt;NA&gt; 41665.14\n70      &lt;NA&gt;       &lt;NA&gt; 7FF38742987329FE 2016-07-28       &lt;NA&gt; 26715.04\n97      &lt;NA&gt;       &lt;NA&gt; B275FB9D13DD7091 2016-07-28       &lt;NA&gt; 29261.31\n102     &lt;NA&gt;       &lt;NA&gt; 13908BA85D11F6DC 2016-07-28       &lt;NA&gt; 29998.73\n106     &lt;NA&gt;       &lt;NA&gt; 8F9B74A5A73579D9 2016-07-28       &lt;NA&gt; 30594.50\n114     &lt;NA&gt;       &lt;NA&gt; 7FF387421A0883F4 2016-07-28       &lt;NA&gt; 26715.04\n      Y_ADDR                  geometry\n51  31853.52 POINT (31505.35 31853.52)\n59  31880.06 POINT (31415.01 31880.06)\n62  45761.17 POINT (23147.94 45761.17)\n65  41376.90  POINT (36446.37 41376.9)\n66  37956.92 POINT (41665.14 37956.92)\n70  29642.15 POINT (26715.04 29642.15)\n97  39169.61 POINT (29261.31 39169.61)\n102 35335.92 POINT (29998.73 35335.92)\n106 38507.25  POINT (30594.5 38507.25)\n114 29642.15 POINT (26715.04 29642.15)\n\n\nThe code below selects the necessary variables of ease of manipulation\n\n\nCode Chunk\neldercare_cleaned &lt;- eldercare %&gt;% \n  filter(INC_CRC != \"7FF38742987329FE\") %&gt;%  #deletes the duplicated row\n  rename(longitude = X_ADDR) %&gt;% \n  rename(latitude = Y_ADDR) %&gt;% \n  select(NAME, longitude, latitude, geometry)\n\n\n\n\nCode Chunk\nst_crs(eldercare_cleaned) &lt;- 3414\n\n\n\n\n\nThe code chunk below checks if there are any duplicates geometries and it returns nil.\n\n\nCode Chunk\nduplicate_geometries &lt;- hawker[duplicated(st_geometry(hawker)), ]\n\n# Display duplicate geometries if any\nif (nrow(duplicate_geometries) &gt; 0) {\n  print(duplicate_geometries)\n} else {\n  print(\"No duplicate geometries found.\")\n}\n\n\n[1] \"No duplicate geometries found.\"\n\n\nAs the CRS of hawker sf is in WGS84, st_transform() is then used to transform it to SVY21.\n\n\nCode Chunk\nhawker &lt;- st_transform(hawker, 3414)\n\n\nThe data is then split into longitude and latitude and selected columns were chosen.\n\n\nCode Chunk\ncoordinates &lt;- st_coordinates(hawker)\nhawker_cleaned &lt;- hawker %&gt;%\n  mutate(longitude = coordinates[, 1],    # First column is longitude\n         latitude = coordinates[, 2]) %&gt;%  \n  select(longitude, latitude, geometry)\n\n\n\n\n\nThe code chunk below checks if there are any duplicates geometries and it returns nil.\n\n\nCode Chunk\nduplicate_kindergarten &lt;- kindergarten[duplicated(st_geometry(kindergarten)), ]\n\n# Display duplicate geometries if any\nif (nrow(duplicate_kindergarten) &gt; 0) {\n  print(duplicate_kindergarten)\n} else {\n  print(\"No duplicate geometries found.\")\n}\n\n\nSimple feature collection with 66 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 12882.62 ymin: 25596.33 xmax: 41916.49 ymax: 48384.34\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n       Name\n271 kml_271\n284 kml_284\n288 kml_288\n291 kml_291\n349 kml_349\n354 kml_354\n355 kml_355\n356 kml_356\n358 kml_358\n360 kml_360\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Description\n271                                     &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;423731&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;252 Tembeling Road  #02-07 S(423731)&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Kindergartens&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;Seeds D' Learning House&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;30BE9FB7740C8654&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20211201143423&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n284                                 &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;287994&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;200 Turf Club Road   S(287994)&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Kindergartens&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;Swallows And Amazons Kindergarten&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;AC4EDF1099057112&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20211201143423&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n288 &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;287994&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;200, TURF CLUB ROAD, #06-08,THE GRANDSTAND, (S)287994&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Kindergartens&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;The Little House (Montessori) Kindergarten&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;AC4EDF101AAB7C05&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20211201143423&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n291                              &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;760102&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;102 Yishun Avenue 5  #03-115 S(760102)&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Kindergartens&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;Tots Town Preschool @ Yishun&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;0CEB47AFBABB448C&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20211201143423&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n349                                          &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;579792&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;4 Bishan Street 13   S(579792)&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Kindergartens&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;Zion Bishan Kindergarten&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;2809ADC0B5232EF6&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20211201143423&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n354                                      &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;519420&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;4 Pasir Ris Drive 6&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Kindergartens&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;Pentecost Methodist Church Kindergarten&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;8C80D608E5D43802&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20211201143423&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n355                                                           &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;299574&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;1 Dunearn Close&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Kindergartens&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;Pibos Garden Preschool&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;7F8CBC68533FFB92&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20211201143423&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n356                                                  &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;423731&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;252 Tembeling Road #01-07&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Kindergartens&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;Pink Tower Montessori&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;30BE9FB7FCB667DE&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20211201143423&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n358                                                 &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;522497&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;497C Tampines Street 45 #01-54&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Kindergartens&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;Prodigy Preschool&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;724ECBCAF5159E3E&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20211201143423&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n360                                               &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;730408&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;408 Woodlands Street 41&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Kindergartens&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;Putra - Putri Kindergarten&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;BA7D15B0567CD551&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20211201143423&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n                         geometry\n271  POINT Z (36012.9 32087.99 0)\n284 POINT Z (23527.86 35684.78 0)\n288 POINT Z (23527.86 35684.78 0)\n291 POINT Z (27493.03 45816.02 0)\n349 POINT Z (30399.37 36579.97 0)\n354 POINT Z (41780.92 39252.46 0)\n355 POINT Z (25915.86 33987.34 0)\n356  POINT Z (36012.9 32087.99 0)\n358 POINT Z (41916.49 37814.51 0)\n360 POINT Z (21217.24 45620.86 0)\n\n\n\n\nCode Chunk\nkindergarten &lt;- st_transform(kindergarten, 3414)\n\n\nThe data is then split into longitude and latitude and selected columns were chosen.\n\n\nCode Chunk\ncoordinates &lt;- st_coordinates(kindergarten)\nkindergarten_cleaned &lt;- kindergarten %&gt;%\n  mutate(longitude = coordinates[, 1],    # First column is longitude\n         latitude = coordinates[, 2]) %&gt;%  \n  select(longitude, latitude, geometry)\n\n\n\n\n\nDespite importing the data and ensuring that postal code is in character form, there are still incomplete postal code as seen in the below code.\n\n\nCode Chunk\nlist(school$postal_code)\n\n\n[[1]]\n  [1] \"738907\" \"737916\" \"768643\" \"768928\" \"579646\" \"159016\" \"544969\" \"569785\"\n  [9] \"569206\" \"569843\" \"569920\" \"569362\" \"487012\" \"139745\" \"309919\" \"139650\"\n [17] \"227988\" \"309918\" \"529366\" \"678117\" \"679697\" \"538403\" \"679944\" \"318990\"\n [25] \"469317\" \"469722\" \"468585\" \"469293\" \"339948\" \"327919\" \"109100\" \"649930\"\n [33] \"648354\" \"547529\" \"399935\" \"658962\" \"159050\" \"689809\" \"679676\" \"598112\"\n [41] \"659634\" \"659633\" \"757714\" \"757699\" \"387621\" \"88256\"  \"518935\" \"579767\"\n [49] \"297822\" \"349692\" \"349700\" \"529894\" \"529896\" \"424821\" \"99757\"  \"449150\"\n [57] \"659401\" \"558979\" \"534793\" \"679287\" \"319765\" \"319764\" \"548595\" \"569405\"\n [65] \"99138\"  \"768959\" \"529392\" \"737924\" \"689905\" \"688845\" \"439012\" \"768547\"\n [73] \"129903\" \"129904\" \"608784\" \"545083\" \"689814\" \"648347\" \"247961\" \"609561\"\n [81] \"479226\" \"479229\" \"659441\" \"689285\" \"569277\" \"659204\" \"436895\" \"529093\"\n [89] \"529258\" \"529231\" \"828869\" \"828814\" \"518866\" \"757521\" \"573838\" \"738908\"\n [97] \"738489\" \"139648\" \"139649\" \"217567\" \"469680\" \"797538\" \"797701\" \"319252\"\n[105] \"648200\" \"739063\" \"739062\" \"609647\" \"649410\" \"158901\" \"159561\" \"389706\"\n[113] \"389705\" \"529176\" \"828848\" \"828866\" \"677744\" \"679938\" \"737942\" \"579807\"\n[121] \"519421\" \"427072\" \"278790\" \"659250\" \"534786\" \"536451\" \"327829\" \"828819\"\n[129] \"534238\" \"534256\" \"649371\" \"768857\" \"269734\" \"737888\" \"768515\" \"569228\"\n[137] \"528906\" \"528933\" \"688258\" \"609476\" \"618652\" \"648368\" \"648348\" \"609790\"\n[145] \"649038\" \"659762\" \"127368\" \"319580\" \"399772\" \"689189\" \"689333\" \"579793\"\n[153] \"579795\" \"618310\" \"659243\" \"519073\" \"408931\" \"408940\" \"368051\" \"738927\"\n[161] \"739110\" \"297754\" \"569948\" \"569384\" \"828867\" \"518798\" \"518901\" \"599986\"\n[169] \"599986\" \"658965\" \"538786\" \"538785\" \"545079\" \"545080\" \"129956\" \"128806\"\n[177] \"288683\" \"556111\" \"268097\" \"288913\" \"769028\" \"768689\" \"148812\" \"139657\"\n[185] \"449149\" \"529283\" \"545088\" \"768960\" \"544974\" \"545081\" \"768692\" \"769026\"\n[193] \"768578\" \"327830\" \"757622\" \"828671\" \"129957\" \"828716\" \"458436\" \"768454\"\n[201] \"168622\" \"544822\" \"519524\" \"518934\" \"518968\" \"529400\" \"536741\" \"536742\"\n[209] \"319320\" \"597610\" \"797702\" \"129857\" \"556094\" \"575566\" \"768687\" \"649076\"\n[217] \"529067\" \"569845\" \"659163\" \"828674\" \"828772\" \"538787\" \"828870\" \"828845\"\n[225] \"128104\" \"738525\" \"149303\" \"148800\" \"149295\" \"99840\"  \"289072\" \"318871\"\n[233] \"575954\" \"469719\" \"689621\" \"649961\" \"237993\" \"737803\" \"738524\" \"545092\"\n[241] \"555855\" \"649295\" \"138572\" \"227968\" \"757715\" \"757704\" \"545166\" \"545090\"\n[249] \"797636\" \"555889\" \"534237\" \"649332\" \"739067\" \"309437\" \"309437\" \"737913\"\n[257] \"689762\" \"737758\" \"544799\" \"529593\" \"357691\" \"359337\" \"359342\" \"469701\"\n[265] \"469700\" \"556742\" \"556140\" \"529706\" \"528986\" \"308274\" \"309331\" \"387724\"\n[273] \"259240\" \"429058\" \"455789\" \"659322\" \"518799\" \"529565\" \"529426\" \"529427\"\n[281] \"439272\" \"437259\" \"438796\" \"449761\" \"569299\" \"688261\" \"479239\" \"469278\"\n[289] \"469300\" \"465561\" \"569730\" \"688268\" \"689143\" \"828728\" \"449035\" \"448880\"\n[297] \"828802\" \"757702\" \"649223\" \"679946\" \"677737\" \"677742\" \"649188\" \"648350\"\n[305] \"519075\" \"579747\" \"738079\" \"738990\" \"738853\" \"738240\" \"738239\" \"739111\"\n[313] \"538882\" \"649036\" \"538784\" \"538789\" \"768611\" \"556108\" \"689100\" \"538720\"\n[321] \"569868\" \"768675\" \"768679\" \"768516\" \"768610\" \"469623\" \"618654\" \"609558\"\n[329] \"649406\" \"529393\" \"658712\" \"538884\" \"169485\" \"679002\" \"677741\" \"556095\"\n[337] \"556123\"\n\n\nThe below code chunk churned out 3 incomplete postal codes.\n\n\nCode Chunk\n# Display postal codes that are not 6 digits\ninvalid_postal_codes &lt;- school %&gt;%\n  filter(nchar(as.character(postal_code)) != 6) %&gt;%\n  select(address, postal_code)\n\n# Show the result\nprint(invalid_postal_codes)\n\n\n# A tibble: 4 × 2\n  address                  postal_code\n  &lt;chr&gt;                    &lt;chr&gt;      \n1 1    Cantonment Close    88256      \n2 1    Bukit Teresa Road   99757      \n3 160  LOWER DELTA ROAD    99138      \n4 1    BUKIT PURMEI AVENUE 99840      \n\n\nThe correct postal code has been changed with the assistance of Google Maps and it piped back to the main df school.\n\n\nCode Chunk\nschool &lt;- school %&gt;% \n  mutate(postal_code = ifelse(postal_code == \"88256\", \"088256\", postal_code)) %&gt;% \n  mutate(postal_code = ifelse(postal_code == \"99757\", \"099757\", postal_code)) %&gt;% \n  mutate(postal_code = ifelse(postal_code == \"99840\", \"099840\", postal_code))\n\n\n\n\nCode Chunk\nschool &lt;- school %&gt;% \n  select(school_name, address, postal_code, mainlevel_code) %&gt;% \n  filter(mainlevel_code ==\"PRIMARY\")\n\n\n\n\nCode Chunk\npostal_list &lt;- sort(unique(school$postal_code)) #parse a list as API cannot read df\n#unique reduces records to pass to portal\n#sort is used to easier to find geo codes\n\n\nFetching Data from onemap API\nAs the primary schools lack of geometry coordinates, API was used to extract the coordinates.\nThis will return WGS84 xy coordinates alongside SVY21 xy coordinates.\n\n\nCode Chunk\nget_coords &lt;- function(postal_list){\n  # Create a data frame to store all retrieved coordinates\n  postal_coords &lt;- data.frame()\n    \n  for (postal in postal_list){\n    r &lt;- GET('https://www.onemap.gov.sg/api/common/elastic/search?',\n           query=list(searchVal=postal,\n                     returnGeom='Y',\n                     getAddrDetails='Y'))\n    data &lt;- fromJSON(rawToChar(r$content))\n    found &lt;- data$found\n    res &lt;- data$results\n    \n    # Create a new data frame for each postal code\n    new_row &lt;- data.frame()\n    \n    # If single result, append \n    if (found == 1){\n      postal_code &lt;- res$POSTAL \n      lat &lt;- res$LATITUDE\n      lng &lt;- res$LONGITUDE\n      new_row &lt;- data.frame(postal_code = postal, \n                           postal_found = postal_code, \n                           latitude_wgs84 = lat,\n                           longitude_wgs84 = lng)\n    }\n    \n    # If multiple results, use the exact postal code match\n    else if (found &gt; 1){\n      # Find exact match for postal code\n      res_match &lt;- res[res$POSTAL == postal, ]\n      \n      # If exact match found, use it\n      if (nrow(res_match) &gt; 0) {\n        postal_code &lt;- res_match$POSTAL[1]\n        lat &lt;- res_match$LATITUDE[1]\n        lng &lt;- res_match$LONGITUDE[1]\n        new_row &lt;- data.frame(postal_code = postal,\n                             postal_found = postal_code,\n                             latitude_wgs84 = lat,\n                             longitude_wgs84 = lng)\n      }\n      # If no exact match, set as NA\n      else {\n        new_row &lt;- data.frame(postal_code = postal,\n                             postal_found = NA,\n                             latitude_wgs84 = NA,\n                             longitude_wgs84 = NA)\n      }\n    }\n    # If no results found\n    else {\n      new_row &lt;- data.frame(postal_code = postal,\n                           postal_found = NA,\n                           latitude_wgs84 = NA,\n                           longitude_wgs84 = NA)\n    }\n    \n    # Add the row\n    postal_coords &lt;- rbind(postal_coords, new_row)\n  }\n  \n  # Convert to sf object with WGS84 coordinates (EPSG:4326)\n  # Filter out rows with NA coordinates first\n  valid_coords &lt;- postal_coords[!is.na(postal_coords$latitude_wgs84) & \n                              !is.na(postal_coords$longitude_wgs84), ]\n  \n  if(nrow(valid_coords) &gt; 0) {\n    coords_sf &lt;- st_as_sf(valid_coords, \n                         coords = c(\"longitude_wgs84\", \"latitude_wgs84\"),\n                         crs = 4326)\n    \n    # Transform to SVY21 (EPSG:3414)\n    coords_svy21 &lt;- st_transform(coords_sf, 3414)\n    \n    # Extract coordinates\n    coords_matrix &lt;- st_coordinates(coords_svy21)\n    \n    # Add SVY21 coordinates back to the original dataframe\n    valid_coords$longitude &lt;- coords_matrix[, 1]  # SVY21 X coordinate\n    valid_coords$latitude &lt;- coords_matrix[, 2]   # SVY21 Y coordinate\n    \n    # Add geometry column\n    valid_coords$geometry &lt;- st_geometry(coords_svy21)\n    \n    # Merge back with rows that had NA coordinates\n    result &lt;- merge(postal_coords, \n                   valid_coords[c(\"postal_code\", \"longitude\", \"latitude\", \"geometry\")], \n                   by = \"postal_code\", all.x = TRUE)\n  } else {\n    # If no valid coordinates, add empty SVY21 columns\n    result &lt;- postal_coords\n    result$longitude &lt;- NA\n    result$latitude &lt;- NA\n    result$geometry &lt;- NA\n  }\n  \n  return(result)\n}\n\n# Usage example:\nadd_list_school &lt;- sort(unique(school$postal_code))\ncoords_school &lt;- get_coords(add_list_school)\nprint(head(coords_school))\n\n\n  postal_code postal_found   latitude_wgs84  longitude_wgs84 longitude latitude\n1      088256       088256 1.27547252623201 103.839962631748  28739.43 28660.79\n2      099757       099757   1.275022722964 103.828157564896  27425.62 28611.06\n3      099840       099840 1.27489701529771 103.824115781658  26975.80 28597.16\n4      109100       109100 1.27612047924037 103.808628535239  25252.19 28732.45\n5      128104       128104  1.3132658326807 103.756629101811  19465.19 32839.91\n6      128806       128806 1.31920159956387 103.761095065761  19962.23 33496.24\n                   geometry\n1 POINT (28739.43 28660.79)\n2 POINT (27425.62 28611.06)\n3  POINT (26975.8 28597.16)\n4 POINT (25252.19 28732.45)\n5 POINT (19465.19 32839.91)\n6 POINT (19962.23 33496.24)\n\n\nCode Chunk\n# Optional: Convert result to sf object for spatial operations\ncoords_school_sf &lt;- st_as_sf(coords_school[!is.na(coords_school$geometry), ])\n\n\nBelow is the code chunk that populates the coordinates in longitude, latitude and postal code against the address in the add_list.\n\n\nCode Chunk\ncoords_school &lt;- get_coords(postal_list)\nprint(coords_school)\n\n\n    postal_code postal_found   latitude_wgs84  longitude_wgs84 longitude\n1        088256       088256 1.27547252623201 103.839962631748  28739.43\n2        099757       099757   1.275022722964 103.828157564896  27425.62\n3        099840       099840 1.27489701529771 103.824115781658  26975.80\n4        109100       109100 1.27612047924037 103.808628535239  25252.19\n5        128104       128104  1.3132658326807 103.756629101811  19465.19\n6        128806       128806 1.31920159956387 103.761095065761  19962.23\n7        129857       129857 1.31666529308968 103.767438999533  20668.24\n8        129903       129903 1.31506321827875 103.763144493687  20190.30\n9        139648       139648 1.30100441916733 103.785455606827  22673.28\n10       148812       148812 1.29981138884057 103.799964819895  24288.03\n11       149303       149303 1.29552905587752 103.807648475656  25143.14\n12       158901       158901 1.28559516494962 103.815547410064  26022.22\n13       159016       159016 1.29133439161334 103.824424680531  27010.19\n14       169485       169485 1.28421153855474 103.825951884637  27180.15\n15       217567       217567 1.31236867681371 103.850766402965  29941.78\n16       227988       227988 1.30935041274966 103.840950265464  28849.34\n17       237993       237993 1.29418347842837 103.836018941571  28300.53\n18       268097       268097 1.32107094241153 103.807681852799  25146.89\n19       278790       278790 1.31667646178347 103.784296227747  22544.29\n20       289072       289072 1.33004178068277 103.806397828938  25004.00\n21       297754       297754 1.34043840661368 103.839811736775  28722.62\n22       309331       309331 1.31781476118551 103.845633334522  29370.51\n23       309437       309437  1.3206340835184 103.828164966953  27426.45\n24       309918       309918 1.31837054523521 103.835609732354  28254.98\n25       319252       319252 1.34032299499938 103.855529906182  30471.88\n26       319320       319320 1.33677864390672 103.855341310212  30450.90\n27       319580       319580  1.3373381935639 103.847148501824  29539.12\n28       319765       319765 1.33275264711587 103.841847263786  28949.15\n29       327829       327829 1.32175999964553 103.857628468881  30705.45\n30       339948       339948 1.32181250780475 103.865404167629  31570.81\n31       349700       349700 1.33566084356265  103.87562006259  32707.71\n32       359337       359337 1.33140255308868 103.865117134385  31538.85\n33       387621       387621 1.32664424544839 103.882227964852  33443.12\n34       387724       387724 1.32439150290682 103.881624837373  33376.00\n35       389706       389706 1.31814416827374 103.883628501601  33599.00\n36       399772       399772 1.31099674009193 103.888352029067  34124.70\n37       408931       408931 1.32858023873554 103.901306904157  35566.41\n38       424821       424821 1.30648535277087 103.911105620496  36656.98\n39       427072       427072 1.31194344271383 103.902902557195  35744.04\n40       437259       437259 1.30498489044822 103.899999965407  35421.03\n41       449149       449149 1.30553224180437 103.917570005713  37376.41\n42       449761       449761 1.30528528687321 103.911553152326  36706.79\n43       455789       455789 1.31878997295135 103.917258129598  37341.65\n44       458436       458436 1.31996876377748 103.923752668632  38064.43\n45       469300       469300 1.31720341663064  103.94575943784  40513.58\n46       469317       469317 1.32344593287992 103.937878976352  39636.53\n47       469623       469623 1.33398037894072 103.932015036296  38983.89\n48       469680       469680 1.32980598760785 103.931710293058  38949.99\n49       469701       469701  1.3347253730189 103.941234868202  40009.96\n50       469719       469719 1.33405659580859 103.934317528484  39240.13\n51       479226       479226 1.33524602829747 103.921286435321  37789.90\n52       479239       479239 1.33109611001931 103.911005345855  36645.74\n53       518798       518798 1.37550677054835 103.934953276155  39310.69\n54       518866       518866 1.37505696315817 103.945289416873  40460.98\n55       518935       518935 1.37246063927999 103.957020286115  41766.50\n56       518968       518968 1.37245132722087 103.962922699031  42423.37\n57       519075       519075 1.36563610941695 103.960861814678  42194.05\n58       519524       519524 1.37801687647812   103.9392021807  39783.53\n59       528906       528906 1.34787074766746 103.939221709073  39785.85\n60       529067       529067 1.35765121877679 103.935246486174  39343.40\n61       529176       529176 1.35711606790292 103.949144797277  40890.13\n62       529258       529258 1.35268400746357 103.961676849165  42284.83\n63       529366       529366 1.34828400809545 103.951482746538  41150.37\n64       529392       529392 1.35061130568554 103.951317297552  41131.94\n65       529393       529393 1.35131569890862 103.950551091373  41046.67\n66       529426       529426  1.3504863946694 103.943573098509  40270.10\n67       529565       529565 1.36048786025607 103.948768900518  40848.28\n68       529706       529706 1.34967978969837 103.937016363397  39540.41\n69       529896       529896 1.34023163951633 103.952080114035  41216.89\n70       534238       534238 1.37738245231461 103.880805274557  33284.68\n71       534793       534793 1.37330297140128 103.897575728303  35151.03\n72       536451       536451 1.36693830877349 103.894114899795  34765.90\n73       536741       536741  1.3502545600894 103.884846876112  33734.52\n74       538720       538720 1.37793264345276 103.885643682171  33823.13\n75       538784       538784 1.37176798317468  103.88280991785  33507.78\n76       538786       538786 1.37362445234082 103.889741457901  34279.17\n77       538787       538787 1.37820062547641 103.894667159512  34827.33\n78       538882       538882 1.35799653782463 103.890216355459  34332.07\n79       544799       544799 1.39528544463377 103.889416413501  34242.94\n80       544822       544822 1.38376923022123 103.891354675017  34458.68\n81       544969       544969 1.39036998654612 103.887165375933  33992.45\n82       544974       544974 1.38255066171151  103.89626457548  35005.09\n83       545080       545080 1.39221149906592 103.891180928186  34439.32\n84       545088       545088 1.38706146494859 103.903202683028  35777.20\n85       545092       545092 1.39333506298213 103.904545253288  35926.59\n86       545166       545166 1.38925044778224 103.899527879229  35368.24\n87       555855       555855  1.3728583654058  103.87477170399  32613.23\n88       556095       556095 1.36026072476019 103.869712517383  32050.22\n89       556108       556108  1.3489768083841 103.868467129954  31911.64\n90       556742       556742 1.34930766795555 103.862309899986  31226.41\n91       558979       558979 1.35742952269615 103.864009397373  31415.53\n92       569228       569228 1.37196445720109 103.851763531681  30052.70\n93       569299       569299 1.36565018546903 103.851009800453  29968.82\n94       569730       569730 1.35994649477568 103.853768994775  30275.89\n95       569785       569785 1.38419941907925 103.841411716006  28900.66\n96       569920       569920 1.36932176584608 103.839630858752  28702.48\n97       569948       569948 1.37614621670476 103.835805246743  28276.74\n98       579646       579646  1.3605834338904 103.833020333986  27966.81\n99       579793       579793 1.34939813669536 103.855018233311  30414.93\n100      597610       597610 1.33807302121151 103.776250903095  21648.98\n101      598112       598112  1.3377498622588 103.766855344779  20603.35\n102      599986       599986 1.33266213599636 103.783382114018  22442.59\n103      609476       609476 1.34843859698946 103.733156566858  16853.08\n104      609558       609558 1.34291981622016 103.740861148879  17710.49\n105      609647       609647 1.33659726465816 103.736089481883  17179.43\n106      618310       618310 1.33820397704287 103.718089546664  15176.23\n107      648200       648200 1.33641298743234  103.69970173931  13129.85\n108      648347       648347 1.35148548772662 103.707578404944  14006.52\n109      648368       648368 1.33917519796104 103.698803732244  13029.92\n110      649036       649036 1.34270639669225 103.687588478681  11781.81\n111      649076       649076 1.34883617453427 103.695003812477  12607.09\n112      649188       649188  1.3477109654722 103.700467520791  13215.14\n113      649223       649223 1.34471211768915 103.698964212988  13047.82\n114      649295       649295 1.34673868889096 103.718457955487  15217.27\n115      649332       649332 1.34686967259067 103.721559387203  15562.43\n116      649930       649930 1.34284013791617 103.712858381937  14594.08\n117      659163       659163 1.34919098857585 103.740719208011  17694.72\n118      659243       659243 1.35424191127295 103.754165828711  19191.20\n119      659401       659401 1.36411948475833 103.749251919049  18644.37\n120      659441       659441  1.3591797447277  103.74847210784  18557.57\n121      659634       659634 1.34586054788695  103.75366641455  19135.59\n122      659762       659762 1.34552849960874 103.756449399351  19445.30\n123      677742       677742 1.38378546546228 103.760273432258  19871.00\n124      677744       677744  1.3858892193692 103.767794469258  20708.01\n125      679002       679002 1.37942873617052  103.76970317201  20920.40\n126      679287       679287   1.366603668561 103.767412204602  20665.41\n127      679676       679676 1.37350089986584 103.769417337654  20888.58\n128      679944       679944 1.38394936211823 103.773632022975  21357.65\n129      679946       679946 1.38917971808454  103.76639772288  20552.58\n130      688261       688261 1.38388660469396 103.753924558586  19164.46\n131      688268       688268 1.40278303219036 103.746801233973  18371.80\n132      689100       689100 1.39672603497195 103.751817992962  18930.07\n133      689189       689189 1.39356050497104 103.747362720939  18434.25\n134      689285       689285  1.3947078296399 103.743201812831  17971.20\n135      689762       689762 1.38143123987698    103.747153671  18410.93\n136      689814       689814 1.38026516111464 103.736554295636  17231.35\n137      689905       689905 1.37775890093367 103.741832917421  17818.78\n138      737803       737803 1.44700426493785 103.801889031334  24502.39\n139      737888       737888 1.42896700684852 103.790607539185  23246.90\n140      737942       737942 1.43989958331782 103.804725587384  24818.04\n141      738079       738079 1.43273620725049  103.79023031392  23204.93\n142      738240       738240 1.43485179380236  103.79741434351  24004.40\n143      738525       738525 1.44203596557592 103.788339762066  22994.56\n144      738853       738853 1.43658270720199 103.791791282236  23378.65\n145      738907       738907  1.4426347903311 103.800040119743  24296.63\n146      738908       738908 1.44414835451585 103.794545295922  23685.14\n147      738927       738927 1.43383904037059 103.773643013846  21359.01\n148      739063       739063 1.43047990810126 103.778192944298  21865.34\n149      739067       739067 1.43240935732525 103.786035065192  22738.06\n150      757521       757521 1.45388170417263 103.817174701639  26203.45\n151      757622       757622 1.45732867302094 103.814075611175  25858.57\n152      757702       757702 1.45168517614776 103.822516208087  26797.87\n153      757714       757714 1.45125024371543 103.815858858929  26057.01\n154      757715       757715  1.4457453096024 103.821152676215  26646.13\n155      768515       768515 1.42768847665227 103.830425490093  27678.04\n156      768611       768611 1.43347186925064 103.837755319448  28493.74\n157      768643       768643 1.43315271543517 103.832942401086  27958.14\n158      768679       768679 1.43397617334964 103.834050796658  28081.48\n159      768687       768687 1.41741627551251 103.830143209875  27646.63\n160      768857       768857 1.42697284512936 103.844240226564  29215.42\n161      768959       768959 1.43839562897006 103.839309173817  28666.66\n162      768960       768960 1.42745159462566 103.848378442893  29675.94\n163      769026       769026 1.42130404159807 103.840792923345  28831.79\n164      769028       769028 1.41590195697649 103.839100590207  28643.46\n165      797538       797538 1.39726966050479 103.880330305684  33231.78\n166      797636       797636  1.3925400930811 103.874984869551  32636.91\n167      797701       797701 1.39030227963797 103.874445458032  32576.88\n168      828671       828671 1.41842322079348 103.905147201163  35993.49\n169      828674       828674 1.41158455562454  103.89890448429  35298.79\n170      828716       828716 1.40507184758584 103.911200193835  36667.16\n171      828728       828728 1.40744168163777 103.898762173662  35282.97\n172      828772       828772 1.40172590450528 103.898794262307  35286.55\n173      828802       828802 1.39889261226315 103.918585924582  37489.11\n174      828819       828819   1.399550034492 103.913404855665  36912.52\n175      828845       828845 1.40505250226058 103.905299026568  36010.44\n176      828848       828848 1.39648195114256  103.91233462071  36793.43\n177      828867       828867 1.39485356081865  103.90857257736  36374.77\n178      828869       828869 1.40009139510996 103.907847599114  36294.07\n    latitude                  geometry\n1   28660.79 POINT (28739.43 28660.79)\n2   28611.06 POINT (27425.62 28611.06)\n3   28597.16  POINT (26975.8 28597.16)\n4   28732.45 POINT (25252.19 28732.45)\n5   32839.91 POINT (19465.19 32839.91)\n6   33496.24 POINT (19962.23 33496.24)\n7   33215.77 POINT (20668.24 33215.77)\n8   33038.64  POINT (20190.3 33038.64)\n9   31484.03 POINT (22673.28 31484.03)\n10  31352.08 POINT (24288.03 31352.08)\n11  30878.55 POINT (25143.14 30878.55)\n12  29780.11 POINT (26022.22 29780.11)\n13  30414.72 POINT (27010.19 30414.72)\n14  29627.11 POINT (27180.15 29627.11)\n15  32740.58 POINT (29941.78 32740.58)\n16  32406.84 POINT (28849.34 32406.84)\n17  30729.75 POINT (28300.53 30729.75)\n18  33702.84 POINT (25146.89 33702.84)\n19  33216.96 POINT (22544.29 33216.96)\n20  34694.80     POINT (25004 34694.8)\n21  35844.39 POINT (28722.62 35844.39)\n22  33342.78 POINT (29370.51 33342.78)\n23  33654.52 POINT (27426.45 33654.52)\n24  33404.23 POINT (28254.98 33404.23)\n25  35831.63 POINT (30471.88 35831.63)\n26  35439.72  POINT (30450.9 35439.72)\n27  35501.58 POINT (29539.12 35501.58)\n28  34994.53 POINT (28949.15 34994.53)\n29  33779.04 POINT (30705.45 33779.04)\n30  33784.85 POINT (31570.81 33784.85)\n31  35316.15 POINT (32707.71 35316.15)\n32  34845.27 POINT (31538.85 34845.27)\n33  34319.15 POINT (33443.12 34319.15)\n34  34070.05    POINT (33376 34070.05)\n35  33379.26    POINT (33599 33379.26)\n36  32588.94  POINT (34124.7 32588.94)\n37  34533.27 POINT (35566.41 34533.27)\n38  32090.16 POINT (36656.98 32090.16)\n39  32693.67 POINT (35744.04 32693.67)\n40  31924.22 POINT (35421.03 31924.22)\n41  31984.80  POINT (37376.41 31984.8)\n42  31957.47 POINT (36706.79 31957.47)\n43  33450.77 POINT (37341.65 33450.77)\n44  33581.14 POINT (38064.43 33581.14)\n45  33275.46 POINT (40513.58 33275.46)\n46  33965.69 POINT (39636.53 33965.69)\n47  35130.51 POINT (38983.89 35130.51)\n48  34668.92 POINT (38949.99 34668.92)\n49  35212.93 POINT (40009.96 35212.93)\n50  35138.95 POINT (39240.13 35138.95)\n51  35270.41  POINT (37789.9 35270.41)\n52  34811.50  POINT (36645.74 34811.5)\n53  39722.31 POINT (39310.69 39722.31)\n54  39672.62 POINT (40460.98 39672.62)\n55  39385.60   POINT (41766.5 39385.6)\n56  39384.60  POINT (42423.37 39384.6)\n57  38630.99 POINT (42194.05 38630.99)\n58  39999.88 POINT (39783.53 39999.88)\n59  36666.47 POINT (39785.85 36666.47)\n60  37747.93  POINT (39343.4 37747.93)\n61  37688.82 POINT (40890.13 37688.82)\n62  37198.82 POINT (42284.83 37198.82)\n63  36712.23 POINT (41150.37 36712.23)\n64  36969.57 POINT (41131.94 36969.57)\n65  37047.45 POINT (41046.67 37047.45)\n66  36955.72  POINT (40270.1 36955.72)\n67  38061.66 POINT (40848.28 38061.66)\n68  36866.50  POINT (39540.41 36866.5)\n69  35821.84 POINT (41216.89 35821.84)\n70  39929.52 POINT (33284.68 39929.52)\n71  39478.48 POINT (35151.03 39478.48)\n72  38774.69  POINT (34765.9 38774.69)\n73  36929.87 POINT (33734.52 36929.87)\n74  39990.37 POINT (33823.13 39990.37)\n75  39308.71 POINT (33507.78 39308.71)\n76  39514.00    POINT (34279.17 39514)\n77  40020.03 POINT (34827.33 40020.03)\n78  37785.95 POINT (34332.07 37785.95)\n79  41909.17 POINT (34242.94 41909.17)\n80  40635.77 POINT (34458.68 40635.77)\n81  41365.63 POINT (33992.45 41365.63)\n82  40501.04 POINT (35005.09 40501.04)\n83  41569.27 POINT (34439.32 41569.27)\n84  40999.84  POINT (35777.2 40999.84)\n85  41693.55 POINT (35926.59 41693.55)\n86  41241.87 POINT (35368.24 41241.87)\n87  39429.26 POINT (32613.23 39429.26)\n88  38036.27 POINT (32050.22 38036.27)\n89  36788.55 POINT (31911.64 36788.55)\n90  36825.12 POINT (31226.41 36825.12)\n91  37723.20  POINT (31415.53 37723.2)\n92  39330.38  POINT (30052.7 39330.38)\n93  38632.18 POINT (29968.82 38632.18)\n94  38001.50  POINT (30275.89 38001.5)\n95  40683.26 POINT (28900.66 40683.26)\n96  39038.16 POINT (28702.48 39038.16)\n97  39792.77 POINT (28276.74 39792.77)\n98  38071.92 POINT (27966.81 38071.92)\n99  36835.12 POINT (30414.93 36835.12)\n100 35582.91 POINT (21648.98 35582.91)\n101 35547.20  POINT (20603.35 35547.2)\n102 34984.58 POINT (22442.59 34984.58)\n103 36729.23 POINT (16853.08 36729.23)\n104 36118.96 POINT (17710.49 36118.96)\n105 35419.87 POINT (17179.43 35419.87)\n106 35597.61 POINT (15176.23 35597.61)\n107 35399.68 POINT (13129.85 35399.68)\n108 37066.28 POINT (14006.52 37066.28)\n109 35705.12 POINT (13029.92 35705.12)\n110 36095.65 POINT (11781.81 36095.65)\n111 36773.40  POINT (12607.09 36773.4)\n112 36648.95 POINT (13215.14 36648.95)\n113 36317.36 POINT (13047.82 36317.36)\n114 36541.34 POINT (15217.27 36541.34)\n115 36555.81 POINT (15562.43 36555.81)\n116 36110.29 POINT (14594.08 36110.29)\n117 36812.40  POINT (17694.72 36812.4)\n118 37370.85  POINT (19191.2 37370.85)\n119 38463.08 POINT (18644.37 38463.08)\n120 37916.87 POINT (18557.57 37916.87)\n121 36444.08 POINT (19135.59 36444.08)\n122 36407.36  POINT (19445.3 36407.36)\n123 40637.61    POINT (19871 40637.61)\n124 40870.21 POINT (20708.01 40870.21)\n125 40155.83  POINT (20920.4 40155.83)\n126 38737.71 POINT (20665.41 38737.71)\n127 39500.36 POINT (20888.58 39500.36)\n128 40655.69 POINT (21357.65 40655.69)\n129 41234.06 POINT (20552.58 41234.06)\n130 40648.81 POINT (19164.46 40648.81)\n131 42738.31  POINT (18371.8 42738.31)\n132 42068.54 POINT (18930.07 42068.54)\n133 41718.53 POINT (18434.25 41718.53)\n134 41845.41  POINT (17971.2 41845.41)\n135 40377.34 POINT (18410.93 40377.34)\n136 40248.44 POINT (17231.35 40248.44)\n137 39971.29 POINT (17818.78 39971.29)\n138 47627.92 POINT (24502.39 47627.92)\n139 45633.47  POINT (23246.9 45633.47)\n140 46842.32 POINT (24818.04 46842.32)\n141 46050.25 POINT (23204.93 46050.25)\n142 46284.17  POINT (24004.4 46284.17)\n143 47078.58 POINT (22994.56 47078.58)\n144 46475.58 POINT (23378.65 46475.58)\n145 47144.77 POINT (24296.63 47144.77)\n146 47312.14 POINT (23685.14 47312.14)\n147 46172.24 POINT (21359.01 46172.24)\n148 45800.79 POINT (21865.34 45800.79)\n149 46014.12 POINT (22738.06 46014.12)\n150 48388.38 POINT (26203.45 48388.38)\n151 48769.53 POINT (25858.57 48769.53)\n152 48145.49 POINT (26797.87 48145.49)\n153 48097.40  POINT (26057.01 48097.4)\n154 47488.69 POINT (26646.13 47488.69)\n155 45492.05 POINT (27678.04 45492.05)\n156 46131.55 POINT (28493.74 46131.55)\n157 46096.26 POINT (27958.14 46096.26)\n158 46187.32 POINT (28081.48 46187.32)\n159 44356.21 POINT (27646.63 44356.21)\n160 45412.93 POINT (29215.42 45412.93)\n161 46676.00    POINT (28666.66 46676)\n162 45465.87 POINT (29675.94 45465.87)\n163 44786.10  POINT (28831.79 44786.1)\n164 44188.76 POINT (28643.46 44188.76)\n165 42128.55 POINT (33231.78 42128.55)\n166 41605.56 POINT (32636.91 41605.56)\n167 41358.12 POINT (32576.88 41358.12)\n168 44467.67 POINT (35993.49 44467.67)\n169 43711.47 POINT (35298.79 43711.47)\n170 42991.37 POINT (36667.16 42991.37)\n171 43253.37 POINT (35282.97 43253.37)\n172 42621.35 POINT (35286.55 42621.35)\n173 42308.13 POINT (37489.11 42308.13)\n174 42380.80  POINT (36912.52 42380.8)\n175 42989.21 POINT (36010.44 42989.21)\n176 42041.54 POINT (36793.43 42041.54)\n177 41861.47 POINT (36374.77 41861.47)\n178 42440.64 POINT (36294.07 42440.64)\n\n\n\n\nCode Chunk\n# Combine latitude and longitude in `coords` int a single \"geometry\" column and convert to an sf object\ncoords_school &lt;- coords_school %&gt;%\n  select(postal_code, geometry)\n\n\nFollowing which, the coords_sf is then combined to the resale_selected df by address, forming a new df resale_geom\n\n\nCode Chunk\nschool_cleaned &lt;- school %&gt;% \n  left_join(coords_school, by = \"postal_code\")\n\n\nIn the code chunk, we will extract the longitude and latitude from the geometry to facilitate transforming into sf.\n\n\nCode Chunk\nschool_cleaned &lt;- school_cleaned %&gt;%\n  mutate(\n    longitude = st_coordinates(geometry)[, 1],  # Extract longitude\n    latitude = st_coordinates(geometry)[, 2]    # Extract latitude\n  )\n\n\nThen, we are able to transform this into sf with the new longitude and latitude columns.\n\n\nCode Chunk\nschool_cleaned&lt;- st_as_sf(school_cleaned, coords = c(\"longitude\", \"latitude\"), crs = 3414)\n\n\n\n\n\nAs the af mall was in WGS84, it will transformed into SVY21 EPSG 3414.\n\n\nCode Chunk\n# Create an sf object with WGS 84 CRS\nmall &lt;- st_as_sf(mall, coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4326)\n# Transform to EPSG:3414\nmall_cleaned &lt;- st_transform(mall, crs = 3414)\n\n\nFrom the code below, we noticed that there are two malls that are duplicated.\n\n\nCode Chunk\ngeom_text &lt;- st_as_text(st_geometry(mall_cleaned))\n\n# Find duplicated geometries\nduplicates &lt;- mall_cleaned[duplicated(geom_text) | duplicated(geom_text, fromLast = TRUE), ]\n\n# Print summary\nprint(paste(\"Number of duplicate geometries found:\", nrow(duplicates)))\n\n\n[1] \"Number of duplicate geometries found: 4\"\n\n\nCode Chunk\nif(nrow(duplicates) &gt; 0) {\n  print(\"\\nDuplicate records:\")\n  print(duplicates)\n}\n\n\n[1] \"\\nDuplicate records:\"\nSimple feature collection with 4 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 29695.5 ymin: 34261.3 xmax: 38710.09 ymax: 36920.73\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 4 × 2\n  `Mall Name`                  geometry\n  &lt;chr&gt;                     &lt;POINT [m]&gt;\n1 Junction 8         (29695.5 36920.73)\n2 Djitsun Mall Bedok (38710.09 34261.3)\n3 Djitsun Mall       (38710.09 34261.3)\n4 Junction 8         (29695.5 36920.73)\n\n\nHence, with the code below, we will remove the duplicate.\n\n\nCode Chunk\n# Convert geometry to text for comparison\ngeom_text &lt;- st_as_text(st_geometry(mall_cleaned))\n\n# Add row numbers to track which entries we keep\nmall_cleaned$row_num &lt;- 1:nrow(mall_cleaned)\n\n# Remove duplicates, keeping first occurrence only\nmall_cleaned&lt;- mall_cleaned[!duplicated(geom_text), ]\n\nmall_cleaned &lt;- mall_cleaned %&gt;% \n  select(`Mall Name`, geometry)\n\n\n\n\n\nThe code chunk below checks if there are any duplicates geometries and it returns nil.\n\n\nCode Chunk\n# Get the geometries as text for easier comparison\ngeom_park &lt;- st_as_text(st_geometry(park))\n\n# Find duplicated geometries\nduplicates_park &lt;- park[duplicated(geom_park) | duplicated(geom_park, fromLast = TRUE), ]\n\n\nAs the sf park was in WGS84, it will transformed into SVY21 EPSG 3414.\n\n\nCode Chunk\n# Create an sf object with WGS 84 CRS\npark &lt;- st_as_sf(park, coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4326)\n\n# Transform to EPSG:3414\npark_cleaned &lt;- st_transform(park, crs = 3414)\n\n\n\n\n\nLet’s take a quick glimpse at the data. We noticed that there duplicated station names, depots, command centres (i.e. BOCC - to which I derived it as a Bus Operations Control Centre, stations that are under construction.\n\n\nCode Chunk\nglimpse(mrt)\n\n\nRows: 230\nColumns: 6\n$ TYP_CD     &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ STN_NAM    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ATTACHEMEN &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ TYP_CD_DES &lt;chr&gt; \"MRT\", \"MRT\", \"MRT\", \"MRT\", \"MRT\", \"MRT\", \"MRT\", \"MRT\", \"LR…\n$ STN_NAM_DE &lt;chr&gt; \"GALI BATU DEPOT\", \"HILLVIEW MRT STATION\", \"BEAUTY WORLD MR…\n$ geometry   &lt;POLYGON [m]&gt; POLYGON ((19210.61 41858.04..., POLYGON ((20650.33 …\n\n\nThus, with the code chunk below, we use the duplicate function to check which are the duplciated stations. Noticeably, these stations are actually interchange where it consists of more than 1 metro line (i.e. Outram Park where it houses East-West, Thomson-EastCoast, and North-East Line). However, Bayshore MRT isn’t an interchange but there are 2 duplicates of it that is likely due to the construction of Bedok South where both links. Regardless, the decision to keep these variables retains as it serves as an important proximate to the residential areas in geospatial setting.\n\n\nCode Chunk\n# Check for duplicates in the STN_NAM_DE column and display the whole rows\nduplicates &lt;- mrt %&gt;%\n  group_by(STN_NAM_DE) %&gt;%\n  filter(n() &gt; 1) %&gt;%\n  ungroup()\n\n# Display duplicates if any\nif (nrow(duplicates) &gt; 0) {\n  print(duplicates)\n} else {\n  print(\"No duplicates found in the STN_NAM_DE column.\")\n}\n\n\nSimple feature collection with 50 features and 5 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 22616.72 ymin: 27478.44 xmax: 42456.47 ymax: 46579.58\nProjected CRS: SVY21\n# A tibble: 50 × 6\n   TYP_CD STN_NAM ATTACHEMEN     TYP_CD_DES STN_NAM_DE                  geometry\n    &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;chr&gt;      &lt;chr&gt;                  &lt;POLYGON [m]&gt;\n 1      0 &lt;NA&gt;    &lt;NA&gt;           MRT        DHOBY GHA… ((29293.51 31312.53, 293…\n 2      0 &lt;NA&gt;    CC15_BSH STN.… MRT        BISHAN MR… ((29683.2 37087.82, 2970…\n 3      0 &lt;NA&gt;    CC13_SER STN.… MRT        SERANGOON… ((32244.31 36987.67, 322…\n 4      0 &lt;NA&gt;    &lt;NA&gt;           MRT        TAMPINES … ((40488.79 37181.92, 404…\n 5      0 &lt;NA&gt;    &lt;NA&gt;           MRT        STEVENS M… ((27140.4 33629.43, 2714…\n 6      0 &lt;NA&gt;    CC10_MPS STN.… MRT        MACPHERSO… ((34310.59 34386.88, 343…\n 7      0 &lt;NA&gt;    C504_S_CGA_MA… MRT        EXPO MRT … ((42293.7 35208.13, 4229…\n 8      0 &lt;NA&gt;    NE1_HBF STN.z… MRT        HARBOURFR… ((26584.75 27512.49, 265…\n 9      0 &lt;NA&gt;    CE2_MRB STN.z… MRT        MARINA BA… ((30397.79 28652.09, 303…\n10      0 &lt;NA&gt;    CC9_PYL STN.z… MRT        PAYA LEBA… ((34495.6 33384.44, 3452…\n# ℹ 40 more rows\n\n\nCode Chunk\nglimpse(duplicates)\n\n\nRows: 50\nColumns: 6\n$ TYP_CD     &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ STN_NAM    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ATTACHEMEN &lt;chr&gt; NA, \"CC15_BSH STN.zip\", \"CC13_SER STN.zip\", NA, NA, \"CC10_M…\n$ TYP_CD_DES &lt;chr&gt; \"MRT\", \"MRT\", \"MRT\", \"MRT\", \"MRT\", \"MRT\", \"MRT\", \"MRT\", \"MR…\n$ STN_NAM_DE &lt;chr&gt; \"DHOBY GHAUT MRT STATION\", \"BISHAN MRT STATION\", \"SERANGOON…\n$ geometry   &lt;POLYGON [m]&gt; POLYGON ((29293.51 31312.53..., POLYGON ((29683.2 3…\n\n\n\n\n\n\n\n\n\nMRT System Map\n\n\nIn the below code chunk, we will cross-check with the MRT map above and remove the names are that are depots, operation centres, sub stations and columns that are not needed. The code chunks includes stations with “MRT STATION” or “LRT STATION”.\n\n\nCode Chunk\nmrt_cleaned &lt;- mrt %&gt;%\n  select(-TYP_CD, -STN_NAM, -ATTACHEMEN) %&gt;%  # Remove specified columns\n  filter(grepl(\"MRT STATION|LRT STATION\", STN_NAM_DE, ignore.case = TRUE))  # Filter rows\n\n# Display the cleaned sf object\nprint(mrt_cleaned)\n\n\nSimple feature collection with 215 features and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 6068.209 ymin: 27478.44 xmax: 45377.5 ymax: 47913.58\nProjected CRS: SVY21\nFirst 10 features:\n   TYP_CD_DES                STN_NAM_DE                       geometry\n1         MRT      HILLVIEW MRT STATION POLYGON ((20650.33 38282.33...\n2         MRT  BEAUTY WORLD MRT STATION POLYGON ((21594.72 35882.94...\n3         MRT          HUME MRT STATION POLYGON ((20808 37457.72, 2...\n4         MRT BUKIT PANJANG MRT STATION POLYGON ((19996.27 40187.21...\n5         MRT        CASHEW MRT STATION POLYGON ((20340.38 39136.76...\n6         MRT   DHOBY GHAUT MRT STATION POLYGON ((29293.51 31312.53...\n7         MRT      LAVENDER MRT STATION POLYGON ((31236.5 32085.76,...\n8         LRT       RENJONG LRT STATION POLYGON ((34382.66 40949.64...\n9         MRT         DOVER MRT STATION POLYGON ((21987.25 32576.91...\n10        LRT       PHOENIX LRT STATION POLYGON ((19602.92 40048.64...\n\n\nIn the list, we noticed that some of the MRT stations are actually under construction. Hence we cross-checked with the MRT map above and exclude those that are under construction.\nFirst, we create a list of excluded MRT list. Then we use Regex to exclude it using if function.\n\n\nCode Chunk\n# Specify the words to exclude\nwords_to_exclude &lt;- c(\"HUME\", \"SUNGEI BEDOK\", \"BEDOK SOUTH\", \"XILIN\", \n                       \"PUNGGOL COAST\", \"BUKIT BROWN\", \"MOUNT PLEASANT\", \n                       \"FOUNDERS' MEMORIAL\")\n\n# Create a regular expression pattern to match any of the words\npattern &lt;- paste(words_to_exclude, collapse = \"|\")\n\n# Ensure the train object is an sf object\nif (!inherits(mrt_cleaned, \"sf\")) {\n  stop(\"The 'train' object is not an sf object.\")\n}\n\n# Filter the train sf object to exclude rows with specified words in STN_NAM_DE\nmrt_cleaned  &lt;- mrt_cleaned %&gt;%\n  filter(!str_detect(STN_NAM_DE, regex(pattern, ignore_case = TRUE)))\n\n# Verify the results\nprint(head(mrt_cleaned))  # Print the first few rows of the filtered object\n\n\nSimple feature collection with 6 features and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 19932.72 ymin: 31220.26 xmax: 31375.01 ymax: 40255.73\nProjected CRS: SVY21\n  TYP_CD_DES                STN_NAM_DE                       geometry\n1        MRT      HILLVIEW MRT STATION POLYGON ((20650.33 38282.33...\n2        MRT  BEAUTY WORLD MRT STATION POLYGON ((21594.72 35882.94...\n3        MRT BUKIT PANJANG MRT STATION POLYGON ((19996.27 40187.21...\n4        MRT        CASHEW MRT STATION POLYGON ((20340.38 39136.76...\n5        MRT   DHOBY GHAUT MRT STATION POLYGON ((29293.51 31312.53...\n6        MRT      LAVENDER MRT STATION POLYGON ((31236.5 32085.76,...\n\n\nCode Chunk\nsummary(mrt_cleaned )      # Summary of the filtered object\n\n\n  TYP_CD_DES         STN_NAM_DE                 geometry  \n Length:211         Length:211         POLYGON      :211  \n Class :character   Class :character   epsg:NA      :  0  \n Mode  :character   Mode  :character   +proj=tmer...:  0  \n\n\nWe will then view() the mrt_cleaned sf and search for the excluded MRT stations in ensuring that it is not there.\n\n\nCode Chunk\nview(mrt_cleaned)\n\n\n\n\nCode Chunk\nst_crs(mrt_cleaned) &lt;- 3414\n\n\n\n\n\nThe code chunk below checks if there are any duplicates geometries and it returns several duplicates.\nIn the table below, we noticed that there are 33 rows of geometries returned with 2-4 duplicates in each geometry. In explaining this issue, we can zoom into the description column and it will surface different supermarket names sharing the same geometry. This is likely due to the changing of the tenants.\n\n\nCode Chunk\nsupermarket = read_rds(\"data/rds/geospatial/supermarket.rds\")\n\n\n\n\nCode Chunk\n# Find duplicated geometries\nduplicate_ids &lt;- supermarket %&gt;%\n  filter(duplicated(st_geometry(.)) | duplicated(st_geometry(.), fromLast = TRUE)) %&gt;%\n  select(geometry) %&gt;%\n  unique() %&gt;%\n  pull(geometry)\n\n# Create a table with duplicates together in a row and count\nduplicates_table &lt;- supermarket %&gt;%\n  filter(st_geometry(geometry) %in% duplicate_ids) %&gt;%\n  group_by(geometry) %&gt;%\n  summarise(duplicate_entries = list(cur_data()), \n            count = n(),         # Count the number of duplicates\n            .groups = 'drop')\n\n# View the duplicated entries together in a row with counts\nif (nrow(duplicates_table) &gt; 0) {\n  print(\"Duplicated geometries found:\")\n  print(duplicates_table)\n  total_duplicates &lt;- sum(duplicates_table$count)\n  cat(\"Total number of duplicates:\", total_duplicates, \"\\n\")\n} else {\n  print(\"No duplicated geometries found.\")\n}\n\n\n[1] \"Duplicated geometries found:\"\nSimple feature collection with 33 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6342 ymin: 1.27031 xmax: 103.9624 ymax: 1.451325\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n# A tibble: 33 × 3\n                  geometry duplicate_entries count\n               &lt;POINT [°]&gt; &lt;list&gt;            &lt;int&gt;\n 1 Z (103.8142 1.322573 0) &lt;tibble [2 × 2]&gt;      2\n 2  Z (103.7627 1.37835 0) &lt;tibble [2 × 2]&gt;      2\n 3 Z (103.8007 1.439916 0) &lt;tibble [2 × 2]&gt;      2\n 4  Z (103.857 1.307785 0) &lt;tibble [2 × 2]&gt;      2\n 5 Z (103.9132 1.402878 0) &lt;tibble [2 × 2]&gt;      2\n 6 Z (103.8435 1.274588 0) &lt;tibble [2 × 2]&gt;      2\n 7 Z (103.9051 1.301191 0) &lt;tibble [4 × 2]&gt;      4\n 8 Z (103.8945 1.314692 0) &lt;tibble [2 × 2]&gt;      2\n 9 Z (103.7653 1.303583 0) &lt;tibble [2 × 2]&gt;      2\n10 Z (103.7882 1.306849 0) &lt;tibble [2 × 2]&gt;      2\n# ℹ 23 more rows\nTotal number of duplicates: 74 \n\n\nIn addressing the duplicate, only one geometry will be retained while the rest will be eradicated.\n\n\nCode Chunk\n# Remove duplicates while retaining unique geometries\nsupermarket &lt;- supermarket %&gt;%\n  distinct(geometry, .keep_all = TRUE)  # Keep all columns for the unique geometries\n\n\nFollowing which, we can check again to see if there’s any duplicates remaining and it returns nil.\n\n\nCode Chunk\n# Find duplicated geometries\nduplicate_ids &lt;- supermarket %&gt;%\n  filter(duplicated(st_geometry(.)) | duplicated(st_geometry(.), fromLast = TRUE)) %&gt;%\n  select(geometry) %&gt;%\n  unique() %&gt;%\n  pull(geometry)\n\n# Create a table with duplicates together in a row and count\nduplicates_table &lt;- supermarket %&gt;%\n  filter(st_geometry(geometry) %in% duplicate_ids) %&gt;%\n  group_by(geometry) %&gt;%\n  summarise(duplicate_entries = list(cur_data()), \n            count = n(),         # Count the number of duplicates\n            .groups = 'drop')\n\n# View the duplicated entries together in a row with counts\nif (nrow(duplicates_table) &gt; 0) {\n  print(\"Duplicated geometries found:\")\n  print(duplicates_table)\n  total_duplicates &lt;- sum(duplicates_table$count)\n  cat(\"Total number of duplicates:\", total_duplicates, \"\\n\")\n} else {\n  print(\"No duplicated geometries found.\")\n}\n\n\n[1] \"No duplicated geometries found.\"\n\n\nThe data is then transformed from WGS84 to SVY21.\n\n\nCode Chunk\n# Create an sf object with WGS 84 CRS\nsupermarket &lt;- st_as_sf(supermarket, coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4326)\n\n# Transform to EPSG:3414\nsupermarket_cleaned &lt;- st_transform(supermarket, crs = 3414)\n\n\n\n\nCode Chunk\ncoordinates &lt;- st_coordinates(supermarket_cleaned)\nsupermarket_cleaned &lt;- supermarket_cleaned %&gt;%\n  mutate(longitude = coordinates[, 1],    # First column is longitude\n         latitude = coordinates[, 2]) %&gt;%  \n  select(longitude, latitude, geometry)"
  },
  {
    "objectID": "TakeHomeExercise/TakeHome3/TakeHome3.html#computing-proximity-to-resale-transactions",
    "href": "TakeHomeExercise/TakeHome3/TakeHome3.html#computing-proximity-to-resale-transactions",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "Code Chunk\n# Create busstop_resale with just the two columns we want\nbusstop_resale &lt;- data.frame(\n  busstop_prox = numeric(nrow(resale_geom)),\n  within_350m_busstop = numeric(nrow(resale_geom)))\n\n# Loop through each resale point\nfor (i in 1:nrow(resale_geom)) {\n  # Get current resale point\n  current_point &lt;- resale_geom[i, ]\n  \n  # Calculate distances to all bus stops\n  distances &lt;- st_distance(current_point, busstop_cleaned)\n  \n  # Store minimum distance in km in busstop_prox\n  busstop_resale$busstop_prox[i] &lt;- as.numeric(min(distances)) / 1000\n  \n  # Count bus stops within 350 meters\n  busstop_resale$within_350m_busstop[i] &lt;- sum(as.numeric(distances) &lt;= 350)\n}\n\n# Verify results\nprint(head(busstop_resale))             # Print the first few rows\n\n\n  busstop_prox within_350m_busstop\n1   0.09807302                  10\n2   0.11396279                   6\n3   0.12282400                   3\n4   0.12282400                   3\n5   0.10566508                   3\n6   0.16643716                   6\n\n\nCode Chunk\nsummary(busstop_resale)                 # Summary of both columns\n\n\n  busstop_prox     within_350m_busstop\n Min.   :0.01543   Min.   : 0.000     \n 1st Qu.:0.07418   1st Qu.: 6.000     \n Median :0.10727   Median : 8.000     \n Mean   :0.11449   Mean   : 7.907     \n 3rd Qu.:0.14582   3rd Qu.:10.000     \n Max.   :0.39147   Max.   :19.000     \n\n\n\n\n\n\n\nCode Chunk\n# Create eldercare_resale with just the two columns we want\neldercare_resale &lt;- data.frame(\n  eldercare_prox = numeric(nrow(resale_geom)),\n  within_350m_eldercare = numeric(nrow(resale_geom)))\n\n# Loop through each point in busstop_resale\nfor (i in 1:nrow(resale_geom)) {\n  # Get current point\n  current_point &lt;- resale_geom[i, ]\n  \n  # Calculate distances to all eldercare centers\n  distances &lt;- st_distance(current_point, eldercare_cleaned)\n  \n  # Store minimum distance in km in eldercare_prox\n  eldercare_resale$eldercare_prox[i] &lt;- as.numeric(min(distances)) / 1000\n  \n  # Count eldercare facilities within 350 meters\n  eldercare_resale$within_350m_eldercare[i] &lt;- sum(as.numeric(distances) &lt;= 350)\n}\n\n# Verify results\nprint(head(eldercare_resale))             # Print the first few rows\n\n\n  eldercare_prox within_350m_eldercare\n1      0.3419924                     1\n2      0.2597442                     1\n3      0.2811279                     1\n4      0.2811279                     1\n5      0.4296838                     0\n6      0.4043587                     0\n\n\nCode Chunk\nsummary(eldercare_resale)   \n\n\n eldercare_prox   within_350m_eldercare\n Min.   :0.0000   Min.   :0.0000       \n 1st Qu.:0.3303   1st Qu.:0.0000       \n Median :0.6308   Median :0.0000       \n Mean   :0.7983   Mean   :0.3898       \n 3rd Qu.:1.0931   3rd Qu.:1.0000       \n Max.   :4.7675   Max.   :7.0000       \n\n\nCode Chunk\nbeep()\n\n\n\n\n\n\n\nCode Chunk\n# Create eldercare_resale with just the two columns we want\nhawker_resale &lt;- data.frame(\n  hawker_prox = numeric(nrow(resale_geom)))\n\n# Loop through each point in busstop_resale\nfor (i in 1:nrow(resale_geom)) {\n  # Get current point\n  current_point &lt;- resale_geom[i, ]\n  \n  # Calculate distances to all eldercare centers\n  distances &lt;- st_distance(current_point, hawker_cleaned)\n  \n  # Store minimum distance in km in eldercare_prox\n  hawker_resale$hawker_prox[i] &lt;- as.numeric(min(distances)) / 1000\n  \n}\n\n# Verify results\nprint(head(hawker_resale))             # Print the first few rows\n\n\n  hawker_prox\n1   0.1861061\n2   0.4248262\n3   0.3108075\n4   0.3108075\n5   0.3143604\n6   0.1378719\n\n\nCode Chunk\nbeep(3)\n\n\n\n\n\n\n\nCode Chunk\n# Create eldercare_resale with just the two columns we want\nkindergarten_resale &lt;- data.frame(\n  kindergarten_prox = numeric(nrow(resale_geom)),\n  within_350m_kindergarten = numeric(nrow(resale_geom)))\n\n# Loop through each point in busstop_resale\nfor (i in 1:nrow(resale_geom)) {\n  # Get current point\n  current_point &lt;- resale_geom[i, ]\n  \n  # Calculate distances to all eldercare centers\n  distances &lt;- st_distance(current_point, kindergarten_cleaned)\n  \n  # Store minimum distance in km in eldercare_prox\n  kindergarten_resale$kindergarten_prox[i] &lt;- as.numeric(min(distances)) / 1000\n  \n  # Count eldercare facilities within 350 meters\n  kindergarten_resale$within_350m_kindergarten[i] &lt;- sum(as.numeric(distances) &lt;= 350)\n}\n\n# Verify results\nprint(head(kindergarten_resale))             # Print the first few rows\n\n\n  kindergarten_prox within_350m_kindergarten\n1         0.5507577                        0\n2         0.2085693                        1\n3         0.2082784                        1\n4         0.2082784                        1\n5         0.1232351                        1\n6         0.1951542                        2\n\n\nCode Chunk\nsummary(kindergarten_resale)   # Summary of both columns\n\n\n kindergarten_prox within_350m_kindergarten\n Min.   :0.0000    Min.   :0.0000          \n 1st Qu.:0.1773    1st Qu.:0.0000          \n Median :0.2810    Median :1.0000          \n Mean   :0.3052    Mean   :0.9791          \n 3rd Qu.:0.4021    3rd Qu.:1.0000          \n Max.   :3.1675    Max.   :8.0000          \n\n\nCode Chunk\nbeep(3)              \n\n\n\n\n\n\n\nCode Chunk\n#change CRS of mall_cleaned to align to CRS EPSG 3414 \nst_crs(mall_cleaned) &lt;- 3414\n\n# Create eldercare_resale with just the two columns we want\nmall_resale &lt;- data.frame(\n  mall_prox = numeric(nrow(resale_geom)))\n\n# Loop through each point in busstop_resale\nfor (i in 1:nrow(resale_geom)) {\n  # Get current point\n  current_point &lt;- resale_geom[i, ]\n  \n  # Calculate distances to all eldercare centers\n  distances &lt;- st_distance(current_point, mall_cleaned)\n  \n  # Store minimum distance in km in eldercare_prox\n  mall_resale$mall_prox[i] &lt;- as.numeric(min(distances)) / 1000\n  \n}\n\n# Verify results\nprint(head(mall_resale))             # Print the first few rows\n\n\n  mall_prox\n1  48.70776\n2  48.80401\n3  48.81649\n4  48.81649\n5  48.60214\n6  48.12243\n\n\nCode Chunk\nbeep(3)\n\n\n\n\n\n\n\nCode Chunk\n# Create eldercare_resale with just the two columns we want\nmrt_resale &lt;- data.frame(\n  mrt_prox = numeric(nrow(resale_geom)))\n\n# Loop through each point in busstop_resale\nfor (i in 1:nrow(resale_geom)) {\n  # Get current point\n  current_point &lt;- resale_geom[i, ]\n  \n  # Calculate distances to all eldercare centers\n  distances &lt;- st_distance(current_point, mrt_cleaned)\n  \n  # Store minimum distance in km in eldercare_prox\n  mrt_resale$mrt_prox[i] &lt;- as.numeric(min(distances)) / 1000\n  \n}\n\n# Verify results\nprint(head(mrt_resale))             # Print the first few rows\n\n\n    mrt_prox\n1 0.90926089\n2 0.27613980\n3 0.33461983\n4 0.33461983\n5 0.06951106\n6 0.39898970\n\n\nCode Chunk\nbeep(3)\n\n\n\n\n\n\n\nCode Chunk\n# Create eldercare_resale with just the two columns we want\npark_resale &lt;- data.frame(\n  park_prox = numeric(nrow(resale_geom)))\n\n# Loop through each point in busstop_resale\nfor (i in 1:nrow(resale_geom)) {\n  # Get current point\n  current_point &lt;- resale_geom[i, ]\n  \n  # Calculate distances to all eldercare centers\n  distances &lt;- st_distance(current_point, park_cleaned)\n  \n  # Store minimum distance in km in eldercare_prox\n  park_resale$park_prox[i] &lt;- as.numeric(min(distances)) / 1000\n  \n}\n\n# Verify results\nprint(head(park_resale))             # Print the first few rows\n\n\n  park_prox\n1 0.2815657\n2 0.3108840\n3 0.3411321\n4 0.3411321\n5 0.2436839\n6 0.1985831\n\n\nCode Chunk\nbeep(3)\n\n\n\n\n\n\n\nCode Chunk\n# Create eldercare_resale with just the two columns we want\nschool_resale &lt;- data.frame(\n  school_prox = numeric(nrow(resale_geom)),\n  within_1km_school = numeric(nrow(resale_geom)))\n\n# Loop through each point in busstop_resale\nfor (i in 1:nrow(resale_geom)) {\n  # Get current point\n  current_point &lt;- resale_geom[i, ]\n  \n  # Calculate distances to all eldercare centers\n  distances &lt;- st_distance(current_point, school_cleaned)\n  \n  # Store minimum distance in km in eldercare_prox\n  school_resale$school_prox[i] &lt;- as.numeric(min(distances)) / 1000\n  \n  # Count eldercare facilities within 1000 meters\n  school_resale$within_1km_school[i] &lt;- sum(as.numeric(distances) &lt;= 1000)\n}\n\n# Verify results\nprint(head(school_resale))             # Print the first few rows\n\n\n  school_prox within_1km_school\n1   0.2279026                 2\n2   0.4437104                 3\n3   0.1210031                 3\n4   0.1210031                 3\n5   0.2472647                 2\n6   0.2692533                 2\n\n\nCode Chunk\nsummary(school_resale)   # Summary of both columns\n\n\n  school_prox      within_1km_school\n Min.   :0.04354   Min.   :0.000    \n 1st Qu.:0.24063   1st Qu.:2.000    \n Median :0.37618   Median :3.000    \n Mean   :0.42942   Mean   :2.979    \n 3rd Qu.:0.54703   3rd Qu.:4.000    \n Max.   :3.29166   Max.   :9.000    \n\n\nCode Chunk\nbeep(3)    \n\n\n\n\n\n\n\nCode Chunk\n# Create eldercare_resale with just the two columns we want\nsupermarket_resale &lt;- data.frame(\n  supermarket_prox = numeric(nrow(resale_geom)))\n\n# Loop through each point in busstop_resale\nfor (i in 1:nrow(resale_geom)) {\n  # Get current point\n  current_point &lt;- resale_geom[i, ]\n  \n  # Calculate distances to all eldercare centers\n  distances &lt;- st_distance(current_point, supermarket_cleaned)\n  \n  # Store minimum distance in km in eldercare_prox\n  supermarket_resale$supermarket_prox[i] &lt;- as.numeric(min(distances)) / 1000\n  \n}\n\n# Verify results\nprint(head(supermarket_resale))             # Print the first few rows\n\n\n  supermarket_prox\n1        0.3872724\n2        0.1666790\n3        0.3212926\n4        0.3212926\n5        0.3413032\n6        0.1577571\n\n\nCode Chunk\nbeep(3)"
  },
  {
    "objectID": "TakeHomeExercise/TakeHome3/TakeHome3.html#visualisations",
    "href": "TakeHomeExercise/TakeHome3/TakeHome3.html#visualisations",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "In the code chunk below, we are able to view the mininum, maximum, median and quadrants of the data. Interesting, the minimum and maximum resale price were $150,000 and $1.58m. And the smallest age of unit was 3 whereas the oldest was 58.\n\n\nCode Chunk\nsummary(resale_geom)\n\n\n   address              town            resale_price        month          \n Length:47423       Length:47423       Min.   : 150000   Length:47423      \n Class :character   Class :character   1st Qu.: 460000   Class :character  \n Mode  :character   Mode  :character   Median : 565000   Mode  :character  \n                                       Mean   : 587538                     \n                                       3rd Qu.: 685000                     \n                                       Max.   :1588000                     \n  flat_type         floor_area_sqm   remaining_lease_yr   longitude    \n Length:47423       Min.   : 31.00   Min.   :41.0       Min.   :11519  \n Class :character   1st Qu.: 74.00   1st Qu.:60.0       1st Qu.:21661  \n Mode  :character   Median : 93.00   Median :73.0       Median :29281  \n                    Mean   : 95.22   Mean   :73.3       Mean   :28655  \n                    3rd Qu.:111.00   3rd Qu.:90.0       3rd Qu.:35078  \n                    Max.   :366.70   Max.   :96.0       Max.   :45192  \n    latitude              geometry        unit_age   \n Min.   :28098   POINT        :47423   Min.   : 3.0  \n 1st Qu.:35670   epsg:3414    :    0   1st Qu.: 9.0  \n Median :38929   +proj=tmer...:    0   Median :26.0  \n Mean   :39069                         Mean   :25.7  \n 3rd Qu.:42401                         3rd Qu.:39.0  \n Max.   :48741                         Max.   :58.0  \n\n\nWe will use tmap to create an iteractive map of the resale prices. Noticebly, the “hotter” resale flats are concentrated in the south and middle area with scatters around the east and west.\n\n\nCode Chunk\ntmap_mode('plot')\ntm_shape(mpsz)+\n  tm_polygons() +\ntm_shape(resale_geom) +  \n  tm_dots(col = \"resale_price\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\n\n\n\n\n\n\n\nCode Chunk\ntmap_mode('plot')\ntm_shape(mpsz)+\n  tm_polygons() +\ntm_shape(resale_geom) +  \n  tm_dots(col = \"unit_age\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\n\n\n\n\n\nIn barchart below, we have populated the percentage of transactions stratified by town. Visibly, Punggol attains the highest 7.7% followed by Woodlands at 7.6%. Hence We would want to predict the HDB resale prices for these two towns.\n\n\nCode Chunk\ntown_percentages &lt;- resale_geom %&gt;%\n  group_by(town) %&gt;%\n  summarise(count = n()) %&gt;%\n  mutate(percentage = (count/sum(count))*100) %&gt;%\n  arrange(desc(percentage))  # Sort in descending order\n\n# Create the bar chart\nggplot(town_percentages, aes(x = reorder(town, -percentage), y = percentage)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  geom_text(aes(label = sprintf(\"%.1f%%\", percentage)), \n            vjust = -0.5, \n            size = 3) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(title = \"Percentage Distribution of Resale Transactions by Town\",\n       x = \"Town\",\n       y = \"Percentage of Total Transactions (%)\")"
  },
  {
    "objectID": "TakeHomeExercise/TakeHome3/TakeHome3.html#final-amendments",
    "href": "TakeHomeExercise/TakeHome3/TakeHome3.html#final-amendments",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "Code Chunk\ndata &lt;- resale_geom %&gt;%\n  cbind(busstop_resale, eldercare_resale, hawker_resale,\n            kindergarten_resale, mall_resale, mrt_resale,\n            park_resale, school_resale, supermarket_resale)\n\n\nFirstly, we will change the column “month” to POSIXct for easier manipulation.\nAs we are focusing on 4-room HDB flats, we will filter according to that.\nLastly, as we’re working with an sf (spatial features) object, we need to use a different approach to remove columns instead of using dplyr. We will use select(!matches) to remove the column address.\n\n\nCode Chunk\ndata &lt;- data %&gt;%\n  mutate(month = as.POSIXct(paste0(month, \"-01\"), format = \"%Y-%m-%d\")) %&gt;% \n  filter(flat_type == \"4 ROOM\") %&gt;% # Change from int to num\n  select(-c(address, flat_type)) %&gt;% \n  mutate(remaining_lease_yr = as.numeric(remaining_lease_yr)) # Change from int to num"
  },
  {
    "objectID": "TakeHomeExercise/TakeHome3/TakeHome3.html#machine-learning",
    "href": "TakeHomeExercise/TakeHome3/TakeHome3.html#machine-learning",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "The entire data are split into training (Jan 2023 - June 2024) and testing (July 2024 - Sept 2024) data sets into by using initial_split() of rsample package. rsample is one of the package of tigymodels.\nIn this section, we will split the data into training data - Jan 2023 to June 2024 & test Data - July 2024 to Sept 2024 that is stratified by Punggol & Woodlands. We can see there are 1549 training records and 300 testing records for Punggol whereas in Woodlands there are 1391 training records and 238 testing records.\n\nPunggolWoodlands\n\n\n\n\nCode Chunk\nset.seed(1234)\ntrain_data_punggol &lt;- data %&gt;%\n  filter(month &gt;= as.POSIXct(\"2023-01-01\") & \n         month &lt;= as.POSIXct(\"2024-06-30\") &\n         town == \"PUNGGOL\") %&gt;% \n  select(-c(month))\n\ntest_data_punggol &lt;- data %&gt;%\n  filter(month &gt;= as.POSIXct(\"2024-07-01\") & \n         month &lt;= as.POSIXct(\"2024-09-30\") &\n         town == \"PUNGGOL\") %&gt;% \n  select(-c(month))\n\n# Optional: to check the filtered data\nprint(paste(\"Number of training records:\", nrow(train_data_punggol)))\n\n\n[1] \"Number of training records: 1549\"\n\n\nCode Chunk\nprint(paste(\"Number of testing records:\", nrow(test_data_punggol)))\n\n\n[1] \"Number of testing records: 300\"\n\n\n\n\nCode Chunk\nwrite_rds(train_data_punggol, \"data/rds/ml/punggol/train_data_punggol.rds\")\nwrite_rds(test_data_punggol, \"data/rds/ml/punggol/test_data_punggol.rds\")\n\n\n\n\n\n\nCode Chunk\nset.seed(1234)\ntrain_data_woodlands &lt;- data %&gt;%\n  filter(month &gt;= as.POSIXct(\"2023-01-01\") & \n         month &lt;= as.POSIXct(\"2024-06-30\") &\n         town == \"WOODLANDS\") %&gt;% \n  select(-c(month))\n\ntest_data_woodlands &lt;- data %&gt;%\n  filter(month &gt;= as.POSIXct(\"2024-07-01\") & \n         month &lt;= as.POSIXct(\"2024-09-30\") &\n         town == \"WOODLANDS\") %&gt;% \n  select(-c(month))\n\n# Optional: to check the filtered data\nprint(paste(\"Number of training records:\", nrow(train_data_woodlands)))\n\n\n[1] \"Number of training records: 1391\"\n\n\nCode Chunk\nprint(paste(\"Number of testing records:\", nrow(test_data_woodlands)))\n\n\n[1] \"Number of testing records: 238\"\n\n\n\n\nCode Chunk\nwrite_rds(train_data_woodlands, \"data/rds/ml/woodlands/train_data_woodlands.rds\")\nwrite_rds(test_data_woodlands, \"data/rds/ml/woodlands/test_data_woodlands.rds\")\n\n\n\n\n\n\n\n\nPrior to loading predictors into predictive model, a correlation matrix will be use to check for sign of multicollinearity. From the correlation matrix below, all correlation values are below 0.8, indicating no sign of multicollinearity.\n\n\nCode Chunk\ndata_nogeo &lt;- data %&gt;%\n   select(-c(month)) %&gt;%\n  st_drop_geometry()\n\n\n\n\nCode Chunk\ncorrplot::corrplot(cor(data_nogeo[, 2:19]), \n                   diag = FALSE, \n                   order = \"AOE\",\n                   tl.pos = \"td\", \n                   tl.cex = 0.5, \n                   method = \"number\", \n                   type = \"upper\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nPunggolWoodlands\n\n\n\n\nCode Chunk\ntrain_data_punggol &lt;- read_rds(\"data/rds/ml/punggol/train_data_punggol.rds\")\ntest_data_punggol &lt;- read_rds(\"data/rds/ml/punggol/test_data_punggol.rds\")\n\n\n\n\n\n\nCode Chunk\ntrain_data_woodlands &lt;- read_rds(\"data/rds/ml/woodlands/train_data_woodlands.rds\")\ntest_data_woodlands &lt;- read_rds(\"data/rds/ml/woodlands/test_data_woodlands.rds\")"
  },
  {
    "objectID": "TakeHomeExercise/TakeHome3/TakeHome3.html#geographically-weighted-regression",
    "href": "TakeHomeExercise/TakeHome3/TakeHome3.html#geographically-weighted-regression",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "Now, we will calibrate a model to predict HDB resale price by using geographically weighted regression method of GWmodel package.\n\n\n\nPunggolWoodlands\n\n\n\n\nCode Chunk\ntrain_data_sp_p &lt;- as_Spatial(train_data_punggol)\ntrain_data_sp_p\n\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1549 \nextent      : 34320.85, 37615.35, 41556.12, 43933.95  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 20\nnames       :    town, resale_price, floor_area_sqm, remaining_lease_yr,        longitude,         latitude, unit_age,       busstop_prox, within_350m_busstop,       eldercare_prox, within_350m_eldercare,      hawker_prox,    kindergarten_prox, within_350m_kindergarten,        mall_prox, ... \nmin values  : PUNGGOL,       456000,             85,                 78, 34320.8500594051, 41556.1217510505,        4, 0.0232098182955299,                   2, 9.08133417710502e-07,                     0, 0.23549332350391, 3.08593427233971e-07,                        0, 54.6825044697734, ... \nmax values  : PUNGGOL,        8e+05,             99,                 95, 37615.3527353946, 43933.9470685477,       21,  0.320676170721283,                  14,     1.69953951976499,                     2, 1.98905282800319,    0.804615824046136,                        3, 56.7005204705412, ... \n\n\n\n\n\n\nCode Chunk\ntrain_data_sp_w &lt;- as_Spatial(train_data_woodlands)\ntrain_data_sp_w\n\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1391 \nextent      : 21177.26, 25268.25, 45373.58, 47860.89  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 20\nnames       :      town, resale_price, floor_area_sqm, remaining_lease_yr,        longitude,         latitude, unit_age,       busstop_prox, within_350m_busstop,     eldercare_prox, within_350m_eldercare,        hawker_prox,    kindergarten_prox, within_350m_kindergarten,        mall_prox, ... \nmin values  : WOODLANDS,       350000,             83,                 49, 21177.2576852251, 45373.5757321722,        4, 0.0199595426953728,                   4, 0.0547017433825683,                     0, 0.0666360841718605, 6.84731226937912e-07,                        0, 50.0713376927117, ... \nmax values  : WOODLANDS,       690000,            119,                 95, 25268.2547251521, 47860.8875505569,       50,  0.253082170943171,                  16,   1.73109192014879,                     3,   1.56805500852822,    0.760166885640459,                        4, 53.6923986369607, ... \n\n\n\n\n\n\n\n\nNext, bw.gwr() of GWmodel package will be used to determine the optimal bandwidth to be used.\n\nPunggolWoodlands\n\n\n\n\nCode Chunk\nbw_adaptive_p &lt;- bw.gwr(resale_price ~ floor_area_sqm +\n                 unit_age + within_350m_busstop +\n                 within_350m_eldercare + hawker_prox + \n                 within_350m_kindergarten + mall_prox +\n                 mrt_prox + park_prox +\n                 within_1km_school +\n                 supermarket_prox,\n                  data=train_data_sp_p,\n                  approach=\"CV\",\n                  kernel=\"gaussian\",\n                  adaptive=TRUE,\n                  longlat=FALSE)\n\n\nTake a cup of tea and have a break, it will take a few minutes.\n          -----A kind suggestion from GWmodel development group\nAdaptive bandwidth: 964 CV score: 2.37468e+12 \nAdaptive bandwidth: 604 CV score: 2.273545e+12 \nAdaptive bandwidth: 379 CV score: 2.179535e+12 \nAdaptive bandwidth: 243 CV score: 2.073315e+12 \nAdaptive bandwidth: 155 CV score: 2.011629e+12 \nAdaptive bandwidth: 105 CV score: 1.983025e+12 \nAdaptive bandwidth: 69 CV score: 1.946889e+12 \nAdaptive bandwidth: 52 CV score: 1.916621e+12 \nAdaptive bandwidth: 36 CV score: 1.894903e+12 \nAdaptive bandwidth: 31 CV score: 1.888189e+12 \nAdaptive bandwidth: 23 CV score: 1.901021e+12 \nAdaptive bandwidth: 31 CV score: 1.888189e+12 \n\n\nCode Chunk\nbeep(3)\n\n\nThe result shows that 31 neighbour points will be the optimal bandwidth to be used if adaptive bandwidth is used for this data set.\n\n\n\n\n\n\nNote\n\n\n\nInsights\nThe result shows that 86 neighbour points will be the optimal bandwidth to be used if adaptive bandwidth is used for this data set.\n\n\n\n\nCode Chunk\nwrite_rds(bw_adaptive_p, \"data/rds/ml/punggol/bw_adaptive_p.rds\")\n\n\n\n\n\n\nCode Chunk\nbw_adaptive_w &lt;- bw.gwr(resale_price ~ floor_area_sqm +\n                 unit_age + within_350m_busstop +\n                 within_350m_eldercare + hawker_prox + \n                 within_350m_kindergarten + mall_prox +\n                 mrt_prox + park_prox +\n                 within_1km_school +\n                 supermarket_prox,\n                  data=train_data_sp_w,\n                  approach=\"CV\",\n                  kernel=\"gaussian\",\n                  adaptive=TRUE,\n                  longlat=FALSE)\n\n\nAdaptive bandwidth: 867 CV score: 1.120907e+12 \nAdaptive bandwidth: 544 CV score: 1.112173e+12 \nAdaptive bandwidth: 343 CV score: 1.101484e+12 \nAdaptive bandwidth: 220 CV score: 1.079251e+12 \nAdaptive bandwidth: 142 CV score: 1.057357e+12 \nAdaptive bandwidth: 96 CV score: 1.047183e+12 \nAdaptive bandwidth: 65 CV score: 1.027565e+12 \nAdaptive bandwidth: 48 CV score: 1.015162e+12 \nAdaptive bandwidth: 35 CV score: 1.307044e+12 \nAdaptive bandwidth: 53 CV score: 1.017467e+12 \nAdaptive bandwidth: 41 CV score: 1.303681e+12 \nAdaptive bandwidth: 48 CV score: 1.015162e+12 \n\n\nCode Chunk\nbeep(3)\n\n\n\n\n\n\n\n\nNote\n\n\n\nInsights\nThe results shows that 48 neighbour points will be the optimal bandwidth to be used if adaptive bandwidth is used for this data set.\n\n\n\n\nCode Chunk\nwrite_rds(bw_adaptive_w, \"data/rds/ml/woodlands/bw_adaptive_w.rds\")\n\n\n\n\n\n\n\n\n\n\nCode Chunk\nbw_adaptive_p &lt;- read_rds(\"data/rds/ml/punggol/bw_adaptive_p.rds\")\n\n\n\n\nCode Chunk\nbw_adaptive_w &lt;- read_rds(\"data/rds/ml/woodlands/bw_adaptive_w.rds\")\n\n\nNow, we can go ahead to calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and Gaussian kernel as shown in the code chunk below.\n\nPunggolWoodlands\n\n\n\n\nCode Chunk\ngwr_adaptive_p &lt;- gwr.basic(formula = resale_price ~ floor_area_sqm +\n                 unit_age + within_350m_busstop +\n                 within_350m_eldercare + hawker_prox + \n                 within_350m_kindergarten + mall_prox +\n                 mrt_prox + park_prox +\n                 within_1km_school +\n                 supermarket_prox,\n                  data=train_data_sp_p,\n                          bw=bw_adaptive_p, \n                          kernel = 'gaussian', \n                          adaptive=TRUE,\n                          longlat = FALSE)\nbeep(3)\n\n\n\n\nCode Chunk\nwrite_rds(gwr_adaptive_p, \"data/rds/ml/punggol/gwr_adaptive_p.rds\")\n\n\n\n\n\n\nCode Chunk\ngwr_adaptive_w &lt;- gwr.basic(formula = resale_price ~ floor_area_sqm +\n                 unit_age + within_350m_busstop +\n                 within_350m_eldercare + hawker_prox + \n                 within_350m_kindergarten + mall_prox +\n                 mrt_prox + park_prox +\n                 within_1km_school +\n                 supermarket_prox,\n                  data=train_data_sp_w,\n                          bw=bw_adaptive_w, \n                          kernel = 'gaussian', \n                          adaptive=TRUE,\n                          longlat = FALSE)\nbeep(3)\n\n\n\n\nCode Chunk\nwrite_rds(gwr_adaptive_w, \"data/rds/ml/woodlands/gwr_adaptive_w.rds\")\n\n\n\n\n\n\n\n\nThe code below can be used to display the model output.\n\nPunggolWoodlands\n\n\n\n\nCode Chunk\ngwr_adaptive_p &lt;- read_rds(\"data/rds/ml/punggol/gwr_adaptive_p.rds\")\n\n\n\n\nCode Chunk\ngwr_adaptive_p\n\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-11-11 01:51:55.810254 \n   Call:\n   gwr.basic(formula = resale_price ~ floor_area_sqm + unit_age + \n    within_350m_busstop + within_350m_eldercare + hawker_prox + \n    within_350m_kindergarten + mall_prox + mrt_prox + park_prox + \n    within_1km_school + supermarket_prox, data = train_data_sp_p, \n    bw = bw_adaptive_p, kernel = \"gaussian\", adaptive = TRUE, \n    longlat = FALSE)\n\n   Dependent (y) variable:  resale_price\n   Independent variables:  floor_area_sqm unit_age within_350m_busstop within_350m_eldercare hawker_prox within_350m_kindergarten mall_prox mrt_prox park_prox within_1km_school supermarket_prox\n   Number of data points: 1549\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n    Min      1Q  Median      3Q     Max \n-136613  -26146     -96   26415  140294 \n\n   Coefficients:\n                              Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)              -2425118.0   181420.5 -13.367  &lt; 2e-16 ***\n   floor_area_sqm              11207.8      669.0  16.754  &lt; 2e-16 ***\n   unit_age                    -3655.1      390.9  -9.351  &lt; 2e-16 ***\n   within_350m_busstop          -601.6      515.7  -1.167 0.243557    \n   within_350m_eldercare       18135.0     2457.4   7.380 2.59e-13 ***\n   hawker_prox                -73597.2     5073.0 -14.508  &lt; 2e-16 ***\n   within_350m_kindergarten     -788.8     2040.8  -0.387 0.699178    \n   mall_prox                   37612.1     3120.5  12.053  &lt; 2e-16 ***\n   mrt_prox                   -81583.3    11403.9  -7.154 1.30e-12 ***\n   park_prox                   64924.6     7539.8   8.611  &lt; 2e-16 ***\n   within_1km_school           -5807.9     1023.9  -5.673 1.68e-08 ***\n   supermarket_prox            41305.5    11410.9   3.620 0.000304 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 39990 on 1537 degrees of freedom\n   Multiple R-squared: 0.5345\n   Adjusted R-squared: 0.5311 \n   F-statistic: 160.4 on 11 and 1537 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 2.457709e+12\n   Sigma(hat): 39858.42\n   AIC:  37237.26\n   AICc:  37237.5\n   BIC:  35853.24\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 31 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                                   Min.     1st Qu.      Median     3rd Qu.\n   Intercept                -3.6026e+08 -1.8903e+07 -5.2468e+06  3.2394e+06\n   floor_area_sqm           -9.3131e+05  4.9672e+03  9.1804e+03  1.6247e+04\n   unit_age                 -7.2954e+03 -2.3589e+03  9.9553e+03  1.8568e+04\n   within_350m_busstop      -2.1840e+04 -3.5978e+03  7.9328e+02  3.5622e+03\n   within_350m_eldercare    -5.0557e+05 -2.9912e+03  1.7167e+04  5.6494e+04\n   hawker_prox              -1.0870e+07 -3.7718e+05 -8.1130e+04  1.2870e+05\n   within_350m_kindergarten -8.6012e+05 -2.8299e+04 -7.3968e+03  1.1648e+04\n   mall_prox                -6.1324e+06 -5.6280e+04  8.8829e+04  3.3087e+05\n   mrt_prox                 -1.5787e+06 -1.4296e+05 -9.5298e+03  2.0465e+05\n   park_prox                -8.1186e+06 -1.7270e+05  1.5589e+04  3.1357e+05\n   within_1km_school        -1.1002e+05 -7.9972e+03  4.7641e+02  8.1719e+03\n   supermarket_prox         -3.4463e+06 -1.1782e+05 -4.4088e+04  5.4404e+04\n                                 Max.\n   Intercept                344376485\n   floor_area_sqm              163021\n   unit_age                     45035\n   within_350m_busstop          20457\n   within_350m_eldercare      3933539\n   hawker_prox                1825411\n   within_350m_kindergarten    323139\n   mall_prox                  6261469\n   mrt_prox                   8775702\n   park_prox                  6609422\n   within_1km_school           119709\n   supermarket_prox          11102587\n   ************************Diagnostic information*************************\n   Number of data points: 1549 \n   Effective number of parameters (2trace(S) - trace(S'S)): 216.9356 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 1332.064 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 36831.82 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 36605.4 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 36183.21 \n   Residual sum of squares: 1.482095e+12 \n   R-square value:  0.7192638 \n   Adjusted R-square value:  0.6735097 \n\n   ***********************************************************************\n   Program stops at: 2024-11-11 01:51:57.180324 \n\n\n\n\n\n\n\n\nNote\n\n\n\nInsights of 2 Regression Analysis in predicting resale price - Punggol\n\nGlobal Regression\n\nResiduals: The residuals indicate the difference between the observed and predicted resale prices. The residuals range from -144,029 to 135,047, with a median of -1,271, suggesting that the model may not perfectly fit all data points, particularly those with extreme values.\nCoefficients: The estimated coefficients for each predictor variable indicate their respective impact on resale price.\n\nSignificant Variables: Variables like floor_area_sqm, unit_age, and hawker_prox are statistically significant with p-values less than 0.001, suggesting strong relationships with the resale price.\nNon-significant Variables: within_350m_busstop did not show significant effects on resale price, with p-values greater than 0.05.\n\nModel Fit: The global regression model explains about 51.57% of the variance in resale price (R-squared = 0.5157). The Adjusted R-squared is 0.5123, showing a modest fit after adjusting for the number of predictors. The F-statistic of 148.8, with a p-value less than 2.2e-16, indicates that the overall model is highly significant.\nAIC/BIC: The Akaike Information Criterion (AIC = 37,298.31) and the Bayesian Information Criterion (BIC = 35,914.29) suggest that the global model provides a reasonable fit, although these values are used for model comparison rather than absolute goodness-of-fit assessment.\n\nGeographically Weighted Regression\n\nThis model was calibrated using the adaptive Gaussian kernel with an adaptive bandwidth of 31, meaning the model uses data from the 31 nearest neighbors to each observation.\n\nModel Calibration: The GWR model was fitted using the same set of predictor variables as the global regression model. The use of an adaptive kernel and Euclidean distance metric enables the model to account for spatial heterogeneity in the relationships between predictors and resale price.\nCoefficient Estimates:\n\nThe GWR model shows significant variation in the coefficient estimates across geographic locations. For instance:\n\nThe coefficient for floor_area_sqm ranges from negative to positive values, indicating that the impact of floor area on resale price varies spatially.\nThe coefficient for unit_age shows a similar spatial variation, with the impact differing from location to location.\nhawker_prox and supermarket_prox also exhibit varying effects on resale price across locations.\n\n\nModel Fit:\nR-squared: The GWR model explains approximately 71.98% of the variance in resale price (R-squared = 0.7198), which is a substantial improvement over the global regression model. The Adjusted R-squared is 0.6743, indicating that the model explains a significant portion of the variance while accounting for the number of predictors.\nAIC/BIC: The GWR model shows lower AIC (36,602.13) and BIC (36,179.57) values than the global regression model, suggesting that the GWR model provides a better fit to the data when accounting for spatial variability.\n\nOverall\nSpatially varying coefficients highlight that the impact of certain factors, such as floor_area_sqm, unit_age, and proximity to amenities like hawker centers and supermarkets, can differ significantly across different locations, suggesting that location-specific interventions or policies might be more effective in real estate pricing.\n\n\n\n\n\n\nCode Chunk\ngwr_adaptive_w &lt;- read_rds(\"data/rds/ml/woodlands/gwr_adaptive_w.rds\")\n\n\n\n\nCode Chunk\ngwr_adaptive_w\n\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-11-11 01:51:57.292957 \n   Call:\n   gwr.basic(formula = resale_price ~ floor_area_sqm + unit_age + \n    within_350m_busstop + within_350m_eldercare + hawker_prox + \n    within_350m_kindergarten + mall_prox + mrt_prox + park_prox + \n    within_1km_school + supermarket_prox, data = train_data_sp_w, \n    bw = bw_adaptive_w, kernel = \"gaussian\", adaptive = TRUE, \n    longlat = FALSE)\n\n   Dependent (y) variable:  resale_price\n   Independent variables:  floor_area_sqm unit_age within_350m_busstop within_350m_eldercare hawker_prox within_350m_kindergarten mall_prox mrt_prox park_prox within_1km_school supermarket_prox\n   Number of data points: 1391\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n    Min      1Q  Median      3Q     Max \n-108605  -18479    -861   17651  118328 \n\n   Coefficients:\n                             Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)              475932.61   72526.66   6.562 7.48e-11 ***\n   floor_area_sqm             2320.91     142.03  16.341  &lt; 2e-16 ***\n   unit_age                  -4309.72      84.93 -50.742  &lt; 2e-16 ***\n   within_350m_busstop         386.65     394.41   0.980   0.3271    \n   within_350m_eldercare      1333.49    1282.39   1.040   0.2986    \n   hawker_prox              -16533.03    3163.02  -5.227 1.99e-07 ***\n   within_350m_kindergarten   2531.64    1171.15   2.162   0.0308 *  \n   mall_prox                 -1004.92    1442.73  -0.697   0.4862    \n   mrt_prox                 -44163.93    4902.52  -9.008  &lt; 2e-16 ***\n   park_prox                -15524.87    2149.37  -7.223 8.38e-13 ***\n   within_1km_school           693.69     670.29   1.035   0.3009    \n   supermarket_prox             23.45    5293.65   0.004   0.9965    \n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 28570 on 1379 degrees of freedom\n   Multiple R-squared: 0.748\n   Adjusted R-squared: 0.746 \n   F-statistic: 372.1 on 11 and 1379 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 1.125708e+12\n   Sigma(hat): 28468.32\n   AIC:  32505.2\n   AICc:  32505.46\n   BIC:  31276.38\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 48 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                                   Min.     1st Qu.      Median     3rd Qu.\n   Intercept                -1.1312e+09 -1.5632e+06  3.9476e+05  1.9149e+06\n   floor_area_sqm           -1.5228e+04  9.8373e+02  2.1764e+03  2.7568e+03\n   unit_age                 -5.2038e+03 -4.5082e+03 -3.9677e+03 -2.2334e+03\n   within_350m_busstop      -5.5262e+03 -1.7354e+03 -5.3815e+02  1.2150e+03\n   within_350m_eldercare    -4.4071e+05 -1.9180e+04 -5.4921e+03  3.5630e+02\n   hawker_prox              -3.5823e+05 -3.8041e+04 -6.8955e+03  4.1019e+04\n   within_350m_kindergarten -3.9285e+04 -4.2950e+03  6.3734e+02  6.3378e+03\n   mall_prox                -2.4362e+05 -2.6020e+04  1.5904e+03  4.0938e+04\n   mrt_prox                 -6.7648e+05 -6.8419e+04 -1.5806e+04  2.0814e+04\n   park_prox                -3.5428e+05 -6.1010e+04 -2.2539e+04  2.6743e+03\n   within_1km_school        -5.6562e+05 -9.8708e+03 -5.0483e+03 -2.4167e+03\n   supermarket_prox         -5.4534e+05 -6.9462e+04 -3.9335e+04 -2.4152e+03\n                                  Max.\n   Intercept                13797986.0\n   floor_area_sqm              18382.0\n   unit_age                    13828.7\n   within_350m_busstop          4947.6\n   within_350m_eldercare     1085786.9\n   hawker_prox              22332689.8\n   within_350m_kindergarten   129965.6\n   mall_prox                21420842.0\n   mrt_prox                  3950844.9\n   park_prox                  787950.9\n   within_1km_school            7588.5\n   supermarket_prox            94873.0\n   ************************Diagnostic information*************************\n   Number of data points: 1391 \n   Effective number of parameters (2trace(S) - trace(S'S)): 131.5628 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 1259.437 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 32347.02 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 32221.73 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 31488.57 \n   Residual sum of squares: 867184876654 \n   R-square value:  0.8058492 \n   Adjusted R-square value:  0.7855518 \n\n   ***********************************************************************\n   Program stops at: 2024-11-11 01:51:58.372047 \n\n\n\n\n\n\n\n\nNote\n\n\n\nInsights of 2 Regression Analysis in predicting resale price - Woodlands\n\nGlobal Regression\n\n\nResiduals: The residuals range from -110,060 to 118,690, with a median of -926. This shows that the model can produce both underestimations and overestimations of resale price, with some extreme residuals indicating potential outliers.\nCoefficients:\n\nIntercept: The base resale price when all predictors are zero is estimated at 435,172.15.\nSignificant Variables:\n\nfloor_area_sqm: Each additional square meter increases the resale price by 2,247.33.\nunit_age: Each additional year of unit age decreases the resale price by 4,322.06.\nhawker_prox: Proximity to hawker centers decreases resale price by -14,811.35.\nmrt_prox: Proximity to MRT stations has a significant negative effect of -45,257.65 on resale price.\npark_prox: Proximity to parks also significantly decreases resale price by -15,817.05.\nwithin_350m_kindergarten: The presence of a kindergarten within 350 meters increases resale price by 3,083.36.\n\n\nModel Fit:\n\nR-squared: The model explains about 74.83% of the variance in resale price, indicating a strong overall fit.\nAdjusted R-squared: 74.63%, which adjusts for the number of predictors.\nF-statistic: The F-statistic is 372.7, with a p-value of less than 2.2e-16, indicating that the model is highly significant.\n\nAIC/BIC:\n\nAIC: 32,503.3, and BIC: 31,274.49, which provide a basis for model comparison and indicate that the global model fits the data reasonably well.\n\n\n\nGeographical Weighted Regression\n\n\nModel Calibration:\n\nAdaptive Bandwidth: 48 neighbors were used in the adaptive bandwidth, ensuring that the local influence of each observation is based on the proximity of other observations.\nKernel Function: A Gaussian kernel was used, which gives more weight to observations closer to each location.\n\nCoefficient Estimates: The coefficients for the predictors vary significantly across geographic locations:\n\nfloor_area_sqm: The effect of floor area on resale price ranges from negative to positive values. In some areas, additional floor space leads to a large increase in resale price, while in others, the impact is smaller.\nunit_age: The effect of unit age varies spatially, with some areas showing a significant negative impact on resale price, while in others, the impact is less pronounced.\nhawker_prox: The proximity to hawker centers shows a negative relationship with resale price in some regions, with a large range in coefficient estimates, suggesting that the effect of hawker centers on property prices varies significantly by location.\nmrt_prox: Proximity to MRT stations shows a negative effect in certain locations, with varying strength across different regions in Woodlands.\npark_prox: Proximity to parks also varies across locations, with some areas showing a stronger negative impact on resale price than others.\n\nModel Fit:\n\nR-squared: The GWR model explains 80.58% of the variance in resale price, which represents a significant improvement over the global model (74.83%). This indicates that incorporating spatial variability greatly enhances model accuracy.\nAdjusted R-squared: 78.52%, further indicating that the GWR model accounts for a significant amount of variance while adjusting for the number of predictors.\n\nAIC/BIC:\n\nAIC: 32,223.42 and BIC: 31,497.62, which are lower than the values for the global model, indicating that the GWR model provides a better fit for the data when accounting for spatial differences.\n\nResidual Sum of Squares (RSS): The GWR model has a residual sum of squares of 8.67507e+11, suggesting that the model has reduced unexplained variance compared to the global model.\n\nOverall\nThe GWR model provides a much better fit for the resale price data in Woodlands compared to the global regression model. The improvement in R-squared (from 74.83% to 80.58%) suggests that there are significant spatial differences in the factors affecting resale prices.\n\nFloor Area: The positive relationship between floor area and resale price varies significantly across Woodlands. In some areas, floor area is a strong predictor of resale price, while in others, it has a smaller effect.\nUnit Age: The negative effect of unit age on resale price is more pronounced in some parts of Woodlands, indicating that older units in certain locations have a larger impact on resale price.\nProximity to Amenities: Variables such as proximity to MRT stations, parks, and hawker centers show varying effects across locations. For example, properties near MRT stations tend to have lower resale prices in some regions, while in others, the proximity to parks and kindergartens can increase prices.\n\n\n\n\n\n\n\n\n\n\nPunggolWoodlands\n\n\n\n\nCode Chunk\ntest_data_sp_p &lt;- as_Spatial(test_data_punggol)\ntest_data_sp_p\n\n\nclass       : SpatialPointsDataFrame \nfeatures    : 300 \nextent      : 34320.85, 37549.49, 41631.21, 43933.95  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 20\nnames       :    town, resale_price, floor_area_sqm, remaining_lease_yr,        longitude,         latitude, unit_age,       busstop_prox, within_350m_busstop,       eldercare_prox, within_350m_eldercare,      hawker_prox,   kindergarten_prox, within_350m_kindergarten,        mall_prox, ... \nmin values  : PUNGGOL,       495000,             85,                 77, 34320.8500594051, 41631.2121389891,        5, 0.0287942812318591,                   2, 9.08133417710502e-07,                     0, 0.28631929470226, 5.4646933332265e-07,                        0, 54.6825044697734, ... \nmax values  : PUNGGOL,       788000,             99,                 94, 37549.4930230187, 43933.9470685477,       22,  0.286960879696002,                  14,     1.69953951976499,                     2, 1.94955185447985,   0.764704244775304,                        3, 56.6972972869625, ... \n\n\n\n\n\n\nCode Chunk\ntest_data_sp_w &lt;- as_Spatial(test_data_woodlands)\ntest_data_sp_w\n\n\nclass       : SpatialPointsDataFrame \nfeatures    : 238 \nextent      : 21177.26, 25186.96, 45373.58, 47860.89  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 20\nnames       :      town, resale_price, floor_area_sqm, remaining_lease_yr,        longitude,         latitude, unit_age,       busstop_prox, within_350m_busstop,     eldercare_prox, within_350m_eldercare,        hawker_prox,  kindergarten_prox, within_350m_kindergarten,        mall_prox, ... \nmin values  : WOODLANDS,       355000,             83,                 48, 21177.2576852251, 45373.5757321722,        5, 0.0205869708371814,                   4, 0.0656405943898966,                     0, 0.0666360841718605, 8.502567959163e-07,                        0, 50.0713376927117, ... \nmax values  : WOODLANDS,       701000,            111,                 94, 25186.9617010174, 47860.8875505569,       51,  0.253082170943171,                  16,   1.65460210369017,                     3,    1.5143514513644,  0.798763870107565,                        4, 53.6923986369607, ... \n\n\n\n\n\n\n\n\n\nPunggolWoodlands\n\n\n\n\nCode Chunk\ngwr_bw_test_adaptive_p &lt;- bw.gwr(resale_price ~ floor_area_sqm +\n                 unit_age + within_350m_busstop +\n                 within_350m_eldercare + hawker_prox + \n                 within_350m_kindergarten + mall_prox +\n                 mrt_prox + park_prox +\n                 within_1km_school +\n                 supermarket_prox,\n                  data=test_data_sp_p,\n                  approach=\"CV\",\n                  kernel=\"gaussian\",\n                  adaptive=TRUE,\n                  longlat=FALSE)\n\n\nAdaptive bandwidth: 193 CV score: 461863083474 \nAdaptive bandwidth: 127 CV score: 444482582747 \nAdaptive bandwidth: 86 CV score: 429860319803 \nAdaptive bandwidth: 60 CV score: 4.15545e+11 \nAdaptive bandwidth: 45 CV score: 405403507470 \nAdaptive bandwidth: 34 CV score: 406000083766 \nAdaptive bandwidth: 50 CV score: 408265912967 \nAdaptive bandwidth: 40 CV score: 406022699918 \nAdaptive bandwidth: 46 CV score: 405735445126 \nAdaptive bandwidth: 42 CV score: 406270763040 \nAdaptive bandwidth: 44 CV score: 405653093262 \nAdaptive bandwidth: 42 CV score: 406270763040 \nAdaptive bandwidth: 43 CV score: 4.05838e+11 \nAdaptive bandwidth: 42 CV score: 406270763040 \nAdaptive bandwidth: 42 CV score: 406270763040 \nAdaptive bandwidth: 41 CV score: 405877056522 \nAdaptive bandwidth: 41 CV score: 405877056522 \nAdaptive bandwidth: 40 CV score: 406022699918 \nAdaptive bandwidth: 40 CV score: 406022699918 \nAdaptive bandwidth: 39 CV score: 406410298495 \nAdaptive bandwidth: 39 CV score: 406410298495 \nAdaptive bandwidth: 38 CV score: 406480426659 \nAdaptive bandwidth: 38 CV score: 406480426659 \nAdaptive bandwidth: 37 CV score: 405643842893 \nAdaptive bandwidth: 37 CV score: 405643842893 \nAdaptive bandwidth: 36 CV score: 405801331672 \nAdaptive bandwidth: 36 CV score: 405801331672 \nAdaptive bandwidth: 35 CV score: 405721054702 \nAdaptive bandwidth: 35 CV score: 405721054702 \nAdaptive bandwidth: 34 CV score: 406000083766 \nAdaptive bandwidth: 34 CV score: 406000083766 \nAdaptive bandwidth: 33 CV score: 405311774904 \n\n\nCode Chunk\nbeep(3)\n\n\n\n\nCode Chunk\nwrite_rds(gwr_bw_test_adaptive_p, \"data/rds/ml/punggol/gwr_bw_test_adaptive_p.rds\")\n\n\n\n\n\n\nCode Chunk\ngwr_bw_test_adaptive_w&lt;- bw.gwr(resale_price ~ floor_area_sqm +\n                 unit_age + within_350m_busstop +\n                 within_350m_eldercare + hawker_prox + \n                 within_350m_kindergarten + mall_prox +\n                 mrt_prox + park_prox +\n                 within_1km_school +\n                 supermarket_prox,\n                  data=test_data_sp_w,\n                  approach=\"CV\",\n                  kernel=\"gaussian\",\n                  adaptive=TRUE,\n                  longlat=FALSE)\n\n\nAdaptive bandwidth: 154 CV score: 141474677352 \nAdaptive bandwidth: 103 CV score: 140090697462 \nAdaptive bandwidth: 70 CV score: 138907367486 \nAdaptive bandwidth: 51 CV score: 137432823055 \nAdaptive bandwidth: 38 CV score: 135519123552 \nAdaptive bandwidth: 31 CV score: 134512516189 \nAdaptive bandwidth: 25 CV score: 134598796510 \nAdaptive bandwidth: 33 CV score: 1.34745e+11 \nAdaptive bandwidth: 28 CV score: 135016725804 \nAdaptive bandwidth: 31 CV score: 134512516189 \n\n\nCode Chunk\nbeep(3)\n\n\n\n\nCode Chunk\nwrite_rds(gwr_bw_test_adaptive_w, \"data/rds/ml/woodlands/gwr_bw_test_adaptive_w.rds\")\n\n\n\n\n\n\n\n\n\nPunggolWoodlands\n\n\n\n\nCode Chunk\ngwr_pred_p &lt;- gwr.predict(formula = resale_price ~ floor_area_sqm +\n                 unit_age + within_350m_busstop +\n                 within_350m_eldercare + hawker_prox + \n                 within_350m_kindergarten + mall_prox +\n                 mrt_prox + park_prox +\n                 within_1km_school +\n                 supermarket_prox, \n                        data=train_data_sp_p, \n                        predictdata = test_data_sp_p, \n                        bw=100, \n                        kernel = 'gaussian', \n                        adaptive=TRUE, \n                        longlat = FALSE)\n\nbeep(3)\n\n\n\n\nCode Chunk\ngwr_pred_p\n\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-11-11 01:51:59.702211 \n   Call:\n   gwr.predict(formula = resale_price ~ floor_area_sqm + unit_age + \n    within_350m_busstop + within_350m_eldercare + hawker_prox + \n    within_350m_kindergarten + mall_prox + mrt_prox + park_prox + \n    within_1km_school + supermarket_prox, data = train_data_sp_p, \n    predictdata = test_data_sp_p, bw = 100, kernel = \"gaussian\", \n    adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable for prediction:  resale_price\n   Independent variables:  floor_area_sqm unit_age within_350m_busstop within_350m_eldercare hawker_prox within_350m_kindergarten mall_prox mrt_prox park_prox within_1km_school supermarket_prox\n   Number of data points: 1549\n   ***********************************************************************\n   *     Results of Geographically Weighted Regression for prediction    *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 100 (number of nearest neighbours)\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                                        Min.     1st Qu.      Median\n   Intercept_coef                -1.1137e+07 -4.0950e+06 -1.8008e+06\n   floor_area_sqm_coef            6.5380e+02  7.4184e+03  9.3916e+03\n   unit_age_coef                 -7.4305e+03 -3.9280e+03 -1.7500e+03\n   within_350m_busstop_coef      -5.3279e+03 -2.1329e+03 -5.4695e+02\n   within_350m_eldercare_coef    -3.0346e+04  2.0333e+03  1.8049e+04\n   hawker_prox_coef              -4.4176e+05 -2.1443e+05 -7.7957e+04\n   within_350m_kindergarten_coef -2.4241e+04 -1.6656e+04 -8.7511e+03\n   mall_prox_coef                -1.9105e+05  9.2298e+03  2.5243e+04\n   mrt_prox_coef                 -1.5919e+05 -8.3799e+04 -5.2551e+04\n   park_prox_coef                -9.6160e+04  1.4933e+04  5.4488e+04\n   within_1km_school_coef        -1.1990e+04 -6.1249e+03 -3.9312e+03\n   supermarket_prox_coef         -1.2277e+05 -5.0793e+04 -2.8030e+04\n                                     3rd Qu.       Max.\n   Intercept_coef                -5.5879e+05 10743979.3\n   floor_area_sqm_coef            1.2740e+04    19011.5\n   unit_age_coef                  4.2753e+03    18101.9\n   within_350m_busstop_coef       1.1316e+03     3852.9\n   within_350m_eldercare_coef     2.8801e+04    65407.6\n   hawker_prox_coef              -1.8368e+04   113086.4\n   within_350m_kindergarten_coef -3.7455e+02    15082.2\n   mall_prox_coef                 6.2916e+04   200516.3\n   mrt_prox_coef                 -1.7414e+04   165209.6\n   park_prox_coef                 1.3493e+05   365020.5\n   within_1km_school_coef        -1.1988e+03     7789.1\n   supermarket_prox_coef          3.2259e+04   117388.1\n\n   ****************       Results of GW prediction       ******************\n                        Min.    1st Qu.     Median    3rd Qu.       Max.\n   prediction         459054     588680     609707     646438     708324\n   prediction_var 1249829990 1264888497 1274282131 1284856043 1412261610\n\n   ***********************************************************************\n   Program stops at: 2024-11-11 01:52:47.242338 \n\n\n\n\nCode Chunk\nwrite_rds(gwr_pred_p, \"data/rds/ml/punggol/gwr_pred_p.rds\")\n\n\n\n\n\n\n\n\nNote\n\n\n\nGWR Coefficient Estimates\nInterpretation:\n\nThe Intercept varies greatly across locations, ranging from a negative coefficient (-1,441,251.41) to a positive coefficient (786,268.8), indicating that baseline resale prices are influenced differently across locations.\nfloor_area_sqm: The effect of floor area on resale price ranges from negative to positive. In some locations, a larger floor area has a smaller impact, while in others it has a much larger effect.\nunit_age: The negative relationship between unit age and resale price is more pronounced in some areas, suggesting that older units may have a larger negative impact on property values in certain locations.\nProximity to amenities: Variables such as hawker_prox, mrt_prox, park_prox, and supermarket_prox show a wide range of coefficients. For example, proximity to hawker centers has a strong negative impact in some regions, while proximity to parks and supermarkets has a positive impact in others.\n\nPrediction Results\nInterpretation:\n\nThe predicted resale prices range from $463,945 to $707,938, with a median prediction of $609,348. These values are within a reasonable range given the data and suggest a good predictive performance.\nThe prediction variance shows a substantial range, from about 1.25 billion to 1.4 billion, indicating that the predicted resale prices vary considerably. This is expected, as some locations may have more variability in resale price due to factors such as proximity to amenities and the overall market conditions.\n\nOverall\nKey findings include:\n\nThe effect of floor area, unit age, and proximity to amenities varies significantly across different locations in Woodlands.\nThe prediction results show a reasonable range of predicted resale prices, with substantial variance indicating the sensitivity of prices to local factors.\n\n\n\n\n\n\n\nCode Chunk\ngwr_pred_w &lt;- gwr.predict(formula = resale_price ~ floor_area_sqm +\n                 unit_age + within_350m_busstop +\n                 within_350m_eldercare + hawker_prox + \n                 within_350m_kindergarten + mall_prox +\n                 mrt_prox + park_prox +\n                 within_1km_school +\n                 supermarket_prox, \n                        data=train_data_sp_w, \n                        predictdata = test_data_sp_w, \n                        bw=100, \n                        kernel = 'gaussian', \n                        adaptive=TRUE, \n                        longlat = FALSE)\n\nbeep(3)\n\n\n\n\nCode Chunk\ngwr_pred_w\n\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-11-11 01:52:47.294196 \n   Call:\n   gwr.predict(formula = resale_price ~ floor_area_sqm + unit_age + \n    within_350m_busstop + within_350m_eldercare + hawker_prox + \n    within_350m_kindergarten + mall_prox + mrt_prox + park_prox + \n    within_1km_school + supermarket_prox, data = train_data_sp_w, \n    predictdata = test_data_sp_w, bw = 100, kernel = \"gaussian\", \n    adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable for prediction:  resale_price\n   Independent variables:  floor_area_sqm unit_age within_350m_busstop within_350m_eldercare hawker_prox within_350m_kindergarten mall_prox mrt_prox park_prox within_1km_school supermarket_prox\n   Number of data points: 1391\n   ***********************************************************************\n   *     Results of Geographically Weighted Regression for prediction    *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 100 (number of nearest neighbours)\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                                        Min.     1st Qu.      Median\n   Intercept_coef                -2.1902e+06 -9.3788e+04  6.7361e+05\n   floor_area_sqm_coef           -3.3112e+03  1.8210e+03  2.2761e+03\n   unit_age_coef                 -4.9718e+03 -4.5761e+03 -4.3469e+03\n   within_350m_busstop_coef      -2.2179e+03 -6.0128e+02 -9.4503e+01\n   within_350m_eldercare_coef    -4.5128e+04 -9.5403e+03 -5.8673e+03\n   hawker_prox_coef              -2.2862e+05 -2.6585e+04 -1.4338e+04\n   within_350m_kindergarten_coef -9.6328e+03 -2.6297e+03 -4.1932e+01\n   mall_prox_coef                -4.8135e+04 -1.0964e+04 -3.7845e+03\n   mrt_prox_coef                 -1.7215e+05 -4.2204e+04 -2.1621e+04\n   park_prox_coef                -1.3435e+05 -4.0264e+04 -2.7108e+04\n   within_1km_school_coef        -9.4865e+03 -5.4721e+03 -3.3587e+03\n   supermarket_prox_coef         -9.7199e+04 -4.7539e+04 -3.3118e+04\n                                     3rd Qu.      Max.\n   Intercept_coef                 1.1117e+06 2976576.6\n   floor_area_sqm_coef            2.5522e+03    3019.2\n   unit_age_coef                 -3.7747e+03   -1769.4\n   within_350m_busstop_coef       7.3075e+02    3301.4\n   within_350m_eldercare_coef    -1.4672e+03    4353.7\n   hawker_prox_coef               3.0435e+03  100863.4\n   within_350m_kindergarten_coef  3.8679e+03   24965.7\n   mall_prox_coef                 1.0691e+04   55502.5\n   mrt_prox_coef                 -7.2434e+03  148627.2\n   park_prox_coef                -1.3584e+04   94005.1\n   within_1km_school_coef        -2.7247e+02    2829.4\n   supermarket_prox_coef         -1.0807e+04   28141.1\n\n   ****************       Results of GW prediction       ******************\n                       Min.   1st Qu.    Median   3rd Qu.      Max.\n   prediction        354574    475267    494160    528380    617913\n   prediction_var 737877010 745713670 752120804 759546466 799428816\n\n   ***********************************************************************\n   Program stops at: 2024-11-11 01:53:19.356233 \n\n\n\n\nCode Chunk\nwrite_rds(gwr_pred_w, \"data/rds/ml/woodlands/gwr_pred_w.rds\")\n\n\n\n\n\n\n\n\nNote\n\n\n\nSummary of GWR Coefficients\nInterpretation:\n\nIntercept: The baseline resale price in Woodlands varies significantly across locations, with a range from 373,726.15 to 929,191.50. This suggests that the average resale price can be strongly influenced by local spatial factors.\nFloor Area: The effect of floor area on resale price varies from a negative to a positive coefficient. In some locations, additional floor area has a relatively small positive impact, while in other locations, the effect is stronger.\nUnit Age: The negative impact of unit age on resale price is consistent across locations. Older units tend to have a more significant negative effect on resale prices, particularly in areas where newer properties dominate.\nProximity to Amenities:\n\nBus Stops: Proximity to bus stops generally has a small positive effect in certain areas (range from -1,933.97 to 3,494.30).\nElder Care Centers: The proximity to eldercare centers has a more varied effect, with the coefficient ranging from negative to positive. In some regions, this proximity significantly increases resale prices.\nHawker Centers: The proximity to hawker centers shows a highly negative impact in many areas, especially in places where hawker centers are abundant and could be seen as less desirable.\nKindergartens: Proximity to kindergartens has a positive effect on property prices in certain locations, especially in family-oriented neighborhoods.\nMalls, MRT, Parks: Proximity to malls and MRT stations tends to have a negative impact on resale prices in specific areas, while proximity to parks shows a more varied effect, with some areas benefiting from this proximity more than others.\nSupermarkets: Proximity to supermarkets has a small negative effect on resale prices in some areas but is generally less influential compared to other amenities.\n\n\nPrediction Results\nInterpretation:\n\nThe predicted resale prices range from $359,170 to $616,562, with a median prediction of $493,786. This shows that the resale price in Woodlands varies significantly across locations.\nThe prediction variance is substantial, ranging from 735 million to 792 million. This suggests considerable variability in the predicted prices, likely due to differing local factors influencing property values across different areas of Woodlands.\n\nOverall\nThe Geographically Weighted Regression (GWR) model provides a more accurate prediction of resale prices in Woodlands compared to traditional global models. By accounting for spatial variability, the GWR model captures how the relationship between property features and resale price changes across different locations.\nKey findings:\n\nFloor area and unit age have spatially varying impacts, with some regions showing stronger relationships between these variables and resale prices.\nProximity to amenities such as hawker centers, MRT stations, and supermarkets shows spatial heterogeneity, with some areas benefiting more from proximity to these amenities than others.\nThe prediction results indicate that resale prices vary widely across Woodlands, with certain regions experiencing higher values based on local factors like proximity to key amenities."
  },
  {
    "objectID": "TakeHomeExercise/TakeHome3/TakeHome3.html#random-forest",
    "href": "TakeHomeExercise/TakeHome3/TakeHome3.html#random-forest",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "PunggolWoodlands\n\n\n\n\nCode Chunk\ncoords_train_p &lt;- st_coordinates(train_data_punggol)\ncoords_test_p &lt;- st_coordinates(test_data_punggol)\n\n\nBefore continue, we write all the output into rds for future used.\n\n\nCode Chunk\ncoords_train_p &lt;- write_rds(coords_train_p, \"data/rds/ml/punggol/coords_train_p.rds\" )\ncoords_test_p &lt;- write_rds(coords_test_p, \"data/rds/ml/punggol/coords_test_p.rds\" )\n\n\n\n\n\n\nCode Chunk\ncoords_train_w &lt;- st_coordinates(train_data_woodlands)\ncoords_test_w &lt;- st_coordinates(test_data_woodlands)\n\n\nBefore continue, we write all the output into rds for future used.\n\n\nCode Chunk\ncoords_train_w &lt;- write_rds(coords_train_w, \"data/rds/ml/coords_train_w.rds\" )\ncoords_test_w &lt;- write_rds(coords_test_w, \"data/rds/ml/coords_test_w.rds\" )\n\n\n\n\n\n\n\n\nFirst, we will drop geometry column of the sf data.frame by using st_drop_geometry() of sf package.\n\nPunggolWoodlands\n\n\n\n\nCode Chunk\ntrain_data_punggol &lt;- train_data_punggol %&gt;% \n  st_drop_geometry() %&gt;% \n  select(-town)\n\n\n\n\n\n\nCode Chunk\ntrain_data_woodlands &lt;- train_data_woodlands %&gt;% \n  st_drop_geometry() %&gt;% \n  select(-town)"
  },
  {
    "objectID": "TakeHomeExercise/TakeHome3/TakeHome3.html#calibrating-random-forest-model",
    "href": "TakeHomeExercise/TakeHome3/TakeHome3.html#calibrating-random-forest-model",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "In this section, you will learn how to calibrate a model to predict HDB resale price by using random forest function of ranger package.\n\nPunggolWoodlands\n\n\n\n\nCode Chunk\nset.seed(1234)\nrf_p &lt;- ranger(resale_price ~ floor_area_sqm +\n                 unit_age + within_350m_busstop +\n                 within_350m_eldercare + hawker_prox + \n                 within_350m_kindergarten + mall_prox +\n                 mrt_prox + park_prox +\n                 within_1km_school +\n                 supermarket_prox,\n             data=train_data_punggol)\nrf_p\n\n\nRanger result\n\nCall:\n ranger(resale_price ~ floor_area_sqm + unit_age + within_350m_busstop +      within_350m_eldercare + hawker_prox + within_350m_kindergarten +      mall_prox + mrt_prox + park_prox + within_1km_school + supermarket_prox,      data = train_data_punggol) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      1549 \nNumber of independent variables:  11 \nMtry:                             3 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       1285768219 \nR squared (OOB):                  0.6229873 \n\n\n\n\nCode Chunk\nwrite_rds(rf_p, \"data/rds/ml/punggol/rf_p.rds\")\n\n\n\n\nCode Chunk\nrf_p &lt;- read_rds(\"data/rds/ml/punggol/rf_p.rds\")\nrf_p\n\n\nRanger result\n\nCall:\n ranger(resale_price ~ floor_area_sqm + unit_age + within_350m_busstop +      within_350m_eldercare + hawker_prox + within_350m_kindergarten +      mall_prox + mrt_prox + park_prox + within_1km_school + supermarket_prox,      data = train_data_punggol) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      1549 \nNumber of independent variables:  11 \nMtry:                             3 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       1285768219 \nR squared (OOB):                  0.6229873 \n\n\n\n\n\n\nCode Chunk\nset.seed(1234)\nrf_w &lt;- ranger(resale_price ~ floor_area_sqm +\n                 unit_age + within_350m_busstop +\n                 within_350m_eldercare + hawker_prox + \n                 within_350m_kindergarten + mall_prox +\n                 mrt_prox + park_prox +\n                 within_1km_school +\n                 supermarket_prox,\n             data=train_data_woodlands)\nrf_w\n\n\nRanger result\n\nCall:\n ranger(resale_price ~ floor_area_sqm + unit_age + within_350m_busstop +      within_350m_eldercare + hawker_prox + within_350m_kindergarten +      mall_prox + mrt_prox + park_prox + within_1km_school + supermarket_prox,      data = train_data_woodlands) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      1391 \nNumber of independent variables:  11 \nMtry:                             3 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       681781375 \nR squared (OOB):                  0.7878284 \n\n\n\n\nCode Chunk\nwrite_rds(rf_w, \"data/rds/ml/woodlands/rf_w.rds\")\n\n\n\n\nCode Chunk\nrf_w &lt;- read_rds(\"data/rds/ml/woodlands/rf_w.rds\")\nrf_w\n\n\nRanger result\n\nCall:\n ranger(resale_price ~ floor_area_sqm + unit_age + within_350m_busstop +      within_350m_eldercare + hawker_prox + within_350m_kindergarten +      mall_prox + mrt_prox + park_prox + within_1km_school + supermarket_prox,      data = train_data_woodlands) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      1391 \nNumber of independent variables:  11 \nMtry:                             3 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       681781375 \nR squared (OOB):                  0.7878284"
  },
  {
    "objectID": "TakeHomeExercise/TakeHome3/TakeHome3.html#calibrating-geographical-random-forest-model",
    "href": "TakeHomeExercise/TakeHome3/TakeHome3.html#calibrating-geographical-random-forest-model",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "In this section, you will learn how to calibrate a model to predict HDB resale price by using grf() of SpatialML package.\n\nPunggolWoodlands\n\n\n\n\nCode Chunk\nset.seed(1234)\ngwRF_adaptive_p &lt;- grf(formula = resale_price ~ floor_area_sqm +\n                 unit_age + within_350m_busstop +\n                 within_350m_eldercare + hawker_prox + \n                 within_350m_kindergarten + mall_prox +\n                 mrt_prox + park_prox +\n                 within_1km_school +\n                 supermarket_prox,\n                      dframe=train_data_punggol, \n                     bw=55,\n                     kernel=\"adaptive\",\n                     coords=coords_train_p,\n                     ntree = 50)\n\n\nRanger result\n\nCall:\n ranger(resale_price ~ floor_area_sqm + unit_age + within_350m_busstop +      within_350m_eldercare + hawker_prox + within_350m_kindergarten +      mall_prox + mrt_prox + park_prox + within_1km_school + supermarket_prox,      data = train_data_punggol, num.trees = 50, mtry = 3, importance = \"impurity\",      num.threads = NULL) \n\nType:                             Regression \nNumber of trees:                  50 \nSample size:                      1549 \nNumber of independent variables:  11 \nMtry:                             3 \nTarget node size:                 5 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       1310728297 \nR squared (OOB):                  0.6156685 \n\n\n          floor_area_sqm                 unit_age      within_350m_busstop \n            6.326567e+11             1.156479e+12             1.244582e+11 \n   within_350m_eldercare              hawker_prox within_350m_kindergarten \n            1.596830e+11             5.174056e+11             1.464665e+11 \n               mall_prox                 mrt_prox                park_prox \n            5.932477e+11             3.333798e+11             2.561672e+11 \n       within_1km_school         supermarket_prox \n            1.749702e+11             3.213997e+11 \n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-126714  -16526    2460    2945   23939  129080 \n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-96078.65 -13775.10    855.28    -58.52  14384.32 119674.72 \n\n\n                               Min         Max       Mean        StD\nfloor_area_sqm                   0 91014202939 5906598515 9572494892\nunit_age                 772992909 70292040371 9653682844 7816982408\nwithin_350m_busstop              0 23284152693 2079171556 2163833036\nwithin_350m_eldercare            0 22318151206  229763601  939144823\nhawker_prox              125992847 77365795037 4763798655 4696341821\nwithin_350m_kindergarten         0 22978183199  737940280 1744984043\nmall_prox                 89425394 51937220048 4875866267 4755292035\nmrt_prox                 131598882 29974992166 4057921881 3311668548\npark_prox                108935979 38285549900 4649691712 4018345994\nwithin_1km_school                0 22017741359  794052428 1434695546\nsupermarket_prox          38701460 63205009429 4233523256 4566974721\n\n\nCode Chunk\nbeep(3)\n\n\n\n\nCode Chunk\nwrite_rds(gwRF_adaptive_p, \"data/rds/ml/punggol/gwRF_adaptive_p.rds\")\n\n\n\n\nCode Chunk\ngwRF_adaptive_p &lt;- read_rds(\"data/rds/ml/punggol/gwRF_adaptive_p.rds\")\n\n\n\n\n\n\nCode Chunk\nset.seed(1234)\ngwRF_adaptive_w &lt;- grf(formula = resale_price ~ floor_area_sqm +\n                 unit_age + within_350m_busstop +\n                 within_350m_eldercare + hawker_prox + \n                 within_350m_kindergarten + mall_prox +\n                 mrt_prox + park_prox +\n                 within_1km_school +\n                 supermarket_prox,\n                      dframe=train_data_woodlands, \n                     bw=55,\n                     kernel=\"adaptive\",\n                     coords=coords_train_w,\n                     ntree = 50)\n\n\nRanger result\n\nCall:\n ranger(resale_price ~ floor_area_sqm + unit_age + within_350m_busstop +      within_350m_eldercare + hawker_prox + within_350m_kindergarten +      mall_prox + mrt_prox + park_prox + within_1km_school + supermarket_prox,      data = train_data_woodlands, num.trees = 50, mtry = 3, importance = \"impurity\",      num.threads = NULL) \n\nType:                             Regression \nNumber of trees:                  50 \nSample size:                      1391 \nNumber of independent variables:  11 \nMtry:                             3 \nTarget node size:                 5 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       699465884 \nR squared (OOB):                  0.7823249 \n\n\n          floor_area_sqm                 unit_age      within_350m_busstop \n            6.498645e+11             1.523757e+12             8.640691e+10 \n   within_350m_eldercare              hawker_prox within_350m_kindergarten \n            2.443447e+10             1.374084e+11             6.735891e+10 \n               mall_prox                 mrt_prox                park_prox \n            2.366135e+11             4.865546e+11             2.638215e+11 \n       within_1km_school         supermarket_prox \n            2.269810e+11             3.863464e+11 \n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-162753.7  -14222.8       0.0     207.3   15208.6  104482.1 \n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-73326.46  -8051.71    102.83    -40.93   8585.95  73157.76 \n\n\n                               Min          Max       Mean         StD\nfloor_area_sqm                   0 119507423558 7235009419 12283139013\nunit_age                 290434405 162351531337 9787543745 16190314956\nwithin_350m_busstop       19024539 101166507333 3038063745  9092850294\nwithin_350m_eldercare            0  43950239157  391375201  1601131616\nhawker_prox              205434461 162703114194 4496604507  7941092644\nwithin_350m_kindergarten         0  28289156658  710152654  1940630865\nmall_prox                165366043  81654791451 4917757094  7011474221\nmrt_prox                 310086561  77191474562 4237235607  5629134036\npark_prox                166726290 137269067979 5310916373 12030607333\nwithin_1km_school                0  70335686672 1024510779  4531548436\nsupermarket_prox         154822760 168893795321 5782731401 13086332964\n\n\nCode Chunk\nbeep(3)\n\n\n\n\nCode Chunk\nwrite_rds(gwRF_adaptive_w, \"data/rds/ml/woodlands/gwRF_adaptive_w.rds\")\n\n\n\n\nCode Chunk\ngwRF_adaptive_w &lt;- read_rds(\"data/rds/ml/woodlands/gwRF_adaptive_w.rds\")\n\n\n\n\n\n\n\n\n\nThe code chunk below will be used to combine the test data with its corresponding coordinates data.\n\nPunggolWoodlands\n\n\n\n\nCode Chunk\ntest_data_punggol &lt;- cbind(test_data_punggol, coords_test_p) %&gt;%\n  st_drop_geometry()\n\n\n\n\n\n\nCode Chunk\ntest_data_woodlands &lt;- cbind(test_data_woodlands, coords_test_w) %&gt;%\n  st_drop_geometry()\n\n\n\n\n\n\n\n\nNext, predict.grf() of spatialML package will be used to predict the resale value by using the test data and gwRF_adaptive model calibrated earlier.\n\nPunggolWoodlands\n\n\n\n\nCode Chunk\ngwRF_pred_p &lt;- predict.grf(gwRF_adaptive_p, \n                           test_data_punggol, \n                           x.var.name=\"X\",\n                           y.var.name=\"Y\", \n                           local.w=1,\n                           global.w=0)\n\n\n\n\nCode Chunk\nGRF_pred_p &lt;- write_rds(gwRF_pred_p, \"data/rds/ml/punggol/gwRF_adaptive_p.rds\")\n\n\n\n\n\n\nCode Chunk\ngwRF_pred_w &lt;- predict.grf(gwRF_adaptive_w, \n                           test_data_woodlands, \n                           x.var.name=\"X\",\n                           y.var.name=\"Y\", \n                           local.w=1,\n                           global.w=0)\n\n\n\n\nCode Chunk\nGRF_pred_w &lt;- write_rds(gwRF_pred_w, \"data/rds/ml/woodlands/gwRF_adaptive_w.rds\")\n\n\n\n\n\n\n\n\nThe output of the predict.grf() is a vector of predicted values. It is wiser to convert it into a data frame for further visualisation and analysis.\n\nPunggolWoodlands\n\n\n\n\nCode Chunk\nGRF_pred_p &lt;- read_rds(\"data/rds/ml/punggol/gwRF_adaptive_p.rds\")\n\n\n\n\nCode Chunk\nGRF_pred_df_p &lt;- as.data.frame(GRF_pred_p)\n\n\nIn the code chunk below, cbind() is used to append the predicted values onto test_data\n\n\nCode Chunk\ntest_data_p &lt;- cbind(test_data_punggol, GRF_pred_df_p)\n\n\n\n\nCode Chunk\nwrite_rds(test_data_p, \"data/rds/ml/punggol/test_data_p.rds\")\n\n\n\n\n\n\nCode Chunk\nGRF_pred_w &lt;- read_rds(\"data/rds/ml/woodlands/gwRF_adaptive_w.rds\")\n\n\n\n\nCode Chunk\nGRF_pred_df_w &lt;- as.data.frame(GRF_pred_w)\n\n\nIn the code chunk below, cbind() is used to append the predicted values onto test_data\n\n\nCode Chunk\ntest_data_w &lt;- cbind(test_data_woodlands, GRF_pred_df_w)\n\n\n\n\nCode Chunk\nwrite_rds(test_data_w, \"data/rds/ml/woodlands/test_data_w.rds\")\n\n\n\n\n\n\n\n\n\nThe root mean square error (RMSE) allows us to measure how far predicted values are from observed values in a regression analysis. In the code chunk below, rmse() of Metrics package is used to compute the RMSE.\n\nPunggolWoodlands\n\n\n\n\nCode Chunk\nrmse(test_data_punggol$resale_price, \n     test_data_p$GRF_pred_p)\n\n\n[1] 49338.38\n\n\n\n\nCode Chunk\nsummary(train_data_punggol$resale_price)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 456000  566000  605000  606094  645000  800000 \n\n\n\n\nCode Chunk\nsummary(test_data_p$resale_price)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 495000  610000  648000  650865  689166  788000 \n\n\n\n\n\n\n\n\nNote\n\n\n\nInterpretation\n\nMagnitude of the Error:\n\nThe RMSE of $49,423.08 indicates that, on average, the model’s predicted resale prices deviate from the actual resale prices by approximately $49,423.08.\nGiven that the mean resale price for the test dataset is $650,865, this RMSE represents about 7.5% of the mean resale price. This indicates a relatively reasonable model performance, with predictions being on average about 7.5% off from actual resale prices.\n\nComparison to Median:\n\nThe median resale price in the test dataset is $648,000.\nThe RMSE of $49,423.08 represents about 7.6% of the median resale price, which suggests that the model’s predictions are, on average, off by 7.6% from the actual resale price at the median.\nSince the RMSE is around 7.5-8% of both the mean and median resale prices, it suggests that the model performs fairly well, but there is still some degree of prediction error.\n\nModel Performance:\n\nAn RMSE of $49,423.08 is typical for real estate predictive models. It shows that while the model has a strong performance, especially considering it accounts for the variance in resale prices, there’s still room for improvement, particularly for properties at the lower and higher price ranges.\n\nRange of Resale Prices:\n\nThe resale prices in the test dataset range from $495,000 to $788,000. Given this price range, the RMSE of $49,423.08 suggests that the model is accurate within 7-8% for most properties, but the error could be higher for properties at the extremes (either low or high price points).\nThe training dataset has similar price distributions, but with a slightly lower mean of $606,094 compared to the test dataset’s mean of $650,865, which suggests that the model may be slightly more optimistic for higher resale prices in the test set.\n\nComparison to the Training Data:\n\nThe training dataset has a slightly higher mean resale price ($606,094) compared to the test dataset ($650,865), suggesting that the model might have been slightly undertrained or overfitted. This could be further analyzed by evaluating how well the model generalizes to the test data.\nThe relatively large spread between the lower and upper bounds of the resale prices (from $456,000 to $800,000 in the training set) might also contribute to some variance in the predictions, though the RMSE indicates that the model is generally effective.\n\n\nConclusion:\nThe RMSE of $$49,423.08 for the Punggol test dataset, where the median resale price is 648,000, indicates that the model’s predictions are on average about 7.6% off from the actual resale prices. This suggests that the model performs reasonably well, with error margins that are typical for real estate prediction models. While this level of error is acceptable, there is still room for improvement, particularly for properties at the lower and higher ends of the price spectrum.\n\n\n\n\n\n\nCode Chunk\nrmse(test_data_woodlands$resale_price, \n     test_data_w$GRF_pred_w)\n\n\n[1] 41784.94\n\n\n\n\nCode Chunk\nsummary(train_data_woodlands$resale_price)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 350000  470000  500000  507168  545000  690000 \n\n\n\n\nCode Chunk\nsummary(test_data_w$resale_price)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 355000  508500  530000  540097  570000  701000 \n\n\n\n\n\n\n\n\nNote\n\n\n\nInterpretation\n1. Magnitude of the Error\n\nThe RMSE of $42,038.73 indicates that, on average, the model’s predicted resale prices deviate from the actual resale prices by $42,038.73\nThis is a 7.8% error when compared to the mean resale price in the test dataset ($540,097). This suggests that, on average, the model’s predictions are off by about 7.8% of the actual resale price, which is a reasonable error margin for many predictive models.\n\n2. Comparison to the Median\n\nThe median resale price in the test dataset is $530,000.\nThe RMSE of $42,038.73 represents about 7.9% of the median resale price. This level of error is typical for real estate predictive models, and the fact that the RMSE is about 7.9% of the median indicates that the model performs well for most of the data points, but there may still be some variability in predictions for properties priced far below or above the median.\n\n3. Model Performance\n\nThe RMSE of $42,038.73 shows that the model has a moderate level of predictive accuracy. An RMSE that is around 7-8% of the median and mean resale prices is common for real estate prediction models, which often deal with diverse factors influencing property prices (e.g., location, condition of the property, etc.).\nThe fact that the RMSE is relatively small compared to the range of resale prices suggests that the model captures the general trends well. However, there could be room for improvement, especially for properties with prices that are on the lower or upper extremes of the price spectrum.\n\n4. Range of Resale Prices\n\nThe range of resale prices in the test dataset spans from $355,000 to $701,000, with the median at $530,000 and the mean at $540,097.\nThe RMSE of $42,038.73 indicates that, on average, the predicted resale prices are within 7-8% of the actual resale prices. This is reasonable for most properties, but properties at the lower end ($355,000) or the upper end ($701,000) of the price range might have higher prediction errors. This suggests that the model works well for mid-range properties, but its accuracy decreases as we move toward the extremes of the price range.\n\n5. Comparison to Training Data\n\nThe median resale price in the training dataset is $$500,000, which is 30,000 lower than the median resale price in the test dataset (5$30,000).\nThe mean resale price in the training dataset is $507,168, which is also slightly lower than the mean of $540,097 in the test dataset.\nThis suggests that the test data might contain higher-value properties than the training data. Despite this difference, the model’s performance (as indicated by the RMSE) is fairly consistent, with the error remaining around 7.8% of the test dataset’s mean resale price.\nThe RMSE of $42,038.73 relative to the training dataset’s median resale price of $500,000 is approximately 8.4%, which is slightly higher than the RMSE relative to the test dataset’s median. This indicates that the model performs slightly better on the test dataset, potentially due to the difference in price distributions.\n\nConclusion:\nThe RMSE of $42,038.73 for the Woodlands test dataset, where the median resale price is $530,000, suggests that the model’s predictions are, on average, about 7.9% off from the actual resale prices. This is a reasonable level of predictive accuracy, indicating that the model is performing well overall, with small but acceptable prediction errors for properties within the typical price range.\nThe model appears to perform slightly better on the test dataset, which has higher property prices than the training dataset. Further model refinement or feature adjustment may help reduce errors for properties at the extremes of the price range. However, for general purposes, the model seems reliable for predicting resale prices within the observed price ranges."
  },
  {
    "objectID": "TakeHomeExercise/TakeHome3/TakeHome3.html#visualising-the-predicted-values",
    "href": "TakeHomeExercise/TakeHome3/TakeHome3.html#visualising-the-predicted-values",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "Alternatively, scatterplot can be used to visualise the actual resale price and the predicted resale price by using the code chunk below.\n\nPunggolWoodlands\n\n\n\n\nCode Chunk\nggplot(data = test_data_p,\n       aes(x = GRF_pred_p,\n           y = resale_price)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nObservation\n\nPositive Correlation:\n\nThere is a clear positive relationship between the predicted resale prices (GRF_pred_p) and the actual resale prices (resale_price). As the predicted resale prices increase, the actual resale prices also tend to increase.\nThis suggests that the model’s predictions are generally in line with the observed data, as higher predicted values correspond to higher actual resale prices.\n\nTrend and Distribution:\n\nThe points follow an upward trend, with a generally tight clustering around the line of best fit (not shown, but implied by the alignment of points). This indicates that the model is effectively capturing the main trend in the data.\nThe scatter is somewhat spread out, indicating some variance between the predicted and actual resale prices. This is typical in any real-world prediction model, as it shows that the model is not perfect and that there are some discrepancies between predicted and actual values.\n\nOutliers:\n\nAlthough the points generally follow a clear upward trend, there might be a few outliers or data points that deviate significantly from the general trend. These outliers could represent cases where the model performs poorly or where there are unusual observations in the data.\n\nModel Fit:\n\nThe strong linear trend suggests that the model has captured the general relationship between the variables, but some level of variability remains. To assess how well the model performs, you would typically calculate performance metrics like R-squared, RMSE, or MAE to quantify how well the model’s predictions match the actual values.\n\n\nConclusion:\nThe scatterplot shows that there is a positive relationship between predicted resale prices (GRF_pred_p) and actual resale prices (resale_price). This suggests that the model is successful in capturing the general trend of the data, but some discrepancies remain, which is expected in any predictive model. Further analysis, such as residual analysis or model evaluation metrics, could be done to refine the model and assess its performance more thoroughly.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nObservation\n\nPositive Correlation:\n\nThere is a strong positive correlation between GRF_pred_w (predicted resale prices) and resale_price (actual resale prices). As the predicted resale prices increase, the actual resale prices also tend to increase.\nThis indicates that the model’s predictions are generally aligned with the observed data, and the higher predicted values tend to correspond to higher actual resale prices.\n\nTrend and Distribution:\n\nThe points show a clear upward trend, indicating a good linear relationship between the predicted and actual values.\nThe scatter around the trend is relatively tight, meaning that the model’s predictions are fairly close to the actual resale prices. This suggests that the model is capturing the trend in the data well.\n\nOutliers:\n\nSome outliers may exist at the higher end of the resale price range. These points appear to deviate slightly from the general trend, but they do not seem to be too far from the line of best fit. These outliers could represent cases where the model does not predict well for certain properties, possibly due to extreme values or unique characteristics of those observations.\n\nModel Fit:\n\nThe strong linear relationship in the scatterplot indicates that the model fits the data well. However, there may still be room for improvement in reducing the variance in predictions, as indicated by the slight spread of points, especially for lower resale prices.\n\nComparison to the Previous Scatterplot:\n\nCompared to the first scatterplot with GRF_pred_p, this scatterplot (with GRF_pred_w) shows a similar pattern of positive correlation between predicted and actual resale prices. Both scatterplots indicate good predictive performance, with minor deviations in predicted values at the lower and higher ends.\n\n\nConclusion:\nThe scatterplot suggests that the model with GRF_pred_w effectively captures the relationship between the predictors and actual resale prices, with strong predictive accuracy. The positive correlation and tight clustering of points indicate that the model is reliable for predicting resale prices. However, further evaluation using residual analysis or model evaluation metrics could help identify areas for improvement, particularly in the presence of outliers."
  },
  {
    "objectID": "TakeHomeExercise/TakeHome3/TakeHome3.html#limitations",
    "href": "TakeHomeExercise/TakeHome3/TakeHome3.html#limitations",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "This study only looks into two the top two towns that has the most transactions and the transactions are slightly below 2 years. Hence, the sample size may be size which inevitably affecting the model. Thus, future proposal may look into top 5 - 10 towns over a span of 5 years. Additonally, only two predictive models were used. Future studies may consider other models such as Xboost in comparing across the models predictive abilities."
  },
  {
    "objectID": "TakeHomeExercise/TakeHome3/TakeHome3.html#conclusion",
    "href": "TakeHomeExercise/TakeHome3/TakeHome3.html#conclusion",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "To summarise, Random Forest is better in predicting HDB resale prices. However, geospatial details are heavily embedded in HDB resale data, we shouldn’t discount the spatial aspects that will affect the resale prices too. If the study’s purpose is to understand spatial patterns, GWR is more suited. Conversely, if accuracy is a priority, Random Forest would be a better choice."
  },
  {
    "objectID": "HandsOnExercise/HandsOn10/HandsOn10b.html",
    "href": "HandsOnExercise/HandsOn10/HandsOn10b.html",
    "title": "Hand-On Exercise 10b",
    "section": "",
    "text": "Spatial Interaction Models (SIMs) are mathematical models for estimating flows between spatial entities developed by Alan Wilson in the late 1960s and early 1970, with considerable uptake and refinement for transport modelling since then Boyce and Williams (2015).\nThere are four main types of traditional SIMs (Wilson 1971):\nOrdinary least square (OLS), log-normal, Poisson and negative binomial (NB) regression methods have been used extensively to calibrate OD flow models by processing flow data as different types of dependent variables. In this chapter, you will gain hands-on experiences on using appropriate R packages to calibrate SIM by using there four regression methods."
  },
  {
    "objectID": "HandsOnExercise/HandsOn10/HandsOn10b.html#the-case-study-and-data",
    "href": "HandsOnExercise/HandsOn10/HandsOn10b.html#the-case-study-and-data",
    "title": "Hand-On Exercise 10b",
    "section": "1 The Case Study and Data",
    "text": "1 The Case Study and Data\nIn this exercise, we are going to calibrate SIM to determine factors affecting the public bus passenger flows during the morning peak in Singapore."
  },
  {
    "objectID": "HandsOnExercise/HandsOn10/HandsOn10b.html#getting-started",
    "href": "HandsOnExercise/HandsOn10/HandsOn10b.html#getting-started",
    "title": "Hand-On Exercise 10b",
    "section": "2 Getting Started",
    "text": "2 Getting Started\nFor the purpose of this exercise, four r packages will be used. They are:\n\nsf for importing, integrating, processing and transforming geospatial data.\ntidyverse for importing, integrating, wrangling and visualising data.\ntmap for creating thematic maps.\n\n\n\nCode Chunk\npacman::p_load(tmap, sf, sp,\n               performance, reshape2,\n               ggpubr, tidyverse)"
  },
  {
    "objectID": "HandsOnExercise/HandsOn10/HandsOn10b.html#the-data",
    "href": "HandsOnExercise/HandsOn10/HandsOn10b.html#the-data",
    "title": "Hand-On Exercise 10b",
    "section": "3 The Data",
    "text": "3 The Data\nThis exercise is a continuation of Chapter 15: Processing and Visualising Flow Data and the following data will be used:\n\nod_data.rds, weekday morning peak passenger flows at planning subzone level.\nmpsz.rds, URA Master Plan 2019 Planning Subzone boundary in simple feature tibble data frame format.\n\nBeside these two data sets, an additional attribute data file called pop.csv will be provided. It"
  },
  {
    "objectID": "HandsOnExercise/HandsOn10/HandsOn10b.html#computing-distance-matrix",
    "href": "HandsOnExercise/HandsOn10/HandsOn10b.html#computing-distance-matrix",
    "title": "Hand-On Exercise 10b",
    "section": "4 Computing Distance Matrix",
    "text": "4 Computing Distance Matrix\nIn spatial interaction, a distance matrix is a table that shows the distance between pairs of locations. For example, in the table below we can see an Euclidean distance of 3926.0025 between MESZ01 and RVSZ05, of 3939.1079 between MESZ01 and SRSZ01, and so on. By definition, an location’s distance from itself, which is shown in the main diagonal of the table, is 0.\n\nIn this section, you will learn how to compute a distance matrix by using URA Master Plan 2019 Planning Subzone boundary in which you saved as an rds file called mpsz.\nFirst, let us import mpsz.rds into R environemnt by using the code chunk below.\n\n\nCode Chunk\nmpsz &lt;- read_rds(\"data/rds/mpsz.rds\")\nmpsz\n\n\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n                 SUBZONE_N SUBZONE_C       PLN_AREA_N PLN_AREA_C       REGION_N\n1              MARINA EAST    MESZ01      MARINA EAST         ME CENTRAL REGION\n2         INSTITUTION HILL    RVSZ05     RIVER VALLEY         RV CENTRAL REGION\n3           ROBERTSON QUAY    SRSZ01  SINGAPORE RIVER         SR CENTRAL REGION\n4  JURONG ISLAND AND BUKOM    WISZ01  WESTERN ISLANDS         WI    WEST REGION\n5             FORT CANNING    MUSZ02           MUSEUM         MU CENTRAL REGION\n6         MARINA EAST (MP)    MPSZ05    MARINE PARADE         MP CENTRAL REGION\n7                   SUDONG    WISZ03  WESTERN ISLANDS         WI    WEST REGION\n8                  SEMAKAU    WISZ02  WESTERN ISLANDS         WI    WEST REGION\n9           SOUTHERN GROUP    SISZ02 SOUTHERN ISLANDS         SI CENTRAL REGION\n10                 SENTOSA    SISZ01 SOUTHERN ISLANDS         SI CENTRAL REGION\n   REGION_C                       geometry\n1        CR MULTIPOLYGON (((33222.98 29...\n2        CR MULTIPOLYGON (((28481.45 30...\n3        CR MULTIPOLYGON (((28087.34 30...\n4        WR MULTIPOLYGON (((14557.7 304...\n5        CR MULTIPOLYGON (((29542.53 31...\n6        CR MULTIPOLYGON (((35279.55 30...\n7        WR MULTIPOLYGON (((15772.59 21...\n8        WR MULTIPOLYGON (((19843.41 21...\n9        CR MULTIPOLYGON (((30870.53 22...\n10       CR MULTIPOLYGON (((26879.04 26...\n\n\nNotice that it is a sf tibble dataframe object class.\n\n4.1 Converting from sf data.table to SpatialPolygonsDataFrame\nThere are at least two ways to compute the required distance matrix. One is based on sf and the other is based on sp. Past experience shown that computing distance matrix by using sf function took relatively longer time that sp method especially the data set is large. In view of this, sp method is used in the code chunks below.\nFirst as.Spatial() will be used to convert mpsz from sf tibble data frame to SpatialPolygonsDataFrame of sp object as shown in the code chunk below.\n\n\nCode Chunk\nmpsz_sp &lt;- as(mpsz, \"Spatial\")\nmpsz_sp\n\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 332 \nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 6\nnames       : SUBZONE_N, SUBZONE_C, PLN_AREA_N, PLN_AREA_C,       REGION_N, REGION_C \nmin values  : ADMIRALTY,    AMSZ01, ANG MO KIO,         AM, CENTRAL REGION,       CR \nmax values  :    YUNNAN,    YSSZ09,     YISHUN,         YS,    WEST REGION,       WR \n\n\n\n\n4.2 Computing the distance matrix\nNext, spDists() of sp package will be used to compute the Euclidean distance between the centroids of the planning subzones.\n\n\n\n\n\n\nQ&A\n\n\n\nDo you know why the distance is calculated between two centroids of a pair of spatial polygons?\n\n\n\n\nCode Chunk\ndist &lt;- spDists(mpsz_sp, \n                longlat = FALSE)\n\n\n\n\nCode Chunk\nhead(dist, n=c(10, 10))\n\n\n           [,1]       [,2]      [,3]      [,4]       [,5]      [,6]      [,7]\n [1,]     0.000  3926.0025  3939.108 20252.964  2989.9839  1431.330 19211.836\n [2,]  3926.003     0.0000   305.737 16513.865   951.8314  5254.066 16242.523\n [3,]  3939.108   305.7370     0.000 16412.062  1045.9088  5299.849 16026.146\n [4,] 20252.964 16513.8648 16412.062     0.000 17450.3044 21665.795  7229.017\n [5,]  2989.984   951.8314  1045.909 17450.304     0.0000  4303.232 17020.916\n [6,]  1431.330  5254.0664  5299.849 21665.795  4303.2323     0.000 20617.082\n [7,] 19211.836 16242.5230 16026.146  7229.017 17020.9161 20617.082     0.000\n [8,] 14960.942 12749.4101 12477.871 11284.279 13336.0421 16281.453  5606.082\n [9,]  7515.256  7934.8082  7649.776 18427.503  7801.6163  8403.896 14810.930\n[10,]  6391.342  4975.0021  4669.295 15469.566  5226.8731  7707.091 13111.391\n           [,8]      [,9]     [,10]\n [1,] 14960.942  7515.256  6391.342\n [2,] 12749.410  7934.808  4975.002\n [3,] 12477.871  7649.776  4669.295\n [4,] 11284.279 18427.503 15469.566\n [5,] 13336.042  7801.616  5226.873\n [6,] 16281.453  8403.896  7707.091\n [7,]  5606.082 14810.930 13111.391\n [8,]     0.000  9472.024  8575.490\n [9,]  9472.024     0.000  3780.800\n[10,]  8575.490  3780.800     0.000\n\n\nNotice that the output dist is a matrix object class of R. Also notice that the column heanders and row headers are not labeled with the planning subzone codes.\n\n\n4.3 Labelling column and row heanders of a distance matrix\nFirst, we will create a list sorted according to the the distance matrix by planning sub-zone code.\n\n\nCode Chunk\nsz_names &lt;- mpsz$SUBZONE_C\n\n\nNext we will attach SUBZONE_C to row and column for distance matrix matching ahead\n\n\nCode Chunk\ncolnames(dist) &lt;- paste0(sz_names)\nrownames(dist) &lt;- paste0(sz_names)\n\n\n\n\n4.4 Pivoting distance value by SUBZONE_C\nNext, we will pivot the distance matrix into a long table by using the row and column subzone codes as show in the code chunk below.\n\n\nCode Chunk\ndistPair &lt;- melt(dist) %&gt;%\n  rename(dist = value)\nhead(distPair, 10)\n\n\n     Var1   Var2      dist\n1  MESZ01 MESZ01     0.000\n2  RVSZ05 MESZ01  3926.003\n3  SRSZ01 MESZ01  3939.108\n4  WISZ01 MESZ01 20252.964\n5  MUSZ02 MESZ01  2989.984\n6  MPSZ05 MESZ01  1431.330\n7  WISZ03 MESZ01 19211.836\n8  WISZ02 MESZ01 14960.942\n9  SISZ02 MESZ01  7515.256\n10 SISZ01 MESZ01  6391.342\n\n\nNotice that the within zone distance is 0.\n\n\n4.5 Updating intra-zonal distances\nIn this section, we are going to append a constant value to replace the intra-zonal distance of 0.\nFirst, we will select and find out the minimum value of the distance by using summary().\n\n\nCode Chunk\ndistPair %&gt;%\n  filter(dist &gt; 0) %&gt;%\n  summary()\n\n\n      Var1             Var2             dist        \n MESZ01 :   331   MESZ01 :   331   Min.   :  173.8  \n RVSZ05 :   331   RVSZ05 :   331   1st Qu.: 7149.5  \n SRSZ01 :   331   SRSZ01 :   331   Median :11890.0  \n WISZ01 :   331   WISZ01 :   331   Mean   :12229.4  \n MUSZ02 :   331   MUSZ02 :   331   3rd Qu.:16401.7  \n MPSZ05 :   331   MPSZ05 :   331   Max.   :49894.4  \n (Other):107906   (Other):107906                    \n\n\nNext, a constant distance value of 50m is added into intra-zones distance.\n\n\nCode Chunk\ndistPair$dist &lt;- ifelse(distPair$dist == 0,\n                        50, distPair$dist)\n\n\nThe code chunk below will be used to check the result data.frame.\n\n\nCode Chunk\ndistPair %&gt;%\n  summary()\n\n\n      Var1             Var2             dist      \n MESZ01 :   332   MESZ01 :   332   Min.   :   50  \n RVSZ05 :   332   RVSZ05 :   332   1st Qu.: 7097  \n SRSZ01 :   332   SRSZ01 :   332   Median :11864  \n WISZ01 :   332   WISZ01 :   332   Mean   :12193  \n MUSZ02 :   332   MUSZ02 :   332   3rd Qu.:16388  \n MPSZ05 :   332   MPSZ05 :   332   Max.   :49894  \n (Other):108232   (Other):108232                  \n\n\nThe code chunk below is used to rename the origin and destination fields.\n\n\nCode Chunk\ndistPair &lt;- distPair %&gt;%\n  rename(orig = Var1,\n         dest = Var2)\n\n\nLastly, the code chunk below is used to save the dataframe for future use.\n\n\nCode Chunk\nwrite_rds(distPair, \"data/rds/distPair.rds\") \n\n\n\n\nCode Chunk\ndistPair &lt;- read_rds(\"data/rds/distPair.rds\")"
  },
  {
    "objectID": "HandsOnExercise/HandsOn10/HandsOn10b.html#preparing-flow-data",
    "href": "HandsOnExercise/HandsOn10/HandsOn10b.html#preparing-flow-data",
    "title": "Hand-On Exercise 10b",
    "section": "5 Preparing flow data",
    "text": "5 Preparing flow data\nThe code chunk below is used import od_data save in Chapter 15 into R environment.\n\n\nCode Chunk\nod_data &lt;- read_rds(\"data/rds/od_data.rds\")\n\n\nNext, we will compute the total passenger trip between and within planning subzones by using the code chunk below. The output is all flow_data.\n\n\nCode Chunk\nflow_data &lt;- od_data %&gt;%\n  group_by(ORIGIN_SZ, DESTIN_SZ) %&gt;% \n  summarize(TRIPS = sum(MORNING_PEAK)) \n\n\nUse the code chunk below to display flow_data dataframe.\n\n\nCode Chunk\nhead(flow_data, 10)\n\n\n# A tibble: 10 × 3\n# Groups:   ORIGIN_SZ [1]\n   ORIGIN_SZ DESTIN_SZ TRIPS\n   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;\n 1 AMSZ01    AMSZ01     1998\n 2 AMSZ01    AMSZ02     8289\n 3 AMSZ01    AMSZ03     8971\n 4 AMSZ01    AMSZ04     2252\n 5 AMSZ01    AMSZ05     6136\n 6 AMSZ01    AMSZ06     2148\n 7 AMSZ01    AMSZ07     1620\n 8 AMSZ01    AMSZ08     1925\n 9 AMSZ01    AMSZ09     1773\n10 AMSZ01    AMSZ10       63\n\n\n\n5.1 Separating intra-flow from passenger volume df\nCode chunk below is used to add three new fields in flow_data dataframe.\n\n\nCode Chunk\nflow_data$FlowNoIntra &lt;- ifelse(\n  flow_data$ORIGIN_SZ == flow_data$DESTIN_SZ, \n  0, flow_data$TRIPS)\nflow_data$offset &lt;- ifelse(\n  flow_data$ORIGIN_SZ == flow_data$DESTIN_SZ, \n  0.000001, 1)\n\n\n\n\n5.2 Combining passenger volume data with distance value\nBefore we can join flow_data and distPair, we need to convert data value type of ORIGIN_SZ and DESTIN_SZ fields of flow_data dataframe into factor data type.\n\n\nCode Chunk\nflow_data$ORIGIN_SZ &lt;- as.factor(flow_data$ORIGIN_SZ)\nflow_data$DESTIN_SZ &lt;- as.factor(flow_data$DESTIN_SZ)\n\n\nNow, left_join() of dplyr will be used to flow_data dataframe and distPair dataframe. The output is called flow_data1.\n\n\nCode Chunk\nflow_data1 &lt;- flow_data %&gt;%\n  left_join (distPair,\n             by = c(\"ORIGIN_SZ\" = \"orig\",\n                    \"DESTIN_SZ\" = \"dest\"))"
  },
  {
    "objectID": "HandsOnExercise/HandsOn10/HandsOn10b.html#preparing-origin-and-destination-attributes",
    "href": "HandsOnExercise/HandsOn10/HandsOn10b.html#preparing-origin-and-destination-attributes",
    "title": "Hand-On Exercise 10b",
    "section": "6 Preparing Origin and Destination Attributes",
    "text": "6 Preparing Origin and Destination Attributes\n\n6.1 Importing population data\n\n\nCode Chunk\npop &lt;- read_csv(\"data/aspatial/pop.csv\")\n\n\n\n\n6.2 Geospatial data wrangling\n\n\nCode Chunk\npop &lt;- pop %&gt;%\n  left_join(mpsz,\n            by = c(\"PA\" = \"PLN_AREA_N\",\n                   \"SZ\" = \"SUBZONE_N\")) %&gt;%\n  select(1:6) %&gt;%\n  rename(SZ_NAME = SZ,\n         SZ = SUBZONE_C)\n\n\n\n\n6.3 Preparing origin attribute\n\n\nCode Chunk\nflow_data1 &lt;- flow_data1 %&gt;%\n  left_join(pop,\n            by = c(ORIGIN_SZ = \"SZ\")) %&gt;%\n  rename(ORIGIN_AGE7_12 = AGE7_12,\n         ORIGIN_AGE13_24 = AGE13_24,\n         ORIGIN_AGE25_64 = AGE25_64) %&gt;%\n  select(-c(PA, SZ_NAME))\n\n\n\n\n6.4 Preparing destination attribute\n\n\nCode Chunk\nflow_data1 &lt;- flow_data1 %&gt;%\n  left_join(pop,\n            by = c(DESTIN_SZ = \"SZ\")) %&gt;%\n  rename(DESTIN_AGE7_12 = AGE7_12,\n         DESTIN_AGE13_24 = AGE13_24,\n         DESTIN_AGE25_64 = AGE25_64) %&gt;%\n  select(-c(PA, SZ_NAME))\n\n\nWe will called the output data file SIM_data. it is in rds data file format.\n\n\nCode Chunk\nwrite_rds(flow_data1, \"data/rds/flow_data_6-9.rds\")"
  },
  {
    "objectID": "HandsOnExercise/HandsOn10/HandsOn10b.html#calibrating-spatial-interaction-models",
    "href": "HandsOnExercise/HandsOn10/HandsOn10b.html#calibrating-spatial-interaction-models",
    "title": "Hand-On Exercise 10b",
    "section": "7 Calibrating Spatial Interaction Models",
    "text": "7 Calibrating Spatial Interaction Models\nIn this section, you will learn how to calibrate Spatial Interaction Models by using Poisson Regression method.\n\n7.1 Importing the modelling data\nFirstly, let us import the modelling data by using the code chunk below.\n\n\nCode Chunk\nflow_data1 &lt;- read_rds(\"data/rds/flow_data_6-9.rds\")\n\n\n\n\n7.2 Visualising the dependent variable\nFirstly, let us plot the distribution of the dependent variable (i.e. TRIPS) by using histogram method by using the code chunk below.\n\n\nCode Chunk\nggplot(data = flow_data1,\n       aes(x = TRIPS)) +\n  geom_histogram()\n\n\n\n\n\n\n\n\n\nNotice that the distribution is highly skewed and not resemble bell shape or also known as normal distribution.\nNext, let us visualise the relation between the dependent variable and one of the key independent variable in Spatial Interaction Model, namely distance.\n\n\nCode Chunk\nggplot(data = flow_data1,\n       aes(x = dist,\n           y = TRIPS)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\n\n\n\n\n\n\n\nNotice that their relationship hardly resemble linear relationship.\nOn the other hand, if we plot the scatter plot by using the log transformed version of both variables, we can see that their relationship is more resemble linear relationship.\n\n\nCode Chunk\nggplot(data = flow_data1,\n       aes(x = log(dist),\n           y = log(TRIPS))) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\n\n\n\n\n\n\n\n\n\n7.3 Checking for variables with zero values\nSince Poisson Regression is based of log and log 0 is undefined, it is important for us to ensure that no 0 values in the explanatory variables.\nIn the code chunk below, summary() of Base R is used to compute the summary statistics of all variables in SIM_data data frame.\n\n\nCode Chunk\nsummary(flow_data1)\n\n\n  ORIGIN_SZ          DESTIN_SZ             TRIPS         FlowNoIntra      \n Length:14734       Length:14734       Min.   :     1   Min.   :     0.0  \n Class :character   Class :character   1st Qu.:    14   1st Qu.:    13.0  \n Mode  :character   Mode  :character   Median :    76   Median :    70.0  \n                                       Mean   :  1021   Mean   :   839.9  \n                                       3rd Qu.:   426   3rd Qu.:   379.0  \n                                       Max.   :232187   Max.   :148274.0  \n     offset              dist       ORIGIN_AGE7_12 ORIGIN_AGE13_24\n Min.   :0.000001   Min.   :   50   Min.   :   0   Min.   :    0  \n 1st Qu.:1.000000   1st Qu.: 3346   1st Qu.: 240   1st Qu.:  440  \n Median :1.000000   Median : 6067   Median : 700   Median : 1350  \n Mean   :0.982150   Mean   : 6880   Mean   :1032   Mean   : 2269  \n 3rd Qu.:1.000000   3rd Qu.: 9729   3rd Qu.:1480   3rd Qu.: 3260  \n Max.   :1.000000   Max.   :26136   Max.   :6340   Max.   :16380  \n ORIGIN_AGE25_64 DESTIN_AGE7_12 DESTIN_AGE13_24 DESTIN_AGE25_64\n Min.   :    0   Min.   :   0   Min.   :    0   Min.   :    0  \n 1st Qu.: 2200   1st Qu.: 240   1st Qu.:  460   1st Qu.: 2200  \n Median : 6810   Median : 720   Median : 1420   Median : 7030  \n Mean   :10487   Mean   :1033   Mean   : 2290   Mean   :10574  \n 3rd Qu.:15770   3rd Qu.:1500   3rd Qu.: 3260   3rd Qu.:15830  \n Max.   :74610   Max.   :6340   Max.   :16380   Max.   :74610  \n\n\nThe print report above reveals that variables ORIGIN_AGE7_12, ORIGIN_AGE13_24, ORIGIN_AGE25_64,DESTIN_AGE7_12, DESTIN_AGE13_24, DESTIN_AGE25_64 consist of 0 values.\nIn view of this, code chunk below will be used to replace zero values to 0.99.\n\n\nCode Chunk\nflow_data1$DESTIN_AGE7_12 &lt;- ifelse(\n  flow_data1$DESTIN_AGE7_12 == 0,\n  0.99, flow_data1$DESTIN_AGE7_12)\nflow_data1$DESTIN_AGE13_24 &lt;- ifelse(\n  flow_data1$DESTIN_AGE13_24 == 0,\n  0.99, flow_data1$DESTIN_AGE13_24)\nflow_data1$DESTIN_AGE25_64 &lt;- ifelse(\n  flow_data1$DESTIN_AGE25_64 == 0,\n  0.99, flow_data1$DESTIN_AGE25_64)\nflow_data1$ORIGIN_AGE7_12 &lt;- ifelse(\n  flow_data1$ORIGIN_AGE7_12 == 0,\n  0.99, flow_data1$ORIGIN_AGE7_12)\nflow_data1$ORIGIN_AGE13_24 &lt;- ifelse(\n  flow_data1$ORIGIN_AGE13_24 == 0,\n  0.99, flow_data1$ORIGIN_AGE13_24)\nflow_data1$ORIGIN_AGE25_64 &lt;- ifelse(\n  flow_data1$ORIGIN_AGE25_64 == 0,\n  0.99, flow_data1$ORIGIN_AGE25_64)\n\n\nYou can run the summary() again.\n\n\nCode Chunk\nsummary(flow_data1)\n\n\n  ORIGIN_SZ          DESTIN_SZ             TRIPS         FlowNoIntra      \n Length:14734       Length:14734       Min.   :     1   Min.   :     0.0  \n Class :character   Class :character   1st Qu.:    14   1st Qu.:    13.0  \n Mode  :character   Mode  :character   Median :    76   Median :    70.0  \n                                       Mean   :  1021   Mean   :   839.9  \n                                       3rd Qu.:   426   3rd Qu.:   379.0  \n                                       Max.   :232187   Max.   :148274.0  \n     offset              dist       ORIGIN_AGE7_12    ORIGIN_AGE13_24   \n Min.   :0.000001   Min.   :   50   Min.   :   0.99   Min.   :    0.99  \n 1st Qu.:1.000000   1st Qu.: 3346   1st Qu.: 240.00   1st Qu.:  440.00  \n Median :1.000000   Median : 6067   Median : 700.00   Median : 1350.00  \n Mean   :0.982150   Mean   : 6880   Mean   :1031.86   Mean   : 2268.84  \n 3rd Qu.:1.000000   3rd Qu.: 9729   3rd Qu.:1480.00   3rd Qu.: 3260.00  \n Max.   :1.000000   Max.   :26136   Max.   :6340.00   Max.   :16380.00  \n ORIGIN_AGE25_64    DESTIN_AGE7_12    DESTIN_AGE13_24    DESTIN_AGE25_64   \n Min.   :    0.99   Min.   :   0.99   Min.   :    0.99   Min.   :    0.99  \n 1st Qu.: 2200.00   1st Qu.: 240.00   1st Qu.:  460.00   1st Qu.: 2200.00  \n Median : 6810.00   Median : 720.00   Median : 1420.00   Median : 7030.00  \n Mean   :10487.62   Mean   :1033.40   Mean   : 2290.35   Mean   :10574.46  \n 3rd Qu.:15770.00   3rd Qu.:1500.00   3rd Qu.: 3260.00   3rd Qu.:15830.00  \n Max.   :74610.00   Max.   :6340.00   Max.   :16380.00   Max.   :74610.00  \n\n\nNotice that all the 0 values have been replaced by 0.99.\n\n\n7.4 Unconstrained Spatial Interaction Model\nIn this section, you will learn how to calibrate an unconstrained spatial interaction model by using glm() of Base Stats. The explanatory variables are origin population by different age cohort, destination population by different age cohort (i.e. ORIGIN_AGE25_64) and distance between origin and destination in km (i.e. dist).\nThe general formula of Unconstrained Spatial Interaction Model\n\nThe code chunk used to calibrate to model is shown below:\n\n\nCode Chunk\nuncSIM &lt;- glm(formula = TRIPS ~ \n                log(ORIGIN_AGE25_64) + \n                log(DESTIN_AGE25_64) +\n                log(dist),\n              family = poisson(link = \"log\"),\n              data = flow_data1,\n              na.action = na.exclude)\nuncSIM\n\n\n\nCall:  glm(formula = TRIPS ~ log(ORIGIN_AGE25_64) + log(DESTIN_AGE25_64) + \n    log(dist), family = poisson(link = \"log\"), data = flow_data1, \n    na.action = na.exclude)\n\nCoefficients:\n         (Intercept)  log(ORIGIN_AGE25_64)  log(DESTIN_AGE25_64)  \n           10.407308              0.244859              0.009562  \n           log(dist)  \n           -0.705896  \n\nDegrees of Freedom: 14733 Total (i.e. Null);  14730 Residual\nNull Deviance:      60800000 \nResidual Deviance: 36430000     AIC: 36520000\n\n\n\n\n7.5 R-squared function\nIn order to measure how much variation of the trips can be accounted by the model we will write a function to calculate R-Squared value as shown below.\n\n\nCode Chunk\nCalcRSquared &lt;- function(observed,estimated){\n  r &lt;- cor(observed,estimated)\n  R2 &lt;- r^2\n  R2\n}\n\n\nNext, we will compute the R-squared of the unconstrained SIM by using the code chunk below.\n\n\nCode Chunk\nCalcRSquared(uncSIM$data$TRIPS, uncSIM$fitted.values)\n\n\n[1] 0.1892576\n\n\n\n\nCode Chunk\nr2_mcfadden(uncSIM)\n\n\n# R2 for Generalized Linear Regression\n       R2: 0.400\n  adj. R2: 0.400\n\n\n\n\n7.6 Origin (Production) constrained SIM\nIn this section, we will fit an origin constrained SIM by using the code3 chunk below.\nThe general formula of Origin Constrained Spatial Interaction Model\n\n\n\nCode Chunk\norcSIM &lt;- glm(formula = TRIPS ~ \n                 ORIGIN_SZ +\n                 log(DESTIN_AGE25_64) +\n                 log(dist),\n              family = poisson(link = \"log\"),\n              data = flow_data1,\n              na.action = na.exclude)\nsummary(orcSIM)\n\n\n\nCall:\nglm(formula = TRIPS ~ ORIGIN_SZ + log(DESTIN_AGE25_64) + log(dist), \n    family = poisson(link = \"log\"), data = flow_data1, na.action = na.exclude)\n\nCoefficients:\n                       Estimate Std. Error   z value Pr(&gt;|z|)    \n(Intercept)           1.211e+01  3.785e-03  3199.012  &lt; 2e-16 ***\nORIGIN_SZAMSZ02       1.008e+00  4.450e-03   226.401  &lt; 2e-16 ***\nORIGIN_SZAMSZ03       5.474e-01  4.563e-03   119.959  &lt; 2e-16 ***\nORIGIN_SZAMSZ04      -7.494e-02  5.187e-03   -14.448  &lt; 2e-16 ***\nORIGIN_SZAMSZ05      -2.006e-01  5.790e-03   -34.650  &lt; 2e-16 ***\nORIGIN_SZAMSZ06       4.193e-01  5.130e-03    81.736  &lt; 2e-16 ***\nORIGIN_SZAMSZ07      -1.372e+00  9.683e-03  -141.686  &lt; 2e-16 ***\nORIGIN_SZAMSZ08      -1.022e+00  8.956e-03  -114.087  &lt; 2e-16 ***\nORIGIN_SZAMSZ09       2.239e-01  5.408e-03    41.396  &lt; 2e-16 ***\nORIGIN_SZAMSZ10       5.061e-01  4.716e-03   107.311  &lt; 2e-16 ***\nORIGIN_SZAMSZ11      -1.856e+00  1.285e-02  -144.414  &lt; 2e-16 ***\nORIGIN_SZAMSZ12      -1.580e+00  1.076e-02  -146.883  &lt; 2e-16 ***\nORIGIN_SZBDSZ01       1.072e+00  4.345e-03   246.734  &lt; 2e-16 ***\nORIGIN_SZBDSZ02       5.198e-01  5.079e-03   102.340  &lt; 2e-16 ***\nORIGIN_SZBDSZ03       9.865e-01  4.490e-03   219.724  &lt; 2e-16 ***\nORIGIN_SZBDSZ04       1.767e+00  3.894e-03   453.646  &lt; 2e-16 ***\nORIGIN_SZBDSZ05       6.395e-01  4.546e-03   140.691  &lt; 2e-16 ***\nORIGIN_SZBDSZ06       9.363e-01  4.543e-03   206.094  &lt; 2e-16 ***\nORIGIN_SZBDSZ07      -1.281e+00  9.558e-03  -133.991  &lt; 2e-16 ***\nORIGIN_SZBDSZ08      -1.167e+00  9.032e-03  -129.194  &lt; 2e-16 ***\nORIGIN_SZBKSZ01      -4.540e-01  6.538e-03   -69.437  &lt; 2e-16 ***\nORIGIN_SZBKSZ02       3.736e-01  5.115e-03    73.050  &lt; 2e-16 ***\nORIGIN_SZBKSZ03       5.841e-01  4.934e-03   118.375  &lt; 2e-16 ***\nORIGIN_SZBKSZ04      -1.177e-01  5.914e-03   -19.895  &lt; 2e-16 ***\nORIGIN_SZBKSZ05      -2.164e-01  5.832e-03   -37.115  &lt; 2e-16 ***\nORIGIN_SZBKSZ06       3.684e-03  5.873e-03     0.627  0.53048    \nORIGIN_SZBKSZ07       7.456e-01  4.426e-03   168.439  &lt; 2e-16 ***\nORIGIN_SZBKSZ08      -2.279e-02  5.348e-03    -4.261 2.04e-05 ***\nORIGIN_SZBKSZ09      -9.572e-02  5.721e-03   -16.733  &lt; 2e-16 ***\nORIGIN_SZBLSZ01      -1.688e+00  1.482e-02  -113.887  &lt; 2e-16 ***\nORIGIN_SZBLSZ02      -2.154e+00  1.924e-02  -111.980  &lt; 2e-16 ***\nORIGIN_SZBLSZ03      -3.249e+00  3.930e-02   -82.662  &lt; 2e-16 ***\nORIGIN_SZBLSZ04      -2.203e+00  2.306e-02   -95.557  &lt; 2e-16 ***\nORIGIN_SZBMSZ01      -1.267e-01  5.222e-03   -24.266  &lt; 2e-16 ***\nORIGIN_SZBMSZ02      -1.075e+00  6.742e-03  -159.386  &lt; 2e-16 ***\nORIGIN_SZBMSZ03      -4.386e-01  5.794e-03   -75.707  &lt; 2e-16 ***\nORIGIN_SZBMSZ04      -6.333e-02  5.157e-03   -12.280  &lt; 2e-16 ***\nORIGIN_SZBMSZ05      -2.256e+00  1.247e-02  -180.957  &lt; 2e-16 ***\nORIGIN_SZBMSZ06      -2.378e+00  1.618e-02  -147.029  &lt; 2e-16 ***\nORIGIN_SZBMSZ07      -4.769e-01  5.653e-03   -84.362  &lt; 2e-16 ***\nORIGIN_SZBMSZ08      -5.652e-01  5.811e-03   -97.259  &lt; 2e-16 ***\nORIGIN_SZBMSZ09      -1.232e+00  8.688e-03  -141.760  &lt; 2e-16 ***\nORIGIN_SZBMSZ10      -1.471e+00  9.130e-03  -161.131  &lt; 2e-16 ***\nORIGIN_SZBMSZ11      -7.866e-01  6.595e-03  -119.263  &lt; 2e-16 ***\nORIGIN_SZBMSZ12      -1.072e+00  9.149e-03  -117.206  &lt; 2e-16 ***\nORIGIN_SZBMSZ13      -1.207e-01  5.691e-03   -21.218  &lt; 2e-16 ***\nORIGIN_SZBMSZ14      -5.376e-01  6.629e-03   -81.098  &lt; 2e-16 ***\nORIGIN_SZBMSZ15      -3.253e-01  6.054e-03   -53.740  &lt; 2e-16 ***\nORIGIN_SZBMSZ16      -1.548e+00  9.144e-03  -169.303  &lt; 2e-16 ***\nORIGIN_SZBMSZ17      -2.169e+00  1.576e-02  -137.622  &lt; 2e-16 ***\nORIGIN_SZBPSZ01       1.369e-01  5.553e-03    24.660  &lt; 2e-16 ***\nORIGIN_SZBPSZ02      -3.292e-02  6.462e-03    -5.094 3.50e-07 ***\nORIGIN_SZBPSZ03       1.491e-01  6.149e-03    24.241  &lt; 2e-16 ***\nORIGIN_SZBPSZ04       3.544e-01  5.084e-03    69.711  &lt; 2e-16 ***\nORIGIN_SZBPSZ05       5.454e-01  4.554e-03   119.764  &lt; 2e-16 ***\nORIGIN_SZBPSZ06      -1.406e+00  9.311e-03  -151.045  &lt; 2e-16 ***\nORIGIN_SZBPSZ07      -1.004e+00  8.575e-03  -117.068  &lt; 2e-16 ***\nORIGIN_SZBSSZ01      -1.625e-02  5.276e-03    -3.080  0.00207 ** \nORIGIN_SZBSSZ02       3.088e-01  4.787e-03    64.495  &lt; 2e-16 ***\nORIGIN_SZBSSZ03       2.555e-01  4.689e-03    54.487  &lt; 2e-16 ***\nORIGIN_SZBTSZ01      -6.646e-02  5.385e-03   -12.340  &lt; 2e-16 ***\nORIGIN_SZBTSZ02      -1.078e+00  7.797e-03  -138.225  &lt; 2e-16 ***\nORIGIN_SZBTSZ03      -2.284e-01  5.727e-03   -39.876  &lt; 2e-16 ***\nORIGIN_SZBTSZ04      -1.053e+00  1.019e-02  -103.339  &lt; 2e-16 ***\nORIGIN_SZBTSZ05      -1.647e+00  1.100e-02  -149.690  &lt; 2e-16 ***\nORIGIN_SZBTSZ06      -7.804e-01  7.181e-03  -108.682  &lt; 2e-16 ***\nORIGIN_SZBTSZ07      -2.298e+00  1.321e-02  -173.921  &lt; 2e-16 ***\nORIGIN_SZBTSZ08      -1.283e+00  9.394e-03  -136.560  &lt; 2e-16 ***\nORIGIN_SZCBSZ01      -1.911e+00  5.483e-02   -34.844  &lt; 2e-16 ***\nORIGIN_SZCCSZ01      -1.758e+00  1.331e-02  -132.099  &lt; 2e-16 ***\nORIGIN_SZCHSZ01      -1.236e+00  1.178e-02  -104.954  &lt; 2e-16 ***\nORIGIN_SZCHSZ02      -5.424e-01  7.940e-03   -68.307  &lt; 2e-16 ***\nORIGIN_SZCHSZ03       4.332e-01  5.841e-03    74.153  &lt; 2e-16 ***\nORIGIN_SZCKSZ01       1.843e-01  5.117e-03    36.007  &lt; 2e-16 ***\nORIGIN_SZCKSZ02       6.800e-01  5.087e-03   133.672  &lt; 2e-16 ***\nORIGIN_SZCKSZ03       8.030e-01  4.522e-03   177.574  &lt; 2e-16 ***\nORIGIN_SZCKSZ04       1.298e+00  4.562e-03   284.446  &lt; 2e-16 ***\nORIGIN_SZCKSZ05       1.011e+00  5.305e-03   190.602  &lt; 2e-16 ***\nORIGIN_SZCKSZ06       1.262e+00  5.042e-03   250.262  &lt; 2e-16 ***\nORIGIN_SZCLSZ01      -6.805e-01  7.661e-03   -88.836  &lt; 2e-16 ***\nORIGIN_SZCLSZ02      -1.837e+00  1.364e-02  -134.665  &lt; 2e-16 ***\nORIGIN_SZCLSZ03      -1.001e+00  7.949e-03  -125.969  &lt; 2e-16 ***\nORIGIN_SZCLSZ04       6.966e-01  4.460e-03   156.204  &lt; 2e-16 ***\nORIGIN_SZCLSZ05      -1.974e+00  1.474e-02  -133.906  &lt; 2e-16 ***\nORIGIN_SZCLSZ06       8.585e-01  4.204e-03   204.230  &lt; 2e-16 ***\nORIGIN_SZCLSZ07      -2.974e-01  5.575e-03   -53.346  &lt; 2e-16 ***\nORIGIN_SZCLSZ08       3.231e-01  5.802e-03    55.688  &lt; 2e-16 ***\nORIGIN_SZCLSZ09      -1.697e+00  1.555e-02  -109.106  &lt; 2e-16 ***\nORIGIN_SZDTSZ02      -4.061e+00  8.341e-02   -48.693  &lt; 2e-16 ***\nORIGIN_SZDTSZ03      -4.031e+00  7.381e-02   -54.618  &lt; 2e-16 ***\nORIGIN_SZDTSZ13      -3.000e+00  3.129e-02   -95.889  &lt; 2e-16 ***\nORIGIN_SZGLSZ01      -1.405e+00  9.192e-03  -152.876  &lt; 2e-16 ***\nORIGIN_SZGLSZ02       2.536e-01  4.889e-03    51.880  &lt; 2e-16 ***\nORIGIN_SZGLSZ03       2.411e-01  4.855e-03    49.649  &lt; 2e-16 ***\nORIGIN_SZGLSZ04       8.350e-01  4.200e-03   198.826  &lt; 2e-16 ***\nORIGIN_SZGLSZ05       6.207e-01  4.375e-03   141.857  &lt; 2e-16 ***\nORIGIN_SZHGSZ01       2.806e-01  4.746e-03    59.121  &lt; 2e-16 ***\nORIGIN_SZHGSZ02       4.917e-01  4.712e-03   104.351  &lt; 2e-16 ***\nORIGIN_SZHGSZ03       2.452e-01  5.113e-03    47.952  &lt; 2e-16 ***\nORIGIN_SZHGSZ04       9.052e-01  4.303e-03   210.358  &lt; 2e-16 ***\nORIGIN_SZHGSZ05       1.170e+00  4.253e-03   275.033  &lt; 2e-16 ***\nORIGIN_SZHGSZ06      -1.016e-01  5.413e-03   -18.773  &lt; 2e-16 ***\nORIGIN_SZHGSZ07       6.984e-01  4.455e-03   156.757  &lt; 2e-16 ***\nORIGIN_SZHGSZ08       1.005e-01  5.354e-03    18.781  &lt; 2e-16 ***\nORIGIN_SZHGSZ09      -5.390e-01  6.962e-03   -77.417  &lt; 2e-16 ***\nORIGIN_SZHGSZ10      -3.512e+00  4.211e-02   -83.388  &lt; 2e-16 ***\nORIGIN_SZJESZ01       4.022e-01  4.869e-03    82.601  &lt; 2e-16 ***\nORIGIN_SZJESZ02       2.273e-01  4.924e-03    46.158  &lt; 2e-16 ***\nORIGIN_SZJESZ03       1.829e-01  5.286e-03    34.598  &lt; 2e-16 ***\nORIGIN_SZJESZ04      -1.177e+00  9.142e-03  -128.767  &lt; 2e-16 ***\nORIGIN_SZJESZ05      -2.065e+00  1.382e-02  -149.494  &lt; 2e-16 ***\nORIGIN_SZJESZ06       2.301e-01  4.853e-03    47.410  &lt; 2e-16 ***\nORIGIN_SZJESZ07      -1.889e+00  1.183e-02  -159.599  &lt; 2e-16 ***\nORIGIN_SZJESZ08      -1.062e+00  1.147e-02   -92.551  &lt; 2e-16 ***\nORIGIN_SZJESZ09       5.237e-01  4.959e-03   105.612  &lt; 2e-16 ***\nORIGIN_SZJESZ10      -1.829e+00  1.800e-02  -101.616  &lt; 2e-16 ***\nORIGIN_SZJESZ11      -2.023e+00  1.931e-02  -104.738  &lt; 2e-16 ***\nORIGIN_SZJWSZ01       2.125e-01  6.405e-03    33.183  &lt; 2e-16 ***\nORIGIN_SZJWSZ02       8.858e-01  4.521e-03   195.929  &lt; 2e-16 ***\nORIGIN_SZJWSZ03       1.269e+00  4.188e-03   302.922  &lt; 2e-16 ***\nORIGIN_SZJWSZ04       1.284e+00  4.280e-03   300.017  &lt; 2e-16 ***\nORIGIN_SZJWSZ05      -1.393e+00  1.252e-02  -111.339  &lt; 2e-16 ***\nORIGIN_SZJWSZ06      -1.015e+00  1.067e-02   -95.109  &lt; 2e-16 ***\nORIGIN_SZJWSZ07      -2.694e+00  2.751e-02   -97.911  &lt; 2e-16 ***\nORIGIN_SZJWSZ08       1.950e+00  4.110e-03   474.430  &lt; 2e-16 ***\nORIGIN_SZJWSZ09       1.831e+00  3.899e-03   469.595  &lt; 2e-16 ***\nORIGIN_SZKLSZ01       1.636e-01  4.902e-03    33.374  &lt; 2e-16 ***\nORIGIN_SZKLSZ02      -5.156e-01  6.321e-03   -81.570  &lt; 2e-16 ***\nORIGIN_SZKLSZ03      -4.145e-01  5.949e-03   -69.666  &lt; 2e-16 ***\nORIGIN_SZKLSZ04      -2.283e+00  1.187e-02  -192.327  &lt; 2e-16 ***\nORIGIN_SZKLSZ05      -8.593e-01  8.272e-03  -103.882  &lt; 2e-16 ***\nORIGIN_SZKLSZ06      -4.709e+00  1.857e-01   -25.352  &lt; 2e-16 ***\nORIGIN_SZKLSZ07      -1.123e+00  8.408e-03  -133.615  &lt; 2e-16 ***\nORIGIN_SZKLSZ08      -1.476e+00  9.152e-03  -161.321  &lt; 2e-16 ***\nORIGIN_SZLKSZ01      -3.273e+00  3.875e-02   -84.465  &lt; 2e-16 ***\nORIGIN_SZMDSZ01      -2.615e+00  2.802e-02   -93.303  &lt; 2e-16 ***\nORIGIN_SZMDSZ02      -8.945e-01  1.035e-02   -86.389  &lt; 2e-16 ***\nORIGIN_SZMDSZ03      -1.998e+00  1.703e-02  -117.297  &lt; 2e-16 ***\nORIGIN_SZMPSZ01      -1.093e+00  8.367e-03  -130.656  &lt; 2e-16 ***\nORIGIN_SZMPSZ02      -5.975e-01  6.898e-03   -86.616  &lt; 2e-16 ***\nORIGIN_SZMPSZ03      -9.706e-03  5.319e-03    -1.825  0.06804 .  \nORIGIN_SZMUSZ02      -3.923e+00  1.038e-01   -37.806  &lt; 2e-16 ***\nORIGIN_SZNTSZ01      -2.829e+00  3.529e-02   -80.157  &lt; 2e-16 ***\nORIGIN_SZNTSZ02      -3.256e+00  2.323e-02  -140.180  &lt; 2e-16 ***\nORIGIN_SZNTSZ03      -9.865e-01  7.777e-03  -126.848  &lt; 2e-16 ***\nORIGIN_SZNTSZ05      -3.353e+00  4.964e-02   -67.546  &lt; 2e-16 ***\nORIGIN_SZNTSZ06      -3.818e+00  5.576e-02   -68.483  &lt; 2e-16 ***\nORIGIN_SZNVSZ01       4.449e-01  4.482e-03    99.269  &lt; 2e-16 ***\nORIGIN_SZNVSZ02      -6.279e-01  6.470e-03   -97.044  &lt; 2e-16 ***\nORIGIN_SZNVSZ03      -1.212e+00  7.788e-03  -155.644  &lt; 2e-16 ***\nORIGIN_SZNVSZ04      -1.469e+00  9.091e-03  -161.543  &lt; 2e-16 ***\nORIGIN_SZNVSZ05      -2.628e+00  1.579e-02  -166.466  &lt; 2e-16 ***\nORIGIN_SZPGSZ01      -9.541e-01  1.223e-02   -78.035  &lt; 2e-16 ***\nORIGIN_SZPGSZ02      -5.353e-01  7.233e-03   -74.009  &lt; 2e-16 ***\nORIGIN_SZPGSZ03       9.574e-01  4.437e-03   215.779  &lt; 2e-16 ***\nORIGIN_SZPGSZ04       1.110e+00  4.417e-03   251.169  &lt; 2e-16 ***\nORIGIN_SZPGSZ05       2.658e-01  5.758e-03    46.156  &lt; 2e-16 ***\nORIGIN_SZPLSZ01      -8.153e-01  1.044e-02   -78.119  &lt; 2e-16 ***\nORIGIN_SZPLSZ02      -1.675e+00  1.478e-02  -113.340  &lt; 2e-16 ***\nORIGIN_SZPLSZ03      -2.963e+00  3.672e-02   -80.686  &lt; 2e-16 ***\nORIGIN_SZPLSZ04      -3.279e+00  3.684e-02   -89.012  &lt; 2e-16 ***\nORIGIN_SZPLSZ05      -2.466e+00  2.245e-02  -109.864  &lt; 2e-16 ***\nORIGIN_SZPNSZ01       1.411e+00  4.584e-03   307.690  &lt; 2e-16 ***\nORIGIN_SZPNSZ02      -5.043e-01  1.108e-02   -45.503  &lt; 2e-16 ***\nORIGIN_SZPNSZ03      -1.878e+00  1.940e-02   -96.796  &lt; 2e-16 ***\nORIGIN_SZPNSZ04      -2.761e+00  3.112e-02   -88.706  &lt; 2e-16 ***\nORIGIN_SZPNSZ05      -2.277e+00  2.628e-02   -86.662  &lt; 2e-16 ***\nORIGIN_SZPRSZ01      -7.934e-01  1.142e-02   -69.499  &lt; 2e-16 ***\nORIGIN_SZPRSZ02       9.414e-01  4.615e-03   203.981  &lt; 2e-16 ***\nORIGIN_SZPRSZ03       7.674e-01  4.626e-03   165.881  &lt; 2e-16 ***\nORIGIN_SZPRSZ04      -3.771e-01  7.516e-03   -50.168  &lt; 2e-16 ***\nORIGIN_SZPRSZ05       1.327e+00  4.325e-03   306.737  &lt; 2e-16 ***\nORIGIN_SZPRSZ06      -4.081e-01  8.651e-03   -47.172  &lt; 2e-16 ***\nORIGIN_SZPRSZ07      -2.151e+00  1.610e-02  -133.558  &lt; 2e-16 ***\nORIGIN_SZPRSZ08       5.293e-04  6.383e-03     0.083  0.93391    \nORIGIN_SZQTSZ01      -4.144e-01  6.846e-03   -60.539  &lt; 2e-16 ***\nORIGIN_SZQTSZ02      -7.967e-01  6.327e-03  -125.933  &lt; 2e-16 ***\nORIGIN_SZQTSZ03      -2.415e-01  5.681e-03   -42.509  &lt; 2e-16 ***\nORIGIN_SZQTSZ04      -1.013e+00  7.129e-03  -142.123  &lt; 2e-16 ***\nORIGIN_SZQTSZ05      -3.923e-01  5.994e-03   -65.446  &lt; 2e-16 ***\nORIGIN_SZQTSZ06      -5.662e-01  6.481e-03   -87.359  &lt; 2e-16 ***\nORIGIN_SZQTSZ07      -1.558e+00  9.635e-03  -161.662  &lt; 2e-16 ***\nORIGIN_SZQTSZ08      -1.577e-01  5.699e-03   -27.665  &lt; 2e-16 ***\nORIGIN_SZQTSZ09      -6.189e-01  6.633e-03   -93.312  &lt; 2e-16 ***\nORIGIN_SZQTSZ10      -4.511e-01  6.512e-03   -69.271  &lt; 2e-16 ***\nORIGIN_SZQTSZ11      -1.455e+00  9.800e-03  -148.421  &lt; 2e-16 ***\nORIGIN_SZQTSZ12      -1.475e+00  1.044e-02  -141.309  &lt; 2e-16 ***\nORIGIN_SZQTSZ13      -3.529e-01  6.413e-03   -55.038  &lt; 2e-16 ***\nORIGIN_SZQTSZ14      -1.591e+00  9.847e-03  -161.565  &lt; 2e-16 ***\nORIGIN_SZQTSZ15      -8.955e-01  1.027e-02   -87.184  &lt; 2e-16 ***\nORIGIN_SZRCSZ01      -1.375e+00  1.265e-02  -108.704  &lt; 2e-16 ***\nORIGIN_SZRCSZ06      -6.196e-01  8.475e-03   -73.116  &lt; 2e-16 ***\nORIGIN_SZRVSZ01      -3.523e+00  3.237e-02  -108.818  &lt; 2e-16 ***\nORIGIN_SZRVSZ02      -2.912e+00  2.776e-02  -104.868  &lt; 2e-16 ***\nORIGIN_SZRVSZ03      -3.145e+00  2.379e-02  -132.232  &lt; 2e-16 ***\nORIGIN_SZRVSZ04      -3.357e+00  5.567e-02   -60.309  &lt; 2e-16 ***\nORIGIN_SZRVSZ05      -2.438e+00  1.644e-02  -148.272  &lt; 2e-16 ***\nORIGIN_SZSBSZ01       5.890e-01  5.529e-03   106.520  &lt; 2e-16 ***\nORIGIN_SZSBSZ02      -7.098e-01  8.213e-03   -86.432  &lt; 2e-16 ***\nORIGIN_SZSBSZ03       9.634e-01  4.611e-03   208.943  &lt; 2e-16 ***\nORIGIN_SZSBSZ04       7.729e-01  5.289e-03   146.136  &lt; 2e-16 ***\nORIGIN_SZSBSZ05      -9.966e-02  6.543e-03   -15.231  &lt; 2e-16 ***\nORIGIN_SZSBSZ06      -1.778e+00  1.719e-02  -103.427  &lt; 2e-16 ***\nORIGIN_SZSBSZ07      -1.161e+00  1.256e-02   -92.436  &lt; 2e-16 ***\nORIGIN_SZSBSZ08      -1.212e+00  1.222e-02   -99.227  &lt; 2e-16 ***\nORIGIN_SZSBSZ09      -5.783e-01  8.579e-03   -67.412  &lt; 2e-16 ***\nORIGIN_SZSESZ02       9.999e-01  4.409e-03   226.798  &lt; 2e-16 ***\nORIGIN_SZSESZ03       1.214e+00  4.164e-03   291.675  &lt; 2e-16 ***\nORIGIN_SZSESZ04       8.141e-01  4.868e-03   167.238  &lt; 2e-16 ***\nORIGIN_SZSESZ05      -2.186e-01  5.915e-03   -36.961  &lt; 2e-16 ***\nORIGIN_SZSESZ06       7.298e-01  4.689e-03   155.641  &lt; 2e-16 ***\nORIGIN_SZSESZ07      -2.543e+00  1.961e-02  -129.689  &lt; 2e-16 ***\nORIGIN_SZSGSZ01      -1.016e+00  8.550e-03  -118.869  &lt; 2e-16 ***\nORIGIN_SZSGSZ02      -1.120e+00  9.589e-03  -116.799  &lt; 2e-16 ***\nORIGIN_SZSGSZ03       2.169e-01  5.167e-03    41.970  &lt; 2e-16 ***\nORIGIN_SZSGSZ04       2.672e-01  4.792e-03    55.757  &lt; 2e-16 ***\nORIGIN_SZSGSZ05      -1.785e+00  1.060e-02  -168.456  &lt; 2e-16 ***\nORIGIN_SZSGSZ06       4.017e-01  4.541e-03    88.470  &lt; 2e-16 ***\nORIGIN_SZSGSZ07      -6.303e-01  6.235e-03  -101.098  &lt; 2e-16 ***\nORIGIN_SZSKSZ01      -1.928e-01  7.765e-03   -24.826  &lt; 2e-16 ***\nORIGIN_SZSKSZ02       3.870e-01  5.689e-03    68.026  &lt; 2e-16 ***\nORIGIN_SZSKSZ03      -6.815e-01  7.983e-03   -85.369  &lt; 2e-16 ***\nORIGIN_SZSKSZ04      -2.528e+00  2.702e-02   -93.548  &lt; 2e-16 ***\nORIGIN_SZSKSZ05      -1.370e+00  1.552e-02   -88.311  &lt; 2e-16 ***\nORIGIN_SZSLSZ01      -3.218e+00  3.058e-02  -105.238  &lt; 2e-16 ***\nORIGIN_SZSLSZ04      -6.800e-01  7.683e-03   -88.497  &lt; 2e-16 ***\nORIGIN_SZSRSZ01      -2.389e+00  1.583e-02  -150.989  &lt; 2e-16 ***\nORIGIN_SZTHSZ01      -2.183e+00  4.887e-02   -44.666  &lt; 2e-16 ***\nORIGIN_SZTHSZ03      -2.243e+00  2.243e-02  -100.025  &lt; 2e-16 ***\nORIGIN_SZTHSZ04      -2.005e+00  2.869e-02   -69.879  &lt; 2e-16 ***\nORIGIN_SZTHSZ06      -2.276e+00  1.784e-02  -127.557  &lt; 2e-16 ***\nORIGIN_SZTMSZ01       4.015e-01  5.814e-03    69.048  &lt; 2e-16 ***\nORIGIN_SZTMSZ02       2.222e+00  3.795e-03   585.568  &lt; 2e-16 ***\nORIGIN_SZTMSZ03       1.412e+00  4.108e-03   343.608  &lt; 2e-16 ***\nORIGIN_SZTMSZ04       9.106e-01  4.742e-03   192.036  &lt; 2e-16 ***\nORIGIN_SZTMSZ05      -3.259e-01  7.534e-03   -43.253  &lt; 2e-16 ***\nORIGIN_SZTNSZ01      -1.806e+00  1.038e-02  -174.076  &lt; 2e-16 ***\nORIGIN_SZTNSZ02      -1.741e+00  9.778e-03  -178.108  &lt; 2e-16 ***\nORIGIN_SZTNSZ03      -2.277e+00  1.338e-02  -170.199  &lt; 2e-16 ***\nORIGIN_SZTNSZ04      -7.703e-01  7.197e-03  -107.032  &lt; 2e-16 ***\nORIGIN_SZTPSZ01      -6.466e-01  6.287e-03  -102.841  &lt; 2e-16 ***\nORIGIN_SZTPSZ02       4.633e-01  4.347e-03   106.578  &lt; 2e-16 ***\nORIGIN_SZTPSZ03      -5.186e-01  6.085e-03   -85.234  &lt; 2e-16 ***\nORIGIN_SZTPSZ04      -2.900e-01  5.779e-03   -50.190  &lt; 2e-16 ***\nORIGIN_SZTPSZ05      -2.169e-01  6.072e-03   -35.720  &lt; 2e-16 ***\nORIGIN_SZTPSZ06       3.357e-01  5.942e-03    56.486  &lt; 2e-16 ***\nORIGIN_SZTPSZ07      -2.517e-01  6.317e-03   -39.846  &lt; 2e-16 ***\nORIGIN_SZTPSZ08      -1.075e+00  9.109e-03  -118.034  &lt; 2e-16 ***\nORIGIN_SZTPSZ09      -3.708e-01  6.189e-03   -59.903  &lt; 2e-16 ***\nORIGIN_SZTPSZ10      -6.889e-01  7.634e-03   -90.229  &lt; 2e-16 ***\nORIGIN_SZTPSZ11       7.661e-02  5.459e-03    14.033  &lt; 2e-16 ***\nORIGIN_SZTPSZ12      -5.971e-01  6.522e-03   -91.552  &lt; 2e-16 ***\nORIGIN_SZTSSZ01      -3.517e+00  4.739e-02   -74.210  &lt; 2e-16 ***\nORIGIN_SZTSSZ02       3.022e-01  7.334e-03    41.203  &lt; 2e-16 ***\nORIGIN_SZTSSZ03       3.730e-01  7.073e-03    52.733  &lt; 2e-16 ***\nORIGIN_SZTSSZ04       3.610e-01  7.463e-03    48.372  &lt; 2e-16 ***\nORIGIN_SZTSSZ05      -1.103e+00  1.404e-02   -78.566  &lt; 2e-16 ***\nORIGIN_SZTSSZ06      -1.310e+00  1.718e-02   -76.286  &lt; 2e-16 ***\nORIGIN_SZWCSZ01      -1.233e-01  7.861e-03   -15.690  &lt; 2e-16 ***\nORIGIN_SZWCSZ02      -2.872e+00  3.159e-02   -90.911  &lt; 2e-16 ***\nORIGIN_SZWCSZ03      -4.138e+00  1.241e-01   -33.349  &lt; 2e-16 ***\nORIGIN_SZWDSZ01       1.370e+00  4.146e-03   330.448  &lt; 2e-16 ***\nORIGIN_SZWDSZ02       1.041e+00  4.747e-03   219.219  &lt; 2e-16 ***\nORIGIN_SZWDSZ03       2.189e+00  4.035e-03   542.344  &lt; 2e-16 ***\nORIGIN_SZWDSZ04       1.142e+00  4.963e-03   230.074  &lt; 2e-16 ***\nORIGIN_SZWDSZ05       5.160e-01  4.998e-03   103.230  &lt; 2e-16 ***\nORIGIN_SZWDSZ06       1.208e+00  4.611e-03   262.019  &lt; 2e-16 ***\nORIGIN_SZWDSZ07      -3.805e-01  8.034e-03   -47.365  &lt; 2e-16 ***\nORIGIN_SZWDSZ08      -4.839e-01  7.878e-03   -61.426  &lt; 2e-16 ***\nORIGIN_SZWDSZ09       1.475e+00  4.401e-03   335.097  &lt; 2e-16 ***\nORIGIN_SZYSSZ01      -1.552e-01  5.643e-03   -27.496  &lt; 2e-16 ***\nORIGIN_SZYSSZ02       8.958e-01  4.973e-03   180.144  &lt; 2e-16 ***\nORIGIN_SZYSSZ03       1.757e+00  4.275e-03   411.050  &lt; 2e-16 ***\nORIGIN_SZYSSZ04       8.439e-01  4.538e-03   185.955  &lt; 2e-16 ***\nORIGIN_SZYSSZ05      -9.995e-02  5.920e-03   -16.884  &lt; 2e-16 ***\nORIGIN_SZYSSZ06      -1.175e+00  1.079e-02  -108.835  &lt; 2e-16 ***\nORIGIN_SZYSSZ07      -1.202e+00  1.127e-02  -106.642  &lt; 2e-16 ***\nORIGIN_SZYSSZ08       1.244e-02  6.104e-03     2.039  0.04148 *  \nORIGIN_SZYSSZ09       1.385e+00  4.239e-03   326.757  &lt; 2e-16 ***\nlog(DESTIN_AGE25_64)  2.298e-02  8.832e-05   260.146  &lt; 2e-16 ***\nlog(dist)            -6.947e-01  1.295e-04 -5363.438  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 60796037  on 14733  degrees of freedom\nResidual deviance: 26726668  on 14453  degrees of freedom\nAIC: 26818857\n\nNumber of Fisher Scoring iterations: 7\n\n\nWe can examine how the constraints hold for destinations this time.\n\n\nCode Chunk\nCalcRSquared(orcSIM$data$TRIPS, orcSIM$fitted.values)\n\n\n[1] 0.4165837\n\n\n\n\n7.7 Destination constrained\nIn this section, we will fit a destination constrained SIM by using the code chunk below.\nThe general formula of Destination Constrained Spatial Interaction Model\n\n\n\nCode Chunk\ndecSIM &lt;- glm(formula = TRIPS ~ \n                DESTIN_SZ + \n                log(ORIGIN_AGE25_64) + \n                log(dist),\n              family = poisson(link = \"log\"),\n              data = flow_data1,\n              na.action = na.exclude)\nsummary(decSIM)\n\n\n\nCall:\nglm(formula = TRIPS ~ DESTIN_SZ + log(ORIGIN_AGE25_64) + log(dist), \n    family = poisson(link = \"log\"), data = flow_data1, na.action = na.exclude)\n\nCoefficients:\n                       Estimate Std. Error   z value Pr(&gt;|z|)    \n(Intercept)          10.8110189  0.0033476  3229.499  &lt; 2e-16 ***\nDESTIN_SZAMSZ02       0.1775885  0.0041530    42.761  &lt; 2e-16 ***\nDESTIN_SZAMSZ03       0.2064091  0.0040888    50.482  &lt; 2e-16 ***\nDESTIN_SZAMSZ04      -0.9406455  0.0060637  -155.127  &lt; 2e-16 ***\nDESTIN_SZAMSZ05      -1.1578100  0.0061804  -187.337  &lt; 2e-16 ***\nDESTIN_SZAMSZ06      -0.8861493  0.0059241  -149.584  &lt; 2e-16 ***\nDESTIN_SZAMSZ07      -1.7712447  0.0096070  -184.370  &lt; 2e-16 ***\nDESTIN_SZAMSZ08      -1.0707197  0.0067763  -158.009  &lt; 2e-16 ***\nDESTIN_SZAMSZ09      -0.9682250  0.0060435  -160.210  &lt; 2e-16 ***\nDESTIN_SZAMSZ10       0.2612773  0.0043738    59.737  &lt; 2e-16 ***\nDESTIN_SZAMSZ11      -0.3714704  0.0086200   -43.094  &lt; 2e-16 ***\nDESTIN_SZAMSZ12       0.0250455  0.0049850     5.024 5.06e-07 ***\nDESTIN_SZBDSZ01       0.5154763  0.0037827   136.271  &lt; 2e-16 ***\nDESTIN_SZBDSZ02      -0.2843120  0.0049517   -57.417  &lt; 2e-16 ***\nDESTIN_SZBDSZ03      -0.0134646  0.0042692    -3.154  0.00161 ** \nDESTIN_SZBDSZ04       1.0014441  0.0034463   290.582  &lt; 2e-16 ***\nDESTIN_SZBDSZ05       0.3721573  0.0038992    95.445  &lt; 2e-16 ***\nDESTIN_SZBDSZ06       0.2013935  0.0042182    47.744  &lt; 2e-16 ***\nDESTIN_SZBDSZ07      -1.0642612  0.0092942  -114.508  &lt; 2e-16 ***\nDESTIN_SZBDSZ08      -1.7769370  0.0105721  -168.077  &lt; 2e-16 ***\nDESTIN_SZBKSZ01      -1.1944766  0.0065580  -182.141  &lt; 2e-16 ***\nDESTIN_SZBKSZ02      -0.2604946  0.0052044   -50.053  &lt; 2e-16 ***\nDESTIN_SZBKSZ03      -0.5905775  0.0055618  -106.184  &lt; 2e-16 ***\nDESTIN_SZBKSZ04      -0.0521573  0.0048274   -10.804  &lt; 2e-16 ***\nDESTIN_SZBKSZ05      -0.8258599  0.0057094  -144.650  &lt; 2e-16 ***\nDESTIN_SZBKSZ06      -0.8696763  0.0060934  -142.725  &lt; 2e-16 ***\nDESTIN_SZBKSZ07       0.2216292  0.0040334    54.949  &lt; 2e-16 ***\nDESTIN_SZBKSZ08      -1.1179375  0.0068749  -162.612  &lt; 2e-16 ***\nDESTIN_SZBKSZ09      -0.2888733  0.0049056   -58.886  &lt; 2e-16 ***\nDESTIN_SZBLSZ01      -0.4487061  0.0070226   -63.894  &lt; 2e-16 ***\nDESTIN_SZBLSZ02       0.6343096  0.0065174    97.326  &lt; 2e-16 ***\nDESTIN_SZBLSZ03       1.3492337  0.0074135   181.997  &lt; 2e-16 ***\nDESTIN_SZBLSZ04      -0.0339193  0.0131568    -2.578  0.00993 ** \nDESTIN_SZBMSZ01      -0.3497912  0.0046910   -74.567  &lt; 2e-16 ***\nDESTIN_SZBMSZ02      -0.5995634  0.0048828  -122.792  &lt; 2e-16 ***\nDESTIN_SZBMSZ03      -0.8726401  0.0056851  -153.495  &lt; 2e-16 ***\nDESTIN_SZBMSZ04      -0.5350402  0.0048888  -109.442  &lt; 2e-16 ***\nDESTIN_SZBMSZ05      -0.4981814  0.0065971   -75.515  &lt; 2e-16 ***\nDESTIN_SZBMSZ06      -2.0640198  0.0123050  -167.739  &lt; 2e-16 ***\nDESTIN_SZBMSZ07      -0.3100988  0.0045283   -68.480  &lt; 2e-16 ***\nDESTIN_SZBMSZ08      -1.2748152  0.0062622  -203.573  &lt; 2e-16 ***\nDESTIN_SZBMSZ09      -2.8056325  0.0143532  -195.471  &lt; 2e-16 ***\nDESTIN_SZBMSZ10      -1.9166407  0.0089273  -214.693  &lt; 2e-16 ***\nDESTIN_SZBMSZ11      -1.7261160  0.0079281  -217.722  &lt; 2e-16 ***\nDESTIN_SZBMSZ12      -1.1495908  0.0077721  -147.912  &lt; 2e-16 ***\nDESTIN_SZBMSZ13      -0.5428008  0.0050824  -106.799  &lt; 2e-16 ***\nDESTIN_SZBMSZ14      -1.1422302  0.0076325  -149.653  &lt; 2e-16 ***\nDESTIN_SZBMSZ15      -1.2217517  0.0068685  -177.878  &lt; 2e-16 ***\nDESTIN_SZBMSZ16      -2.4074288  0.0107900  -223.116  &lt; 2e-16 ***\nDESTIN_SZBMSZ17      -2.6985491  0.0164771  -163.776  &lt; 2e-16 ***\nDESTIN_SZBPSZ01      -0.6183085  0.0054605  -113.233  &lt; 2e-16 ***\nDESTIN_SZBPSZ02      -1.4579175  0.0083271  -175.080  &lt; 2e-16 ***\nDESTIN_SZBPSZ03      -1.0775392  0.0075109  -143.463  &lt; 2e-16 ***\nDESTIN_SZBPSZ04      -0.6645303  0.0058070  -114.436  &lt; 2e-16 ***\nDESTIN_SZBPSZ05       0.3449386  0.0039504    87.318  &lt; 2e-16 ***\nDESTIN_SZBPSZ06      -0.9360064  0.0077394  -120.941  &lt; 2e-16 ***\nDESTIN_SZBPSZ07      -0.6850065  0.0077761   -88.091  &lt; 2e-16 ***\nDESTIN_SZBSSZ01      -0.3144210  0.0045803   -68.647  &lt; 2e-16 ***\nDESTIN_SZBSSZ02      -0.7531935  0.0051075  -147.469  &lt; 2e-16 ***\nDESTIN_SZBSSZ03       0.1964072  0.0038255    51.342  &lt; 2e-16 ***\nDESTIN_SZBTSZ01       0.0749897  0.0041584    18.033  &lt; 2e-16 ***\nDESTIN_SZBTSZ02      -0.8214254  0.0065659  -125.105  &lt; 2e-16 ***\nDESTIN_SZBTSZ03      -0.1672596  0.0047942   -34.888  &lt; 2e-16 ***\nDESTIN_SZBTSZ04      -1.7727273  0.0103706  -170.938  &lt; 2e-16 ***\nDESTIN_SZBTSZ05      -0.8162630  0.0067401  -121.105  &lt; 2e-16 ***\nDESTIN_SZBTSZ06      -0.8159130  0.0059754  -136.546  &lt; 2e-16 ***\nDESTIN_SZBTSZ07      -2.1139258  0.0105602  -200.178  &lt; 2e-16 ***\nDESTIN_SZBTSZ08      -1.3565179  0.0086828  -156.231  &lt; 2e-16 ***\nDESTIN_SZCBSZ01      -4.6643129  0.3162417   -14.749  &lt; 2e-16 ***\nDESTIN_SZCCSZ01      -1.0088833  0.0080155  -125.866  &lt; 2e-16 ***\nDESTIN_SZCHSZ01      -1.1909317  0.0095262  -125.017  &lt; 2e-16 ***\nDESTIN_SZCHSZ02       0.0890035  0.0052277    17.025  &lt; 2e-16 ***\nDESTIN_SZCHSZ03       1.4883985  0.0039094   380.724  &lt; 2e-16 ***\nDESTIN_SZCKSZ01      -0.1684738  0.0047561   -35.422  &lt; 2e-16 ***\nDESTIN_SZCKSZ02      -0.4314614  0.0051537   -83.720  &lt; 2e-16 ***\nDESTIN_SZCKSZ03       0.6413457  0.0038639   165.983  &lt; 2e-16 ***\nDESTIN_SZCKSZ04      -0.6370791  0.0059869  -106.412  &lt; 2e-16 ***\nDESTIN_SZCKSZ05      -0.4185112  0.0065348   -64.044  &lt; 2e-16 ***\nDESTIN_SZCKSZ06       0.7003888  0.0045139   155.163  &lt; 2e-16 ***\nDESTIN_SZCLSZ01       0.3751343  0.0047400    79.143  &lt; 2e-16 ***\nDESTIN_SZCLSZ02      -2.2913668  0.0133371  -171.804  &lt; 2e-16 ***\nDESTIN_SZCLSZ03      -1.0498490  0.0076548  -137.149  &lt; 2e-16 ***\nDESTIN_SZCLSZ04      -0.1118915  0.0044886   -24.928  &lt; 2e-16 ***\nDESTIN_SZCLSZ05      -1.3113032  0.0084067  -155.983  &lt; 2e-16 ***\nDESTIN_SZCLSZ06       0.1661786  0.0040203    41.334  &lt; 2e-16 ***\nDESTIN_SZCLSZ07      -0.6429895  0.0052617  -122.202  &lt; 2e-16 ***\nDESTIN_SZCLSZ08      -0.4271702  0.0057208   -74.670  &lt; 2e-16 ***\nDESTIN_SZCLSZ09       0.3882136  0.0063758    60.888  &lt; 2e-16 ***\nDESTIN_SZDTSZ02      -3.0106480  0.0348374   -86.420  &lt; 2e-16 ***\nDESTIN_SZDTSZ03      -1.4195712  0.0144110   -98.506  &lt; 2e-16 ***\nDESTIN_SZDTSZ13      -2.2368573  0.0161427  -138.567  &lt; 2e-16 ***\nDESTIN_SZGLSZ01       0.0013721  0.0051224     0.268  0.78881    \nDESTIN_SZGLSZ02      -0.3376674  0.0046195   -73.097  &lt; 2e-16 ***\nDESTIN_SZGLSZ03       0.3659900  0.0038384    95.350  &lt; 2e-16 ***\nDESTIN_SZGLSZ04       0.2969928  0.0038026    78.103  &lt; 2e-16 ***\nDESTIN_SZGLSZ05       0.1786445  0.0038853    45.980  &lt; 2e-16 ***\nDESTIN_SZHGSZ01       0.2979206  0.0038825    76.735  &lt; 2e-16 ***\nDESTIN_SZHGSZ02      -0.5701034  0.0051182  -111.388  &lt; 2e-16 ***\nDESTIN_SZHGSZ03      -1.0387610  0.0061020  -170.233  &lt; 2e-16 ***\nDESTIN_SZHGSZ04      -0.2264881  0.0043617   -51.926  &lt; 2e-16 ***\nDESTIN_SZHGSZ05      -0.2287090  0.0044851   -50.993  &lt; 2e-16 ***\nDESTIN_SZHGSZ06      -0.7896437  0.0054081  -146.010  &lt; 2e-16 ***\nDESTIN_SZHGSZ07       0.2268880  0.0040336    56.249  &lt; 2e-16 ***\nDESTIN_SZHGSZ08      -0.4260784  0.0048967   -87.013  &lt; 2e-16 ***\nDESTIN_SZHGSZ09       0.1027784  0.0051341    20.019  &lt; 2e-16 ***\nDESTIN_SZHGSZ10      -2.8571803  0.0262064  -109.026  &lt; 2e-16 ***\nDESTIN_SZJESZ01      -0.0843635  0.0048222   -17.495  &lt; 2e-16 ***\nDESTIN_SZJESZ02      -0.5197682  0.0051511  -100.904  &lt; 2e-16 ***\nDESTIN_SZJESZ03      -0.6250311  0.0056619  -110.392  &lt; 2e-16 ***\nDESTIN_SZJESZ04      -0.3937360  0.0065536   -60.080  &lt; 2e-16 ***\nDESTIN_SZJESZ05      -0.9748291  0.0097665   -99.814  &lt; 2e-16 ***\nDESTIN_SZJESZ06       0.3642736  0.0040600    89.722  &lt; 2e-16 ***\nDESTIN_SZJESZ07      -1.1571882  0.0081557  -141.887  &lt; 2e-16 ***\nDESTIN_SZJESZ08      -0.5955747  0.0078071   -76.286  &lt; 2e-16 ***\nDESTIN_SZJESZ09      -0.3629500  0.0053966   -67.256  &lt; 2e-16 ***\nDESTIN_SZJESZ10       0.7691552  0.0069348   110.912  &lt; 2e-16 ***\nDESTIN_SZJESZ11       0.9365743  0.0065801   142.335  &lt; 2e-16 ***\nDESTIN_SZJWSZ01      -0.4568805  0.0064536   -70.795  &lt; 2e-16 ***\nDESTIN_SZJWSZ02      -0.2880426  0.0051632   -55.788  &lt; 2e-16 ***\nDESTIN_SZJWSZ03       0.6680404  0.0039264   170.142  &lt; 2e-16 ***\nDESTIN_SZJWSZ04       0.9492158  0.0037186   255.262  &lt; 2e-16 ***\nDESTIN_SZJWSZ05      -0.1938053  0.0060810   -31.871  &lt; 2e-16 ***\nDESTIN_SZJWSZ06       0.3813164  0.0054551    69.900  &lt; 2e-16 ***\nDESTIN_SZJWSZ07      -1.2676010  0.0280038   -45.265  &lt; 2e-16 ***\nDESTIN_SZJWSZ08       0.5013149  0.0044573   112.471  &lt; 2e-16 ***\nDESTIN_SZJWSZ09       1.4161404  0.0033937   417.291  &lt; 2e-16 ***\nDESTIN_SZKLSZ01      -0.6909444  0.0051540  -134.059  &lt; 2e-16 ***\nDESTIN_SZKLSZ02      -0.8146023  0.0057129  -142.589  &lt; 2e-16 ***\nDESTIN_SZKLSZ03      -1.3956114  0.0065167  -214.161  &lt; 2e-16 ***\nDESTIN_SZKLSZ04      -1.9070281  0.0087370  -218.270  &lt; 2e-16 ***\nDESTIN_SZKLSZ05      -0.9293576  0.0071070  -130.766  &lt; 2e-16 ***\nDESTIN_SZKLSZ06      -2.5402234  0.0362062   -70.160  &lt; 2e-16 ***\nDESTIN_SZKLSZ07      -1.2017213  0.0065751  -182.769  &lt; 2e-16 ***\nDESTIN_SZKLSZ08      -0.6083433  0.0050916  -119.480  &lt; 2e-16 ***\nDESTIN_SZLKSZ01      -1.5186810  0.0204155   -74.389  &lt; 2e-16 ***\nDESTIN_SZMDSZ01      -1.4601772  0.0198347   -73.617  &lt; 2e-16 ***\nDESTIN_SZMDSZ02      -1.1554609  0.0111345  -103.773  &lt; 2e-16 ***\nDESTIN_SZMDSZ03      -2.9919337  0.0250838  -119.277  &lt; 2e-16 ***\nDESTIN_SZMPSZ01      -1.1705809  0.0077128  -151.771  &lt; 2e-16 ***\nDESTIN_SZMPSZ02      -0.9380957  0.0060321  -155.517  &lt; 2e-16 ***\nDESTIN_SZMPSZ03      -0.1761013  0.0046389   -37.962  &lt; 2e-16 ***\nDESTIN_SZMUSZ02      -2.4525115  0.0199630  -122.853  &lt; 2e-16 ***\nDESTIN_SZNTSZ01      -3.6605524  0.0447752   -81.754  &lt; 2e-16 ***\nDESTIN_SZNTSZ02      -2.0082021  0.0108736  -184.686  &lt; 2e-16 ***\nDESTIN_SZNTSZ03      -1.2387489  0.0076141  -162.691  &lt; 2e-16 ***\nDESTIN_SZNTSZ05      -1.8054361  0.0249540   -72.351  &lt; 2e-16 ***\nDESTIN_SZNTSZ06      -2.9500517  0.0428601   -68.830  &lt; 2e-16 ***\nDESTIN_SZNVSZ01      -0.4089022  0.0044288   -92.327  &lt; 2e-16 ***\nDESTIN_SZNVSZ02      -0.6865452  0.0052770  -130.102  &lt; 2e-16 ***\nDESTIN_SZNVSZ03      -0.7333670  0.0054243  -135.199  &lt; 2e-16 ***\nDESTIN_SZNVSZ04      -2.2095097  0.0106997  -206.503  &lt; 2e-16 ***\nDESTIN_SZNVSZ05      -1.8721104  0.0089058  -210.212  &lt; 2e-16 ***\nDESTIN_SZPGSZ01      -1.8756618  0.0153008  -122.586  &lt; 2e-16 ***\nDESTIN_SZPGSZ02      -0.9435337  0.0067224  -140.356  &lt; 2e-16 ***\nDESTIN_SZPGSZ03       0.3458476  0.0040152    86.134  &lt; 2e-16 ***\nDESTIN_SZPGSZ04      -0.0271485  0.0044805    -6.059 1.37e-09 ***\nDESTIN_SZPGSZ05      -0.8920273  0.0070730  -126.117  &lt; 2e-16 ***\nDESTIN_SZPLSZ01      -0.2153087  0.0068270   -31.538  &lt; 2e-16 ***\nDESTIN_SZPLSZ02      -1.3646116  0.0131155  -104.046  &lt; 2e-16 ***\nDESTIN_SZPLSZ03      -0.0869245  0.0095838    -9.070  &lt; 2e-16 ***\nDESTIN_SZPLSZ04      -0.2574560  0.0093336   -27.584  &lt; 2e-16 ***\nDESTIN_SZPLSZ05      -0.7186364  0.0116835   -61.509  &lt; 2e-16 ***\nDESTIN_SZPNSZ01       1.1326963  0.0049977   226.643  &lt; 2e-16 ***\nDESTIN_SZPNSZ02       1.6516855  0.0064492   256.106  &lt; 2e-16 ***\nDESTIN_SZPNSZ03       0.8504093  0.0077034   110.394  &lt; 2e-16 ***\nDESTIN_SZPNSZ04       1.6891381  0.0075802   222.836  &lt; 2e-16 ***\nDESTIN_SZPNSZ05       0.7402750  0.0115948    63.845  &lt; 2e-16 ***\nDESTIN_SZPRSZ01      -1.0257636  0.0084652  -121.175  &lt; 2e-16 ***\nDESTIN_SZPRSZ02      -0.2028503  0.0049839   -40.701  &lt; 2e-16 ***\nDESTIN_SZPRSZ03       0.5560483  0.0038496   144.442  &lt; 2e-16 ***\nDESTIN_SZPRSZ04      -0.6824142  0.0079047   -86.330  &lt; 2e-16 ***\nDESTIN_SZPRSZ05       0.0316117  0.0044946     7.033 2.02e-12 ***\nDESTIN_SZPRSZ06       0.3706283  0.0052006    71.267  &lt; 2e-16 ***\nDESTIN_SZPRSZ07      -1.4740460  0.0117304  -125.661  &lt; 2e-16 ***\nDESTIN_SZPRSZ08      -0.7869180  0.0064862  -121.321  &lt; 2e-16 ***\nDESTIN_SZQTSZ01      -1.2790095  0.0085392  -149.781  &lt; 2e-16 ***\nDESTIN_SZQTSZ02      -1.4989188  0.0073423  -204.149  &lt; 2e-16 ***\nDESTIN_SZQTSZ03      -0.9334132  0.0064035  -145.765  &lt; 2e-16 ***\nDESTIN_SZQTSZ04      -1.0506142  0.0065335  -160.805  &lt; 2e-16 ***\nDESTIN_SZQTSZ05      -0.9765013  0.0058471  -167.006  &lt; 2e-16 ***\nDESTIN_SZQTSZ06      -1.2206088  0.0063560  -192.042  &lt; 2e-16 ***\nDESTIN_SZQTSZ07      -1.6794007  0.0108727  -154.460  &lt; 2e-16 ***\nDESTIN_SZQTSZ08      -0.1214413  0.0047980   -25.311  &lt; 2e-16 ***\nDESTIN_SZQTSZ09      -0.5252607  0.0057371   -91.555  &lt; 2e-16 ***\nDESTIN_SZQTSZ10      -0.5981644  0.0054192  -110.378  &lt; 2e-16 ***\nDESTIN_SZQTSZ11      -0.0766021  0.0053446   -14.333  &lt; 2e-16 ***\nDESTIN_SZQTSZ12      -0.6153017  0.0070680   -87.054  &lt; 2e-16 ***\nDESTIN_SZQTSZ13      -0.1690535  0.0051315   -32.944  &lt; 2e-16 ***\nDESTIN_SZQTSZ14      -0.5398362  0.0062233   -86.744  &lt; 2e-16 ***\nDESTIN_SZQTSZ15      -0.1873015  0.0073132   -25.611  &lt; 2e-16 ***\nDESTIN_SZRCSZ01      -0.5875494  0.0071798   -81.833  &lt; 2e-16 ***\nDESTIN_SZRCSZ06      -2.0856090  0.0188789  -110.473  &lt; 2e-16 ***\nDESTIN_SZRVSZ01      -2.6183708  0.0162319  -161.310  &lt; 2e-16 ***\nDESTIN_SZRVSZ02      -3.1882190  0.0326141   -97.756  &lt; 2e-16 ***\nDESTIN_SZRVSZ03      -2.5981974  0.0135074  -192.353  &lt; 2e-16 ***\nDESTIN_SZRVSZ04      -1.9741504  0.0154961  -127.396  &lt; 2e-16 ***\nDESTIN_SZRVSZ05      -3.1547734  0.0256310  -123.084  &lt; 2e-16 ***\nDESTIN_SZSBSZ01      -0.3097949  0.0060601   -51.121  &lt; 2e-16 ***\nDESTIN_SZSBSZ02      -1.1229132  0.0076338  -147.097  &lt; 2e-16 ***\nDESTIN_SZSBSZ03       0.6289715  0.0041400   151.926  &lt; 2e-16 ***\nDESTIN_SZSBSZ04       0.1419430  0.0051357    27.638  &lt; 2e-16 ***\nDESTIN_SZSBSZ05      -0.9256413  0.0071963  -128.628  &lt; 2e-16 ***\nDESTIN_SZSBSZ06      -2.3487368  0.0221611  -105.984  &lt; 2e-16 ***\nDESTIN_SZSBSZ07      -0.7864630  0.0181706   -43.282  &lt; 2e-16 ***\nDESTIN_SZSBSZ08       1.3240051  0.0051598   256.599  &lt; 2e-16 ***\nDESTIN_SZSBSZ09       0.8431156  0.0048330   174.449  &lt; 2e-16 ***\nDESTIN_SZSESZ02      -0.2385874  0.0046618   -51.180  &lt; 2e-16 ***\nDESTIN_SZSESZ03       0.5439188  0.0036932   147.276  &lt; 2e-16 ***\nDESTIN_SZSESZ04      -0.6715716  0.0054222  -123.856  &lt; 2e-16 ***\nDESTIN_SZSESZ05      -0.3601932  0.0047508   -75.818  &lt; 2e-16 ***\nDESTIN_SZSESZ06      -0.6088413  0.0057017  -106.782  &lt; 2e-16 ***\nDESTIN_SZSESZ07      -2.9477507  0.0226797  -129.973  &lt; 2e-16 ***\nDESTIN_SZSGSZ01      -0.5100640  0.0058280   -87.519  &lt; 2e-16 ***\nDESTIN_SZSGSZ02      -0.0439941  0.0051633    -8.520  &lt; 2e-16 ***\nDESTIN_SZSGSZ03      -0.3700648  0.0047152   -78.483  &lt; 2e-16 ***\nDESTIN_SZSGSZ04      -0.3021335  0.0046865   -64.468  &lt; 2e-16 ***\nDESTIN_SZSGSZ05      -2.2253287  0.0097908  -227.288  &lt; 2e-16 ***\nDESTIN_SZSGSZ06       0.2963602  0.0037948    78.097  &lt; 2e-16 ***\nDESTIN_SZSGSZ07      -0.5940373  0.0051371  -115.637  &lt; 2e-16 ***\nDESTIN_SZSISZ01      -1.4528976  0.0257790   -56.360  &lt; 2e-16 ***\nDESTIN_SZSKSZ01      -0.0374952  0.0066885    -5.606 2.07e-08 ***\nDESTIN_SZSKSZ02       0.7271418  0.0050281   144.617  &lt; 2e-16 ***\nDESTIN_SZSKSZ03      -0.0640794  0.0059146   -10.834  &lt; 2e-16 ***\nDESTIN_SZSKSZ04      -0.5610767  0.0139676   -40.170  &lt; 2e-16 ***\nDESTIN_SZSKSZ05       0.1510974  0.0104871    14.408  &lt; 2e-16 ***\nDESTIN_SZSLSZ01      -0.5823031  0.0083356   -69.858  &lt; 2e-16 ***\nDESTIN_SZSLSZ04      -0.8166665  0.0070329  -116.122  &lt; 2e-16 ***\nDESTIN_SZSRSZ01      -2.3241796  0.0127215  -182.696  &lt; 2e-16 ***\nDESTIN_SZTHSZ01      -2.8157635  0.0366840   -76.757  &lt; 2e-16 ***\nDESTIN_SZTHSZ03      -2.1005978  0.0250842   -83.742  &lt; 2e-16 ***\nDESTIN_SZTHSZ04      -2.1246250  0.0213690   -99.425  &lt; 2e-16 ***\nDESTIN_SZTHSZ06      -1.4571092  0.0150031   -97.121  &lt; 2e-16 ***\nDESTIN_SZTMSZ01      -0.1234559  0.0055152   -22.385  &lt; 2e-16 ***\nDESTIN_SZTMSZ02       1.5961628  0.0032599   489.635  &lt; 2e-16 ***\nDESTIN_SZTMSZ03       0.6977233  0.0037138   187.875  &lt; 2e-16 ***\nDESTIN_SZTMSZ04       0.8606606  0.0037592   228.947  &lt; 2e-16 ***\nDESTIN_SZTMSZ05       0.3750655  0.0051281    73.140  &lt; 2e-16 ***\nDESTIN_SZTNSZ01      -1.2624562  0.0066979  -188.485  &lt; 2e-16 ***\nDESTIN_SZTNSZ02      -2.0761581  0.0096538  -215.062  &lt; 2e-16 ***\nDESTIN_SZTNSZ03      -2.1128125  0.0115717  -182.584  &lt; 2e-16 ***\nDESTIN_SZTNSZ04      -1.2417494  0.0068502  -181.271  &lt; 2e-16 ***\nDESTIN_SZTPSZ01      -0.7094356  0.0055768  -127.211  &lt; 2e-16 ***\nDESTIN_SZTPSZ02       0.1491604  0.0037260    40.032  &lt; 2e-16 ***\nDESTIN_SZTPSZ03      -0.4973355  0.0054878   -90.626  &lt; 2e-16 ***\nDESTIN_SZTPSZ04      -1.5160395  0.0071592  -211.761  &lt; 2e-16 ***\nDESTIN_SZTPSZ05      -0.9196565  0.0056750  -162.054  &lt; 2e-16 ***\nDESTIN_SZTPSZ06      -0.2710649  0.0062637   -43.276  &lt; 2e-16 ***\nDESTIN_SZTPSZ07      -2.0198681  0.0116556  -173.296  &lt; 2e-16 ***\nDESTIN_SZTPSZ08      -1.4881412  0.0085532  -173.987  &lt; 2e-16 ***\nDESTIN_SZTPSZ09      -0.5901273  0.0059394   -99.358  &lt; 2e-16 ***\nDESTIN_SZTPSZ10      -1.1215711  0.0084488  -132.749  &lt; 2e-16 ***\nDESTIN_SZTPSZ11      -0.4837089  0.0050905   -95.022  &lt; 2e-16 ***\nDESTIN_SZTPSZ12      -0.8653927  0.0061326  -141.113  &lt; 2e-16 ***\nDESTIN_SZTSSZ01      -0.5515103  0.0208541   -26.446  &lt; 2e-16 ***\nDESTIN_SZTSSZ02       0.8373778  0.0093757    89.314  &lt; 2e-16 ***\nDESTIN_SZTSSZ03       1.7021888  0.0064394   264.340  &lt; 2e-16 ***\nDESTIN_SZTSSZ04       1.5355016  0.0067855   226.292  &lt; 2e-16 ***\nDESTIN_SZTSSZ05       1.6932319  0.0073725   229.668  &lt; 2e-16 ***\nDESTIN_SZTSSZ06       0.4567808  0.0137927    33.118  &lt; 2e-16 ***\nDESTIN_SZWCSZ01       1.3967640  0.0045392   307.711  &lt; 2e-16 ***\nDESTIN_SZWCSZ02      -0.4560229  0.0122949   -37.090  &lt; 2e-16 ***\nDESTIN_SZWCSZ03      -2.0710051  0.0325121   -63.699  &lt; 2e-16 ***\nDESTIN_SZWDSZ01       1.5137342  0.0034774   435.310  &lt; 2e-16 ***\nDESTIN_SZWDSZ02      -0.3005475  0.0055149   -54.497  &lt; 2e-16 ***\nDESTIN_SZWDSZ03       1.2514543  0.0036112   346.550  &lt; 2e-16 ***\nDESTIN_SZWDSZ04      -0.1702528  0.0058295   -29.205  &lt; 2e-16 ***\nDESTIN_SZWDSZ05      -0.0005419  0.0053911    -0.101  0.91994    \nDESTIN_SZWDSZ06       0.5203361  0.0040318   129.058  &lt; 2e-16 ***\nDESTIN_SZWDSZ07       0.6006472  0.0061745    97.279  &lt; 2e-16 ***\nDESTIN_SZWDSZ08       0.6650867  0.0060867   109.268  &lt; 2e-16 ***\nDESTIN_SZWDSZ09       0.6237312  0.0044830   139.132  &lt; 2e-16 ***\nDESTIN_SZYSSZ01       1.0471638  0.0038255   273.732  &lt; 2e-16 ***\nDESTIN_SZYSSZ02       0.2341114  0.0048213    48.558  &lt; 2e-16 ***\nDESTIN_SZYSSZ03      -0.0916446  0.0051335   -17.852  &lt; 2e-16 ***\nDESTIN_SZYSSZ04      -0.0085536  0.0048684    -1.757  0.07892 .  \nDESTIN_SZYSSZ05      -1.5775071  0.0100297  -157.283  &lt; 2e-16 ***\nDESTIN_SZYSSZ06      -1.8130307  0.0098617  -183.846  &lt; 2e-16 ***\nDESTIN_SZYSSZ07      -1.1703963  0.0111525  -104.945  &lt; 2e-16 ***\nDESTIN_SZYSSZ08       0.5253514  0.0039556   132.813  &lt; 2e-16 ***\nDESTIN_SZYSSZ09       0.4353435  0.0038890   111.943  &lt; 2e-16 ***\nlog(ORIGIN_AGE25_64)  0.2249135  0.0001404  1602.353  &lt; 2e-16 ***\nlog(dist)            -0.6989356  0.0001287 -5431.279  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 60796037  on 14733  degrees of freedom\nResidual deviance: 26208384  on 14452  degrees of freedom\nAIC: 26300575\n\nNumber of Fisher Scoring iterations: 7\n\n\nWe can examine how the constraints hold for destinations this time.\n\n\nCode Chunk\nCalcRSquared(decSIM$data$TRIPS, decSIM$fitted.values)\n\n\n[1] 0.4972985\n\n\n\n\n7.8 Doubly constrained\nIn this section, we will fit a doubly constrained SIM by using the code chunk below.\nThe general formula of Doubly Constrained Spatial Interaction Model\n\n\n\nCode Chunk\ndbcSIM &lt;- glm(formula = TRIPS ~ \n                ORIGIN_SZ + \n                DESTIN_SZ + \n                log(dist),\n              family = poisson(link = \"log\"),\n              data = flow_data1,\n              na.action = na.exclude)\nsummary(dbcSIM)\n\n\n\nCall:\nglm(formula = TRIPS ~ ORIGIN_SZ + DESTIN_SZ + log(dist), family = poisson(link = \"log\"), \n    data = flow_data1, na.action = na.exclude)\n\nCoefficients:\n                  Estimate Std. Error   z value Pr(&gt;|z|)    \n(Intercept)     12.4165310  0.0043949  2825.242  &lt; 2e-16 ***\nORIGIN_SZAMSZ02  0.9496891  0.0045740   207.630  &lt; 2e-16 ***\nORIGIN_SZAMSZ03  0.5519174  0.0046672   118.253  &lt; 2e-16 ***\nORIGIN_SZAMSZ04  0.1028140  0.0052468    19.596  &lt; 2e-16 ***\nORIGIN_SZAMSZ05  0.0822549  0.0058663    14.022  &lt; 2e-16 ***\nORIGIN_SZAMSZ06  0.6617809  0.0052580   125.861  &lt; 2e-16 ***\nORIGIN_SZAMSZ07 -0.9508298  0.0097681   -97.340  &lt; 2e-16 ***\nORIGIN_SZAMSZ08 -0.7271779  0.0090946   -79.958  &lt; 2e-16 ***\nORIGIN_SZAMSZ09  0.4896781  0.0055203    88.704  &lt; 2e-16 ***\nORIGIN_SZAMSZ10  0.4819428  0.0048175   100.040  &lt; 2e-16 ***\nORIGIN_SZAMSZ11 -1.7719841  0.0130695  -135.582  &lt; 2e-16 ***\nORIGIN_SZAMSZ12 -1.7679107  0.0108777  -162.526  &lt; 2e-16 ***\nORIGIN_SZBDSZ01  0.8314812  0.0045187   184.010  &lt; 2e-16 ***\nORIGIN_SZBDSZ02  0.4305836  0.0052535    81.961  &lt; 2e-16 ***\nORIGIN_SZBDSZ03  0.8009370  0.0046384   172.676  &lt; 2e-16 ***\nORIGIN_SZBDSZ04  1.4562985  0.0040456   359.971  &lt; 2e-16 ***\nORIGIN_SZBDSZ05  0.4501939  0.0046960    95.867  &lt; 2e-16 ***\nORIGIN_SZBDSZ06  0.7745026  0.0047424   163.314  &lt; 2e-16 ***\nORIGIN_SZBDSZ07 -1.1784123  0.0098105  -120.117  &lt; 2e-16 ***\nORIGIN_SZBDSZ08 -0.9830996  0.0091135  -107.873  &lt; 2e-16 ***\nORIGIN_SZBKSZ01 -0.3042966  0.0067086   -45.359  &lt; 2e-16 ***\nORIGIN_SZBKSZ02  0.4801541  0.0054160    88.655  &lt; 2e-16 ***\nORIGIN_SZBKSZ03  0.7823931  0.0052007   150.440  &lt; 2e-16 ***\nORIGIN_SZBKSZ04 -0.1292545  0.0061735   -20.937  &lt; 2e-16 ***\nORIGIN_SZBKSZ05 -0.0258584  0.0060192    -4.296 1.74e-05 ***\nORIGIN_SZBKSZ06  0.1994719  0.0061206    32.590  &lt; 2e-16 ***\nORIGIN_SZBKSZ07  0.7434860  0.0046598   159.553  &lt; 2e-16 ***\nORIGIN_SZBKSZ08  0.1625007  0.0055219    29.428  &lt; 2e-16 ***\nORIGIN_SZBKSZ09 -0.0864293  0.0059533   -14.518  &lt; 2e-16 ***\nORIGIN_SZBLSZ01 -2.1022485  0.0150316  -139.855  &lt; 2e-16 ***\nORIGIN_SZBLSZ02 -2.9460181  0.0195760  -150.491  &lt; 2e-16 ***\nORIGIN_SZBLSZ03 -4.9412872  0.0398540  -123.985  &lt; 2e-16 ***\nORIGIN_SZBLSZ04 -2.8143593  0.0239209  -117.653  &lt; 2e-16 ***\nORIGIN_SZBMSZ01 -0.0264561  0.0053639    -4.932 8.13e-07 ***\nORIGIN_SZBMSZ02 -0.8656513  0.0068511  -126.353  &lt; 2e-16 ***\nORIGIN_SZBMSZ03 -0.1723467  0.0059613   -28.911  &lt; 2e-16 ***\nORIGIN_SZBMSZ04  0.2169844  0.0053578    40.499  &lt; 2e-16 ***\nORIGIN_SZBMSZ05 -2.0252956  0.0126107  -160.602  &lt; 2e-16 ***\nORIGIN_SZBMSZ06 -1.7642018  0.0163931  -107.619  &lt; 2e-16 ***\nORIGIN_SZBMSZ07 -0.3271629  0.0058137   -56.274  &lt; 2e-16 ***\nORIGIN_SZBMSZ08 -0.2533255  0.0059335   -42.694  &lt; 2e-16 ***\nORIGIN_SZBMSZ09 -0.7712635  0.0087939   -87.704  &lt; 2e-16 ***\nORIGIN_SZBMSZ10 -1.0098048  0.0092519  -109.145  &lt; 2e-16 ***\nORIGIN_SZBMSZ11 -0.3816187  0.0067302   -56.702  &lt; 2e-16 ***\nORIGIN_SZBMSZ12 -0.6666616  0.0095680   -69.676  &lt; 2e-16 ***\nORIGIN_SZBMSZ13 -0.0076108  0.0059040    -1.289  0.19737    \nORIGIN_SZBMSZ14 -0.1682476  0.0069391   -24.246  &lt; 2e-16 ***\nORIGIN_SZBMSZ15  0.0904585  0.0062822    14.399  &lt; 2e-16 ***\nORIGIN_SZBMSZ16 -1.1808741  0.0092258  -127.997  &lt; 2e-16 ***\nORIGIN_SZBMSZ17 -1.7189127  0.0158408  -108.512  &lt; 2e-16 ***\nORIGIN_SZBPSZ01  0.4294645  0.0058051    73.980  &lt; 2e-16 ***\nORIGIN_SZBPSZ02  0.5028906  0.0068169    73.771  &lt; 2e-16 ***\nORIGIN_SZBPSZ03  0.6656178  0.0066126   100.658  &lt; 2e-16 ***\nORIGIN_SZBPSZ04  0.5203612  0.0053224    97.769  &lt; 2e-16 ***\nORIGIN_SZBPSZ05  0.5377769  0.0047907   112.256  &lt; 2e-16 ***\nORIGIN_SZBPSZ06 -1.2327809  0.0094950  -129.835  &lt; 2e-16 ***\nORIGIN_SZBPSZ07 -0.9035255  0.0088739  -101.818  &lt; 2e-16 ***\nORIGIN_SZBSSZ01  0.1210027  0.0053990    22.412  &lt; 2e-16 ***\nORIGIN_SZBSSZ02  0.4618449  0.0048641    94.951  &lt; 2e-16 ***\nORIGIN_SZBSSZ03  0.2160739  0.0047835    45.170  &lt; 2e-16 ***\nORIGIN_SZBTSZ01 -0.1108042  0.0055599   -19.929  &lt; 2e-16 ***\nORIGIN_SZBTSZ02 -0.8911221  0.0079213  -112.498  &lt; 2e-16 ***\nORIGIN_SZBTSZ03 -0.2203980  0.0059325   -37.151  &lt; 2e-16 ***\nORIGIN_SZBTSZ04 -0.6427946  0.0105438   -60.964  &lt; 2e-16 ***\nORIGIN_SZBTSZ05 -1.4662312  0.0111784  -131.166  &lt; 2e-16 ***\nORIGIN_SZBTSZ06 -0.6105884  0.0073456   -83.123  &lt; 2e-16 ***\nORIGIN_SZBTSZ07 -1.9041317  0.0132781  -143.404  &lt; 2e-16 ***\nORIGIN_SZBTSZ08 -1.0627939  0.0095982  -110.728  &lt; 2e-16 ***\nORIGIN_SZCBSZ01 -2.9365941  0.0548632   -53.526  &lt; 2e-16 ***\nORIGIN_SZCCSZ01 -1.5313555  0.0134599  -113.772  &lt; 2e-16 ***\nORIGIN_SZCHSZ01 -1.2034494  0.0119468  -100.734  &lt; 2e-16 ***\nORIGIN_SZCHSZ02 -0.8299415  0.0081984  -101.232  &lt; 2e-16 ***\nORIGIN_SZCHSZ03 -0.5143946  0.0061944   -83.042  &lt; 2e-16 ***\nORIGIN_SZCKSZ01  0.2372583  0.0053612    44.255  &lt; 2e-16 ***\nORIGIN_SZCKSZ02  0.9124836  0.0054472   167.515  &lt; 2e-16 ***\nORIGIN_SZCKSZ03  0.7237808  0.0048401   149.539  &lt; 2e-16 ***\nORIGIN_SZCKSZ04  1.6884022  0.0050169   336.540  &lt; 2e-16 ***\nORIGIN_SZCKSZ05  1.3932005  0.0062346   223.464  &lt; 2e-16 ***\nORIGIN_SZCKSZ06  1.0670053  0.0066112   161.394  &lt; 2e-16 ***\nORIGIN_SZCLSZ01 -0.8602837  0.0079240  -108.567  &lt; 2e-16 ***\nORIGIN_SZCLSZ02 -1.3853421  0.0137444  -100.793  &lt; 2e-16 ***\nORIGIN_SZCLSZ03 -0.8582608  0.0081177  -105.727  &lt; 2e-16 ***\nORIGIN_SZCLSZ04  0.7836027  0.0046427   168.782  &lt; 2e-16 ***\nORIGIN_SZCLSZ05 -1.8121756  0.0148960  -121.655  &lt; 2e-16 ***\nORIGIN_SZCLSZ06  0.8296870  0.0043909   188.955  &lt; 2e-16 ***\nORIGIN_SZCLSZ07 -0.2325219  0.0057432   -40.487  &lt; 2e-16 ***\nORIGIN_SZCLSZ08  0.2714336  0.0062625    43.342  &lt; 2e-16 ***\nORIGIN_SZCLSZ09 -2.2223744  0.0160946  -138.082  &lt; 2e-16 ***\nORIGIN_SZDTSZ02 -4.0704970  0.0834192   -48.796  &lt; 2e-16 ***\nORIGIN_SZDTSZ03 -3.4529031  0.0738295   -46.769  &lt; 2e-16 ***\nORIGIN_SZDTSZ13 -2.8301983  0.0313085   -90.397  &lt; 2e-16 ***\nORIGIN_SZGLSZ01 -1.4674986  0.0093137  -157.563  &lt; 2e-16 ***\nORIGIN_SZGLSZ02  0.2749369  0.0050051    54.931  &lt; 2e-16 ***\nORIGIN_SZGLSZ03  0.0781954  0.0049748    15.718  &lt; 2e-16 ***\nORIGIN_SZGLSZ04  0.8167797  0.0043260   188.808  &lt; 2e-16 ***\nORIGIN_SZGLSZ05  0.5277509  0.0044879   117.595  &lt; 2e-16 ***\nORIGIN_SZHGSZ01  0.2323885  0.0048555    47.861  &lt; 2e-16 ***\nORIGIN_SZHGSZ02  0.5707182  0.0048256   118.268  &lt; 2e-16 ***\nORIGIN_SZHGSZ03  0.4231170  0.0052149    81.136  &lt; 2e-16 ***\nORIGIN_SZHGSZ04  0.9341168  0.0044128   211.681  &lt; 2e-16 ***\nORIGIN_SZHGSZ05  1.2192790  0.0043790   278.437  &lt; 2e-16 ***\nORIGIN_SZHGSZ06  0.0490041  0.0054961     8.916  &lt; 2e-16 ***\nORIGIN_SZHGSZ07  0.6337041  0.0045735   138.559  &lt; 2e-16 ***\nORIGIN_SZHGSZ08  0.0312612  0.0054684     5.717 1.09e-08 ***\nORIGIN_SZHGSZ09 -0.6985397  0.0071800   -97.289  &lt; 2e-16 ***\nORIGIN_SZHGSZ10 -2.9958967  0.0422303   -70.942  &lt; 2e-16 ***\nORIGIN_SZJESZ01  0.4363431  0.0051329    85.010  &lt; 2e-16 ***\nORIGIN_SZJESZ02  0.3460900  0.0051372    67.370  &lt; 2e-16 ***\nORIGIN_SZJESZ03  0.2928005  0.0055108    53.132  &lt; 2e-16 ***\nORIGIN_SZJESZ04 -1.1924298  0.0093540  -127.478  &lt; 2e-16 ***\nORIGIN_SZJESZ05 -2.0178136  0.0139479  -144.668  &lt; 2e-16 ***\nORIGIN_SZJESZ06  0.1637633  0.0050685    32.310  &lt; 2e-16 ***\nORIGIN_SZJESZ07 -1.8227460  0.0119383  -152.680  &lt; 2e-16 ***\nORIGIN_SZJESZ08 -1.1556281  0.0117870   -98.043  &lt; 2e-16 ***\nORIGIN_SZJESZ09  0.4766229  0.0052813    90.248  &lt; 2e-16 ***\nORIGIN_SZJESZ10 -2.6868992  0.0186864  -143.789  &lt; 2e-16 ***\nORIGIN_SZJESZ11 -3.0618150  0.0199755  -153.278  &lt; 2e-16 ***\nORIGIN_SZJWSZ01  0.4417418  0.0068611    64.383  &lt; 2e-16 ***\nORIGIN_SZJWSZ02  0.9738087  0.0047905   203.281  &lt; 2e-16 ***\nORIGIN_SZJWSZ03  1.1548028  0.0045180   255.599  &lt; 2e-16 ***\nORIGIN_SZJWSZ04  0.9078417  0.0046668   194.532  &lt; 2e-16 ***\nORIGIN_SZJWSZ05 -1.7092500  0.0127422  -134.141  &lt; 2e-16 ***\nORIGIN_SZJWSZ06 -1.3284287  0.0109785  -121.002  &lt; 2e-16 ***\nORIGIN_SZJWSZ07 -2.3231549  0.0281427   -82.549  &lt; 2e-16 ***\nORIGIN_SZJWSZ08  1.9386127  0.0046041   421.059  &lt; 2e-16 ***\nORIGIN_SZJWSZ09  1.3987549  0.0042610   328.266  &lt; 2e-16 ***\nORIGIN_SZKLSZ01  0.2617735  0.0050089    52.261  &lt; 2e-16 ***\nORIGIN_SZKLSZ02 -0.4325093  0.0064279   -67.286  &lt; 2e-16 ***\nORIGIN_SZKLSZ03 -0.2787173  0.0060380   -46.161  &lt; 2e-16 ***\nORIGIN_SZKLSZ04 -1.9432693  0.0119163  -163.076  &lt; 2e-16 ***\nORIGIN_SZKLSZ05 -0.5420067  0.0085529   -63.371  &lt; 2e-16 ***\nORIGIN_SZKLSZ06 -4.2949009  0.1857686   -23.120  &lt; 2e-16 ***\nORIGIN_SZKLSZ07 -0.8576946  0.0085178  -100.694  &lt; 2e-16 ***\nORIGIN_SZKLSZ08 -1.3840925  0.0092323  -149.918  &lt; 2e-16 ***\nORIGIN_SZLKSZ01 -2.8108510  0.0392356   -71.640  &lt; 2e-16 ***\nORIGIN_SZMDSZ01 -1.6745388  0.0296543   -56.469  &lt; 2e-16 ***\nORIGIN_SZMDSZ02 -0.8193738  0.0106631   -76.842  &lt; 2e-16 ***\nORIGIN_SZMDSZ03 -1.5088267  0.0172032   -87.706  &lt; 2e-16 ***\nORIGIN_SZMPSZ01 -0.9860154  0.0085053  -115.929  &lt; 2e-16 ***\nORIGIN_SZMPSZ02 -0.5958875  0.0070097   -85.008  &lt; 2e-16 ***\nORIGIN_SZMPSZ03 -0.0490122  0.0054582    -8.980  &lt; 2e-16 ***\nORIGIN_SZMUSZ02 -3.5233367  0.1037749   -33.952  &lt; 2e-16 ***\nORIGIN_SZNTSZ01 -2.6451541  0.0353125   -74.907  &lt; 2e-16 ***\nORIGIN_SZNTSZ02 -2.7710546  0.0232841  -119.011  &lt; 2e-16 ***\nORIGIN_SZNTSZ03 -0.6123404  0.0079083   -77.430  &lt; 2e-16 ***\nORIGIN_SZNTSZ05 -2.9257445  0.0496704   -58.903  &lt; 2e-16 ***\nORIGIN_SZNTSZ06 -3.3260031  0.0557966   -59.609  &lt; 2e-16 ***\nORIGIN_SZNVSZ01  0.6421306  0.0046037   139.482  &lt; 2e-16 ***\nORIGIN_SZNVSZ02 -0.4251550  0.0065890   -64.525  &lt; 2e-16 ***\nORIGIN_SZNVSZ03 -1.0765622  0.0078766  -136.679  &lt; 2e-16 ***\nORIGIN_SZNVSZ04 -1.2289504  0.0091468  -134.358  &lt; 2e-16 ***\nORIGIN_SZNVSZ05 -2.3551389  0.0158219  -148.853  &lt; 2e-16 ***\nORIGIN_SZPGSZ01  0.1518212  0.0154825     9.806  &lt; 2e-16 ***\nORIGIN_SZPGSZ02 -0.4062609  0.0073780   -55.064  &lt; 2e-16 ***\nORIGIN_SZPGSZ03  0.8976913  0.0046122   194.636  &lt; 2e-16 ***\nORIGIN_SZPGSZ04  1.1161685  0.0045850   243.437  &lt; 2e-16 ***\nORIGIN_SZPGSZ05  0.4794249  0.0060213    79.621  &lt; 2e-16 ***\nORIGIN_SZPLSZ01 -0.8322377  0.0107898   -77.132  &lt; 2e-16 ***\nORIGIN_SZPLSZ02 -1.2968937  0.0149841   -86.551  &lt; 2e-16 ***\nORIGIN_SZPLSZ03 -3.2744991  0.0374541   -87.427  &lt; 2e-16 ***\nORIGIN_SZPLSZ04 -3.5423615  0.0372570   -95.079  &lt; 2e-16 ***\nORIGIN_SZPLSZ05 -2.4343705  0.0227807  -106.861  &lt; 2e-16 ***\nORIGIN_SZPNSZ01  0.8052461  0.0056124   143.476  &lt; 2e-16 ***\nORIGIN_SZPNSZ02 -1.8042362  0.0128222  -140.712  &lt; 2e-16 ***\nORIGIN_SZPNSZ03 -2.6363996  0.0200058  -131.782  &lt; 2e-16 ***\nORIGIN_SZPNSZ04 -4.8427070  0.0320126  -151.275  &lt; 2e-16 ***\nORIGIN_SZPNSZ05 -3.6613775  0.0285686  -128.161  &lt; 2e-16 ***\nORIGIN_SZPRSZ01 -0.5645384  0.0117126   -48.199  &lt; 2e-16 ***\nORIGIN_SZPRSZ02  0.9145886  0.0048137   189.998  &lt; 2e-16 ***\nORIGIN_SZPRSZ03  0.4478971  0.0048102    93.113  &lt; 2e-16 ***\nORIGIN_SZPRSZ04 -0.5312444  0.0079019   -67.230  &lt; 2e-16 ***\nORIGIN_SZPRSZ05  1.1462662  0.0045250   253.318  &lt; 2e-16 ***\nORIGIN_SZPRSZ06 -0.7392744  0.0090347   -81.826  &lt; 2e-16 ***\nORIGIN_SZPRSZ07 -2.1667862  0.0162528  -133.318  &lt; 2e-16 ***\nORIGIN_SZPRSZ08 -0.1327079  0.0065712   -20.195  &lt; 2e-16 ***\nORIGIN_SZQTSZ01  0.1062151  0.0071538    14.847  &lt; 2e-16 ***\nORIGIN_SZQTSZ02 -0.4993990  0.0064382   -77.568  &lt; 2e-16 ***\nORIGIN_SZQTSZ03  0.1161844  0.0058822    19.752  &lt; 2e-16 ***\nORIGIN_SZQTSZ04 -0.8102612  0.0072742  -111.389  &lt; 2e-16 ***\nORIGIN_SZQTSZ05 -0.0417272  0.0061917    -6.739 1.59e-11 ***\nORIGIN_SZQTSZ06 -0.2521417  0.0066449   -37.945  &lt; 2e-16 ***\nORIGIN_SZQTSZ07 -1.2395975  0.0097496  -127.143  &lt; 2e-16 ***\nORIGIN_SZQTSZ08 -0.1105467  0.0059364   -18.622  &lt; 2e-16 ***\nORIGIN_SZQTSZ09 -0.5078461  0.0067895   -74.798  &lt; 2e-16 ***\nORIGIN_SZQTSZ10 -0.3866593  0.0066995   -57.714  &lt; 2e-16 ***\nORIGIN_SZQTSZ11 -1.5264609  0.0099770  -152.998  &lt; 2e-16 ***\nORIGIN_SZQTSZ12 -1.3866518  0.0106887  -129.730  &lt; 2e-16 ***\nORIGIN_SZQTSZ13 -0.3764286  0.0066707   -56.430  &lt; 2e-16 ***\nORIGIN_SZQTSZ14 -1.4907399  0.0100120  -148.896  &lt; 2e-16 ***\nORIGIN_SZQTSZ15 -1.0552239  0.0108792   -96.994  &lt; 2e-16 ***\nORIGIN_SZRCSZ01 -1.3136074  0.0126986  -103.445  &lt; 2e-16 ***\nORIGIN_SZRCSZ06 -0.2418276  0.0085509   -28.281  &lt; 2e-16 ***\nORIGIN_SZRVSZ01 -2.9263747  0.0324968   -90.051  &lt; 2e-16 ***\nORIGIN_SZRVSZ02 -2.2980940  0.0278202   -82.605  &lt; 2e-16 ***\nORIGIN_SZRVSZ03 -2.4663765  0.0238674  -103.336  &lt; 2e-16 ***\nORIGIN_SZRVSZ04 -3.1853677  0.0556939   -57.194  &lt; 2e-16 ***\nORIGIN_SZRVSZ05 -1.5695490  0.0166684   -94.163  &lt; 2e-16 ***\nORIGIN_SZSBSZ01  0.7674590  0.0061811   124.163  &lt; 2e-16 ***\nORIGIN_SZSBSZ02 -0.7307279  0.0084105   -86.883  &lt; 2e-16 ***\nORIGIN_SZSBSZ03  0.5920074  0.0050167   118.008  &lt; 2e-16 ***\nORIGIN_SZSBSZ04  0.3684857  0.0058575    62.908  &lt; 2e-16 ***\nORIGIN_SZSBSZ05 -0.0036863  0.0068459    -0.538  0.59026    \nORIGIN_SZSBSZ06 -1.1939284  0.0181541   -65.766  &lt; 2e-16 ***\nORIGIN_SZSBSZ07 -0.4896579  0.0135618   -36.106  &lt; 2e-16 ***\nORIGIN_SZSBSZ08 -2.1221691  0.0127258  -166.762  &lt; 2e-16 ***\nORIGIN_SZSBSZ09 -1.2032410  0.0089611  -134.273  &lt; 2e-16 ***\nORIGIN_SZSESZ02  1.0721820  0.0045336   236.498  &lt; 2e-16 ***\nORIGIN_SZSESZ03  1.0808012  0.0042923   251.801  &lt; 2e-16 ***\nORIGIN_SZSESZ04  1.0137448  0.0050668   200.076  &lt; 2e-16 ***\nORIGIN_SZSESZ05 -0.1678679  0.0060206   -27.882  &lt; 2e-16 ***\nORIGIN_SZSESZ06  0.9165834  0.0048323   189.677  &lt; 2e-16 ***\nORIGIN_SZSESZ07 -2.2499789  0.0196327  -114.603  &lt; 2e-16 ***\nORIGIN_SZSGSZ01 -0.9369800  0.0087282  -107.351  &lt; 2e-16 ***\nORIGIN_SZSGSZ02 -1.1690716  0.0097131  -120.360  &lt; 2e-16 ***\nORIGIN_SZSGSZ03  0.2604352  0.0052709    49.410  &lt; 2e-16 ***\nORIGIN_SZSGSZ04  0.3468823  0.0048897    70.942  &lt; 2e-16 ***\nORIGIN_SZSGSZ05 -1.5927797  0.0106308  -149.827  &lt; 2e-16 ***\nORIGIN_SZSGSZ06  0.3605651  0.0046361    77.774  &lt; 2e-16 ***\nORIGIN_SZSGSZ07 -0.5333873  0.0063119   -84.504  &lt; 2e-16 ***\nORIGIN_SZSKSZ01 -0.2706750  0.0082836   -32.676  &lt; 2e-16 ***\nORIGIN_SZSKSZ02  0.0970953  0.0063378    15.320  &lt; 2e-16 ***\nORIGIN_SZSKSZ03 -0.6954342  0.0082538   -84.256  &lt; 2e-16 ***\nORIGIN_SZSKSZ04 -2.3863580  0.0284607   -83.847  &lt; 2e-16 ***\nORIGIN_SZSKSZ05 -1.5443140  0.0179059   -86.246  &lt; 2e-16 ***\nORIGIN_SZSLSZ01 -2.9450656  0.0307283   -95.842  &lt; 2e-16 ***\nORIGIN_SZSLSZ04 -0.5739349  0.0077851   -73.722  &lt; 2e-16 ***\nORIGIN_SZSRSZ01 -1.6136735  0.0160199  -100.729  &lt; 2e-16 ***\nORIGIN_SZTHSZ01 -2.6034976  0.0489378   -53.200  &lt; 2e-16 ***\nORIGIN_SZTHSZ03 -1.2770601  0.0229815   -55.569  &lt; 2e-16 ***\nORIGIN_SZTHSZ04 -2.0110399  0.0287527   -69.943  &lt; 2e-16 ***\nORIGIN_SZTHSZ06 -1.7720116  0.0180394   -98.230  &lt; 2e-16 ***\nORIGIN_SZTMSZ01  0.1254729  0.0060924    20.595  &lt; 2e-16 ***\nORIGIN_SZTMSZ02  1.6667504  0.0039836   418.403  &lt; 2e-16 ***\nORIGIN_SZTMSZ03  1.0941176  0.0042911   254.976  &lt; 2e-16 ***\nORIGIN_SZTMSZ04  0.3209520  0.0050349    63.746  &lt; 2e-16 ***\nORIGIN_SZTMSZ05 -0.8155124  0.0079342  -102.785  &lt; 2e-16 ***\nORIGIN_SZTNSZ01 -1.4237298  0.0104636  -136.064  &lt; 2e-16 ***\nORIGIN_SZTNSZ02 -1.2718890  0.0098660  -128.916  &lt; 2e-16 ***\nORIGIN_SZTNSZ03 -1.7960517  0.0134675  -133.362  &lt; 2e-16 ***\nORIGIN_SZTNSZ04 -0.3508142  0.0073556   -47.694  &lt; 2e-16 ***\nORIGIN_SZTPSZ01 -0.3841699  0.0064137   -59.898  &lt; 2e-16 ***\nORIGIN_SZTPSZ02  0.5315265  0.0044497   119.451  &lt; 2e-16 ***\nORIGIN_SZTPSZ03 -0.4669723  0.0062160   -75.124  &lt; 2e-16 ***\nORIGIN_SZTPSZ04 -0.0617169  0.0058830   -10.491  &lt; 2e-16 ***\nORIGIN_SZTPSZ05  0.0713309  0.0062133    11.480  &lt; 2e-16 ***\nORIGIN_SZTPSZ06  0.6800356  0.0069456    97.909  &lt; 2e-16 ***\nORIGIN_SZTPSZ07 -0.0432782  0.0064382    -6.722 1.79e-11 ***\nORIGIN_SZTPSZ08 -0.6976429  0.0092416   -75.490  &lt; 2e-16 ***\nORIGIN_SZTPSZ09 -0.3708833  0.0063548   -58.363  &lt; 2e-16 ***\nORIGIN_SZTPSZ10 -0.4063575  0.0077803   -52.229  &lt; 2e-16 ***\nORIGIN_SZTPSZ11  0.1040282  0.0056115    18.538  &lt; 2e-16 ***\nORIGIN_SZTPSZ12 -0.5104672  0.0066261   -77.039  &lt; 2e-16 ***\nORIGIN_SZTSSZ01 -3.5036830  0.0487290   -71.901  &lt; 2e-16 ***\nORIGIN_SZTSSZ02 -0.0386819  0.0094886    -4.077 4.57e-05 ***\nORIGIN_SZTSSZ03 -0.3862387  0.0095139   -40.597  &lt; 2e-16 ***\nORIGIN_SZTSSZ04 -0.6380676  0.0099905   -63.867  &lt; 2e-16 ***\nORIGIN_SZTSSZ05 -2.7354613  0.0162414  -168.425  &lt; 2e-16 ***\nORIGIN_SZTSSZ06 -2.6310865  0.0255772  -102.868  &lt; 2e-16 ***\nORIGIN_SZWCSZ01 -1.1561047  0.0087394  -132.286  &lt; 2e-16 ***\nORIGIN_SZWCSZ02 -2.6956217  0.0319117   -84.471  &lt; 2e-16 ***\nORIGIN_SZWCSZ03 -4.3526889  0.1241082   -35.072  &lt; 2e-16 ***\nORIGIN_SZWDSZ01  0.8712417  0.0043720   199.277  &lt; 2e-16 ***\nORIGIN_SZWDSZ02  0.9119539  0.0050326   181.210  &lt; 2e-16 ***\nORIGIN_SZWDSZ03  1.6205678  0.0045250   358.136  &lt; 2e-16 ***\nORIGIN_SZWDSZ04  1.2081941  0.0054272   222.618  &lt; 2e-16 ***\nORIGIN_SZWDSZ05  0.4284783  0.0052752    81.224  &lt; 2e-16 ***\nORIGIN_SZWDSZ06  0.9018716  0.0049820   181.028  &lt; 2e-16 ***\nORIGIN_SZWDSZ07 -0.6444820  0.0084731   -76.062  &lt; 2e-16 ***\nORIGIN_SZWDSZ08 -0.8764983  0.0082622  -106.085  &lt; 2e-16 ***\nORIGIN_SZWDSZ09  1.3292589  0.0048663   273.158  &lt; 2e-16 ***\nORIGIN_SZYSSZ01 -0.4780462  0.0058489   -81.733  &lt; 2e-16 ***\nORIGIN_SZYSSZ02  0.9323419  0.0054402   171.380  &lt; 2e-16 ***\nORIGIN_SZYSSZ03  2.0577240  0.0046737   440.274  &lt; 2e-16 ***\nORIGIN_SZYSSZ04  0.8697472  0.0047269   184.000  &lt; 2e-16 ***\nORIGIN_SZYSSZ05  0.1662764  0.0060376    27.540  &lt; 2e-16 ***\nORIGIN_SZYSSZ06 -0.8115617  0.0109084   -74.398  &lt; 2e-16 ***\nORIGIN_SZYSSZ07 -0.8971248  0.0119220   -75.250  &lt; 2e-16 ***\nORIGIN_SZYSSZ08 -0.2738680  0.0063553   -43.093  &lt; 2e-16 ***\nORIGIN_SZYSSZ09  1.2274518  0.0044951   273.066  &lt; 2e-16 ***\nDESTIN_SZAMSZ02 -0.0516322  0.0042829   -12.055  &lt; 2e-16 ***\nDESTIN_SZAMSZ03  0.0801823  0.0041904    19.135  &lt; 2e-16 ***\nDESTIN_SZAMSZ04 -0.9282211  0.0061322  -151.368  &lt; 2e-16 ***\nDESTIN_SZAMSZ05 -1.0794168  0.0062543  -172.588  &lt; 2e-16 ***\nDESTIN_SZAMSZ06 -0.8839603  0.0060851  -145.267  &lt; 2e-16 ***\nDESTIN_SZAMSZ07 -1.5835093  0.0096846  -163.508  &lt; 2e-16 ***\nDESTIN_SZAMSZ08 -0.9756903  0.0068829  -141.756  &lt; 2e-16 ***\nDESTIN_SZAMSZ09 -1.0362692  0.0061651  -168.087  &lt; 2e-16 ***\nDESTIN_SZAMSZ10 -0.1227646  0.0044788   -27.410  &lt; 2e-16 ***\nDESTIN_SZAMSZ11 -0.4802374  0.0088108   -54.506  &lt; 2e-16 ***\nDESTIN_SZAMSZ12  0.2142621  0.0050653    42.300  &lt; 2e-16 ***\nDESTIN_SZBDSZ01  0.3582789  0.0039578    90.524  &lt; 2e-16 ***\nDESTIN_SZBDSZ02 -0.4368229  0.0051384   -85.012  &lt; 2e-16 ***\nDESTIN_SZBDSZ03 -0.1568727  0.0044329   -35.388  &lt; 2e-16 ***\nDESTIN_SZBDSZ04  0.6731669  0.0036215   185.882  &lt; 2e-16 ***\nDESTIN_SZBDSZ05  0.3647198  0.0040496    90.062  &lt; 2e-16 ***\nDESTIN_SZBDSZ06  0.0589240  0.0044352    13.286  &lt; 2e-16 ***\nDESTIN_SZBDSZ07 -0.6648168  0.0095742   -69.438  &lt; 2e-16 ***\nDESTIN_SZBDSZ08 -1.7214136  0.0106600  -161.483  &lt; 2e-16 ***\nDESTIN_SZBKSZ01 -1.2688264  0.0067263  -188.637  &lt; 2e-16 ***\nDESTIN_SZBKSZ02 -0.3912129  0.0055446   -70.558  &lt; 2e-16 ***\nDESTIN_SZBKSZ03 -0.8663392  0.0058693  -147.605  &lt; 2e-16 ***\nDESTIN_SZBKSZ04 -0.1247273  0.0051254   -24.335  &lt; 2e-16 ***\nDESTIN_SZBKSZ05 -0.7407774  0.0059120  -125.300  &lt; 2e-16 ***\nDESTIN_SZBKSZ06 -0.9934643  0.0063345  -156.834  &lt; 2e-16 ***\nDESTIN_SZBKSZ07  0.0882230  0.0042928    20.551  &lt; 2e-16 ***\nDESTIN_SZBKSZ08 -1.1134447  0.0070752  -157.372  &lt; 2e-16 ***\nDESTIN_SZBKSZ09 -0.1788171  0.0051327   -34.839  &lt; 2e-16 ***\nDESTIN_SZBLSZ01 -0.7696433  0.0071898  -107.047  &lt; 2e-16 ***\nDESTIN_SZBLSZ02  0.4076650  0.0068001    59.950  &lt; 2e-16 ***\nDESTIN_SZBLSZ03  1.5398488  0.0078230   196.836  &lt; 2e-16 ***\nDESTIN_SZBLSZ04 -0.3499486  0.0136985   -25.546  &lt; 2e-16 ***\nDESTIN_SZBMSZ01 -0.2114705  0.0048311   -43.773  &lt; 2e-16 ***\nDESTIN_SZBMSZ02 -0.3316806  0.0049958   -66.391  &lt; 2e-16 ***\nDESTIN_SZBMSZ03 -0.5134774  0.0058534   -87.723  &lt; 2e-16 ***\nDESTIN_SZBMSZ04 -0.2205274  0.0051028   -43.217  &lt; 2e-16 ***\nDESTIN_SZBMSZ05 -0.2101165  0.0067710   -31.032  &lt; 2e-16 ***\nDESTIN_SZBMSZ06 -1.3832385  0.0124821  -110.818  &lt; 2e-16 ***\nDESTIN_SZBMSZ07 -0.0133462  0.0046787    -2.853  0.00434 ** \nDESTIN_SZBMSZ08 -0.9056756  0.0063868  -141.805  &lt; 2e-16 ***\nDESTIN_SZBMSZ09 -2.3175407  0.0144523  -160.358  &lt; 2e-16 ***\nDESTIN_SZBMSZ10 -1.3973725  0.0090463  -154.470  &lt; 2e-16 ***\nDESTIN_SZBMSZ11 -1.3950206  0.0080459  -173.383  &lt; 2e-16 ***\nDESTIN_SZBMSZ12 -0.6882789  0.0081539   -84.411  &lt; 2e-16 ***\nDESTIN_SZBMSZ13 -0.2729120  0.0052969   -51.523  &lt; 2e-16 ***\nDESTIN_SZBMSZ14 -0.7581980  0.0080215   -94.521  &lt; 2e-16 ***\nDESTIN_SZBMSZ15 -0.9323237  0.0071093  -131.142  &lt; 2e-16 ***\nDESTIN_SZBMSZ16 -2.0655530  0.0108490  -190.391  &lt; 2e-16 ***\nDESTIN_SZBMSZ17 -2.5124893  0.0165366  -151.935  &lt; 2e-16 ***\nDESTIN_SZBPSZ01 -0.8203274  0.0057682  -142.216  &lt; 2e-16 ***\nDESTIN_SZBPSZ02 -1.5284265  0.0087447  -174.783  &lt; 2e-16 ***\nDESTIN_SZBPSZ03 -1.2434382  0.0080852  -153.792  &lt; 2e-16 ***\nDESTIN_SZBPSZ04 -0.7778558  0.0060900  -127.727  &lt; 2e-16 ***\nDESTIN_SZBPSZ05  0.1782204  0.0042331    42.101  &lt; 2e-16 ***\nDESTIN_SZBPSZ06 -0.6758807  0.0079728   -84.773  &lt; 2e-16 ***\nDESTIN_SZBPSZ07 -0.5029450  0.0081151   -61.976  &lt; 2e-16 ***\nDESTIN_SZBSSZ01 -0.1269916  0.0046949   -27.049  &lt; 2e-16 ***\nDESTIN_SZBSSZ02 -0.7536917  0.0051895  -145.233  &lt; 2e-16 ***\nDESTIN_SZBSSZ03  0.2747969  0.0039115    70.254  &lt; 2e-16 ***\nDESTIN_SZBTSZ01  0.1708577  0.0043381    39.385  &lt; 2e-16 ***\nDESTIN_SZBTSZ02 -0.6820190  0.0067243  -101.427  &lt; 2e-16 ***\nDESTIN_SZBTSZ03  0.0610599  0.0049825    12.255  &lt; 2e-16 ***\nDESTIN_SZBTSZ04 -1.3199639  0.0107063  -123.288  &lt; 2e-16 ***\nDESTIN_SZBTSZ05 -0.4174991  0.0069221   -60.314  &lt; 2e-16 ***\nDESTIN_SZBTSZ06 -0.5260242  0.0061145   -86.029  &lt; 2e-16 ***\nDESTIN_SZBTSZ07 -1.6678047  0.0106335  -156.844  &lt; 2e-16 ***\nDESTIN_SZBTSZ08 -0.7999935  0.0089175   -89.711  &lt; 2e-16 ***\nDESTIN_SZCBSZ01 -5.6321332  0.3162476   -17.809  &lt; 2e-16 ***\nDESTIN_SZCCSZ01 -0.9342781  0.0081409  -114.763  &lt; 2e-16 ***\nDESTIN_SZCHSZ01 -1.2808546  0.0096774  -132.355  &lt; 2e-16 ***\nDESTIN_SZCHSZ02  0.0067332  0.0054322     1.239  0.21516    \nDESTIN_SZCHSZ03  1.0988838  0.0041378   265.570  &lt; 2e-16 ***\nDESTIN_SZCKSZ01 -0.3192235  0.0050632   -63.048  &lt; 2e-16 ***\nDESTIN_SZCKSZ02 -0.7776453  0.0055279  -140.676  &lt; 2e-16 ***\nDESTIN_SZCKSZ03  0.2772358  0.0042541    65.170  &lt; 2e-16 ***\nDESTIN_SZCKSZ04 -1.3842048  0.0065159  -212.436  &lt; 2e-16 ***\nDESTIN_SZCKSZ05 -1.2051808  0.0076814  -156.897  &lt; 2e-16 ***\nDESTIN_SZCKSZ06  0.1321955  0.0061568    21.472  &lt; 2e-16 ***\nDESTIN_SZCLSZ01  0.1942449  0.0049977    38.867  &lt; 2e-16 ***\nDESTIN_SZCLSZ02 -2.0828648  0.0134597  -154.749  &lt; 2e-16 ***\nDESTIN_SZCLSZ03 -0.8823728  0.0078307  -112.681  &lt; 2e-16 ***\nDESTIN_SZCLSZ04 -0.2311432  0.0047194   -48.977  &lt; 2e-16 ***\nDESTIN_SZCLSZ05 -1.0113430  0.0085536  -118.237  &lt; 2e-16 ***\nDESTIN_SZCLSZ06  0.0694682  0.0042166    16.475  &lt; 2e-16 ***\nDESTIN_SZCLSZ07 -0.4953961  0.0054184   -91.429  &lt; 2e-16 ***\nDESTIN_SZCLSZ08 -0.3849563  0.0061404   -62.693  &lt; 2e-16 ***\nDESTIN_SZCLSZ09  0.4201808  0.0067112    62.609  &lt; 2e-16 ***\nDESTIN_SZDTSZ02 -2.6513032  0.0348725   -76.029  &lt; 2e-16 ***\nDESTIN_SZDTSZ03 -1.5192228  0.0144477  -105.153  &lt; 2e-16 ***\nDESTIN_SZDTSZ13 -2.2041951  0.0161726  -136.292  &lt; 2e-16 ***\nDESTIN_SZGLSZ01 -0.0139744  0.0052464    -2.664  0.00773 ** \nDESTIN_SZGLSZ02 -0.2850816  0.0047467   -60.059  &lt; 2e-16 ***\nDESTIN_SZGLSZ03  0.3511872  0.0039473    88.969  &lt; 2e-16 ***\nDESTIN_SZGLSZ04  0.2909117  0.0039436    73.769  &lt; 2e-16 ***\nDESTIN_SZGLSZ05  0.1845361  0.0040011    46.121  &lt; 2e-16 ***\nDESTIN_SZHGSZ01  0.1418382  0.0039875    35.571  &lt; 2e-16 ***\nDESTIN_SZHGSZ02 -0.7233151  0.0052374  -138.105  &lt; 2e-16 ***\nDESTIN_SZHGSZ03 -1.1918463  0.0062129  -191.834  &lt; 2e-16 ***\nDESTIN_SZHGSZ04 -0.4380360  0.0044839   -97.691  &lt; 2e-16 ***\nDESTIN_SZHGSZ05 -0.5671024  0.0046427  -122.149  &lt; 2e-16 ***\nDESTIN_SZHGSZ06 -0.8271411  0.0054935  -150.566  &lt; 2e-16 ***\nDESTIN_SZHGSZ07  0.0721800  0.0041589    17.356  &lt; 2e-16 ***\nDESTIN_SZHGSZ08 -0.4297429  0.0050021   -85.913  &lt; 2e-16 ***\nDESTIN_SZHGSZ09 -0.2085461  0.0052544   -39.690  &lt; 2e-16 ***\nDESTIN_SZHGSZ10 -2.9169699  0.0262698  -111.039  &lt; 2e-16 ***\nDESTIN_SZJESZ01 -0.2822473  0.0051166   -55.163  &lt; 2e-16 ***\nDESTIN_SZJESZ02 -0.6761389  0.0053635  -126.063  &lt; 2e-16 ***\nDESTIN_SZJESZ03 -0.7371756  0.0058983  -124.980  &lt; 2e-16 ***\nDESTIN_SZJESZ04 -0.4593491  0.0067970   -67.581  &lt; 2e-16 ***\nDESTIN_SZJESZ05 -1.1418012  0.0099049  -115.277  &lt; 2e-16 ***\nDESTIN_SZJESZ06  0.1759680  0.0042791    41.123  &lt; 2e-16 ***\nDESTIN_SZJESZ07 -1.2260587  0.0082714  -148.229  &lt; 2e-16 ***\nDESTIN_SZJESZ08 -0.8547001  0.0080417  -106.283  &lt; 2e-16 ***\nDESTIN_SZJESZ09 -0.4306353  0.0057006   -75.542  &lt; 2e-16 ***\nDESTIN_SZJESZ10  0.6584971  0.0073664    89.392  &lt; 2e-16 ***\nDESTIN_SZJESZ11  0.9661208  0.0070491   137.056  &lt; 2e-16 ***\nDESTIN_SZJWSZ01 -0.9128436  0.0069529  -131.290  &lt; 2e-16 ***\nDESTIN_SZJWSZ02 -0.7285851  0.0054839  -132.859  &lt; 2e-16 ***\nDESTIN_SZJWSZ03  0.2601455  0.0043215    60.198  &lt; 2e-16 ***\nDESTIN_SZJWSZ04  0.6860274  0.0041135   166.775  &lt; 2e-16 ***\nDESTIN_SZJWSZ05 -0.4684576  0.0062875   -74.506  &lt; 2e-16 ***\nDESTIN_SZJWSZ06 -0.2459774  0.0057575   -42.723  &lt; 2e-16 ***\nDESTIN_SZJWSZ07 -1.8854234  0.0287721   -65.529  &lt; 2e-16 ***\nDESTIN_SZJWSZ08 -0.5523308  0.0051054  -108.186  &lt; 2e-16 ***\nDESTIN_SZJWSZ09  0.8818747  0.0037800   233.301  &lt; 2e-16 ***\nDESTIN_SZKLSZ01 -0.5814386  0.0052711  -110.308  &lt; 2e-16 ***\nDESTIN_SZKLSZ02 -0.7090577  0.0058161  -121.914  &lt; 2e-16 ***\nDESTIN_SZKLSZ03 -1.2191910  0.0065984  -184.772  &lt; 2e-16 ***\nDESTIN_SZKLSZ04 -1.6961428  0.0087866  -193.038  &lt; 2e-16 ***\nDESTIN_SZKLSZ05 -0.6927144  0.0073574   -94.153  &lt; 2e-16 ***\nDESTIN_SZKLSZ06 -2.2967464  0.0362605   -63.340  &lt; 2e-16 ***\nDESTIN_SZKLSZ07 -0.9536980  0.0066777  -142.819  &lt; 2e-16 ***\nDESTIN_SZKLSZ08 -0.4565596  0.0051736   -88.249  &lt; 2e-16 ***\nDESTIN_SZLKSZ01 -1.7277135  0.0207336   -83.329  &lt; 2e-16 ***\nDESTIN_SZMDSZ01 -1.7155417  0.0210080   -81.661  &lt; 2e-16 ***\nDESTIN_SZMDSZ02 -1.3694928  0.0114174  -119.948  &lt; 2e-16 ***\nDESTIN_SZMDSZ03 -2.7183729  0.0252678  -107.582  &lt; 2e-16 ***\nDESTIN_SZMPSZ01 -0.8051991  0.0078564  -102.490  &lt; 2e-16 ***\nDESTIN_SZMPSZ02 -0.7627000  0.0061386  -124.246  &lt; 2e-16 ***\nDESTIN_SZMPSZ03 -0.0649484  0.0047787   -13.591  &lt; 2e-16 ***\nDESTIN_SZMUSZ02 -1.9549128  0.0200160   -97.667  &lt; 2e-16 ***\nDESTIN_SZNTSZ01 -3.3048398  0.0448053   -73.760  &lt; 2e-16 ***\nDESTIN_SZNTSZ02 -1.6454847  0.0109337  -150.497  &lt; 2e-16 ***\nDESTIN_SZNTSZ03 -1.1389723  0.0077396  -147.161  &lt; 2e-16 ***\nDESTIN_SZNTSZ05 -2.0264109  0.0250226   -80.983  &lt; 2e-16 ***\nDESTIN_SZNTSZ06 -3.3496282  0.0428989   -78.082  &lt; 2e-16 ***\nDESTIN_SZNVSZ01 -0.3407614  0.0045493   -74.905  &lt; 2e-16 ***\nDESTIN_SZNVSZ02 -0.4987695  0.0053942   -92.465  &lt; 2e-16 ***\nDESTIN_SZNVSZ03 -0.4936107  0.0055158   -89.491  &lt; 2e-16 ***\nDESTIN_SZNVSZ04 -1.9141281  0.0107557  -177.964  &lt; 2e-16 ***\nDESTIN_SZNVSZ05 -1.5378263  0.0089577  -171.677  &lt; 2e-16 ***\nDESTIN_SZPGSZ01 -1.7744485  0.0194346   -91.304  &lt; 2e-16 ***\nDESTIN_SZPGSZ02 -0.9282918  0.0069006  -134.523  &lt; 2e-16 ***\nDESTIN_SZPGSZ03  0.0885025  0.0042145    21.000  &lt; 2e-16 ***\nDESTIN_SZPGSZ04 -0.3879375  0.0046862   -82.784  &lt; 2e-16 ***\nDESTIN_SZPGSZ05 -0.9649873  0.0074625  -129.311  &lt; 2e-16 ***\nDESTIN_SZPLSZ01 -0.6159175  0.0070845   -86.939  &lt; 2e-16 ***\nDESTIN_SZPLSZ02 -1.7551386  0.0133081  -131.885  &lt; 2e-16 ***\nDESTIN_SZPLSZ03 -0.1378379  0.0098704   -13.965  &lt; 2e-16 ***\nDESTIN_SZPLSZ04 -0.1411200  0.0096446   -14.632  &lt; 2e-16 ***\nDESTIN_SZPLSZ05 -0.8483196  0.0119048   -71.259  &lt; 2e-16 ***\nDESTIN_SZPNSZ01 -0.1579087  0.0057330   -27.544  &lt; 2e-16 ***\nDESTIN_SZPNSZ02  1.0243480  0.0076680   133.587  &lt; 2e-16 ***\nDESTIN_SZPNSZ03  0.0451598  0.0081444     5.545 2.94e-08 ***\nDESTIN_SZPNSZ04  1.8941928  0.0087479   216.530  &lt; 2e-16 ***\nDESTIN_SZPNSZ05  1.0341581  0.0130830    79.046  &lt; 2e-16 ***\nDESTIN_SZPRSZ01 -1.4038513  0.0086911  -161.527  &lt; 2e-16 ***\nDESTIN_SZPRSZ02 -0.4942539  0.0052403   -94.319  &lt; 2e-16 ***\nDESTIN_SZPRSZ03  0.4219510  0.0040281   104.751  &lt; 2e-16 ***\nDESTIN_SZPRSZ04 -0.4841099  0.0083498   -57.979  &lt; 2e-16 ***\nDESTIN_SZPRSZ05 -0.2988481  0.0047512   -62.899  &lt; 2e-16 ***\nDESTIN_SZPRSZ06  0.0012333  0.0054530     0.226  0.82108    \nDESTIN_SZPRSZ07 -1.1417482  0.0118845   -96.070  &lt; 2e-16 ***\nDESTIN_SZPRSZ08 -0.8259249  0.0066757  -123.720  &lt; 2e-16 ***\nDESTIN_SZQTSZ01 -1.2134330  0.0089222  -136.002  &lt; 2e-16 ***\nDESTIN_SZQTSZ02 -1.2397956  0.0074512  -166.388  &lt; 2e-16 ***\nDESTIN_SZQTSZ03 -0.7448659  0.0066511  -111.992  &lt; 2e-16 ***\nDESTIN_SZQTSZ04 -0.6243112  0.0066812   -93.443  &lt; 2e-16 ***\nDESTIN_SZQTSZ05 -0.6102589  0.0060458  -100.940  &lt; 2e-16 ***\nDESTIN_SZQTSZ06 -0.9164592  0.0065095  -140.788  &lt; 2e-16 ***\nDESTIN_SZQTSZ07 -1.4600643  0.0109976  -132.762  &lt; 2e-16 ***\nDESTIN_SZQTSZ08  0.0004582  0.0050178     0.091  0.92724    \nDESTIN_SZQTSZ09 -0.5226213  0.0058901   -88.728  &lt; 2e-16 ***\nDESTIN_SZQTSZ10 -0.3867082  0.0055876   -69.208  &lt; 2e-16 ***\nDESTIN_SZQTSZ11  0.0260589  0.0055065     4.732 2.22e-06 ***\nDESTIN_SZQTSZ12 -0.3387634  0.0072779   -46.547  &lt; 2e-16 ***\nDESTIN_SZQTSZ13 -0.0512118  0.0053664    -9.543  &lt; 2e-16 ***\nDESTIN_SZQTSZ14 -0.2555346  0.0063792   -40.057  &lt; 2e-16 ***\nDESTIN_SZQTSZ15 -0.1820651  0.0077537   -23.481  &lt; 2e-16 ***\nDESTIN_SZRCSZ01 -0.4641196  0.0072515   -64.003  &lt; 2e-16 ***\nDESTIN_SZRCSZ06 -2.0929548  0.0189106  -110.676  &lt; 2e-16 ***\nDESTIN_SZRVSZ01 -1.7885682  0.0163492  -109.398  &lt; 2e-16 ***\nDESTIN_SZRVSZ02 -3.1669721  0.0326320   -97.051  &lt; 2e-16 ***\nDESTIN_SZRVSZ03 -2.0306835  0.0135749  -149.591  &lt; 2e-16 ***\nDESTIN_SZRVSZ04 -1.5113470  0.0155637   -97.107  &lt; 2e-16 ***\nDESTIN_SZRVSZ05 -2.3683855  0.0259334   -91.326  &lt; 2e-16 ***\nDESTIN_SZSBSZ01 -0.5841063  0.0068588   -85.162  &lt; 2e-16 ***\nDESTIN_SZSBSZ02 -1.0777704  0.0078288  -137.667  &lt; 2e-16 ***\nDESTIN_SZSBSZ03  0.4734371  0.0045880   103.190  &lt; 2e-16 ***\nDESTIN_SZSBSZ04  0.0546094  0.0057517     9.494  &lt; 2e-16 ***\nDESTIN_SZSBSZ05 -0.9588198  0.0075242  -127.431  &lt; 2e-16 ***\nDESTIN_SZSBSZ06 -1.8528944  0.0234040   -79.170  &lt; 2e-16 ***\nDESTIN_SZSBSZ07 -1.8403768  0.0195878   -93.955  &lt; 2e-16 ***\nDESTIN_SZSBSZ08  0.9205969  0.0055698   165.285  &lt; 2e-16 ***\nDESTIN_SZSBSZ09  0.5166486  0.0051939    99.472  &lt; 2e-16 ***\nDESTIN_SZSESZ02 -0.5728211  0.0048270  -118.669  &lt; 2e-16 ***\nDESTIN_SZSESZ03  0.2554787  0.0038335    66.645  &lt; 2e-16 ***\nDESTIN_SZSESZ04 -0.8982794  0.0056698  -158.432  &lt; 2e-16 ***\nDESTIN_SZSESZ05 -0.4661655  0.0048578   -95.962  &lt; 2e-16 ***\nDESTIN_SZSESZ06 -0.8392849  0.0059198  -141.777  &lt; 2e-16 ***\nDESTIN_SZSESZ07 -3.2182325  0.0227089  -141.717  &lt; 2e-16 ***\nDESTIN_SZSGSZ01 -0.2751206  0.0059581   -46.176  &lt; 2e-16 ***\nDESTIN_SZSGSZ02 -0.2951806  0.0052515   -56.209  &lt; 2e-16 ***\nDESTIN_SZSGSZ03 -0.4469508  0.0048181   -92.766  &lt; 2e-16 ***\nDESTIN_SZSGSZ04 -0.2842809  0.0047961   -59.274  &lt; 2e-16 ***\nDESTIN_SZSGSZ05 -2.0643753  0.0098252  -210.109  &lt; 2e-16 ***\nDESTIN_SZSGSZ06  0.2501247  0.0038873    64.343  &lt; 2e-16 ***\nDESTIN_SZSGSZ07 -0.5743750  0.0052184  -110.067  &lt; 2e-16 ***\nDESTIN_SZSISZ01 -1.1030669  0.0259113   -42.571  &lt; 2e-16 ***\nDESTIN_SZSKSZ01 -0.5462538  0.0071443   -76.460  &lt; 2e-16 ***\nDESTIN_SZSKSZ02  0.2965180  0.0056707    52.290  &lt; 2e-16 ***\nDESTIN_SZSKSZ03 -0.4521490  0.0062177   -72.719  &lt; 2e-16 ***\nDESTIN_SZSKSZ04 -0.6665145  0.0148252   -44.958  &lt; 2e-16 ***\nDESTIN_SZSKSZ05 -0.1474142  0.0121958   -12.087  &lt; 2e-16 ***\nDESTIN_SZSLSZ01 -0.8855715  0.0084587  -104.693  &lt; 2e-16 ***\nDESTIN_SZSLSZ04 -1.1787840  0.0071355  -165.200  &lt; 2e-16 ***\nDESTIN_SZSRSZ01 -1.6435064  0.0128822  -127.580  &lt; 2e-16 ***\nDESTIN_SZTHSZ01 -3.4388625  0.0367651   -93.536  &lt; 2e-16 ***\nDESTIN_SZTHSZ03 -2.5809435  0.0256853  -100.483  &lt; 2e-16 ***\nDESTIN_SZTHSZ04 -2.4887189  0.0214441  -116.056  &lt; 2e-16 ***\nDESTIN_SZTHSZ06 -1.7965101  0.0152160  -118.067  &lt; 2e-16 ***\nDESTIN_SZTMSZ01 -0.3251891  0.0058067   -56.002  &lt; 2e-16 ***\nDESTIN_SZTMSZ02  1.1558743  0.0034703   333.080  &lt; 2e-16 ***\nDESTIN_SZTMSZ03  0.4525619  0.0039244   115.319  &lt; 2e-16 ***\nDESTIN_SZTMSZ04  0.8223271  0.0040060   205.274  &lt; 2e-16 ***\nDESTIN_SZTMSZ05  0.3880619  0.0054308    71.456  &lt; 2e-16 ***\nDESTIN_SZTNSZ01 -0.9533112  0.0067853  -140.496  &lt; 2e-16 ***\nDESTIN_SZTNSZ02 -1.5909451  0.0097396  -163.348  &lt; 2e-16 ***\nDESTIN_SZTNSZ03 -1.6470771  0.0116598  -141.261  &lt; 2e-16 ***\nDESTIN_SZTNSZ04 -1.0686173  0.0069848  -152.993  &lt; 2e-16 ***\nDESTIN_SZTPSZ01 -0.5180183  0.0056886   -91.063  &lt; 2e-16 ***\nDESTIN_SZTPSZ02  0.2160781  0.0038283    56.443  &lt; 2e-16 ***\nDESTIN_SZTPSZ03 -0.2479956  0.0056651   -43.776  &lt; 2e-16 ***\nDESTIN_SZTPSZ04 -1.5015463  0.0072444  -207.271  &lt; 2e-16 ***\nDESTIN_SZTPSZ05 -0.9551144  0.0057981  -164.729  &lt; 2e-16 ***\nDESTIN_SZTPSZ06 -0.4846634  0.0074621   -64.950  &lt; 2e-16 ***\nDESTIN_SZTPSZ07 -1.9753440  0.0118295  -166.984  &lt; 2e-16 ***\nDESTIN_SZTPSZ08 -1.3455063  0.0086909  -154.817  &lt; 2e-16 ***\nDESTIN_SZTPSZ09 -0.3556620  0.0061296   -58.024  &lt; 2e-16 ***\nDESTIN_SZTPSZ10 -1.3213501  0.0085951  -153.733  &lt; 2e-16 ***\nDESTIN_SZTPSZ11 -0.3877006  0.0052409   -73.977  &lt; 2e-16 ***\nDESTIN_SZTPSZ12 -0.7064020  0.0062472  -113.075  &lt; 2e-16 ***\nDESTIN_SZTSSZ01 -0.8827157  0.0218327   -40.431  &lt; 2e-16 ***\nDESTIN_SZTSSZ02 -0.6067055  0.0115514   -52.522  &lt; 2e-16 ***\nDESTIN_SZTSSZ03  0.4380259  0.0086774    50.479  &lt; 2e-16 ***\nDESTIN_SZTSSZ04  0.4902124  0.0089922    54.515  &lt; 2e-16 ***\nDESTIN_SZTSSZ05  1.4336278  0.0093410   153.477  &lt; 2e-16 ***\nDESTIN_SZTSSZ06  0.9223573  0.0209024    44.127  &lt; 2e-16 ***\nDESTIN_SZWCSZ01  1.1559309  0.0051787   223.208  &lt; 2e-16 ***\nDESTIN_SZWCSZ02 -1.2664455  0.0126131  -100.407  &lt; 2e-16 ***\nDESTIN_SZWCSZ03 -2.7360882  0.0325753   -83.993  &lt; 2e-16 ***\nDESTIN_SZWDSZ01  0.8193492  0.0037301   219.657  &lt; 2e-16 ***\nDESTIN_SZWDSZ02 -0.7852474  0.0058655  -133.875  &lt; 2e-16 ***\nDESTIN_SZWDSZ03  0.5742422  0.0041884   137.104  &lt; 2e-16 ***\nDESTIN_SZWDSZ04 -0.8391525  0.0065075  -128.951  &lt; 2e-16 ***\nDESTIN_SZWDSZ05 -0.3510692  0.0057253   -61.319  &lt; 2e-16 ***\nDESTIN_SZWDSZ06  0.1358804  0.0043968    30.905  &lt; 2e-16 ***\nDESTIN_SZWDSZ07 -0.2207379  0.0066369   -33.259  &lt; 2e-16 ***\nDESTIN_SZWDSZ08 -0.0264655  0.0065065    -4.068 4.75e-05 ***\nDESTIN_SZWDSZ09 -0.2065828  0.0050524   -40.888  &lt; 2e-16 ***\nDESTIN_SZYSSZ01  0.7467996  0.0040979   182.238  &lt; 2e-16 ***\nDESTIN_SZYSSZ02 -0.3002718  0.0053434   -56.195  &lt; 2e-16 ***\nDESTIN_SZYSSZ03 -1.1087686  0.0057219  -193.778  &lt; 2e-16 ***\nDESTIN_SZYSSZ04 -0.3748076  0.0051481   -72.805  &lt; 2e-16 ***\nDESTIN_SZYSSZ05 -1.7909654  0.0102064  -175.475  &lt; 2e-16 ***\nDESTIN_SZYSSZ06 -1.8519179  0.0099601  -185.933  &lt; 2e-16 ***\nDESTIN_SZYSSZ07 -0.9246626  0.0118101   -78.294  &lt; 2e-16 ***\nDESTIN_SZYSSZ08  0.4403129  0.0041268   106.697  &lt; 2e-16 ***\nDESTIN_SZYSSZ09  0.0267012  0.0041393     6.451 1.11e-10 ***\nlog(dist)       -0.6721961  0.0001353 -4969.566  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 60796037  on 14733  degrees of freedom\nResidual deviance: 20988409  on 14175  degrees of freedom\nAIC: 21081154\n\nNumber of Fisher Scoring iterations: 7\n\n\nWe can examine how the constraints hold for destinations this time.\n\n\nCode Chunk\nCalcRSquared(dbcSIM$data$TRIPS, dbcSIM$fitted.values)\n\n\n[1] 0.5739638\n\n\nNotice that there is a relatively greater improvement in the R^2 value.\n\n\n7.9 Model comparison\nAnother useful model performance measure for continuous dependent variable is Root Mean Squared Error. In this sub-section, you will learn how to use compare_performance() of performance package\nFirst of all, let us create a list called model_list by using the code chun below.\n\n\nCode Chunk\nmodel_list &lt;- list(unconstrained=uncSIM,\n                   originConstrained=orcSIM,\n                   destinationConstrained=decSIM,\n                   doublyConstrained=dbcSIM)\n\n\nNext, we will compute the RMSE of all the models in model_list file by using the code chunk below.\n\n\nCode Chunk\ncompare_performance(model_list,\n                    metrics = \"RMSE\")\n\n\n# Comparison of Model Performance Indices\n\nName                   | Model |     RMSE\n-----------------------------------------\nunconstrained          |   glm | 4288.012\noriginConstrained      |   glm | 3659.954\ndestinationConstrained |   glm | 3389.556\ndoublyConstrained      |   glm | 3252.297\n\n\nThe print above reveals that doubly constrained SIM is the best model among all the four SIMs because it has the smallest RMSE value of 1487.111.\n\n\n7.10 Visualising fitted values\nIn this section, you will learn how to visualise the observed values and the fitted values.\nFirstly we will extract the fitted values from each model by using the code chunk below.\n\n\nCode Chunk\ndf &lt;- as.data.frame(uncSIM$fitted.values) %&gt;%\n  round(digits = 0)\n\n\nNext, we will join the values to SIM_data data frame.\n\n\nCode Chunk\nSIM_data &lt;- flow_data1 %&gt;%\n  cbind(df) %&gt;%\n  rename(uncTRIPS = \"uncSIM$fitted.values\")\n\n\nRepeat the same step by for Origin Constrained SIM (i.e. orcSIM)\n\n\nCode Chunk\ndf &lt;- as.data.frame(orcSIM$fitted.values) %&gt;%\n  round(digits = 0)\n\n\n\n\nCode Chunk\nSIM_data &lt;- SIM_data %&gt;%\n  cbind(df) %&gt;%\n  rename(orcTRIPS = \"orcSIM$fitted.values\")\n\n\nRepeat the same step by for Destination Constrained SIM (i.e. decSIM)\n\n\nCode Chunk\ndf &lt;- as.data.frame(decSIM$fitted.values) %&gt;%\n  round(digits = 0)\n\n\n\n\nCode Chunk\nSIM_data &lt;- SIM_data %&gt;%\n  cbind(df) %&gt;%\n  rename(decTRIPS = \"decSIM$fitted.values\")\n\n\nRepeat the same step by for Doubly Constrained SIM (i.e. dbcSIM)\n\n\nCode Chunk\ndf &lt;- as.data.frame(dbcSIM$fitted.values) %&gt;%\n  round(digits = 0)\n\n\n\n\nCode Chunk\nSIM_data &lt;- SIM_data %&gt;%\n  cbind(df) %&gt;%\n  rename(dbcTRIPS = \"dbcSIM$fitted.values\")\n\n\n\n\nCode Chunk\nunc_p &lt;- ggplot(data = SIM_data,\n                aes(x = uncTRIPS,\n                    y = TRIPS)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\norc_p &lt;- ggplot(data = SIM_data,\n                aes(x = orcTRIPS,\n                    y = TRIPS)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\ndec_p &lt;- ggplot(data = SIM_data,\n                aes(x = decTRIPS,\n                    y = TRIPS)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\ndbc_p &lt;- ggplot(data = SIM_data,\n                aes(x = dbcTRIPS,\n                    y = TRIPS)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\nNow, we will put all the graphs into a single visual for better comparison by using the code chunk below.\n\n\nCode Chunk\nggarrange(unc_p, orc_p, dec_p, dbc_p,\n          ncol = 2,\n          nrow = 2)"
  },
  {
    "objectID": "HandsOnExercise/HandsOn10/HandsOn10a.html",
    "href": "HandsOnExercise/HandsOn10/HandsOn10a.html",
    "title": "Hands-On Exercise 10a",
    "section": "",
    "text": "Spatial interaction represent the flow of people, material, or information between locations in geographical space. It encompasses everything from freight shipments, energy flows, and the global trade in rare antiquities, to flight schedules, rush hour woes, and pedestrian foot traffic.\nEach spatial interaction, as an analogy for a set of movements, is composed of a discrete origin/destination pair. Each pair can be represented as a cell in a matrix where rows are related to the locations (centroids) of origin, while columns are related to locations (centroids) of destination. Such a matrix is commonly known as an origin/destination matrix, or a spatial interaction matrix.\nIn this hands-on exercise, you will learn how to build an OD matrix by using Passenger Volume by Origin Destination Bus Stops data set downloaded from LTA DataMall. By the end of this hands-on exercise, you will be able:\n\nto import and extract OD data for a selected time interval,\nto import and save geospatial data (i.e. bus stops and mpsz) into sf tibble data frame objects,\nto populate planning subzone code into bus stops sf tibble data frame,\nto construct desire lines geospatial data from the OD data, and\nto visualise passenger volume by origin and destination bus stops by using the desire lines data."
  },
  {
    "objectID": "HandsOnExercise/HandsOn10/HandsOn10a.html#overview",
    "href": "HandsOnExercise/HandsOn10/HandsOn10a.html#overview",
    "title": "Hands-On Exercise 10a",
    "section": "",
    "text": "Spatial interaction represent the flow of people, material, or information between locations in geographical space. It encompasses everything from freight shipments, energy flows, and the global trade in rare antiquities, to flight schedules, rush hour woes, and pedestrian foot traffic.\nEach spatial interaction, as an analogy for a set of movements, is composed of a discrete origin/destination pair. Each pair can be represented as a cell in a matrix where rows are related to the locations (centroids) of origin, while columns are related to locations (centroids) of destination. Such a matrix is commonly known as an origin/destination matrix, or a spatial interaction matrix.\nIn this hands-on exercise, you will learn how to build an OD matrix by using Passenger Volume by Origin Destination Bus Stops data set downloaded from LTA DataMall. By the end of this hands-on exercise, you will be able:\n\nto import and extract OD data for a selected time interval,\nto import and save geospatial data (i.e. bus stops and mpsz) into sf tibble data frame objects,\nto populate planning subzone code into bus stops sf tibble data frame,\nto construct desire lines geospatial data from the OD data, and\nto visualise passenger volume by origin and destination bus stops by using the desire lines data."
  },
  {
    "objectID": "HandsOnExercise/HandsOn10/HandsOn10a.html#getting-started",
    "href": "HandsOnExercise/HandsOn10/HandsOn10a.html#getting-started",
    "title": "Hands-On Exercise 10a",
    "section": "2 Getting Started",
    "text": "2 Getting Started\nFor the purpose of this exercise, five r packages will be used. They are:\n\nsf for importing, integrating, processing and transforming geospatial data.\ntidyverse for importing, integrating, wrangling and visualising data.\ntmap for creating elegent and cartographic quality thematic maps.\nstplanr provides functions for solving common problems in transport planning and modelling such as downloading and cleaning transport datasets; creating geographic “desire lines” from origin-destination (OD) data; route assignment, locally and interfaces to routing services such as CycleStreets.net; calculation of route segment attributes such as bearing and aggregate flow; and ‘travel watershed’ analysis.\nDT provides an R interface to the JavaScript library DataTables. R data objects (matrices or data frames) can be displayed as tables on HTML pages, and DataTables provides filtering, pagination, sorting, and many other features in the tables.\n\n\n\nCode Chunk\npacman::p_load(tmap, sf, DT, stplanr, tidyverse)"
  },
  {
    "objectID": "HandsOnExercise/HandsOn10/HandsOn10a.html#preparing-the-flow-data",
    "href": "HandsOnExercise/HandsOn10/HandsOn10a.html#preparing-the-flow-data",
    "title": "Hands-On Exercise 10a",
    "section": "3 Preparing the Flow Data",
    "text": "3 Preparing the Flow Data\n\n3.1 Importing the OD data\nFirstly, we will import the Passenger Volume by Origin Destination Bus Stops data set downloaded from LTA DataMall by using read_csv() of readr package.\n\n\nCode Chunk\nodbus &lt;- read_csv(\"data/aspatial/origin_destination_bus_202210.csv\")\n\n\nLet use display the odbus tibble data table by using the code chunk below.\n\n\nCode Chunk\nglimpse(odbus)\n\n\nRows: 5,122,925\nColumns: 7\n$ YEAR_MONTH          &lt;chr&gt; \"2022-10\", \"2022-10\", \"2022-10\", \"2022-10\", \"2022-…\n$ DAY_TYPE            &lt;chr&gt; \"WEEKDAY\", \"WEEKENDS/HOLIDAY\", \"WEEKENDS/HOLIDAY\",…\n$ TIME_PER_HOUR       &lt;dbl&gt; 10, 10, 7, 11, 16, 16, 20, 7, 7, 11, 11, 8, 11, 11…\n$ PT_TYPE             &lt;chr&gt; \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"…\n$ ORIGIN_PT_CODE      &lt;dbl&gt; 65239, 65239, 23519, 52509, 54349, 54349, 43371, 8…\n$ DESTINATION_PT_CODE &lt;dbl&gt; 65159, 65159, 23311, 42041, 53241, 53241, 14139, 9…\n$ TOTAL_TRIPS         &lt;dbl&gt; 2, 1, 2, 1, 1, 4, 1, 3, 1, 5, 2, 5, 15, 40, 1, 1, …\n\n\nA quick check of odbus tibble data frame shows that the values in OROGIN_PT_CODE and DESTINATON_PT_CODE are in numeric data type. Hence, the code chunk below is used to convert these data values into character data type.\n\n\nCode Chunk\nodbus$ORIGIN_PT_CODE &lt;- as.factor(odbus$ORIGIN_PT_CODE)\nodbus$DESTINATION_PT_CODE &lt;- as.factor(odbus$DESTINATION_PT_CODE) \n\n\n\n\n3.2 Extracting the study data\nFor the purpose of this exercise, we will extract commuting flows on weekday and between 6 and 9 o’clock.\n\n\nCode Chunk\nodbus6_9 &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 6 &\n           TIME_PER_HOUR &lt;= 9) %&gt;%\n  group_by(ORIGIN_PT_CODE,\n           DESTINATION_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\n\nTable below shows the content of odbus6_9\n\n\nCode Chunk\ndatatable(odbus6_9)\n\n\n\n\n\n\nWe will save the output in rds format for future used.\n\n\nCode Chunk\nwrite_rds(odbus6_9, \"data/rds/odbus6_9.rds\")\n\n\nThe code chunk below will be used to import the save odbus6_9.rds into R environment.\n\n\nCode Chunk\nodbus6_9 &lt;- read_rds(\"data/rds/odbus6_9.rds\")"
  },
  {
    "objectID": "HandsOnExercise/HandsOn10/HandsOn10a.html#working-with-geospatial-data",
    "href": "HandsOnExercise/HandsOn10/HandsOn10a.html#working-with-geospatial-data",
    "title": "Hands-On Exercise 10a",
    "section": "4 Working with Geospatial Data",
    "text": "4 Working with Geospatial Data\nFor the purpose of this exercise, two geospatial data will be used. They are:\n\nBusStop: This data provides the location of bus stop as at last quarter of 2022.\nMPSZ-2019: This data provides the sub-zone boundary of URA Master Plan 2019.\n\nBoth data sets are in ESRI shapefile format.\n\n4.1 Importing geospatial data\nTwo geospatial data will be used in this exercise, they are:\n\n\nCode Chunk\nbusstop &lt;- st_read(dsn = \"data/geospatial\",\n                   layer = \"BusStop\") %&gt;%\n  st_transform(crs = 3414)\n\n\nReading layer `BusStop' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/HandsOnExercise/HandsOn10/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5159 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\n\n\n\nCode Chunk\nmpsz &lt;- st_read(dsn = \"data/geospatial\",\n                   layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs = 3414)\n\n\nReading layer `MPSZ-2019' from data source \n  `/Users/joshuatingsiyuan14/Desktop/isaiahting/ISSS626_GAA/HandsOnExercise/HandsOn10/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\nCode Chunk\nmpsz\n\n\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n                 SUBZONE_N SUBZONE_C       PLN_AREA_N PLN_AREA_C       REGION_N\n1              MARINA EAST    MESZ01      MARINA EAST         ME CENTRAL REGION\n2         INSTITUTION HILL    RVSZ05     RIVER VALLEY         RV CENTRAL REGION\n3           ROBERTSON QUAY    SRSZ01  SINGAPORE RIVER         SR CENTRAL REGION\n4  JURONG ISLAND AND BUKOM    WISZ01  WESTERN ISLANDS         WI    WEST REGION\n5             FORT CANNING    MUSZ02           MUSEUM         MU CENTRAL REGION\n6         MARINA EAST (MP)    MPSZ05    MARINE PARADE         MP CENTRAL REGION\n7                   SUDONG    WISZ03  WESTERN ISLANDS         WI    WEST REGION\n8                  SEMAKAU    WISZ02  WESTERN ISLANDS         WI    WEST REGION\n9           SOUTHERN GROUP    SISZ02 SOUTHERN ISLANDS         SI CENTRAL REGION\n10                 SENTOSA    SISZ01 SOUTHERN ISLANDS         SI CENTRAL REGION\n   REGION_C                       geometry\n1        CR MULTIPOLYGON (((33222.98 29...\n2        CR MULTIPOLYGON (((28481.45 30...\n3        CR MULTIPOLYGON (((28087.34 30...\n4        WR MULTIPOLYGON (((14557.7 304...\n5        CR MULTIPOLYGON (((29542.53 31...\n6        CR MULTIPOLYGON (((35279.55 30...\n7        WR MULTIPOLYGON (((15772.59 21...\n8        WR MULTIPOLYGON (((19843.41 21...\n9        CR MULTIPOLYGON (((30870.53 22...\n10       CR MULTIPOLYGON (((26879.04 26...\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nst_read() function of sf package is used to import the shapefile into R as sf data frame.\nst_transform() function of sf package is used to transform the projection to crs 3414.\n\n\n\nThe code chunk below will be used to write mpsz sf tibble data frame into an rds file for future use.\n\n\nCode Chunk\nmpsz &lt;- write_rds(mpsz, \"data/rds/mpsz.rds\")"
  },
  {
    "objectID": "HandsOnExercise/HandsOn10/HandsOn10a.html#geospatial-data-wrangling",
    "href": "HandsOnExercise/HandsOn10/HandsOn10a.html#geospatial-data-wrangling",
    "title": "Hands-On Exercise 10a",
    "section": "5 Geospatial data wrangling",
    "text": "5 Geospatial data wrangling\n\n5.1 Combining Busstop and mpsz\nCode chunk below populates the planning subzone code (i.e. SUBZONE_C) of mpsz sf data frame into busstop sf data frame.\n\n\nCode Chunk\nbusstop_mpsz &lt;- st_intersection(busstop, mpsz) %&gt;%\n  select(BUS_STOP_N, SUBZONE_C) %&gt;%\n  st_drop_geometry()\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nst_intersection() is used to perform point and polygon overly and the output will be in point sf object.\nselect() of dplyr package is then use to retain only BUS_STOP_N and SUBZONE_C in the busstop_mpsz sf data frame.\nfive bus stops are excluded in the resultant data frame because they are outside of Singapore bpundary.\n\n\n\n\n\nCode Chunk\ndatatable(busstop_mpsz)\n\n\n\n\n\n\nBefore moving to the next step, it is wise to save the output into rds format.\n\n\nCode Chunk\nwrite_rds(busstop_mpsz, \"data/rds/busstop_mpsz.rds\")  \n\n\nNext, we are going to append the planning subzone code from busstop_mpsz data frame onto odbus6_9 data frame.\n\n\nCode Chunk\nod_data &lt;- left_join(odbus6_9 , busstop_mpsz,\n            by = c(\"ORIGIN_PT_CODE\" = \"BUS_STOP_N\")) %&gt;%\n  rename(ORIGIN_BS = ORIGIN_PT_CODE,\n         ORIGIN_SZ = SUBZONE_C,\n         DESTIN_BS = DESTINATION_PT_CODE)\n\n\nBefore continue, it is a good practice for us to check for duplicating records.\n\n\nCode Chunk\nduplicate &lt;- od_data %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\n\nIf duplicated records are found, the code chunk below will be used to retain the unique records.\n\n\nCode Chunk\nod_data &lt;- unique(od_data)\n\n\nIt will be a good practice to confirm if the duplicating records issue has been addressed fully.\nNext, we will update od_data data frame cwith the planning subzone codes.\n\n\nCode Chunk\nod_data &lt;- left_join(od_data , busstop_mpsz,\n            by = c(\"DESTIN_BS\" = \"BUS_STOP_N\")) \n\n\n\n\nCode Chunk\nduplicate &lt;- od_data %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\n\n\n\nCode Chunk\nod_data &lt;- unique(od_data)\n\n\n\n\nCode Chunk\nod_data &lt;- od_data %&gt;%\n  rename(DESTIN_SZ = SUBZONE_C) %&gt;%\n  drop_na() %&gt;%\n  group_by(ORIGIN_SZ, DESTIN_SZ) %&gt;%\n  summarise(MORNING_PEAK = sum(TRIPS))\n\n\nIt is time to save the output into an rds file format.\n\n\nCode Chunk\nwrite_rds(od_data, \"data/rds/od_data_fii.rds\")\n\n\n\n\nCode Chunk\nod_data_fii &lt;- read_rds(\"data/rds/od_data.rds\")"
  },
  {
    "objectID": "HandsOnExercise/HandsOn10/HandsOn10a.html#visualising-spatial-interaction",
    "href": "HandsOnExercise/HandsOn10/HandsOn10a.html#visualising-spatial-interaction",
    "title": "Hands-On Exercise 10a",
    "section": "6 Visualising Spatial Interaction",
    "text": "6 Visualising Spatial Interaction\nIn this section, you will learn how to prepare a desire line by using stplanr package.\n\n6.1 Removing intra-zonal flows\nWe will not plot the intra-zonal flows. The code chunk below will be used to remove intra-zonal flows.\n\n\nCode Chunk\nod_data_fij &lt;- od_data[od_data$ORIGIN_SZ!=od_data$DESTIN_SZ,]\n\n\n\n\nCode Chunk\nwrite_rds(od_data_fij, \"data/rds/od_data_fij.rds\")\n\n\n\n\nCode Chunk\nod_data_fij &lt;- read_rds(\"data/rds/od_data_fij.rds\")\n\n\n\n\n6.2 Creating desire lines\nIn this code chunk below, od2line() of stplanr package is used to create the desire lines.\n\n\nCode Chunk\nflowLine &lt;- od2line(flow = od_data_fij, \n                    zones = mpsz,\n                    zone_code = \"SUBZONE_C\")\n\n\n\n\nCode Chunk\nwrite_rds(flowLine, \"data/rds/flowLine.rds\")\n\n\n\n\nCode Chunk\nflowLine &lt;- read_rds(\"data/rds/flowLine.rds\")\n\n\n\n\n6.3 Visualising the desire lines\nTo visualise the resulting desire lines, the code chunk below is used.\n\n\nCode Chunk\ntm_shape(mpsz) +\n  tm_polygons() +\nflowLine %&gt;%  \ntm_shape() +\n  tm_lines(lwd = \"MORNING_PEAK\",\n           style = \"quantile\",\n           scale = c(0.1, 1, 3, 5, 7, 10),\n           n = 6,\n           alpha = 0.3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBe patient, the rendering process takes more time because of the transparency argument (i.e. alpha)\n\n\nWhen the flow data are very messy and highly skewed like the one shown above, it is wiser to focus on selected flows, for example flow greater than or equal to 5000 as shown below.\n\n\nCode Chunk\ntm_shape(mpsz) +\n  tm_polygons() +\nflowLine %&gt;%  \n  filter(MORNING_PEAK &gt;= 5000) %&gt;%\ntm_shape() +\n  tm_lines(lwd = \"MORNING_PEAK\",\n           style = \"quantile\",\n           scale = c(0.1, 1, 3, 5, 7, 10),\n           n = 6,\n           alpha = 0.3)"
  },
  {
    "objectID": "InClassExercise/InClass9/data/geospatial/ELDERCARE.html",
    "href": "InClassExercise/InClass9/data/geospatial/ELDERCARE.html",
    "title": "Navigating Geospatial Mines",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;  ELDERCARE  ENG dataset\n\nELDERCARE\n\n                 0 0     false"
  },
  {
    "objectID": "HandsOnExercise/HandsOn10/data/geospatial/MPSZ-2019.html",
    "href": "HandsOnExercise/HandsOn10/data/geospatial/MPSZ-2019.html",
    "title": "Navigating Geospatial Mines",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "InClassExercise/InClass9/InClass9.html",
    "href": "InClassExercise/InClass9/InClass9.html",
    "title": "In-Class Exercise 9",
    "section": "",
    "text": "Code Chunk\npacman::p_load(sf, tmap, spdep, GWmodel, SpatialML, rsample, Metrics, tidyverse, knitr, kableExtra)\n\n\nELDERCARE is in shapefile format, the code chunk below will be used:\n\n\nCode Chunk\neldercare &lt;- st_read(dsn = \"data/EldercareServicesSHP\",\n                     layer = \"ELDERCARE\") %&gt;% \n  st_transform(crs=3414)\n\n\nCode chunk below is used to import fml file.\n\n\nCode Chunk\nCHAS &lt;- st_read(\"data/CHASClinics.kml\") %&gt;% \n  st_transform(crs=3414)\n\n\nSt_buffer of sf package is used to a buffer of 1km of around each eldercare features\n\n\nCode Chunk\nbuffer_km &lt;- st_buffer(eldercare, \n                        dist = 1000)\n\n\nthe code chunk belwo is used to plot the newly created buffers and the CHAS clinic\nPLOT THE POLYGON FIRST then the point\n\n\nCode Chunk\ntmap_mode('plot') #view\ntm_shape(buffer_km) +\n  tm_polygons(alpha=.3) +\ntm_shape(CHAS) +\n  tm_dots()\n\n\ncode chunk below is used to count the number of CHAS clinics with 1km of each eldercare centre\n\n\nCode Chunk\nbuffer_km$pts_counts &lt;- lengths( #length returns in the new field name pts count\n  st_intersects(buffer_km, CHAS))\n\n\n\n\n\n\n\n\nspatstat do not like point z\n\n\n\n\n\n\n\n\nCode Chunk\nmdata &lt;- read_rds(\"data/rds/model/mdata.rds\") %&gt;%\n  st_jitter(amount= 2) #in metres\n#jitter is displacement of the same point\n\n\nif use gwr, coordinates that are the same will return nothing. thus, need to st_jitter to shift alittle. Shouldn’t use decimal as if might be 0.\nEg: 10 transactions in one hdb flat. then need to jitter to displace the distance alittle.\n\n\n\n\n\n\nwhat are the codes to see overlapping\n\n\n\n\n\n\n\n\nCode Chunk\npacman::p_load(SpatialACC, sf, tidyverse, tmap, ggstatsplot)\n\n\n\n\nCode Chunk\nmpsz &lt;- st_read(dsn = \"data/geospatial\",\n                layer=\"MP14_SUBZONE_NO_SEA_PL\")\n\n\n\n\nCode Chunk\nhexagons &lt;- st_read(dsn = \"data/geospatial\",\n                    layer = \"hexagons\")\n\n\n\n\nCode Chunk\neldercare &lt;-st_read(dsn = \"data/geospatial\",\n                    layer = \"ELDERCARE\")\n\n\n\n\nCode Chunk\nODMatrix &lt;- read_csv(\"data/aspatial/OD_Matrix.csv\",\n                     skip = 0)\n\n\n\n\nCode Chunk\neldercare &lt;- eldercare %&gt;% \n  select(fid, ADDRESSPOS) %&gt;%\n  mutate(capacity = 100)\n\n\n\n\nCode Chunk\nhexagons &lt;- hexagons %&gt;% \n  select(fid) %&gt;% \n  mutate(demand=100)\n\n\n\n\nCode Chunk\ndistmat &lt;- ODMatrix %&gt;% \n  select(origin_id, destination_id, total_cost) %&gt;% \n  spread(destination_id, total_cost) %&gt;% #converts to matrix instead making it into long and thin \n  select(-c('origin_id'))\n\n\n\n\nCode Chunk\ndistmat_km &lt;- as.matrix(distmat/1000)\n\n\n\n\nCode Chunk\nacc_Hansen &lt;- data.frame(ac(hexagons$demand, #distance\n                            eldercare$capacity, #origin\n                            distmat_km, \n                            #d0 = 50, #distance limits\n                            power = 2, #can change the power\n                            family = \"Hansen\"))\n\n\n\n\nCode Chunk\ncolnames(acc_Hansen) &lt;- \"accHansen\"\n\nacc_Hansen &lt;- as_tibble(acc_Hansen)\n\nhexagon_Hansen &lt;- bind_cols(hexagons, acc_Hansen)\n\n\n\n\nCode Chunk\nmapex &lt;- st_bbox(hexagons)\n\ntmap_mode(\"plot\")\ntm_shape(hexagon_Hansen,\n         bbox = mapex) + \n  tm_fill(col = \"accHansen\",\n          n = 10,\n          style = \"quantile\",\n          border.col = \"black\",\n          border.lwd = 1) +\ntm_shape(eldercare) +\n  tm_symbols(size = 0.1) +\n  tm_layout(main.title = \"Accessibility to eldercare: Hansen method\",\n            main.title.position = \"center\",\n            main.title.size = 2,\n            legend.outside = FALSE,\n            legend.height = 0.45, \n            legend.width = 3.0,\n            legend.format = list(digits = 6),\n            legend.position = c(\"right\", \"top\"),\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.5)"
  },
  {
    "objectID": "InClassExercise/InClass9/data/geospatial/hexagons.html",
    "href": "InClassExercise/InClass9/data/geospatial/hexagons.html",
    "title": "Navigating Geospatial Mines",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n                 0 0     false"
  },
  {
    "objectID": "TakeHomeExercise/TakeHome3/TakeHome3.html#building-a-non-spatial-multiple-linear-regression",
    "href": "TakeHomeExercise/TakeHome3/TakeHome3.html#building-a-non-spatial-multiple-linear-regression",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "PunggolWoodlands\n\n\n\n\nCode Chunk\n# Fit the linear model for resale_price using the specified variables\nprice_mlr_p &lt;- lm(resale_price ~ floor_area_sqm +\n                 unit_age + within_350m_busstop +\n                 within_350m_eldercare + hawker_prox + \n                 within_350m_kindergarten + mall_prox +\n                 mrt_prox + park_prox +\n                 within_1km_school +\n                 supermarket_prox,\n               data = train_data_punggol)\n\n# Summary of the linear model\nsummary(price_mlr_p)\n\n\n\nCall:\nlm(formula = resale_price ~ floor_area_sqm + unit_age + within_350m_busstop + \n    within_350m_eldercare + hawker_prox + within_350m_kindergarten + \n    mall_prox + mrt_prox + park_prox + within_1km_school + supermarket_prox, \n    data = train_data_punggol)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-136613  -26146     -96   26415  140294 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              -2425118.0   181420.5 -13.367  &lt; 2e-16 ***\nfloor_area_sqm              11207.8      669.0  16.754  &lt; 2e-16 ***\nunit_age                    -3655.1      390.9  -9.351  &lt; 2e-16 ***\nwithin_350m_busstop          -601.6      515.7  -1.167 0.243557    \nwithin_350m_eldercare       18135.0     2457.4   7.380 2.59e-13 ***\nhawker_prox                -73597.2     5073.0 -14.508  &lt; 2e-16 ***\nwithin_350m_kindergarten     -788.8     2040.8  -0.387 0.699178    \nmall_prox                   37612.1     3120.5  12.053  &lt; 2e-16 ***\nmrt_prox                   -81583.3    11403.9  -7.154 1.30e-12 ***\npark_prox                   64924.6     7539.8   8.611  &lt; 2e-16 ***\nwithin_1km_school           -5807.9     1023.9  -5.673 1.68e-08 ***\nsupermarket_prox            41305.5    11410.9   3.620 0.000304 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 39990 on 1537 degrees of freedom\nMultiple R-squared:  0.5345,    Adjusted R-squared:  0.5311 \nF-statistic: 160.4 on 11 and 1537 DF,  p-value: &lt; 2.2e-16\n\n\nCode Chunk\nbeep(3)\n\n\n\n\nCode Chunk\nwrite_rds(price_mlr_p, \"data/rds/ml/punggol/price_mlr_p.rds\" ) \n\n\n\n\n\n\nCode Chunk\n# Fit the linear model for resale_price using the specified variables\nprice_mlr_w &lt;- lm(resale_price ~ floor_area_sqm +\n                 unit_age + within_350m_busstop +\n                 within_350m_eldercare + hawker_prox + \n                 within_350m_kindergarten + mall_prox +\n                 mrt_prox + park_prox +\n                 within_1km_school +\n                 supermarket_prox,\n               data = train_data_woodlands)\n\n# Summary of the linear model\nsummary(price_mlr_w)\n\n\n\nCall:\nlm(formula = resale_price ~ floor_area_sqm + unit_age + within_350m_busstop + \n    within_350m_eldercare + hawker_prox + within_350m_kindergarten + \n    mall_prox + mrt_prox + park_prox + within_1km_school + supermarket_prox, \n    data = train_data_woodlands)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-108605  -18479    -861   17651  118328 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              475932.61   72526.66   6.562 7.48e-11 ***\nfloor_area_sqm             2320.91     142.03  16.341  &lt; 2e-16 ***\nunit_age                  -4309.72      84.93 -50.742  &lt; 2e-16 ***\nwithin_350m_busstop         386.65     394.41   0.980   0.3271    \nwithin_350m_eldercare      1333.49    1282.39   1.040   0.2986    \nhawker_prox              -16533.03    3163.02  -5.227 1.99e-07 ***\nwithin_350m_kindergarten   2531.64    1171.15   2.162   0.0308 *  \nmall_prox                 -1004.92    1442.73  -0.697   0.4862    \nmrt_prox                 -44163.93    4902.52  -9.008  &lt; 2e-16 ***\npark_prox                -15524.87    2149.37  -7.223 8.38e-13 ***\nwithin_1km_school           693.69     670.29   1.035   0.3009    \nsupermarket_prox             23.45    5293.65   0.004   0.9965    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 28570 on 1379 degrees of freedom\nMultiple R-squared:  0.748, Adjusted R-squared:  0.746 \nF-statistic: 372.1 on 11 and 1379 DF,  p-value: &lt; 2.2e-16\n\n\nCode Chunk\nbeep(3)\n\n\n\n\nCode Chunk\nwrite_rds(price_mlr_w, \"data/rds/ml/woodlands/price_mlr_w.rds\" )"
  },
  {
    "objectID": "TakeHomeExercise/TakeHome3/TakeHome3.html#evaluation-of-model-geographically-weighted-regression-vs-random-forest",
    "href": "TakeHomeExercise/TakeHome3/TakeHome3.html#evaluation-of-model-geographically-weighted-regression-vs-random-forest",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "PunggolWoodlands\n\n\nTo compare the two methods—Random Forest (RF) and Geographically Weighted Regression (GWR)—for predicting resale prices in Punggol, let’s focus on the prediction accuracy and model performance.\n1. Prediction Accuracy:\nRandom Forest:\n\nRMSE: The Random Forest model has an RMSE of 49,423.08, which measures the average error between the predicted and actual resale prices. A lower RMSE indicates better accuracy.\nPrice Distribution:\n\nTraining Data: Min 456,000, Median 605,000, Max 800,000.\nTest Data: Min 495,000, Median 648,000, Max 788,000.\n\n\nThe predicted resale prices in the test data are within the expected range and the RMSE indicates that the Random Forest model performs fairly well.\nGWR:\n\nPredicted Values: The predicted resale prices from the GWR model range from 463,945 to 707,938, with a median of 609,348.\nPrediction Variance: The variance of predictions ranges from 1.25 billion to 1.4 billion, indicating some spread in the predictions, which is typical of GWR since it accounts for spatial variability.\n\nThe predicted resale prices from GWR are also within the expected range, with a similar spread as the actual test data.\n\nModel Calibration:\n\nRandom Forest:\n\nStrength: Random Forest is well-suited for handling complex, non-linear relationships between predictors without explicit assumptions about their functional form.\nWeakness: It is harder to interpret and doesn’t directly provide insights into how individual variables (like distance to bus stops, malls, etc.) influence the resale prices.\n\nGWR:\n\nStrength: GWR allows for spatial variation in the coefficients, meaning it can show how the effect of each predictor (like floor_area_sqm, unit_age, etc.) changes across different locations. This makes GWR especially valuable for understanding local variations in resale prices.\nWeakness: GWR requires more complex interpretation, as it provides different coefficients for each observation based on the local neighborhood.\n\n\nWhich Model is Better?\n\n\nPrediction Accuracy: Both models provide similar ranges of predicted resale prices. However, Random Forest has a slightly better RMSE (49,423.08), which suggests it may perform slightly better in terms of overall prediction accuracy.\nInterpretability: GWR excels in interpretability, as it provides insights into how different factors affect resale prices in different parts of Punggol, thanks to its spatially varying coefficients. This is valuable if you need to understand the localized impact of each predictor.\n\n\n\n1. Prediction Accuracy:\nRandom Forest:\n\nRMSE: The RMSE for the Random Forest model is 42,038.73. Lower RMSE indicates better prediction accuracy, and this value is quite reasonable, suggesting good performance for predicting resale prices.\nPrice Summary:\n\nTraining Data: Min 350,000, Median 500,000, Max 690,000.\nTest Data: Min 355,000, Median 530,000, Max 701,000.\n\nThe test data’s price distribution suggests that the model is predicting within the expected range.\n\nGWR:\n\nPrediction: The GWR model gives predicted resale prices ranging from 359,170 to 616,562, with a median of 493,786. This is fairly close to the actual resale prices in the test data (from 355,000 to 701,000), indicating that GWR is also performing reasonably well for predictions.\nPrediction Variance: The prediction variance ranges from 735,748,898 to 792,698,476, suggesting that the GWR model’s predictions have some spread, but this is expected since GWR accounts for spatial variation.\n\n\nModel Calibration:\n\nRandom Forest:\n\nRandom Forest tends to perform well when capturing complex, non-linear relationships between predictors. It doesn’t offer easy interpretability, but it can model interactions between variables without explicitly specifying them. Given that the RMSE is relatively low (42,038.73), the model seems to be doing well in predicting resale prices.\n\nGWR:\n\nModel Calibration Information: The coefficients for the GWR model vary spatially, and the values seem reasonable (e.g., for floor_area_sqm, the coefficient ranges from -3419.735 to 3037.7). These coefficients represent the local effect of each variable on resale prices.\nInterpretability: One key strength of GWR is that it allows you to understand how each variable affects resale prices differently in various spatial locations (e.g., the effect of proximity to bus stops, hawker centers, or supermarkets varies across the region).\n\n\nWhich Model is Better?\n\n\nPrediction Accuracy: Both models give similar ranges for predicted resale prices, and both seem to predict the prices reasonably well. However, Random Forest generally provides a more reliable prediction with lower RMSE (42,038.73), indicating better overall performance in terms of predictive accuracy.\nInterpretability: If you need to understand how each factor influences resale prices in different regions of Woodlands, GWR is a better choice because of its spatially varying coefficients and ability to show how variables interact with location."
  },
  {
    "objectID": "InClassExercise/InClass8/InClass8.html#installing-loading-r-packages",
    "href": "InClassExercise/InClass8/InClass8.html#installing-loading-r-packages",
    "title": "In-Class Exercise 8",
    "section": "1 Installing & Loading R Packages",
    "text": "1 Installing & Loading R Packages\n\n\nCode Chunk\npacman::p_load(sf, spdep, GWmodel, SpatialML, \n               tmap, rsample, Metrics, tidyverse,\n               knitr, kableExtra)"
  },
  {
    "objectID": "InClassExercise/InClass8/InClass8.html#data-import",
    "href": "InClassExercise/InClass8/InClass8.html#data-import",
    "title": "In-Class Exercise 8",
    "section": "2 Data Import",
    "text": "2 Data Import\n\n\nCode Chunk\nmdata &lt;- read_rds(\"data/model/mdata.rds\")"
  },
  {
    "objectID": "InClassExercise/InClass8/InClass8.html#data-sampling",
    "href": "InClassExercise/InClass8/InClass8.html#data-sampling",
    "title": "In-Class Exercise 8",
    "section": "3 Data Sampling",
    "text": "3 Data Sampling\nCalibrating predictive models are computational intensive, especially random forest method is used. For quick prototyping, a 10% sample will be selected at random from the data by using the code chunk below.\n\n\nCode Chunk\nset.seed(1234)\nHDB_sample &lt;- mdata %&gt;%\n  sample_n(1500)"
  },
  {
    "objectID": "InClassExercise/InClass8/InClass8.html#checking-of-overlapping-point",
    "href": "InClassExercise/InClass8/InClass8.html#checking-of-overlapping-point",
    "title": "In-Class Exercise 8",
    "section": "4 Checking of overlapping point",
    "text": "4 Checking of overlapping point\nWhen using GWmodel to calibrate explanatory or predictive models, it is very important to ensure that there are no overlapping point features. The code chunk below is used to check if there are overlapping point features.\n\n\nCode Chunk\noverlapping_points &lt;- HDB_sample %&gt;%\n  mutate(overlap = lengths(st_equals(., .)) &gt; 1)"
  },
  {
    "objectID": "InClassExercise/InClass8/InClass8.html#spatial-jitter",
    "href": "InClassExercise/InClass8/InClass8.html#spatial-jitter",
    "title": "In-Class Exercise 8",
    "section": "5 Spatial Jitter",
    "text": "5 Spatial Jitter\nIn the code code chunk below, st_jitter() of sf package is used to move the point features by 5m to avoid overlapping point features.\n\n\nCode Chunk\nHDB_sample &lt;- HDB_sample %&gt;%\n  st_jitter(amount = 5)\n\n\nThe entire data are split into training and test data sets with 65% and 35% respectively by using initial_split() of rsample package. rsample is one of the package of tigymodels.\n\n\nCode Chunk\nset.seed(1234)\nresale_split &lt;- initial_split(HDB_sample, \n                              prop = 6.67/10,)\ntrain_data &lt;- training(resale_split)\ntest_data &lt;- testing(resale_split)"
  },
  {
    "objectID": "InClassExercise/InClass8/InClass8.html#multicollinearity-check",
    "href": "InClassExercise/InClass8/InClass8.html#multicollinearity-check",
    "title": "In-Class Exercise 8",
    "section": "6 Multicollinearity check",
    "text": "6 Multicollinearity check\nIn order to avoid multicollineariy. In the code chunk below, ggcorrmat() of ggstatsplot is used to plot a correlation matrix to check if there are pairs of highly correlated independent variables.\n\n\nCode Chunk\nmdata_nogeo &lt;- mdata %&gt;%\n  st_drop_geometry()\nggstatsplot::ggcorrmat(mdata_nogeo[, 2:17])"
  },
  {
    "objectID": "InClassExercise/InClass8/InClass8.html#building-a-non-spatial-multiple-linear-regression",
    "href": "InClassExercise/InClass8/InClass8.html#building-a-non-spatial-multiple-linear-regression",
    "title": "In-Class Exercise 8",
    "section": "7 Building a non-spatial multiple linear regression",
    "text": "7 Building a non-spatial multiple linear regression\n\n\nCode Chunk\nprice_mlr &lt;- lm(resale_price ~ floor_area_sqm +\n                  storey_order + remaining_lease_mths +\n                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n                  PROX_MRT + PROX_PARK + PROX_MALL + \n                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                data=train_data)\nolsrr::ols_regress(price_mlr)\n\n\n                              Model Summary                                \n--------------------------------------------------------------------------\nR                           0.862       RMSE                    60813.316 \nR-Squared                   0.742       MSE                3754578098.252 \nAdj. R-Squared              0.739       Coef. Var                  14.255 \nPred R-Squared              0.734       AIC                     24901.005 \nMAE                     45987.256       SBC                     24979.529 \n--------------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                     ANOVA                                      \n-------------------------------------------------------------------------------\n                    Sum of                                                     \n                   Squares         DF         Mean Square       F         Sig. \n-------------------------------------------------------------------------------\nRegression    1.065708e+13         14    761220078101.236    202.745    0.0000 \nResidual      3.698259e+12        985      3754578098.252                      \nTotal         1.435534e+13        999                                          \n-------------------------------------------------------------------------------\n\n                                               Parameter Estimates                                                 \n------------------------------------------------------------------------------------------------------------------\n                   model          Beta    Std. Error    Std. Beta       t        Sig          lower         upper \n------------------------------------------------------------------------------------------------------------------\n             (Intercept)    115703.696     34303.409                   3.373    0.001     48387.533    183019.860 \n          floor_area_sqm      2778.618       292.262        0.165      9.507    0.000      2205.089      3352.146 \n            storey_order     12698.165      1070.950        0.211     11.857    0.000     10596.559     14799.771 \n    remaining_lease_mths       350.252        14.596        0.450     23.997    0.000       321.610       378.894 \n                PROX_CBD    -16225.588       630.092       -0.572    -25.751    0.000    -17462.065    -14989.110 \n        PROX_ELDERLYCARE    -11330.930      3220.845       -0.061     -3.518    0.000    -17651.436     -5010.423 \n             PROX_HAWKER    -19964.070      4021.046       -0.087     -4.965    0.000    -27854.872    -12073.268 \n                PROX_MRT    -39652.516      5412.288       -0.130     -7.326    0.000    -50273.456    -29031.577 \n               PROX_PARK    -15878.322      4609.199       -0.061     -3.445    0.001    -24923.300     -6833.344 \n               PROX_MALL    -15910.922      6438.111       -0.048     -2.471    0.014    -28544.911     -3276.933 \n        PROX_SUPERMARKET    -18928.514     13304.965       -0.025     -1.423    0.155    -45037.848      7180.821 \nWITHIN_350M_KINDERGARTEN      9309.735      2024.293        0.079      4.599    0.000      5337.313     13282.157 \n   WITHIN_350M_CHILDCARE     -1619.514      1180.948       -0.026     -1.371    0.171     -3936.977       697.948 \n         WITHIN_350M_BUS      -447.695       738.715       -0.011     -0.606    0.545     -1897.331      1001.940 \n       WITHIN_1KM_PRISCH    -10698.012      1543.511       -0.138     -6.931    0.000    -13726.960     -7669.065 \n------------------------------------------------------------------------------------------------------------------"
  },
  {
    "objectID": "InClassExercise/InClass8/InClass8.html#multicollinearity-check-with-vif",
    "href": "InClassExercise/InClass8/InClass8.html#multicollinearity-check-with-vif",
    "title": "In-Class Exercise 8",
    "section": "8 Multicollinearity check with VIF",
    "text": "8 Multicollinearity check with VIF\n\n8.1 VIF\n\n\nCode Chunk\nvif &lt;- performance::check_collinearity(price_mlr)\nkable(vif, \n      caption = \"Variance Inflation Factor (VIF) Results\") %&gt;%\n  kable_styling(font_size = 18) \n\n\n\n\nVariance Inflation Factor (VIF) Results\n\n\nTerm\nVIF\nVIF_CI_low\nVIF_CI_high\nSE_factor\nTolerance\nTolerance_CI_low\nTolerance_CI_high\n\n\n\n\nfloor_area_sqm\n1.146686\n1.085743\n1.250945\n1.070834\n0.8720785\n0.7993954\n0.9210287\n\n\nstorey_order\n1.206020\n1.135720\n1.312734\n1.098189\n0.8291736\n0.7617690\n0.8804986\n\n\nremaining_lease_mths\n1.343645\n1.254833\n1.463410\n1.159157\n0.7442440\n0.6833358\n0.7969186\n\n\nPROX_CBD\n1.887898\n1.733977\n2.074096\n1.374008\n0.5296898\n0.4821378\n0.5767088\n\n\nPROX_ELDERLYCARE\n1.140418\n1.080572\n1.244716\n1.067904\n0.8768712\n0.8033960\n0.9254357\n\n\nPROX_HAWKER\n1.183865\n1.116887\n1.289223\n1.088056\n0.8446907\n0.7756609\n0.8953457\n\n\nPROX_MRT\n1.211390\n1.140307\n1.318485\n1.100632\n0.8254980\n0.7584464\n0.8769566\n\n\nPROX_PARK\n1.186122\n1.118797\n1.291599\n1.089092\n0.8430839\n0.7742340\n0.8938169\n\n\nPROX_MALL\n1.435504\n1.335252\n1.565736\n1.198125\n0.6966193\n0.6386771\n0.7489224\n\n\nPROX_SUPERMARKET\n1.226727\n1.153448\n1.335000\n1.107577\n0.8151773\n0.7490638\n0.8669656\n\n\nWITHIN_350M_KINDERGARTEN\n1.123989\n1.067172\n1.228865\n1.060183\n0.8896886\n0.8137594\n0.9370564\n\n\nWITHIN_350M_CHILDCARE\n1.387119\n1.292841\n1.511748\n1.177760\n0.7209189\n0.6614860\n0.7734902\n\n\nWITHIN_350M_BUS\n1.193498\n1.125056\n1.299398\n1.092473\n0.8378731\n0.7695869\n0.8888447\n\n\nWITHIN_1KM_PRISCH\n1.508943\n1.399770\n1.647930\n1.228390\n0.6627154\n0.6068219\n0.7144029\n\n\n\n\n\n\n\n\n\n\n8.2 Plotting VIF\n\n\nCode Chunk\nvif &lt;- performance::check_collinearity(price_mlr)\nkable(vif, \n      caption = \"Variance Inflation Factor (VIF) Results\") %&gt;%\n  kable_styling(font_size = 18) \n\n\n\n\nVariance Inflation Factor (VIF) Results\n\n\nTerm\nVIF\nVIF_CI_low\nVIF_CI_high\nSE_factor\nTolerance\nTolerance_CI_low\nTolerance_CI_high\n\n\n\n\nfloor_area_sqm\n1.146686\n1.085743\n1.250945\n1.070834\n0.8720785\n0.7993954\n0.9210287\n\n\nstorey_order\n1.206020\n1.135720\n1.312734\n1.098189\n0.8291736\n0.7617690\n0.8804986\n\n\nremaining_lease_mths\n1.343645\n1.254833\n1.463410\n1.159157\n0.7442440\n0.6833358\n0.7969186\n\n\nPROX_CBD\n1.887898\n1.733977\n2.074096\n1.374008\n0.5296898\n0.4821378\n0.5767088\n\n\nPROX_ELDERLYCARE\n1.140418\n1.080572\n1.244716\n1.067904\n0.8768712\n0.8033960\n0.9254357\n\n\nPROX_HAWKER\n1.183865\n1.116887\n1.289223\n1.088056\n0.8446907\n0.7756609\n0.8953457\n\n\nPROX_MRT\n1.211390\n1.140307\n1.318485\n1.100632\n0.8254980\n0.7584464\n0.8769566\n\n\nPROX_PARK\n1.186122\n1.118797\n1.291599\n1.089092\n0.8430839\n0.7742340\n0.8938169\n\n\nPROX_MALL\n1.435504\n1.335252\n1.565736\n1.198125\n0.6966193\n0.6386771\n0.7489224\n\n\nPROX_SUPERMARKET\n1.226727\n1.153448\n1.335000\n1.107577\n0.8151773\n0.7490638\n0.8669656\n\n\nWITHIN_350M_KINDERGARTEN\n1.123989\n1.067172\n1.228865\n1.060183\n0.8896886\n0.8137594\n0.9370564\n\n\nWITHIN_350M_CHILDCARE\n1.387119\n1.292841\n1.511748\n1.177760\n0.7209189\n0.6614860\n0.7734902\n\n\nWITHIN_350M_BUS\n1.193498\n1.125056\n1.299398\n1.092473\n0.8378731\n0.7695869\n0.8888447\n\n\nWITHIN_1KM_PRISCH\n1.508943\n1.399770\n1.647930\n1.228390\n0.6627154\n0.6068219\n0.7144029"
  },
  {
    "objectID": "InClassExercise/InClass8/InClass8.html#predictive-modelling-with-gwr",
    "href": "InClassExercise/InClass8/InClass8.html#predictive-modelling-with-gwr",
    "title": "In-Class Exercise 8",
    "section": "9 Predictive Modelling with gwr",
    "text": "9 Predictive Modelling with gwr\n\n9.1 Computing adaptive bandwidth\n\n\nCode Chunk\nbw_adaptive &lt;- bw.gwr(resale_price ~ floor_area_sqm +\n                  storey_order + remaining_lease_mths +\n                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n                  PROX_MRT + PROX_PARK + PROX_MALL + \n                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                  data=train_data,\n                  approach=\"CV\",\n                  kernel=\"gaussian\",\n                  adaptive=TRUE,\n                  longlat=FALSE)\n\n\nAdaptive bandwidth: 625 CV score: 3.459032e+12 \nAdaptive bandwidth: 394 CV score: 3.231786e+12 \nAdaptive bandwidth: 250 CV score: 2.914736e+12 \nAdaptive bandwidth: 162 CV score: 2.610897e+12 \nAdaptive bandwidth: 107 CV score: 2.240188e+12 \nAdaptive bandwidth: 73 CV score: 1.971641e+12 \nAdaptive bandwidth: 52 CV score: 1.797271e+12 \nAdaptive bandwidth: 39 CV score: 1.659472e+12 \nAdaptive bandwidth: 31 CV score: 1.573963e+12 \nAdaptive bandwidth: 26 CV score: 1.550147e+12 \nAdaptive bandwidth: 23 CV score: 1.542544e+12 \nAdaptive bandwidth: 21 CV score: 1.518885e+12 \nAdaptive bandwidth: 19 CV score: 1.515965e+12 \nAdaptive bandwidth: 19 CV score: 1.515965e+12 \n\n\n\n\nCode Chunk\nbw_adaptive\n\n\n[1] 19"
  },
  {
    "objectID": "InClassExercise/InClass8/InClass8.html#model-calibration",
    "href": "InClassExercise/InClass8/InClass8.html#model-calibration",
    "title": "In-Class Exercise 8",
    "section": "10 Model Calibration",
    "text": "10 Model Calibration\n\n10.1 Code\n\n\nCode Chunk\ngwr_adaptive &lt;- gwr.basic(formula = resale_price ~\n                            floor_area_sqm + storey_order +\n                            remaining_lease_mths + PROX_CBD + \n                            PROX_ELDERLYCARE + PROX_HAWKER +\n                            PROX_MRT + PROX_PARK + PROX_MALL + \n                            PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                            WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                            WITHIN_1KM_PRISCH,\n                          data=train_data,\n                          bw=bw_adaptive, \n                          kernel = 'gaussian', \n                          adaptive=TRUE,\n                          longlat = FALSE)"
  },
  {
    "objectID": "InClassExercise/InClass8/InClass8.html#predicting-modelling-with-mlr",
    "href": "InClassExercise/InClass8/InClass8.html#predicting-modelling-with-mlr",
    "title": "In-Class Exercise 8",
    "section": "11 Predicting Modelling with MLR",
    "text": "11 Predicting Modelling with MLR\n\n11.1 Predicting with test data\n\n11.1.1 Test data bw\n\n\nCode Chunk\ngwr_bw_test_adaptive &lt;- bw.gwr(resale_price ~ floor_area_sqm +\n                  storey_order + remaining_lease_mths +\n                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n                  PROX_MRT + PROX_PARK + PROX_MALL + \n                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                  data=test_data,\n                  approach=\"CV\",\n                  kernel=\"gaussian\",\n                  adaptive=TRUE,\n                  longlat=FALSE)\n\n\nAdaptive bandwidth: 316 CV score: 1.752181e+12 \nAdaptive bandwidth: 203 CV score: 1.635856e+12 \nAdaptive bandwidth: 132 CV score: 1.452381e+12 \nAdaptive bandwidth: 89 CV score: 1.292305e+12 \nAdaptive bandwidth: 61 CV score: 1.115867e+12 \nAdaptive bandwidth: 45 CV score: 1.007764e+12 \nAdaptive bandwidth: 34 CV score: 886240690081 \nAdaptive bandwidth: 28 CV score: 859792519354 \nAdaptive bandwidth: 23 CV score: 856247388820 \nAdaptive bandwidth: 21 CV score: 846203688027 \nAdaptive bandwidth: 19 CV score: 837013751205 \nAdaptive bandwidth: 18 CV score: 8.32968e+11 \nAdaptive bandwidth: 17 CV score: 834218488853 \nAdaptive bandwidth: 18 CV score: 8.32968e+11 \n\n\n\n\nCode Chunk\ngwr_pred &lt;- gwr.predict(formula = resale_price ~\n                          floor_area_sqm + storey_order +\n                          remaining_lease_mths + PROX_CBD + \n                          PROX_ELDERLYCARE + PROX_HAWKER + \n                          PROX_MRT + PROX_PARK + PROX_MALL + \n                          PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                          WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + \n                          WITHIN_1KM_PRISCH, \n                        data=train_data, \n                        predictdata = test_data, \n                        bw=bw_adaptive, \n                        kernel = 'gaussian', \n                        adaptive=TRUE, \n                        longlat = FALSE)"
  },
  {
    "objectID": "InClassExercise/InClass8/InClass8.html#predictive-modelling-rf-method",
    "href": "InClassExercise/InClass8/InClass8.html#predictive-modelling-rf-method",
    "title": "In-Class Exercise 8",
    "section": "12 Predictive Modelling: RF method",
    "text": "12 Predictive Modelling: RF method\n\n12.1 Data Preparation\nirstly, code chunk below is used to extract the coordinates of training and test data sets\n\n\nCode Chunk\ncoords &lt;- st_coordinates(HDB_sample)\ncoords_train &lt;- st_coordinates(train_data)\ncoords_test &lt;- st_coordinates(test_data)\n\n\nNext, code chunk below is used to drop the geometry column of both training and test data sets.\n\n\nCode Chunk\ntrain_data_nogeom &lt;- train_data %&gt;%\n  st_drop_geometry()\n\n\n\n\n12.2 Calibrating RF model\n\n\nCode Chunk\nset.seed(1234)\nrf &lt;- ranger(resale_price ~ floor_area_sqm + storey_order + \n               remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE + \n               PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL + \n               PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n               WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + \n               WITHIN_1KM_PRISCH,\n             data=train_data_nogeom)\n\n\n\n\n12.3 Model output\n\n\nCode Chunk\nrf\n\n\nRanger result\n\nCall:\n ranger(resale_price ~ floor_area_sqm + storey_order + remaining_lease_mths +      PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK +      PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +      WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH,      data = train_data_nogeom) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      1000 \nNumber of independent variables:  14 \nMtry:                             3 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       2283173758 \nR squared (OOB):                  0.8411121"
  },
  {
    "objectID": "InClassExercise/InClass8/InClass8.html#predicting-modelling-spatialml-method",
    "href": "InClassExercise/InClass8/InClass8.html#predicting-modelling-spatialml-method",
    "title": "In-Class Exercise 8",
    "section": "13 Predicting Modelling:: SpatialML method",
    "text": "13 Predicting Modelling:: SpatialML method\n\n\nCode Chunk\nset.seed(1234)\ngwRF_adaptive &lt;- grf(formula = resale_price ~ floor_area_sqm + \n                       storey_order + remaining_lease_mths + \n                       PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + \n                       PROX_MRT + PROX_PARK + PROX_MALL + \n                       PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                       WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                       WITHIN_1KM_PRISCH,\n                     dframe=train_data_nogeom, \n                     bw=55,\n                     kernel=\"adaptive\",\n                     coords=coords_train)\n\n\nRanger result\n\nCall:\n ranger(resale_price ~ floor_area_sqm + storey_order + remaining_lease_mths +      PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK +      PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +      WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH,      data = train_data_nogeom, num.trees = 500, mtry = 4, importance = \"impurity\",      num.threads = NULL) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      1000 \nNumber of independent variables:  14 \nMtry:                             4 \nTarget node size:                 5 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       2074009887 \nR squared (OOB):                  0.8556679 \n\n\n          floor_area_sqm             storey_order     remaining_lease_mths \n            6.858004e+11             1.422012e+12             2.486068e+12 \n                PROX_CBD         PROX_ELDERLYCARE              PROX_HAWKER \n            4.638477e+12             5.479231e+11             6.521805e+11 \n                PROX_MRT                PROX_PARK                PROX_MALL \n            8.441291e+11             5.449257e+11             4.405820e+11 \n        PROX_SUPERMARKET WITHIN_350M_KINDERGARTEN    WITHIN_350M_CHILDCARE \n            3.776907e+11             1.277977e+11             2.369706e+11 \n         WITHIN_350M_BUS        WITHIN_1KM_PRISCH \n            2.184408e+11             7.997281e+11 \n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-221080.0  -21530.3    -895.4    -271.1   20305.0  296404.4 \n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-38672.86  -3227.99   -178.43    -71.12   2831.66  42872.18 \n\n\n                                Min          Max         Mean          StD\nfloor_area_sqm            696919686 192865321765  24393255920  32201130831\nstorey_order              517785315 319895669535  27780386721  50723636275\nremaining_lease_mths     2957069739 610102523068 100407363399 137038263288\nPROX_CBD                  962467743 343722236312  30938242726  48859511192\nPROX_ELDERLYCARE         1897793617 150381230352  23249130891  25384606673\nPROX_HAWKER              1035238085 214011373785  20753714949  24523766193\nPROX_MRT                 1130621303 267464693079  28928314326  46397690636\nPROX_PARK                1137628579 179606297600  19965989625  21334314778\nPROX_MALL                1278390826 271590767488  27256268534  40402723719\nPROX_SUPERMARKET          991735075 176833138872  18996601028  27059219290\nWITHIN_350M_KINDERGARTEN  173811210  49153266122   5742647253   7358559963\nWITHIN_350M_CHILDCARE     445891832 182607185549  18494577567  33745415431\nWITHIN_350M_BUS           590779439 142390607080   9066032058  11317330875\nWITHIN_1KM_PRISCH         220777233  60582254997   6408000068   7434712348"
  },
  {
    "objectID": "InClassExercise/InClass8/InClass8.html#predicting-by-using-the-test-data",
    "href": "InClassExercise/InClass8/InClass8.html#predicting-by-using-the-test-data",
    "title": "In-Class Exercise 8",
    "section": "14 Predicting by using the test data",
    "text": "14 Predicting by using the test data\n\n14.1 Preparing the test data\n\n\nCode Chunk\ntest_data_nogeom &lt;- cbind(\n  test_data, coords_test) %&gt;%\n  st_drop_geometry()\n\n\n\n\n14.2 Predicting with test data\nIn the code chunk below, predict.grf() of spatialML for predicting re-sale prices in the test data set (i.e. test_data_nogeom)\n\n\nCode Chunk\ngwRF_pred &lt;- predict.grf(gwRF_adaptive, \n                           test_data_nogeom, \n                           x.var.name=\"X\",\n                           y.var.name=\"Y\", \n                           local.w=1,\n                           global.w=0)\n\n\n##Creating DF Next, the code chunk below is used to convert the output from predict.grf() into a data.frame.\n\n\nCode Chunk\nGRF_pred_df &lt;- as.data.frame(gwRF_pred)\n\n\nThen, cbind() is used to append fields in GRF_pred_df data.frame onto test_data.\n\n\nCode Chunk\ntest_data_pred &lt;- cbind(test_data, \n                        GRF_pred_df)"
  }
]