---
title: "In-Class Exercise 6"
author: "Joshua TING"
date: "30 September, 2024"
date-modified: "last-modified"
format:
  html:
    code-fold: true
    code-summary: "Code Chunk"
    number-sections: true
execute:
  eval: false #r will run through all codes
  echo: true #r will display all code chunk
  warning: false #for mark down
  freeze: true #r will not render all existing  html files
  message: false #avoid printing warning message
editor: source
---

```{r}
pacman::p_load(sf, sfdep, tmap, plotly, tidyverse, kendell)
```

```{r}
hunan <- st_read(dsn = "data/geospatial",
                  layer = "Hunan")
```

```{r}
GDPPC <- read_csv("data/aspatial/Hunan_GDPPC.csv")
```

```{r}
GDPPC_st <- spacetime(GDPPC, hunan, #GDPPC is attirbute while hunan is geospatial
                      .loc_col = "County", 
                      .time_col = "Year") #cannot use original time and date fiekld
```

The above code has multiple layer due to different timeS.

Hint:Need to convert day to integer OR drop the day. Cannot need to have decimals places. Need to use lubridate to change to POSIXct.

Below code is to check if it is a time-space cube.

```{r}
is_spacetime_cube(GDPPC_st)
```

### [Spacetime Cubes](https://sfdep.josiahparry.com/articles/spacetime-s3)

A spacetime object is a spacetime cube if every location has a value for every time index. Another way of saying this is that each location contains a regular time-series.

In ESRI terminology, the basic unit of a spacetime cube is a *bin*. A bin is the unique combination of a location and time index. For each time index, the collection of every location is called a *time slice*. In every location, the collection of every bin at each time index is referred to as a a *bin time-series*.

[![](images/GUID-0FEECE1A-6B54-44B4-AE49-05E7EA849A8B-web.png)](https://sfdep.josiahparry.com/articles/spacetime-s3)

## Inputing Computing Gi\*

*Good in detecting hotspot (incidences) and coldspot (low incidences)*

```{r}
GDPPC_nb <- GDPPC_st %>%
  activate("geometry") %>%
  mutate(nb = include_self(
    st_contiguity(geometry)),
    wt = st_inverse_distance(nb,
                              geometry,
                              scale = 1,
                              alpha = 1),
    .before = 1) %>%
  set_nbs("nb") %>%
  set_wts("wt")
```

::: callout-note
## Things to learn from above code chunk

-   Activate() of dplyr package is used to activate the geometry context

-   mutate() of dplyr package is used to create two new columns nb and wt

-   then we will activate the data context again and copy over the nb and wt columns to each time-slice using set_nbs() and set_wts()

-   row order is very important so do not rearrange the observations after using set_nbs() or set_wts()
:::

We can use these news columns to manually calculate the local Gi\* for each location. We can do this by grouping by Year and using local_gstar_perm() of sfdep package. After which, we use unnest() to unnest gi_star of the newly created gi_starts data.frame.

```{r}
gi_stars <- GDPPC_nb %>%
  group_by(Year) %>%
  mutate(gi_star = local_gstar_perm(
    GDPPC, nb, wt)) %>%
  tidyr::unnest(gi_star)
```

## Mann-Kendall Test

A monotonic series or function is one that only increases (or decreases) and never changes direction. As long as the function either stays flats

```{r}
cbg <- gi_stars %>%
  ungroup() %>%
  filter(County == "Changsha") |>
  select(County, Year, gi_star)
```

Next we plot the result by using ggplot2 functions.

```{r}
eval: false
ggplot(data = cbg,
       aes(X = Year,
           y = gi_star)) +
  geom_line() +
  theme_light()
```

```{r}
cbg %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>%
  tidyr::unnest_wider(mk)
```

In above result, **sl** is the p-value. With reference to the results, we will reject the hypothesis null and infer that a slight upward trend.

```{r}
ehsa <- gi_stars %>%
  group_by(County) %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>%
  tidyr::unnest_wider(mk)
head(ehsa)
```

Mann-Kendall test data.frame

## Performing Emerging Hotspot Analysis

Lastly, we will perform EHSA analysis by using emerging_hotspot_analysis() of sfdep package. It takes a spacetime object x (i.e. GDPPC_st), and the quoted name of the variable of interest (i.e. GDPPC) for .var argument. The k argument is used to specify the number of time lags which is set to 1 by default. Lastly, nsim map numbers of simulation is to be performed

```{r}
ehsa <- emerging_hotspot_analysis(
  x = GDPPC_st,
  .var = "GDPPC",
  k = 1,
  nsim = 99
)
```

```{r}
ggplot(data = ehsa,
       aes = (x = classification)) +
  geom_bar()
```

```{r}
hunan_ehsa <- hunan %>%
  left_join(ehsa,
              by = join_by(County == location))
```

```{r}
ehsa_sig <- hunan_ehsa %>%
  filter(p_value <0.05)
tmap_mode("plot")
tm_shape(hunan_ehsa) +
  tm_polygons() +
  tm_borders(alpha = 0.5) +
tm_shape(ehsa_sig) +
  tm_fill("classification") +
  tm_borders(alpha = 0.4)
```
